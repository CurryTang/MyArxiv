{"2024-12-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2412.10369v1","updated":"2024-12-13T18:58:48Z","published":"2024-12-13T18:58:48Z","title":"A Grounded Typology of Word Classes","summary":"  We propose a grounded approach to meaning in language typology. We treat data\nfrom perceptual modalities, such as images, as a language-agnostic\nrepresentation of meaning. Hence, we can quantify the function--form\nrelationship between images and captions across languages. Inspired by\ninformation theory, we define \"groundedness\", an empirical measure of\ncontextual semantic contentfulness (formulated as a difference in surprisal)\nwhich can be computed with multilingual multimodal language models. As a proof\nof concept, we apply this measure to the typology of word classes. Our measure\ncaptures the contentfulness asymmetry between functional (grammatical) and\nlexical (content) classes across languages, but contradicts the view that\nfunctional classes do not convey content. Moreover, we find universal trends in\nthe hierarchy of groundedness (e.g., nouns > adjectives > verbs), and show that\nour measure partly correlates with psycholinguistic concreteness norms in\nEnglish. We release a dataset of groundedness scores for 30 languages. Our\nresults suggest that the grounded typology approach can provide quantitative\nevidence about semantic function in language.\n","authors":["Coleman Haley","Sharon Goldwater","Edoardo Ponti"],"pdf_url":"https://arxiv.org/pdf/2412.10369v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.10321v1","updated":"2024-12-13T18:00:57Z","published":"2024-12-13T18:00:57Z","title":"AdvPrefix: An Objective for Nuanced LLM Jailbreaks","summary":"  Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks.\n","authors":["Sicheng Zhu","Brandon Amos","Yuandong Tian","Chuan Guo","Ivan Evtimov"],"pdf_url":"https://arxiv.org/pdf/2412.10321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10319v1","updated":"2024-12-13T17:59:52Z","published":"2024-12-13T17:59:52Z","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods","summary":"  Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.\n","authors":["Yucheng Li","Huiqiang Jiang","Qianhui Wu","Xufang Luo","Surin Ahn","Chengruidong Zhang","Amir H. Abdi","Dongsheng Li","Jianfeng Gao","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2412.10319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02820v2","updated":"2024-12-13T17:53:25Z","published":"2024-11-05T05:41:41Z","title":"DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving","summary":"  Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.\n","authors":["Yuhan Liu","Yuyang Huang","Jiayi Yao","Zhuohan Gu","Kuntai Du","Hanchen Li","Yihua Cheng","Junchen Jiang","Shan Lu","Madan Musuvathi","Esha Choukse"],"pdf_url":"https://arxiv.org/pdf/2411.02820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10312v1","updated":"2024-12-13T17:52:48Z","published":"2024-12-13T17:52:48Z","title":"Interlocking-free Selective Rationalization Through Genetic-based\n  Learning","summary":"  A popular end-to-end architecture for selective rationalization is the\nselect-then-predict pipeline, comprising a generator to extract highlights fed\nto a predictor. Such a cooperative system suffers from suboptimal equilibrium\nminima due to the dominance of one of the two modules, a phenomenon known as\ninterlocking. While several contributions aimed at addressing interlocking,\nthey only mitigate its effect, often by introducing feature-based heuristics,\nsampling, and ad-hoc regularizations. We present GenSPP, the first\ninterlocking-free architecture for selective rationalization that does not\nrequire any learning overhead, as the above-mentioned. GenSPP avoids\ninterlocking by performing disjoint training of the generator and predictor via\ngenetic global search. Experiments on a synthetic and a real-world benchmark\nshow that our model outperforms several state-of-the-art competitors.\n","authors":["Federico Ruggeri","Gaetano Signorelli"],"pdf_url":"https://arxiv.org/pdf/2412.10312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10302v1","updated":"2024-12-13T17:37:48Z","published":"2024-12-13T17:37:48Z","title":"DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding","summary":"  We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.\n","authors":["Zhiyu Wu","Xiaokang Chen","Zizheng Pan","Xingchao Liu","Wen Liu","Damai Dai","Huazuo Gao","Yiyang Ma","Chengyue Wu","Bingxuan Wang","Zhenda Xie","Yu Wu","Kai Hu","Jiawei Wang","Yaofeng Sun","Yukun Li","Yishi Piao","Kang Guan","Aixin Liu","Xin Xie","Yuxiang You","Kai Dong","Xingkai Yu","Haowei Zhang","Liang Zhao","Yisong Wang","Chong Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.10302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06223v3","updated":"2024-12-13T17:29:55Z","published":"2024-09-10T05:26:53Z","title":"Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models","summary":"  The Audio Question Answering (AQA) task includes audio event classification,\naudio captioning, and open-ended reasoning. Recently, AQA has garnered\nattention due to the advent of Large Audio Language Models (LALMs). Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext-only Large Language Models (LLMs) through a projection module. While LALMs\nexcel in general audio understanding, they are limited in temporal reasoning,\nwhich may hinder their commercial applications and on-device deployment. This\npaper addresses these challenges and limitations in audio temporal reasoning.\nFirst, we introduce a data augmentation technique for generating reliable audio\ntemporal questions and answers using an LLM. Second, we perform a further\nfine-tuning of an existing baseline using curriculum learning strategy to\nspecialize in temporal reasoning without compromising performance on fine-tuned\ntasks. We demonstrate the performance of our model using state-of-the-art LALMs\non public audio benchmark datasets. Third, we implement our AQA model on-device\nlocally and investigate its CPU inference for edge applications.\n","authors":["Arvind Krishna Sridhar","Yinyi Guo","Erik Visser"],"pdf_url":"https://arxiv.org/pdf/2409.06223v3.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.10291v1","updated":"2024-12-13T17:21:29Z","published":"2024-12-13T17:21:29Z","title":"Still \"Talking About Large Language Models\": Some Clarifications","summary":"  My paper \"Talking About Large Language Models\" has more than once been\ninterpreted as advocating a reductionist stance towards large language models.\nBut the paper was not intended that way, and I do not endorse such positions.\nThis short note situates the paper in the context of a larger philosophical\nproject that is concerned with the (mis)use of words rather than metaphysics,\nin the spirit of Wittgenstein's later writing.\n","authors":["Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2412.10291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10281v1","updated":"2024-12-13T17:03:56Z","published":"2024-12-13T17:03:56Z","title":"One world, one opinion? The superstar effect in LLM responses","summary":"  As large language models (LLMs) are shaping the way information is shared and\naccessed online, their opinions have the potential to influence a wide\naudience. This study examines who the LLMs view as the most prominent figures\nacross various fields, using prompts in ten different languages to explore the\ninfluence of linguistic diversity. Our findings reveal low diversity in\nresponses, with a small number of figures dominating recognition across\nlanguages (also known as the \"superstar effect\"). These results highlight the\nrisk of narrowing global knowledge representation when LLMs retrieve subjective\ninformation.\n","authors":["Sofie Goethals","Lauren Rhue"],"pdf_url":"https://arxiv.org/pdf/2412.10281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04482v2","updated":"2024-12-13T16:56:21Z","published":"2024-11-20T15:44:58Z","title":"NLP Cluster Analysis of Common Core State Standards and NAEP Item\n  Specifications","summary":"  Camilli (2024) proposed a methodology using natural language processing (NLP)\nto map the relationship of a set of content standards to item specifications.\nThis study provided evidence that NLP can be used to improve the mapping\nprocess. As part of this investigation, the nominal classifications of\nstandards and items specifications were used to examine construct equivalence.\nIn the current paper, we determine the strength of empirical support for the\nsemantic distinctiveness of these classifications, which are known as \"domains\"\nfor Common Core standards, and \"strands\" for National Assessment of Educational\nProgress (NAEP) item specifications. This is accomplished by separate k-means\nclustering for standards and specifications of their corresponding embedding\nvectors. We then briefly illustrate an application of these findings.\n","authors":["Gregory Camilli","Larry Suter"],"pdf_url":"https://arxiv.org/pdf/2412.04482v2.pdf","comment":"10 pages, 5 tables"},{"id":"http://arxiv.org/abs/2412.10271v1","updated":"2024-12-13T16:46:03Z","published":"2024-12-13T16:46:03Z","title":"Benchmarking Linguistic Diversity of Large Language Models","summary":"  The development and evaluation of Large Language Models (LLMs) has primarily\nfocused on their task-solving capabilities, with recent models even surpassing\nhuman performance in some areas. However, this focus often neglects whether\nmachine-generated language matches the human level of diversity, in terms of\nvocabulary choice, syntactic construction, and expression of meaning, raising\nquestions about whether the fundamentals of language generation have been fully\naddressed. This paper emphasizes the importance of examining the preservation\nof human linguistic richness by language models, given the concerning surge in\nonline content produced or aided by LLMs. We propose a comprehensive framework\nfor evaluating LLMs from various linguistic diversity perspectives including\nlexical, syntactic, and semantic dimensions. Using this framework, we benchmark\nseveral state-of-the-art LLMs across all diversity dimensions, and conduct an\nin-depth case study for syntactic diversity. Finally, we analyze how different\ndevelopment and deployment choices impact the linguistic diversity of LLM\noutputs.\n","authors":["Yanzhu Guo","Guokan Shang","Chloé Clavel"],"pdf_url":"https://arxiv.org/pdf/2412.10271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10266v1","updated":"2024-12-13T16:34:39Z","published":"2024-12-13T16:34:39Z","title":"Reasoner Outperforms: Generative Stance Detection with Rationalization\n  for Social Media","summary":"  Stance detection is crucial for fostering a human-centric Web by analyzing\nuser-generated content to identify biases and harmful narratives that undermine\ntrust. With the development of Large Language Models (LLMs), existing\napproaches treat stance detection as a classification problem, providing robust\nmethodologies for modeling complex group interactions and advancing\ncapabilities in natural language tasks. However, these methods often lack\ninterpretability, limiting their ability to offer transparent and\nunderstandable justifications for predictions. This study adopts a generative\napproach, where stance predictions include explicit, interpretable rationales,\nand integrates them into smaller language models through single-task and\nmultitask learning. We find that incorporating reasoning into stance detection\nenables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot\nperformance, achieving an improvement of up to 9.57%. Moreover, our results\nshow that reasoning capabilities enhance multitask learning performance but may\nreduce effectiveness in single-task settings. Crucially, we demonstrate that\nfaithful rationales improve rationale distillation into SLMs, advancing efforts\nto build interpretable, trustworthy systems for addressing discrimination,\nfostering trust, and promoting equitable engagement on social media.\n","authors":["Jiaqing Yuan","Ruijie Xi","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2412.10266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10257v1","updated":"2024-12-13T16:26:34Z","published":"2024-12-13T16:26:34Z","title":"Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models","summary":"  The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.002).\n","authors":["Harry J. Davies","Giorgos Iacovides","Danilo P. Mandic"],"pdf_url":"https://arxiv.org/pdf/2412.10257v1.pdf","comment":"14 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2412.10244v1","updated":"2024-12-13T16:13:35Z","published":"2024-12-13T16:13:35Z","title":"Efficient Continual Pre-training of LLMs for Low-resource Languages","summary":"  Open-source Large Language models (OsLLMs) propel the democratization of\nnatural language research by giving the flexibility to augment or update model\nparameters for performance improvement. Nevertheless, like proprietary LLMs,\nOs-LLMs offer poorer performance on low-resource languages (LRLs) than\nhigh-resource languages (HRLs), owing to smaller amounts of training data and\nunderrepresented vocabulary. On the other hand, continual pre-training (CPT)\nwith large amounts of language-specific data is a costly proposition in terms\nof data acquisition and computational resources. Our goal is to drastically\nreduce CPT cost. To that end, we first develop a new algorithm to select a\nsubset of texts from a larger corpus. We show the effectiveness of our\ntechnique using very little CPT data. In search of further improvement, we\ndesign a new algorithm to select tokens to include in the LLM vocabulary. We\nexperiment with the recent Llama-3 model and nine Indian languages with diverse\nscripts and extent of resource availability. For evaluation, we use\nIndicGenBench, a generation task benchmark dataset for Indic languages. We\nexperiment with various CPT corpora and augmented vocabulary size and offer\ninsights across language families.\n","authors":["Arijit Nag","Soumen Chakrabarti","Animesh Mukherjee","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2412.10244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12691v4","updated":"2024-12-13T15:56:43Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12435v2","updated":"2024-12-13T15:50:26Z","published":"2024-09-19T03:29:40Z","title":"Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language\n  Models","summary":"  We introduce a novel analysis that leverages linguistic minimal pairs to\nprobe the internal linguistic representations of Large Language Models (LLMs).\nBy measuring the similarity between LLM activation differences across minimal\npairs, we quantify the and gain insight into the linguistic knowledge captured\nby LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs\nin three languages, reveal properties of linguistic similarity from four key\naspects: consistency across LLMs, relation to theoretical categorizations,\ndependency to semantic context, and cross-lingual alignment of relevant\nphenomena. Our findings suggest that 1) linguistic similarity is significantly\ninfluenced by training data exposure, leading to higher cross-LLM agreement in\nhigher-resource languages. 2) Linguistic similarity strongly aligns with\nfine-grained theoretical linguistic categories but weakly with broader ones. 3)\nLinguistic similarity shows a weak correlation with semantic similarity,\nshowing its context-dependent nature. 4) LLMs exhibit limited cross-lingual\nalignment in their understanding of relevant linguistic phenomena. This work\ndemonstrates the potential of minimal pairs as a window into the neural\nrepresentations of language in LLMs, shedding light on the relationship between\nLLMs and linguistic theory. Codes and data are available at\nhttps://github.com/ChenDelong1999/Linguistic-Similarity\n","authors":["Xinyu Zhou","Delong Chen","Samuel Cahyawijaya","Xufeng Duan","Zhenguang G. Cai"],"pdf_url":"https://arxiv.org/pdf/2409.12435v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2412.10220v1","updated":"2024-12-13T15:45:45Z","published":"2024-12-13T15:45:45Z","title":"How good is my story? Towards quantitative metrics for evaluating\n  LLM-generated XAI narratives","summary":"  A rapidly developing application of LLMs in XAI is to convert quantitative\nexplanations such as SHAP into user-friendly narratives to explain the\ndecisions made by smaller prediction models. Evaluating the narratives without\nrelying on human preference studies or surveys is becoming increasingly\nimportant in this field. In this work we propose a framework and explore\nseveral automated metrics to evaluate LLM-generated narratives for explanations\nof tabular classification tasks. We apply our approach to compare several\nstate-of-the-art LLMs across different datasets and prompt types. As a\ndemonstration of their utility, these metrics allow us to identify new\nchallenges related to LLM hallucinations for XAI narratives.\n","authors":["Timour Ichmoukhamedov","James Hinns","David Martens"],"pdf_url":"https://arxiv.org/pdf/2412.10220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12741v3","updated":"2024-12-13T15:37:01Z","published":"2024-09-19T13:03:24Z","title":"Fine Tuning Large Language Models for Medicine: The Role and Importance\n  of Direct Preference Optimization","summary":"  Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique.\n","authors":["Thomas Savage","Stephen Ma","Abdessalem Boukil","Vishwesh Patel","Ekanath Rangan","Ivan Lopez","Jonathan H Chen"],"pdf_url":"https://arxiv.org/pdf/2409.12741v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10207v1","updated":"2024-12-13T15:30:20Z","published":"2024-12-13T15:30:20Z","title":"Retrieval-Augmented Semantic Parsing: Using Large Language Models to\n  Improve Generalization","summary":"  Open-domain semantic parsing remains a challenging task, as models often rely\non heuristics and struggle to handle unseen concepts. In this paper, we\ninvestigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external lexical knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing.\n","authors":["Xiao Zhang","Qianru Meng","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2412.10207v1.pdf","comment":"Submitted to ARR"},{"id":"http://arxiv.org/abs/2406.18916v2","updated":"2024-12-13T15:15:46Z","published":"2024-06-27T06:13:05Z","title":"TrustUQA: A Trustful Framework for Unified Structured Data Question\n  Answering","summary":"  Natural language question answering (QA) over structured data sources such as\ntables and knowledge graphs have been widely investigated, especially with\nLarge Language Models (LLMs) in recent years. The main solutions include\nquestion to formal query parsing and retrieval-based answer generation.\nHowever, current methods of the former often suffer from weak generalization,\nfailing to dealing with multi-types of sources, while the later is limited in\ntrustfulness. In this paper, we propose TrustUQA, a trustful QA framework that\ncan simultaneously support multiple types of structured data in a unified way.\nTo this end, it adopts an LLM-friendly and unified knowledge representation\nmethod called Condition Graph(CG), and uses an LLM and demonstration-based\ntwo-level method for CG querying. For enhancement, it is also equipped with\ndynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks\ncovering 3 types of structured data. It outperforms 2 existing unified\nstructured data QA methods. In comparison with the baselines that are specific\nto one data type, it achieves state-of-the-art on 2 of the datasets. Further\nmore, we have demonstrated the potential of our method for more general QA\ntasks, QA over mixed structured data and QA across structured data. The code is\navailable at https://github.com/zjukg/TrustUQA.\n","authors":["Wen Zhang","Long Jin","Yushan Zhu","Jiaoyan Chen","Zhiwei Huang","Junjie Wang","Yin Hua","Lei Liang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18916v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.11242v3","updated":"2024-12-13T14:59:58Z","published":"2024-07-15T21:10:40Z","title":"Bridging Sequence-Structure Alignment in RNA Foundation Models","summary":"  The alignment between RNA sequences and structures in foundation models (FMs)\nhas yet to be thoroughly investigated. Existing FMs have struggled to establish\nsequence-structure alignment, hindering the free flow of genomic information\nbetween RNA sequences and structures. In this study, we introduce OmniGenome,\nan RNA FM trained to align RNA sequences with respect to secondary structures\nbased on structure-contextualised modelling. The alignment enables free and\nbidirectional mappings between sequences and structures by utilising the\nflexible RNA modelling paradigm that supports versatile input and output\nmodalities, i.e., sequence and/or structure as input/output. We implement RNA\ndesign and zero-shot secondary structure prediction as case studies to evaluate\nthe Seq2Str and Str2Seq mapping capacity of OmniGenome. Results on the EternaV2\nbenchmark show that OmniGenome solved 74% of puzzles, whereas existing FMs only\nsolved up to 3% of the puzzles due to the oversight of sequence-structure\nalignment. We leverage four comprehensive in-silico genome modelling benchmarks\nto evaluate performance across a diverse set of genome downstream tasks, where\nthe results show that OmniGenome achieves state-of-the-art performance on RNA\nand DNA benchmarks, even without any training on DNA genomes.\n","authors":["Heng Yang","Renzhi Chen","Ke Li"],"pdf_url":"https://arxiv.org/pdf/2407.11242v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10151v1","updated":"2024-12-13T14:11:26Z","published":"2024-12-13T14:11:26Z","title":"VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval\n  Augmented Generation","summary":"  We propose the VLR-Bench, a visual question answering (VQA) benchmark for\nevaluating vision language models (VLMs) based on retrieval augmented\ngeneration (RAG). Unlike existing evaluation datasets for external\nknowledge-based VQA, the proposed VLR-Bench includes five input passages. This\nallows testing of the ability to determine which passage is useful for\nanswering a given query, a capability lacking in previous research. In this\ncontext, we constructed a dataset of 32,000 automatically generated\ninstruction-following examples, which we denote as VLR-IF. This dataset is\nspecifically designed to enhance the RAG capabilities of VLMs by enabling them\nto learn how to generate appropriate answers based on input passages. We\nevaluated the validity of the proposed benchmark and training data and verified\nits performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3\nmodel. The proposed VLR-Bench and VLR-IF datasets are publicly available\nonline.\n","authors":["Hyeonseok Lim","Dongjae Shin","Seohyun Song","Inho Won","Minjun Kim","Junghun Yuk","Haneol Jang","KyungTae Lim"],"pdf_url":"https://arxiv.org/pdf/2412.10151v1.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025), 19 pages"},{"id":"http://arxiv.org/abs/2412.10139v1","updated":"2024-12-13T13:41:24Z","published":"2024-12-13T13:41:24Z","title":"TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse\n  Analysis with Prompt Engineering","summary":"  The capacity of LLMs to carry out automated qualitative analysis has been\nquestioned by corpus linguists, and it has been argued that corpus-based\ndiscourse analysis incorporating LLMs is hindered by issues of unsatisfying\nperformance, hallucination, and irreproducibility. Our proposed method,\nTACOMORE, aims to address these concerns by serving as an effective prompting\nframework in this domain. The framework consists of four principles, i.e.,\nTask, Context, Model and Reproducibility, and specifies five fundamental\nelements of a good prompt, i.e., Role Description, Task Definition, Task\nProcedures, Contextual Information and Output Format. We conduct experiments on\nthree LLMs, i.e., GPT-4o, Gemini-1.5-Pro and Gemini-1.5.Flash, and find that\nTACOMORE helps improve LLM performance in three representative discourse\nanalysis tasks, i.e., the analysis of keywords, collocates and concordances,\nbased on an open corpus of COVID-19 research articles. Our findings show the\nefficacy of the proposed prompting framework TACOMORE in corpus-based discourse\nanalysis in terms of Accuracy, Ethicality, Reasoning, and Reproducibility, and\nprovide novel insights into the application and evaluation of LLMs in automated\nqualitative studies.\n","authors":["Bingru Li","Han Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10138v1","updated":"2024-12-13T13:41:18Z","published":"2024-12-13T13:41:18Z","title":"ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL","summary":"  Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by\nlarge language models (LLMs), the latest state-of-the-art techniques are still\ntrapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which\nlimits their applicability in open scenarios. To address this challenge, we\npropose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to\nimprove the comprehensive capabilities of open-source LLMs for Text2SQL,\nthereby providing a more practical solution. Our approach begins with\nmulti-task supervised fine-tuning (SFT) using various synthetic training data\nrelated to SQL generation. Unlike existing SFT-based Text2SQL methods, we\nintroduced several additional SFT tasks, including schema linking, noise\ncorrection, and continuation writing. Engaging in a variety of SQL generation\ntasks enhances the model's understanding of SQL syntax and improves its ability\nto generate high-quality SQL queries. Additionally, inspired by the\ncollaborative modes of LLM agents, we introduce a Multitask Collaboration\nPrompting (MCP) strategy. This strategy leverages collaboration across several\nSQL-related tasks to reduce hallucinations during SQL generation, thereby\nmaximizing the potential of enhancing Text2SQL performance through explicit\nmultitask capabilities. Extensive experiments and in-depth analyses have been\nperformed on eight open-source LLMs and five widely-used benchmarks. The\nresults demonstrate that our proposal outperforms the latest Text2SQL methods\nand yields leading performance.\n","authors":["Yang Qin","Chao Chen","Zhihang Fu","Ze Chen","Dezhong Peng","Peng Hu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2412.10138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10136v1","updated":"2024-12-13T13:32:59Z","published":"2024-12-13T13:32:59Z","title":"Can LLMs Convert Graphs to Text-Attributed Graphs?","summary":"  Graphs are ubiquitous data structures found in numerous real-world\napplications, such as drug discovery, recommender systems, and social network\nanalysis. Graph neural networks (GNNs) have become a popular tool to learn node\nembeddings through message passing on these structures. However, a significant\nchallenge arises when applying GNNs to multiple graphs with different feature\nspaces, as existing GNN architectures are not designed for cross-graph feature\nalignment. To address this, recent approaches introduce text-attributed graphs,\nwhere each node is associated with a textual description, enabling the use of a\nshared textual encoder to project nodes from different graphs into a unified\nfeature space. While promising, this method relies heavily on the availability\nof text-attributed data, which can be difficult to obtain in practice. To\nbridge this gap, we propose a novel method named Topology-Aware Node\ndescription Synthesis (TANS), which leverages large language models (LLMs) to\nautomatically convert existing graphs into text-attributed graphs. The key idea\nis to integrate topological information with each node's properties, enhancing\nthe LLMs' ability to explain how graph topology influences node semantics. We\nevaluate our TANS on text-rich, text-limited, and text-free graphs,\ndemonstrating that it enables a single GNN to operate across diverse graphs.\nNotably, on text-free graphs, our method significantly outperforms existing\napproaches that manually design node features, showcasing the potential of LLMs\nfor preprocessing graph-structured data, even in the absence of textual\ninformation. The code and data are available at\nhttps://github.com/Zehong-Wang/TANS.\n","authors":["Zehong Wang","Sidney Liu","Zheyuan Zhang","Tianyi Ma","Chuxu Zhang","Yanfang Ye"],"pdf_url":"https://arxiv.org/pdf/2412.10136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10135v1","updated":"2024-12-13T13:32:13Z","published":"2024-12-13T13:32:13Z","title":"ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers","summary":"  As large language models (LLMs) grow in size, traditional full fine-tuning\nbecomes increasingly impractical due to its high computational and storage\ncosts. Although popular parameter-efficient fine-tuning methods, such as LoRA,\nhave significantly reduced the number of tunable parameters, there is still\nroom for further optimization. In this work, we propose ASLoRA, a cross-layer\nparameter-sharing strategy combining global sharing with partial adaptive\nsharing. Specifically, we share the low-rank matrix A across all layers and\nadaptively merge matrix B during training. This sharing mechanism not only\nmitigates overfitting effectively but also captures inter-layer dependencies,\nsignificantly enhancing the model's representational capability. We conduct\nextensive experiments on various NLP tasks, showing that ASLoRA outperforms\nLoRA while using less than 25% of the parameters, highlighting its flexibility\nand superior parameter efficiency. Furthermore, in-depth analyses of the\nadaptive sharing strategy confirm its significant advantages in enhancing both\nmodel flexibility and task adaptability.\n","authors":["Junyan Hu","Xue Xiao","Mengqi Zhang","Xiao Chen","Zhaochun Ren","Zhumin Chen","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2412.10135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10121v1","updated":"2024-12-13T13:06:58Z","published":"2024-12-13T13:06:58Z","title":"Familiarity: Better Evaluation of Zero-Shot Named Entity Recognition by\n  Quantifying Label Shifts in Synthetic Training Data","summary":"  Zero-shot named entity recognition (NER) is the task of detecting named\nentities of specific types (such as 'Person' or 'Medicine') without any\ntraining examples. Current research increasingly relies on large synthetic\ndatasets, automatically generated to cover tens of thousands of distinct entity\ntypes, to train zero-shot NER models. However, in this paper, we find that\nthese synthetic datasets often contain entity types that are semantically\nhighly similar to (or even the same as) those in standard evaluation\nbenchmarks. Because of this overlap, we argue that reported F1 scores for\nzero-shot NER overestimate the true capabilities of these approaches. Further,\nwe argue that current evaluation setups provide an incomplete picture of\nzero-shot abilities since they do not quantify the label shift (i.e., the\nsimilarity of labels) between training and evaluation datasets. To address\nthese issues, we propose Familiarity, a novel metric that captures both the\nsemantic similarity between entity types in training and evaluation, as well as\ntheir frequency in the training data, to provide an estimate of label shift. It\nallows researchers to contextualize reported zero-shot NER scores when using\ncustom synthetic training datasets. Further, it enables researchers to generate\nevaluation setups of various transfer difficulties for fine-grained analysis of\nzero-shot NER.\n","authors":["Jonas Golde","Patrick Haller","Max Ploner","Fabio Barth","Nicolaas Jedema","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2412.10121v1.pdf","comment":"8 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2412.10110v1","updated":"2024-12-13T12:51:50Z","published":"2024-12-13T12:51:50Z","title":"Label-template based Few-Shot Text Classification with Contrastive\n  Learning","summary":"  As an algorithmic framework for learning to learn, meta-learning provides a\npromising solution for few-shot text classification. However, most existing\nresearch fail to give enough attention to class labels. Traditional basic\nframework building meta-learner based on prototype networks heavily relies on\ninter-class variance, and it is easily influenced by noise. To address these\nlimitations, we proposes a simple and effective few-shot text classification\nframework. In particular, the corresponding label templates are embed into\ninput sentences to fully utilize the potential value of class labels, guiding\nthe pre-trained model to generate more discriminative text representations\nthrough the semantic information conveyed by labels. With the continuous\ninfluence of label semantics, supervised contrastive learning is utilized to\nmodel the interaction information between support samples and query samples.\nFurthermore, the averaging mechanism is replaced with an attention mechanism to\nhighlight vital semantic information. To verify the proposed scheme, four\ntypical datasets are employed to assess the performance of different methods.\nExperimental results demonstrate that our method achieves substantial\nperformance enhancements and outperforms existing state-of-the-art models on\nfew-shot text classification tasks.\n","authors":["Guanghua Hou","Shuhui Cao","Deqiang Ouyang","Ning Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12046v2","updated":"2024-12-13T12:50:18Z","published":"2024-02-19T10:59:29Z","title":"Citation Amnesia: On The Recency Bias of NLP and Other Academic Fields","summary":"  This study examines the tendency to cite older work across 20 fields of study\nover 43 years (1980--2023). We put NLP's propensity to cite older work in the\ncontext of these 20 other fields to analyze whether NLP shows similar temporal\ncitation patterns to these other fields over time or whether differences can be\nobserved. Our analysis, based on a dataset of approximately 240 million papers,\nreveals a broader scientific trend: many fields have markedly declined in\nciting older works (e.g., psychology, computer science). We term this decline a\n'citation age recession', analogous to how economists define periods of reduced\neconomic activity. The trend is strongest in NLP and ML research (-12.8% and\n-5.5% in citation age from previous peaks). Our results suggest that citing\nmore recent works is not directly driven by the growth in publication rates\n(-3.4% across fields; -5.2% in humanities; -5.5% in formal sciences) -- even\nwhen controlling for an increase in the volume of papers. Our findings raise\nquestions about the scientific community's engagement with past literature,\nparticularly for NLP, and the potential consequences of neglecting older but\nrelevant research. The data and a demo showcasing our results are publicly\navailable.\n","authors":["Jan Philip Wahle","Terry Ruas","Mohamed Abdalla","Bela Gipp","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2402.12046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10105v1","updated":"2024-12-13T12:46:33Z","published":"2024-12-13T12:46:33Z","title":"MALAMUTE: A Multilingual, Highly-granular, Template-free,\n  Education-based Probing Dataset","summary":"  Language models (LMs) have excelled in various broad domains. However, to\nensure their safe and effective integration into real-world educational\nsettings, they must demonstrate proficiency in specific, granular areas of\nknowledge. Existing cloze-style benchmarks, commonly used to evaluate LMs'\nknowledge, have three major limitations. They: 1) do not cover the educational\ndomain; 2) typically focus on low-complexity, generic knowledge or broad\ndomains, which do not adequately assess the models' knowledge in specific\nsubjects; and 3) often rely on templates that can bias model predictions. Here,\nwe introduce MALAMUTE, a multilingual, template-free, and highly granular\nprobing dataset comprising expert-written, peer-reviewed probes from 71\nuniversity-level textbooks across three languages (English, Spanish, and\nPolish). MALAMUTE is the first education-based cloze-style dataset. It covers\neight domains, each with up to 14 subdomains, further broken down into concepts\nand concept-based prompts, totaling 33,361 university curriculum concepts and\n116,887 prompts. MALAMUTE's fine granularity, educational focus, and inclusion\nof both sentence-level and paragraph-level prompts make it an ideal tool for\nevaluating LMs' course-related knowledge. Our evaluation of masked and causal\nLMs on MALAMUTE shows that despite overall proficiency, they have significant\ngaps in knowledge when examined closely on specific subjects, hindering their\nsafe use in classrooms and underscoring the need for further development.\n","authors":["Sagi Shaier","George Arthur Baker","Chiranthan Sridhar","Lawrence E Hunter","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2412.10105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10104v1","updated":"2024-12-13T12:45:14Z","published":"2024-12-13T12:45:14Z","title":"RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for\n  Real Estate Sector","summary":"  The real estate market relies heavily on structured data, such as property\ndetails, market trends, and price fluctuations. However, the lack of\nspecialized Tabular Question Answering datasets in this domain limits the\ndevelopment of automated question-answering systems. To fill this gap, we\nintroduce RETQA, the first large-scale open-domain Chinese Tabular Question\nAnswering dataset for Real Estate. RETQA comprises 4,932 tables and 20,762\nquestion-answer pairs across 16 sub-fields within three major domains: property\ninformation, real estate company finance information and land auction\ninformation. Compared with existing tabular question answering datasets, RETQA\nposes greater challenges due to three key factors: long-table structures,\nopen-domain retrieval, and multi-domain queries. To tackle these challenges, we\npropose the SLUTQA framework, which integrates large language models with\nspoken language understanding tasks to enhance retrieval and answering\naccuracy. Extensive experiments demonstrate that SLUTQA significantly improves\nthe performance of large language models on RETQA by in-context learning. RETQA\nand SLUTQA provide essential resources for advancing tabular question answering\nresearch in the real estate domain, addressing critical challenges in\nopen-domain and long-table question-answering. The dataset and code are\npublicly available at \\url{https://github.com/jensen-w/RETQA}.\n","authors":["Zhensheng Wang","Wenmian Yang","Kun Zhou","Yiquan Zhang","Weijia Jia"],"pdf_url":"https://arxiv.org/pdf/2412.10104v1.pdf","comment":"This paper is accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10103v1","updated":"2024-12-13T12:42:51Z","published":"2024-12-13T12:42:51Z","title":"AMuSeD: An Attentive Deep Neural Network for Multimodal Sarcasm\n  Detection Incorporating Bi-modal Data Augmentation","summary":"  Detecting sarcasm effectively requires a nuanced understanding of context,\nincluding vocal tones and facial expressions. The progression towards\nmultimodal computational methods in sarcasm detection, however, faces\nchallenges due to the scarcity of data. To address this, we present AMuSeD\n(Attentive deep neural network for MUltimodal Sarcasm dEtection incorporating\nbi-modal Data augmentation). This approach utilizes the Multimodal Sarcasm\nDetection Dataset (MUStARD) and introduces a two-phase bimodal data\naugmentation strategy. The first phase involves generating varied text samples\nthrough Back Translation from several secondary languages. The second phase\ninvolves the refinement of a FastSpeech 2-based speech synthesis system,\ntailored specifically for sarcasm to retain sarcastic intonations. Alongside a\ncloud-based Text-to-Speech (TTS) service, this Fine-tuned FastSpeech 2 system\nproduces corresponding audio for the text augmentations. We also investigate\nvarious attention mechanisms for effectively merging text and audio data,\nfinding self-attention to be the most efficient for bimodal integration. Our\nexperiments reveal that this combined augmentation and attention approach\nachieves a significant F1-score of 81.0% in text-audio modalities, surpassing\neven models that use three modalities from the MUStARD dataset.\n","authors":["Xiyuan Gao","Shubhi Bansal","Kushaan Gowda","Zhu Li","Shekhar Nayak","Nagendra Kumar","Matt Coler"],"pdf_url":"https://arxiv.org/pdf/2412.10103v1.pdf","comment":"This is a preprint version of the paper, submitted and under review\n  at the IEEE Transactions on Affective Computing"},{"id":"http://arxiv.org/abs/2412.07646v3","updated":"2024-12-13T12:35:21Z","published":"2024-12-10T16:32:19Z","title":"Searching for Structure: Investigating Emergent Communication with Large\n  Language Models","summary":"  Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field.\n","authors":["Tom Kouwenhoven","Max Peeperkorn","Tessa Verhoef"],"pdf_url":"https://arxiv.org/pdf/2412.07646v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10095v1","updated":"2024-12-13T12:31:06Z","published":"2024-12-13T12:31:06Z","title":"HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language\n  Transfer and Automatic Data Annotation","summary":"  In this paper we present our submission for the NorSID Shared Task as part of\nthe 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks:\nIntent Detection, Slot Filling and Dialect Identification, evaluated using data\nin different dialects of the Norwegian language. For Intent Detection and Slot\nFilling, we have fine-tuned a multitask model in a cross-lingual setting, to\nleverage the xSID dataset available in 17 languages. In the case of Dialect\nIdentification, our final submission consists of a model fine-tuned on the\nprovided development set, which has obtained the highest scores within our\nexperiments. Our final results on the test set show that our models do not drop\nin performance compared to the development set, likely due to the\ndomain-specificity of the dataset and the similar distribution of both subsets.\nFinally, we also report an in-depth analysis of the provided datasets and their\nartifacts, as well as other sets of experiments that have been carried out but\ndid not yield the best results. Additionally, we present an analysis on the\nreasons why some methods have been more successful than others; mainly the\nimpact of the combination of languages and domain-specificity of the training\ndata on the results.\n","authors":["Jaione Bengoetxea","Mikel Zubillaga","Ekhi Azurmendi","Maite Heredia","Julen Etxaniz","Markel Ferro","Jeremy Barnes"],"pdf_url":"https://arxiv.org/pdf/2412.10095v1.pdf","comment":"Vardial 2025 NorSID Shared Task"},{"id":"http://arxiv.org/abs/2412.09612v2","updated":"2024-12-13T12:27:52Z","published":"2024-12-12T18:59:40Z","title":"Olympus: A Universal Task Router for Computer Vision Tasks","summary":"  We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttp://yuanze-lin.me/Olympus_page/\n","authors":["Yuanze Lin","Yunsheng Li","Dongdong Chen","Weijian Xu","Ronald Clark","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2412.09612v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.21013v2","updated":"2024-12-13T12:27:37Z","published":"2024-10-28T13:36:46Z","title":"Frequency matters: Modeling irregular morphological patterns in Spanish\n  with Transformers","summary":"  The present paper evaluates the learning behaviour of a transformer-based\nneural network with regard to an irregular inflectional paradigm. We apply the\nparadigm cell filling problem to irregular patterns. We approach this problem\nusing the morphological reinflection task and model it as a character\nsequence-to-sequence learning problem. The test case under investigation are\nirregular verbs in Spanish. Besides many regular verbs in Spanish L-shaped\nverbs the first person singular indicative stem irregularly matches the\nsubjunctive paradigm, while other indicative forms remain unaltered. We examine\nthe role of frequency during learning and compare models under differing input\nfrequency conditions. We train the model on a corpus of Spanish with a\nrealistic distribution of regular and irregular verbs to compare it with models\ntrained on input with augmented distributions of (ir)regular words. We explore\nhow the neural models learn this L-shaped pattern using post-hoc analyses. Our\nexperiments show that, across frequency conditions, the models are surprisingly\ncapable of learning the irregular pattern. Furthermore, our post-hoc analyses\nreveal the possible sources of errors. All code and data are available at\n\\url{https://anonymous.4open.science/r/modeling_spanish_acl-7567/} under MIT\nlicense.\n","authors":["Akhilesh Kakolu Ramarao","Kevin Tang","Dinah Baer-Henney"],"pdf_url":"https://arxiv.org/pdf/2410.21013v2.pdf","comment":"Typos and grammatical corrections"},{"id":"http://arxiv.org/abs/2412.10079v1","updated":"2024-12-13T12:13:19Z","published":"2024-12-13T12:13:19Z","title":"Lost in the Middle, and In-Between: Enhancing Language Models' Ability\n  to Reason Over Long Contexts in Multi-Hop QA","summary":"  Previous work finds that recent long-context language models fail to make\nequal use of information in the middle of their inputs, preferring pieces of\ninformation located at the tail ends which creates an undue bias in situations\nwhere we would like models to be equally capable of using different parts of\nthe input. Thus far, the problem has mainly only been considered in settings\nwith single pieces of critical information, leading us to question what happens\nwhen multiple necessary pieces of information are spread out over the inputs.\nHere, we demonstrate the effects of the \"lost in the middle\" problem in the\nmulti-hop question answering setting -- in which multiple reasoning \"hops\" over\ndisconnected documents are required -- and show that performance degrades not\nonly with respect to the distance of information from the edges of the context,\nbut also between pieces of information. Additionally, we experiment with means\nof alleviating the problem by reducing superfluous document contents through\nknowledge graph triple extraction and summarization, and prompting models to\nreason more thoroughly using chain-of-thought prompting.\n","authors":["George Arthur Baker","Ankush Raut","Sagi Shaier","Lawrence E Hunter","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2412.10079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09383v2","updated":"2024-12-13T12:08:28Z","published":"2024-12-12T15:50:55Z","title":"Neural Text Normalization for Luxembourgish using Real-Life Variation\n  Data","summary":"  Orthographic variation is very common in Luxembourgish texts due to the\nabsence of a fully-fledged standard variety. Additionally, developing NLP tools\nfor Luxembourgish is a difficult task given the lack of annotated and parallel\ndata, which is exacerbated by ongoing standardization. In this paper, we\npropose the first sequence-to-sequence normalization models using the ByT5 and\nmT5 architectures with training data obtained from word-level real-life\nvariation data. We perform a fine-grained, linguistically-motivated evaluation\nto test byte-based, word-based and pipeline-based models for their strengths\nand weaknesses in text normalization. We show that our sequence model using\nreal-life variation data is an effective approach for tailor-made normalization\nin Luxembourgish.\n","authors":["Anne-Marie Lutgen","Alistair Plum","Christoph Purschke","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2412.09383v2.pdf","comment":"Accepted at VarDial 2025"},{"id":"http://arxiv.org/abs/2412.01408v3","updated":"2024-12-13T11:59:06Z","published":"2024-12-02T11:51:19Z","title":"Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings\n  with Few-Shot Learning","summary":"  Online abusive content detection, particularly in low-resource settings and\nwithin the audio modality, remains underexplored. We investigate the potential\nof pre-trained audio representations for detecting abusive language in\nlow-resource languages, in this case, in Indian languages using Few Shot\nLearning (FSL). Leveraging powerful representations from models such as Wav2Vec\nand Whisper, we explore cross-lingual abuse detection using the ADIMA dataset\nwith FSL. Our approach integrates these representations within the\nModel-Agnostic Meta-Learning (MAML) framework to classify abusive language in\n10 languages. We experiment with various shot sizes (50-200) evaluating the\nimpact of limited data on performance. Additionally, a feature visualization\nstudy was conducted to better understand model behaviour. This study highlights\nthe generalization ability of pre-trained models in low-resource scenarios and\noffers valuable insights into detecting abusive language in multilingual\ncontexts.\n","authors":["Aditya Narayan Sankaran","Reza Farahbakhsh","Noel Crespi"],"pdf_url":"https://arxiv.org/pdf/2412.01408v3.pdf","comment":"Accepted as part of the proceedings of COLING 2025"},{"id":"http://arxiv.org/abs/2409.16667v3","updated":"2024-12-13T11:50:50Z","published":"2024-09-25T06:54:29Z","title":"A Character-Centric Creative Story Generation via Imagination","summary":"  Creative story generation has long been a goal of NLP research. While\nexisting methodologies have aimed to generate long and coherent stories, they\nfall significantly short of human capabilities in terms of diversity and\ncharacter depth. To address this, we introduce a novel story generation\nframework called CCI (Character-centric Creative story generation via\nImagination). CCI features two modules for creative story generation: IG\n(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we\nutilize a text-to-image model to create visual representations of key story\nelements, such as characters, backgrounds, and main plots, in a more novel and\nconcrete manner than text-only approaches. The MW module uses these story\nelements to generate multiple persona-description candidates and selects the\nbest one to insert into the story, thereby enhancing the richness and depth of\nthe narrative. We compared the stories generated by CCI and baseline models\nthrough statistical analysis, as well as human and LLM evaluations. The results\nshowed that the IG and MW modules significantly improve various aspects of the\nstories' creativity. Furthermore, our framework enables interactive multi-modal\nstory generation with users, opening up new possibilities for human-LLM\nintegration in cultural development. Project page : https://www.2024cci.p-e.kr/\n","authors":["Kyeongman Park","Minbeom Kim","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2409.16667v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10056v1","updated":"2024-12-13T11:38:10Z","published":"2024-12-13T11:38:10Z","title":"GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?","summary":"  Large Language Models (LLMs) are commonly evaluated using human-crafted\nbenchmarks, under the premise that higher scores implicitly reflect stronger\nhuman-like performance. However, there is growing concern that LLMs may ``game\"\nthese benchmarks due to data leakage, achieving high scores while struggling\nwith tasks simple for humans. To substantively address the problem, we create\nGAOKAO-Eval, a comprehensive benchmark based on China's National College\nEntrance Examination (Gaokao), and conduct ``closed-book\" evaluations for\nrepresentative models released prior to Gaokao. Contrary to prevailing\nconsensus, even after addressing data leakage and comprehensiveness,\nGAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned\ncapabilities. To better understand this mismatch, We introduce the Rasch model\nfrom cognitive psychology to analyze LLM scoring patterns and identify two key\ndiscrepancies: 1) anomalous consistent performance across various question\ndifficulties, and 2) high variance in performance on questions of similar\ndifficulty. In addition, We identified inconsistent grading of LLM-generated\nanswers among teachers and recurring mistake patterns. we find that the\nphenomenons are well-grounded in the motivations behind OpenAI o1, and o1's\nreasoning-as-difficulties can mitigate the mismatch. These results show that\nGAOKAO-Eval can reveal limitations in LLM capabilities not captured by current\nbenchmarks and highlight the need for more LLM-aligned difficulty analysis.\n","authors":["Zhikai Lei","Tianyi Liang","Hanglei Hu","Jin Zhang","Yunhua Zhou","Yunfan Shao","Linyang Li","Chenchui Li","Changbo Wang","Hang Yan","Qipeng Guo"],"pdf_url":"https://arxiv.org/pdf/2412.10056v1.pdf","comment":"10 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.10054v1","updated":"2024-12-13T11:35:00Z","published":"2024-12-13T11:35:00Z","title":"Unsupervised Named Entity Disambiguation for Low Resource Domains","summary":"  In the ever-evolving landscape of natural language processing and information\nretrieval, the need for robust and domain-specific entity linking algorithms\nhas become increasingly apparent. It is crucial in a considerable number of\nfields such as humanities, technical writing and biomedical sciences to enrich\ntexts with semantics and discover more knowledge. The use of Named Entity\nDisambiguation (NED) in such domains requires handling noisy texts, low\nresource settings and domain-specific KBs. Existing approaches are mostly\ninappropriate for such scenarios, as they either depend on training data or are\nnot flexible enough to work with domain-specific KBs. Thus in this work, we\npresent an unsupervised approach leveraging the concept of Group Steiner Trees\n(GST), which can identify the most relevant candidates for entity\ndisambiguation using the contextual similarities across candidate entities for\nall the mentions present in a document. We outperform the state-of-the-art\nunsupervised methods by more than 40\\% (in avg.) in terms of Precision@1 across\nvarious domain-specific datasets.\n","authors":["Debarghya Datta","Soumajit Pramanik"],"pdf_url":"https://arxiv.org/pdf/2412.10054v1.pdf","comment":"Accepted in EMNLP-2024"},{"id":"http://arxiv.org/abs/2410.15633v2","updated":"2024-12-13T11:16:57Z","published":"2024-10-21T04:30:53Z","title":"GATEAU: Selecting Influential Sample for Long Context Alignment","summary":"  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03536v3","updated":"2024-12-13T10:55:37Z","published":"2024-07-03T22:45:36Z","title":"Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias","summary":"  The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla, (2) a curated dataset for bias measurement\nbenchmarking and (3) testing two different probing techniques for bias\ndetection in the context of Bangla. This is the first work of such kind\ninvolving bias assessment of LLMs for Bangla to the best of our knowledge. All\nour code and resources are publicly available for the progress of bias related\nresearch in Bangla NLP.\n","authors":["Jayanta Sadhu","Maneesha Rani Saha","Rifat Shahriyar"],"pdf_url":"https://arxiv.org/pdf/2407.03536v3.pdf","comment":"Accepted at The First Workshop on Language Models for Low-Resource\n  Languages (LoResLM) at COLING 2025"},{"id":"http://arxiv.org/abs/2402.13125v2","updated":"2024-12-13T10:48:13Z","published":"2024-02-20T16:38:33Z","title":"TreeEval: Benchmark-Free Evaluation of Large Language Models through\n  Tree Planning","summary":"  Recently, numerous new benchmarks have been established to evaluate the\nperformance of large language models (LLMs) via either computing a holistic\nscore or employing another LLM as a judge. However, these approaches suffer\nfrom data leakage due to the open access of the benchmark and inflexible\nevaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a\nbenchmark-free evaluation method for LLMs that let a high-performance LLM host\nan irreproducible evaluation session and essentially avoids the data leakage.\nMoreover, this LLM performs as an examiner to raise up a series of questions\nunder a topic with a tree planing strategy, which considers the current\nevaluation status to decide the next question generation and ensures the\ncompleteness and efficiency of the evaluation process. We evaluate $6$ models\nof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately\nachieved the highest correlation coefficient with AlpacaEval2.0 using only\naround $45$ questions. We also conduct more analysis to show the robustness and\nreliability of TreeEval. Our code can be accessed via the provided\nhttps://github.com/Ashura5/TreeEval.\n","authors":["Xiang Li","Yunshi Lan","Chao Yang"],"pdf_url":"https://arxiv.org/pdf/2402.13125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06175v2","updated":"2024-12-13T10:11:31Z","published":"2024-11-09T13:17:39Z","title":"Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs","summary":"  This paper introduces a novel semi-supervised learning framework specifically\ndesigned for text classification tasks, effectively addressing the challenge of\nvast datasets with limited labeled examples. By integrating multi-level\nsimilarity based data augmentation techniques from Retrieval-Augmented\nGeneration (RAG) to Large Language Model (LLM) rewriting and traditional word\nsubstitution-we constructed an intelligent augmentation pipeline. This\nframework innovatively employs the selection of representative landmarks\nthrough clustering, which serve as intermediaries in the retrieval and\nrewriting processes, ensuring that the augmented data maintains a distribution\nsimilar to the original dataset. Empirical results show that even in complex\ntext document classification scenarios with over 100 categories, our method\nachieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and\nWeb of Science datasets, respectively. These findings highlight the\neffectiveness and broad applicability of our semi-supervised learning approach\nfor text classification tasks.\n","authors":["Shan Zhong","Jiahao Zeng","Yongxin Yu","Bohong Lin"],"pdf_url":"https://arxiv.org/pdf/2411.06175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11112v2","updated":"2024-12-13T10:01:19Z","published":"2024-09-17T12:06:05Z","title":"Strategic Insights in Human and Large Language Model Tactics at Word\n  Guessing Games","summary":"  At the beginning of 2022, a simplistic word-guessing game took the world by\nstorm and was further adapted to many languages beyond the original English\nversion. In this paper, we examine the strategies of daily word-guessing game\nplayers that have evolved during a period of over two years. A survey gathered\nfrom 25% of frequent players reveals their strategies and motivations for\ncontinuing the daily journey. We also explore the capability of several popular\nopen-access large language model systems and open-source models at\ncomprehending and playing the game in two different languages. Results\nhighlight the struggles of certain models to maintain correct guess length and\ngenerate repetitions, as well as hallucinations of non-existent words and\ninflections.\n","authors":["Matīss Rikters","Sanita Reinsone"],"pdf_url":"https://arxiv.org/pdf/2409.11112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10008v1","updated":"2024-12-13T09:47:26Z","published":"2024-12-13T09:47:26Z","title":"Automated Collection of Evaluation Dataset for Semantic Search in\n  Low-Resource Domain Language","summary":"  Domain-specific languages that use a lot of specific terminology often fall\ninto the category of low-resource languages. Collecting test datasets in a\nnarrow domain is time-consuming and requires skilled human resources with\ndomain knowledge and training for the annotation task. This study addresses the\nchallenge of automated collecting test datasets to evaluate semantic search in\nlow-resource domain-specific German language of the process industry. Our\napproach proposes an end-to-end annotation pipeline for automated query\ngeneration to the score reassessment of query-document pairs. To overcome the\nlack of text encoders trained in the German chemistry domain, we explore a\nprinciple of an ensemble of \"weak\" text encoders trained on common knowledge\ndatasets. We combine individual relevance scores from diverse models to\nretrieve document candidates and relevance scores generated by an LLM, aiming\nto achieve consensus on query-document alignment. Evaluation results\ndemonstrate that the ensemble method significantly improves alignment with\nhuman-assigned relevance scores, outperforming individual models in both\ninter-coder agreement and accuracy metrics. These findings suggest that\nensemble learning can effectively adapt semantic search systems for\nspecialized, low-resource languages, offering a practical solution to resource\nlimitations in domain-specific contexts.\n","authors":["Anastasia Zhukova","Christian E. Matt","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2412.10008v1.pdf","comment":"accepted in the First Workshop on Language Models for Low-Resource\n  Languages (LoResLM) co-located with the 31st International Conference on\n  Computational Linguistics (COLING 2025)"},{"id":"http://arxiv.org/abs/2412.10006v1","updated":"2024-12-13T09:45:30Z","published":"2024-12-13T09:45:30Z","title":"The role of inhibitory control in garden-path sentence processing: A\n  Chinese-English bilingual perspective","summary":"  In reading garden-path sentences, people must resolve competing\ninterpretations, though initial misinterpretations can linger despite\nreanalysis. This study examines the role of inhibitory control (IC) in managing\nthese misinterpretations among Chinese-English bilinguals. Using self-paced\nreading tasks, we investigated how IC influences recovery from garden-path\nsentences in Chinese (L1) and its interaction with language proficiency during\nEnglish (L2) processing. Results indicate that IC does not affect garden-path\nrecovery in Chinese, suggesting reliance on semantic context may reduce the\nneed for IC. In contrast, findings for English L2 learners reveal a complex\nrelationship between language proficiency and IC: Participants with low L2\nproficiency but high IC showed lingering misinterpretations, while those with\nhigh proficiency exhibited none. These results support and extend the Model of\nCognitive Control (Ness et al., 2023). Moreover, our comparison of three Stroop\ntask versions identifies L1 colour-word Stroop task as the preferred measure of\nIC in bilingual research.\n","authors":["Xiaohui Rao","Haoze Li","Xiaofang Lin","Lijuan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.10006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09318v2","updated":"2024-12-13T09:30:36Z","published":"2024-12-12T14:43:03Z","title":"Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction","summary":"  LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.\n","authors":["Jing Liu","Abdellah Fourtassi"],"pdf_url":"https://arxiv.org/pdf/2412.09318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09993v1","updated":"2024-12-13T09:29:27Z","published":"2024-12-13T09:29:27Z","title":"A Comparative Study of LLMs, NMT Models, and Their Combination in\n  Persian-English Idiom Translation","summary":"  Large language models (LLMs) have shown superior capabilities in translating\nfigurative language compared to neural machine translation (NMT) systems.\nHowever, the impact of different prompting methods and LLM-NMT combinations on\nidiom translation has yet to be thoroughly investigated. This paper introduces\ntwo parallel datasets of sentences containing idiomatic expressions for\nPersian$\\rightarrow$English and English$\\rightarrow$Persian translations, with\nPersian idioms sampled from our PersianIdioms resource, a collection of 2,200\nidioms and their meanings. Using these datasets, we evaluate various open- and\nclosed-source LLMs, NMT models, and their combinations. Translation quality is\nassessed through idiom translation accuracy and fluency. We also find that\nautomatic evaluation methods like LLM-as-a-judge, BLEU and BERTScore are\neffective for comparing different aspects of model performance. Our experiments\nreveal that Claude-3.5-Sonnet delivers outstanding results in both translation\ndirections. For English$\\rightarrow$Persian, combining weaker LLMs with Google\nTranslate improves results, while Persian$\\rightarrow$English translations\nbenefit from single prompts for simpler models and complex prompts for advanced\nones.\n","authors":["Sara Rezaeimanesh","Faezeh Hosseini","Yadollah Yaghoobzadeh"],"pdf_url":"https://arxiv.org/pdf/2412.09993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09990v1","updated":"2024-12-13T09:23:58Z","published":"2024-12-13T09:23:58Z","title":"Small Language Model as Data Prospector for Large Language Model","summary":"  The quality of instruction data directly affects the performance of\nfine-tuned Large Language Models (LLMs). Previously, \\cite{li2023one} proposed\n\\texttt{NUGGETS}, which identifies and selects high-quality quality data from a\nlarge dataset by identifying those individual instruction examples that can\nsignificantly improve the performance of different tasks after being learnt as\none-shot instances. In this work, we propose \\texttt{SuperNUGGETS}, an improved\nvariant of \\texttt{NUGGETS} optimised for efficiency and performance. Our\n\\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large\nlanguage model (LLM) to filter the data for outstanding one-shot instances and\nrefines the predefined set of tests. The experimental results show that the\nperformance of \\texttt{SuperNUGGETS} only decreases by 1-2% compared to\n\\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58.\nCompared to the original \\texttt{NUGGETS}, our \\texttt{SuperNUGGETS} has a\nhigher utility value due to the significantly lower resource consumption.\n","authors":["Shiwen Ni","Haihong Wu","Di Yang","Qiang Qu","Hamid Alinejad-Rokny","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04680v2","updated":"2024-12-13T08:44:55Z","published":"2024-08-08T04:49:21Z","title":"Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications","summary":"  The ability of large language models (LLMs) to transform, interpret, and\ncomprehend vast quantities of heterogeneous data presents a significant\nopportunity to enhance data-driven care delivery. However, the sensitive nature\nof protected health information (PHI) raises valid concerns about data privacy\nand trust in remote LLM platforms. In addition, the cost associated with\ncloud-based artificial intelligence (AI) services continues to impede\nwidespread adoption. To address these challenges, we propose a shift in the LLM\nexecution environment from opaque, centralized cloud providers to a\ndecentralized and dynamic fog computing architecture. By executing open-weight\nLLMs in more trusted environments, such as the user's edge device or a fog\nlayer within a local network, we aim to mitigate the privacy, trust, and\nfinancial challenges associated with cloud-based LLMs. We further present\nSpeziLLM, an open-source framework designed to facilitate rapid and seamless\nleveraging of different LLM execution layers and lowering barriers to LLM\nintegration in digital health applications. We demonstrate SpeziLLM's broad\napplicability across six digital health applications, showcasing its\nversatility in various healthcare settings.\n","authors":["Philipp Zagar","Vishnu Ravi","Lauren Aalami","Stephan Krusche","Oliver Aalami","Paul Schmiedmayer"],"pdf_url":"https://arxiv.org/pdf/2408.04680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09957v1","updated":"2024-12-13T08:33:26Z","published":"2024-12-13T08:33:26Z","title":"Romanized to Native Malayalam Script Transliteration Using an\n  Encoder-Decoder Framework","summary":"  In this work, we present the development of a reverse transliteration model\nto convert romanized Malayalam to native script using an encoder-decoder\nframework built with attention-based bidirectional Long Short Term Memory\n(Bi-LSTM) architecture. To train the model, we have used curated and combined\ncollection of 4.3 million transliteration pairs derived from publicly available\nIndic language translitertion datasets, Dakshina and Aksharantar. We evaluated\nthe model on two different test dataset provided by IndoNLP-2025-Shared-Task\nthat contain, (1) General typing patterns and (2) Adhoc typing patterns,\nrespectively. On the Test Set-1, we obtained a character error rate (CER) of\n7.4%. However upon Test Set-2, with adhoc typing patterns, where most vowel\nindicators are missing, our model gave a CER of 22.7%.\n","authors":["Bajiyo Baiju","Kavya Manohar","Leena G Pillai","Elizabeth Sherly"],"pdf_url":"https://arxiv.org/pdf/2412.09957v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2404.19252v2","updated":"2024-12-13T08:12:22Z","published":"2024-04-30T04:16:55Z","title":"ViTHSD: Exploiting Hatred by Targets for Hate Speech Detection on\n  Vietnamese Social Media Texts","summary":"  The growth of social networks makes toxic content spread rapidly. Hate speech\ndetection is a task to help decrease the number of harmful comments. With the\ndiversity in the hate speech created by users, it is necessary to interpret the\nhate speech besides detecting it. Hence, we propose a methodology to construct\na system for targeted hate speech detection from online streaming texts from\nsocial media. We first introduce the ViTHSD - a targeted hate speech detection\ndataset for Vietnamese Social Media Texts. The dataset contains 10K comments,\neach comment is labeled to specific targets with three levels: clean,\noffensive, and hate. There are 5 targets in the dataset, and each target is\nlabeled with the corresponding level manually by humans with strict annotation\nguidelines. The inter-annotator agreement obtained from the dataset is 0.45 by\nCohen's Kappa index, which is indicated as a moderate level. Then, we construct\na baseline for this task by combining the Bi-GRU-LSTM-CNN with the pre-trained\nlanguage model to leverage the power of text representation of BERTology.\nFinally, we suggest a methodology to integrate the baseline model for targeted\nhate speech detection into the online streaming system for practical\napplication in preventing hateful and offensive content on social media.\n","authors":["Cuong Nhat Vo","Khanh Bao Huynh","Son T. Luu","Trong-Hop Do"],"pdf_url":"https://arxiv.org/pdf/2404.19252v2.pdf","comment":"Accepted for publication at Journal of Computational Social Science"},{"id":"http://arxiv.org/abs/2412.09946v1","updated":"2024-12-13T08:10:56Z","published":"2024-12-13T08:10:56Z","title":"Enhancing Nursing and Elderly Care with Large Language Models: An\n  AI-Driven Framework","summary":"  This paper explores the application of large language models (LLMs) in\nnursing and elderly care, focusing on AI-driven patient monitoring and\ninteraction. We introduce a novel Chinese nursing dataset and implement\nincremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to\nenhance LLM performance in specialized tasks. Using LangChain, we develop a\ndynamic nursing assistant capable of real-time care and personalized\ninterventions. Experimental results demonstrate significant improvements,\npaving the way for AI-driven solutions to meet the growing demands of\nhealthcare in aging populations.\n","authors":["Qiao Sun","Jiexin Xie","Nanyang Ye","Qinying Gu","Shijie Guo"],"pdf_url":"https://arxiv.org/pdf/2412.09946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04617v2","updated":"2024-12-13T08:05:53Z","published":"2024-10-06T20:34:03Z","title":"Evaluation of Code LLMs on Geospatial Code Generation","summary":"  Software development support tools have been studied for a long time, with\nrecent approaches using Large Language Models (LLMs) for code generation. These\nmodels can generate Python code for data science and machine learning\napplications. LLMs are helpful for software engineers because they increase\nproductivity in daily work. An LLM can also serve as a \"mentor\" for\ninexperienced software developers, and be a viable learning support.\nHigh-quality code generation with LLMs can also be beneficial in geospatial\ndata science. However, this domain poses different challenges, and code\ngeneration LLMs are typically not evaluated on geospatial tasks. Here, we show\nhow we constructed an evaluation benchmark for code generation models, based on\na selection of geospatial tasks. We categorised geospatial tasks based on their\ncomplexity and required tools. Then, we created a dataset with tasks that test\nmodel capabilities in spatial reasoning, spatial data processing, and\ngeospatial tools usage. The dataset consists of specific coding problems that\nwere manually created for high quality. For every problem, we proposed a set of\ntest scenarios that make it possible to automatically check the generated code\nfor correctness. In addition, we tested a selection of existing code generation\nLLMs for code generation in the geospatial domain. We share our dataset and\nreproducible evaluation code on a public GitHub repository, arguing that this\ncan serve as an evaluation benchmark for new LLMs in the future. Our dataset\nwill hopefully contribute to the development new models capable of solving\ngeospatial coding tasks with high accuracy. These models will enable the\ncreation of coding assistants tailored for geospatial applications.\n","authors":["Piotr Gramacki","Bruno Martins","Piotr Szymański"],"pdf_url":"https://arxiv.org/pdf/2410.04617v2.pdf","comment":"7th ACM SIGSPATIAL International Workshop on AI for Geographic\n  Knowledge Discovery (GeoAI'24)"},{"id":"http://arxiv.org/abs/2305.11626v2","updated":"2024-12-13T07:32:04Z","published":"2023-05-19T12:09:49Z","title":"CCT-Code: Cross-Consistency Training for Multilingual Clone Detection\n  and Code Search","summary":"  We consider the well-known and important tasks of clone detection and\ninformation retrieval for source code. The most standard setup is to search\nclones inside the same language code snippets. But it is also useful to find\ncode snippets with identical behaviour in different programming languages.\nNevertheless multi- and cross-lingual clone detection has been little studied\nin literature. We present a novel training procedure, cross-consistency\ntraining (CCT) leveraging cross-lingual similarity, that we apply to train\nlanguage models on source code in various programming languages. We show that\nthis training is effective both for encoder- and decoder-based models. The\ntrained encoder-based CCT-LM model achieves a new state of the art on POJ-104\n(monolingual C++ clone detection benchmark) with 96.73\\% MAP and AdvTest\n(monolingual Python code search benchmark) with 47.18\\% MRR. The decoder-based\nCCT-LM model shows comparable performance in these tasks. In addition, we\nformulate the multi- and cross-lingual clone detection problem and present XCD,\na new benchmark dataset produced from CodeForces submissions.\n","authors":["Anton Tikhonov","Nikita Sorokin","Dmitry Abulkhanov","Irina Piontkovskaya","Sergey Nikolenko","Valentin Malykh"],"pdf_url":"https://arxiv.org/pdf/2305.11626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09925v1","updated":"2024-12-13T07:27:42Z","published":"2024-12-13T07:27:42Z","title":"Simulating Hard Attention Using Soft Attention","summary":"  We study conditions under which transformers using soft attention can\nsimulate hard attention, that is, effectively focus all attention on a subset\nof positions. First, we examine several variants of linear temporal logic,\nwhose formulas have been previously been shown to be computable using hard\nattention transformers. We demonstrate how soft attention transformers can\ncompute formulas of these logics using unbounded positional embeddings or\ntemperature scaling. Second, we demonstrate how temperature scaling allows\nsoftmax transformers to simulate a large subclass of average-hard attention\ntransformers, those that have what we call the uniform-tieless property.\n","authors":["Andy Yang","Lena Strobl","David Chiang","Dana Angluin"],"pdf_url":"https://arxiv.org/pdf/2412.09925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09922v1","updated":"2024-12-13T07:22:13Z","published":"2024-12-13T07:22:13Z","title":"Low-Resource Fast Text Classification Based on Intra-Class and\n  Inter-Class Distance Calculation","summary":"  In recent years, text classification methods based on neural networks and\npre-trained models have gained increasing attention and demonstrated excellent\nperformance. However, these methods still have some limitations in practical\napplications: (1) They typically focus only on the matching similarity between\nsentences. However, there exists implicit high-value information both within\nsentences of the same class and across different classes, which is very crucial\nfor classification tasks. (2) Existing methods such as pre-trained language\nmodels and graph-based approaches often consume substantial memory for training\nand text-graph construction. (3) Although some low-resource methods can achieve\ngood performance, they often suffer from excessively long processing times. To\naddress these challenges, we propose a low-resource and fast text\nclassification model called LFTC. Our approach begins by constructing a\ncompressor list for each class to fully mine the regularity information within\nintra-class data. We then remove redundant information irrelevant to the target\nclassification to reduce processing time. Finally, we compute the similarity\ndistance between text pairs for classification. We evaluate LFTC on 9 publicly\navailable benchmark datasets, and the results demonstrate significant\nimprovements in performance and processing time, especially under limited\ncomputational and data resources, highlighting its superior advantages.\n","authors":["Yanxu Mao","Peipei Liu","Tiehan Cui","Congying Liu","Datao You"],"pdf_url":"https://arxiv.org/pdf/2412.09922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13748v2","updated":"2024-12-13T06:55:46Z","published":"2024-06-19T18:01:08Z","title":"Learn and Unlearn in Multilingual LLMs","summary":"  This paper investigates the propagation of harmful information in\nmultilingual large language models (LLMs) and evaluates the efficacy of various\nunlearning methods. We demonstrate that fake information, regardless of the\nlanguage it is in, once introduced into these models through training data, can\nspread across different languages, compromising the integrity and reliability\nof the generated content. Our findings reveal that standard unlearning\ntechniques, which typically focus on English data, are insufficient in\nmitigating the spread of harmful content in multilingual contexts and could\ninadvertently reinforce harmful content across languages. We show that only by\naddressing harmful responses in both English and the original language of the\nharmful data can we effectively eliminate generations for all languages. This\nunderscores the critical need for comprehensive unlearning strategies that\nconsider the multilingual nature of modern LLMs to enhance their safety and\nreliability across diverse linguistic landscapes.\n","authors":["Taiming Lu","Philipp Koehn"],"pdf_url":"https://arxiv.org/pdf/2406.13748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09906v1","updated":"2024-12-13T06:45:26Z","published":"2024-12-13T06:45:26Z","title":"Enhancing the Reasoning Capabilities of Small Language Models via\n  Solution Guidance Fine-Tuning","summary":"  Large language models (LLMs) have demonstrated remarkable performance across\na wide range of tasks. Advances in prompt engineering and fine-tuning\ntechniques have further enhanced their ability to address complex reasoning\nchallenges. However, these advanced capabilities are often exclusive to models\nexceeding 100 billion parameters. Although Chain-of-Thought (CoT) fine-tuning\nmethods have been explored for smaller models (under 10 billion parameters),\nthey typically depend on extensive CoT training data, which can introduce\ninconsistencies and limit effectiveness in low-data settings. To overcome these\nlimitations, this paper introduce a new reasoning strategy Solution Guidance\n(SG) and a plug-and-play training paradigm Solution-Guidance Fine-Tuning (SGFT)\nfor enhancing the reasoning capabilities of small language models. SG focuses\non problem understanding and decomposition at the semantic and logical levels,\nrather than specific computations, which can effectively improve the SLMs'\ngeneralization and reasoning abilities. With only a small amount of SG training\ndata, SGFT can fine-tune a SLM to produce accurate problem-solving guidances,\nwhich can then be flexibly fed to any SLM as prompts, enabling it to generate\ncorrect answers directly. Experimental results demonstrate that our method\nsignificantly improves the performance of SLMs on various reasoning tasks,\nenhancing both their practicality and efficiency within resource-constrained\nenvironments.\n","authors":["Jing Bi","Yuting Wu","Weiwei Xing","Zhenjie Wei"],"pdf_url":"https://arxiv.org/pdf/2412.09906v1.pdf","comment":"11 pages, 4 figures, to be published in The 31st International\n  Conference on Computational Linguistics (COLING 2025)"},{"id":"http://arxiv.org/abs/2412.08038v2","updated":"2024-12-13T06:39:00Z","published":"2024-12-11T02:37:32Z","title":"Bootstrapping Heterogeneous Graph Representation Learning via Large\n  Language Models: A Generalized Approach","summary":"  Graph representation learning methods are highly effective in handling\ncomplex non-Euclidean data by capturing intricate relationships and features\nwithin graph structures. However, traditional methods face challenges when\ndealing with heterogeneous graphs that contain various types of nodes and edges\ndue to the diverse sources and complex nature of the data. Existing\nHeterogeneous Graph Neural Networks (HGNNs) have shown promising results but\nrequire prior knowledge of node and edge types and unified node feature\nformats, which limits their applicability. Recent advancements in graph\nrepresentation learning using Large Language Models (LLMs) offer new solutions\nby integrating LLMs' data processing capabilities, enabling the alignment of\nvarious graph representations. Nevertheless, these methods often overlook\nheterogeneous graph data and require extensive preprocessing. To address these\nlimitations, we propose a novel method that leverages the strengths of both LLM\nand GNN, allowing for the processing of graph data with any format and type of\nnodes and edges without the need for type information or special preprocessing.\nOur method employs LLM to automatically summarize and classify different data\nformats and types, aligns node features, and uses a specialized GNN for\ntargeted learning, thus obtaining effective graph representations for\ndownstream tasks. Theoretical analysis and experimental validation have\ndemonstrated the effectiveness of our method.\n","authors":["Hang Gao","Chenhao Zhang","Fengge Wu","Junsuo Zhao","Changwen Zheng","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.08038v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09900v1","updated":"2024-12-13T06:35:55Z","published":"2024-12-13T06:35:55Z","title":"Analyzing Fairness of Computer Vision and Natural Language Processing\n  Models","summary":"  Machine learning (ML) algorithms play a crucial role in decision making\nacross diverse fields such as healthcare, finance, education, and law\nenforcement. Despite their widespread adoption, these systems raise ethical and\nsocial concerns due to potential biases and fairness issues. This study focuses\non evaluating and improving the fairness of Computer Vision and Natural\nLanguage Processing (NLP) models applied to unstructured datasets, emphasizing\nhow biased predictions can reinforce existing systemic inequalities. A publicly\navailable dataset from Kaggle was utilized to simulate a practical scenario for\nexamining fairness in ML workflows. To address and mitigate biases, the study\nemployed two leading fairness libraries: Fairlearn by Microsoft, and AIF360 by\nIBM. These tools offer comprehensive frameworks for fairness analysis,\nincluding metrics evaluation, result visualization, and bias mitigation\ntechniques. The research aims to measure bias levels in ML models, compare the\neffectiveness of these fairness libraries, and provide actionable\nrecommendations for practitioners. The results demonstrate that each library\npossesses distinct strengths and limitations in evaluating and mitigating\nfairness. By systematically analyzing these tools, the study contributes\nvaluable insights to the growing field of ML fairness, offering practical\nguidance for integrating fairness solutions into real world applications. This\nresearch underscores the importance of building more equitable and responsible\nmachine learning systems.\n","authors":["Ahmed Rashed","Abdelkrim Kallich","Mohamed Eltayeb"],"pdf_url":"https://arxiv.org/pdf/2412.09900v1.pdf","comment":"16 pages, 1 table, 4 figures"},{"id":"http://arxiv.org/abs/2412.09263v2","updated":"2024-12-13T06:28:11Z","published":"2024-12-12T13:21:09Z","title":"First Train to Generate, then Generate to Train: UnitedSynT5 for\n  Few-Shot NLI","summary":"  Natural Language Inference (NLI) tasks require identifying the relationship\nbetween sentence pairs, typically classified as entailment, contradiction, or\nneutrality. While the current state-of-the-art (SOTA) model, Entailment\nFew-Shot Learning (EFL), achieves a 93.1% accuracy on the Stanford Natural\nLanguage Inference (SNLI) dataset, further advancements are constrained by the\ndataset's limitations. To address this, we propose a novel approach leveraging\nsynthetic data augmentation to enhance dataset diversity and complexity. We\npresent UnitedSynT5, an advanced extension of EFL that leverages a T5-based\ngenerator to synthesize additional premise-hypothesis pairs, which are\nrigorously cleaned and integrated into the training data. These augmented\nexamples are processed within the EFL framework, embedding labels directly into\nhypotheses for consistency. We train a GTR-T5-XL model on this expanded\ndataset, achieving a new benchmark of 94.7% accuracy on the SNLI dataset, 94.0%\naccuracy on the E-SNLI dataset, and 92.6% accuracy on the MultiNLI dataset,\nsurpassing the previous SOTA models. This research demonstrates the potential\nof synthetic data augmentation in improving NLI models, offering a path forward\nfor further advancements in natural language understanding tasks.\n","authors":["Sourav Banerjee","Anush Mahajan","Ayushi Agarwal","Eishkaran Singh"],"pdf_url":"https://arxiv.org/pdf/2412.09263v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2406.00627v4","updated":"2024-12-13T06:13:08Z","published":"2024-06-02T06:09:56Z","title":"Role-playing Prompt Framework: Generation and Evaluation","summary":"  Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis paper introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance.\n","authors":["Xun Liu","Zhengwei Ni"],"pdf_url":"https://arxiv.org/pdf/2406.00627v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09884v1","updated":"2024-12-13T05:52:37Z","published":"2024-12-13T05:52:37Z","title":"Benchmarking Table Comprehension In The Wild","summary":"  Large Language Models (LLMs), while being increasingly dominant on a myriad\nof knowledge-intensive activities, have only had limited success understanding\nlengthy table-text mixtures, such as academic papers and financial reports.\nRecent advances of long-context LLMs have opened up new possibilities for this\nfield. Nonetheless, we identify two roadblocks: (1) Prior benchmarks of table\nquestion answering (TableQA) have focused on isolated tables without context,\nmaking it hard to evaluate models in real-world scenarios. (2) Prior benchmarks\nhave focused on some narrow skill sets of table comprehension such as table\nrecognition, data manipulation/calculation, table summarization etc., while a\nskilled human employs those skills collectively. In this work, we introduce\nTableQuest, a new benchmark designed to evaluate the holistic table\ncomprehension capabilities of LLMs in the natural table-rich context of\nfinancial reports. We employ a rigorous data processing and filtering procedure\nto ensure that the question-answer pairs are logical, reasonable, and diverse.\nWe experiment with 7 state-of-the-art models, and find that despite reasonable\naccuracy in locating facts, they often falter when required to execute more\nsophisticated reasoning or multi-step calculations. We conclude with a\nqualitative study of the failure modes and discuss the challenges of\nconstructing a challenging benchmark. We make the evaluation data, judging\nprocedure and results of this study publicly available to facilitate research\nin this field.\n","authors":["Yikang Pan","Yi Zhu","Rand Xie","Yizhi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09884v1.pdf","comment":"Accepted at TRL Workshop@Neurips 2024. Link to data\n  https://github.com/boson-ai/Table_eval_public"},{"id":"http://arxiv.org/abs/2412.09879v1","updated":"2024-12-13T05:50:22Z","published":"2024-12-13T05:50:22Z","title":"On the Limit of Language Models as Planning Formalizers","summary":"  Large Language Models have been shown to fail to create executable and\nverifiable plans in grounded environments. An emerging line of work shows\nsuccess in using LLM as a formalizer to generate a formal representation (e.g.,\nPDDL) of the planning domain, which can be deterministically solved to find a\nplan. We systematically evaluate this methodology while bridging some major\ngaps. While previous work only generates a partial PDDL representation given\ntemplated and thus unrealistic environment descriptions, we generate the\ncomplete representation given descriptions of various naturalness levels. Among\nan array of observations critical to improve LLMs' formal planning ability, we\nnote that large enough models can effectively formalize descriptions as PDDL,\noutperforming those directly generating plans, while being robust to lexical\nperturbation. As the descriptions become more natural-sounding, we observe a\ndecrease in performance and provide detailed error analysis.\n","authors":["Cassie Huang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09871v1","updated":"2024-12-13T05:33:32Z","published":"2024-12-13T05:33:32Z","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","summary":"  We introduce the Byte Latent Transformer (BLT), a new byte-level LLM\narchitecture that, for the first time, matches tokenization-based LLM\nperformance at scale with significant improvements in inference efficiency and\nrobustness. BLT encodes bytes into dynamically sized patches, which serve as\nthe primary units of computation. Patches are segmented based on the entropy of\nthe next byte, allocating more compute and model capacity where increased data\ncomplexity demands it. We present the first FLOP controlled scaling study of\nbyte-level models up to 8B parameters and 4T training bytes. Our results\ndemonstrate the feasibility of scaling models trained on raw bytes without a\nfixed vocabulary. Both training and inference efficiency improve due to\ndynamically selecting long patches when data is predictable, along with\nqualitative improvements on reasoning and long tail generalization. Overall,\nfor fixed inference costs, BLT shows significantly better scaling than\ntokenization-based models, by simultaneously growing both patch and model size.\n","authors":["Artidoro Pagnoni","Ram Pasunuru","Pedro Rodriguez","John Nguyen","Benjamin Muller","Margaret Li","Chunting Zhou","Lili Yu","Jason Weston","Luke Zettlemoyer","Gargi Ghosh","Mike Lewis","Ari Holtzman","Srinivasan Iyer"],"pdf_url":"https://arxiv.org/pdf/2412.09871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09867v1","updated":"2024-12-13T05:19:49Z","published":"2024-12-13T05:19:49Z","title":"Human-Like Embodied AI Interviewer: Employing Android ERICA in Real\n  International Conference","summary":"  This paper introduces the human-like embodied AI interviewer which integrates\nandroid robots equipped with advanced conversational capabilities, including\nattentive listening, conversational repairs, and user fluency adaptation.\nMoreover, it can analyze and present results post-interview. We conducted a\nreal-world case study at SIGDIAL 2024 with 42 participants, of whom 69%\nreported positive experiences. This study demonstrated the system's\neffectiveness in conducting interviews just like a human and marked the first\nemployment of such a system at an international conference. The demonstration\nvideo is available at https://youtu.be/jCuw9g99KuE.\n","authors":["Zi Haur Pang","Yahui Fu","Divesh Lala","Mikey Elmers","Koji Inoue","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2412.09867v1.pdf","comment":"This paper has been accepted for demonstration presentation at\n  International Conference on Computational Linguistics (COLING 2025)"},{"id":"http://arxiv.org/abs/2401.07977v3","updated":"2024-12-13T05:19:47Z","published":"2024-01-15T21:43:46Z","title":"Towards Efficient Methods in Medical Question Answering using Knowledge\n  Graph Embeddings","summary":"  In Natural Language Processing (NLP), Machine Reading Comprehension (MRC) is\nthe task of answering a question based on a given context. To handle questions\nin the medical domain, modern language models such as BioBERT, SciBERT and even\nChatGPT are trained on vast amounts of in-domain medical corpora. However,\nin-domain pre-training is expensive in terms of time and resources. In this\npaper, we propose a resource-efficient approach for injecting domain knowledge\ninto a model without relying on such domain-specific pre-training.\n  Knowledge graphs are powerful resources for accessing medical information.\nBuilding on existing work, we introduce a method using Multi-Layer Perceptrons\n(MLPs) for aligning and integrating embeddings extracted from medical knowledge\ngraphs with the embedding spaces of pre-trained language models (LMs). The\naligned embeddings are fused with open-domain LMs BERT and RoBERTa that are\nfine-tuned for two MRC tasks, span detection (COVID-QA) and multiple-choice\nquestions (PubMedQA). We compare our method to prior techniques that rely on a\nvocabulary overlap for embedding alignment and show how our method circumvents\nthis requirement to deliver better performance. On both datasets, our method\nallows BERT/RoBERTa to either perform on par (occasionally exceeding) with\nstronger domain-specific models or show improvements in general over prior\ntechniques. With the proposed approach, we signal an alternative method to\nin-domain pre-training to achieve domain proficiency. Our code is available\nhere.\n","authors":["Saptarshi Sengupta","Connor Heaton","Suhan Cui","Soumalya Sarkar","Prasenjit Mitra"],"pdf_url":"https://arxiv.org/pdf/2401.07977v3.pdf","comment":"Accepted to the MABM workshop at IEEE BIBM 2024"},{"id":"http://arxiv.org/abs/2412.09859v1","updated":"2024-12-13T04:59:50Z","published":"2024-12-13T04:59:50Z","title":"Financial Sentiment Analysis: Leveraging Actual and Synthetic Data for\n  Supervised Fine-tuning","summary":"  The Efficient Market Hypothesis (EMH) highlights the essence of financial\nnews in stock price movement. Financial news comes in the form of corporate\nannouncements, news titles, and other forms of digital text. The generation of\ninsights from financial news can be done with sentiment analysis.\nGeneral-purpose language models are too general for sentiment analysis in\nfinance. Curated labeled data for fine-tuning general-purpose language models\nare scare, and existing fine-tuned models for sentiment analysis in finance do\nnot capture the maximum context width. We hypothesize that using actual and\nsynthetic data can improve performance. We introduce BertNSP-finance to\nconcatenate shorter financial sentences into longer financial sentences, and\nfinbert-lc to determine sentiment from digital text. The results show improved\nperformance on the accuracy and the f1 score for the financial phrasebank data\nwith $50\\%$ and $100\\%$ agreement levels.\n","authors":["Abraham Atsiwo"],"pdf_url":"https://arxiv.org/pdf/2412.09859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08966v3","updated":"2024-12-13T04:59:10Z","published":"2024-02-14T06:20:48Z","title":"Pretraining Vision-Language Model for Difference Visual Question\n  Answering in Longitudinal Chest X-rays","summary":"  Difference visual question answering (diff-VQA) is a challenging task that\nrequires answering complex questions based on differences between a pair of\nimages. This task is particularly important in reading chest X-ray images\nbecause radiologists often compare multiple images of the same patient taken at\ndifferent times to track disease progression and changes in its severity in\ntheir clinical practice. However, previous works focused on designing specific\nnetwork architectures for the diff-VQA task, missing opportunities to enhance\nthe model's performance using a pretrained vision-language model (VLM). Here,\nwe introduce a novel VLM called PLURAL, which is pretrained on natural and\nlongitudinal chest X-ray data for the diff-VQA task. The model is developed\nusing a step-by-step approach, starting with being pretrained on natural images\nand texts, followed by being trained using longitudinal chest X-ray data. The\nlongitudinal data consist of pairs of X-ray images, along with question-answer\nsets and radiologist's reports that describe the changes in lung abnormalities\nand diseases over time. Our experimental results show that the PLURAL model\noutperforms state-of-the-art methods not only in diff-VQA for longitudinal\nX-rays but also in conventional VQA for a single X-ray image. Through extensive\nexperiments, we demonstrate the effectiveness of the proposed VLM architecture\nand pretraining method in improving the model's performance.\n","authors":["Yeongjae Cho","Taehee Kim","Heejun Shin","Sungzoon Cho","Dongmyung Shin"],"pdf_url":"https://arxiv.org/pdf/2402.08966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08967v3","updated":"2024-12-13T04:44:11Z","published":"2024-01-17T04:43:21Z","title":"ReFT: Reasoning with Reinforced Fine-Tuning","summary":"  One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT.\n","authors":["Trung Quoc Luong","Xinbo Zhang","Zhanming Jie","Peng Sun","Xiaoran Jin","Hang Li"],"pdf_url":"https://arxiv.org/pdf/2401.08967v3.pdf","comment":"ACL 2024 main conference; adjust with reviewer comments; 13 pages"},{"id":"http://arxiv.org/abs/2412.07144v2","updated":"2024-12-13T04:05:05Z","published":"2024-12-10T03:06:28Z","title":"Political Actor Agent: Simulating Legislative System for Roll Call Votes\n  Prediction with Large Language Models","summary":"  Predicting roll call votes through modeling political actors has emerged as a\nfocus in quantitative political science and computer science. Widely used\nembedding-based methods generate vectors for legislators from diverse data sets\nto predict legislative behaviors. However, these methods often contend with\nchallenges such as the need for manually predefined features, reliance on\nextensive training data, and a lack of interpretability. Achieving more\ninterpretable predictions under flexible conditions remains an unresolved\nissue. This paper introduces the Political Actor Agent (PAA), a novel\nagent-based framework that utilizes Large Language Models to overcome these\nlimitations. By employing role-playing architectures and simulating legislative\nsystem, PAA provides a scalable and interpretable paradigm for predicting\nroll-call votes. Our approach not only enhances the accuracy of predictions but\nalso offers multi-view, human-understandable decision reasoning, providing new\ninsights into political actor behaviors. We conducted comprehensive experiments\nusing voting records from the 117-118th U.S. House of Representatives,\nvalidating the superior performance and interpretability of PAA. This study not\nonly demonstrates PAA's effectiveness but also its potential in political\nscience research.\n","authors":["Hao Li","Ruoyuan Gong","Hao Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.07144v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2408.12325v3","updated":"2024-12-13T04:03:07Z","published":"2024-08-22T12:00:31Z","title":"Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators","summary":"  Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.\n","authors":["Dingkang Yang","Dongling Xiao","Jinjie Wei","Mingcheng Li","Zhaoyu Chen","Ke Li","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.12325v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.06724v2","updated":"2024-12-13T03:43:35Z","published":"2024-12-09T18:13:27Z","title":"AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and\n  Benchmark","summary":"  We investigate the reasoning capabilities of large language models (LLMs) for\nautomatically generating data-cleaning workflows. To evaluate LLMs' ability to\ncomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data\nCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations\nto repair three types of data quality issues: duplicates, missing values, and\ninconsistent data formats. Given a dirty table and a purpose (expressed as a\nquery), this pipeline generates a minimal, clean table sufficient to address\nthe purpose and the data cleaning workflow used to produce the table. The\nplanning process involves three main LLM-driven components: (1) Select Target\nColumns: Identifies a set of target columns related to the purpose. (2) Inspect\nColumn Quality: Assesses the data quality for each target column and generates\na Data Quality Report as operation objectives. (3) Generate Operation &\nArguments: Predicts the next operation and arguments based on the data quality\nreport results. Additionally, we propose a data cleaning benchmark to evaluate\nthe capability of LLM agents to automatically generate workflows that address\ndata cleaning purposes of varying difficulty levels. The benchmark comprises\nthe annotated datasets as a collection of purpose, raw table, clean table, data\ncleaning workflow, and answer set. In our experiments, we evaluated three LLMs\nthat auto-generate purpose-driven data cleaning workflows. The results indicate\nthat LLMs perform well in planning and generating data-cleaning workflows\nwithout the need for fine-tuning.\n","authors":["Lan Li","Liri Fang","Vetle I. Torvik"],"pdf_url":"https://arxiv.org/pdf/2412.06724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09827v1","updated":"2024-12-13T03:38:49Z","published":"2024-12-13T03:38:49Z","title":"Low-Rank Adaptation with Task-Relevant Feature Enhancement for\n  Fine-tuning Language Models","summary":"  Fine-tuning pre-trained large language models in a parameter-efficient manner\nis widely studied for its effectiveness and efficiency. LoRA is one of the most\nwidely used methods, which assumes that the optimization process is essentially\nlow dimensional. Although LoRA has demonstrated commendable performance, there\nremains a significant performance gap between LoRA and full fine-tuning when\nlearning new tasks. In this work, we propose Low-Rank Adaptation with\nTask-Relevant Feature Enhancement(LoRATRF) for enhancing task-relevant features\nfrom the perspective of editing neural network representations. To prioritize\ntask-relevant features, a task-aware filter that selectively extracts valuable\nknowledge from hidden representations for the target or current task is\ndesigned. As the experiments on a vareity of datasets including NLU,\ncommonsense reasoning and mathematical reasoning tasks demonstrates, our method\nreduces 33.71% parameters and achieves better performance on a variety of\ndatasets in comparison with SOTA low-rank methods.\n","authors":["Changqun Li","Chaofan Ding","Kexin Luan","Xinhan Di"],"pdf_url":"https://arxiv.org/pdf/2412.09827v1.pdf","comment":"6 Pages, 3 figures accepted by AAAI 2025 CoLoRAI - Connecting\n  Low-Rank Representations in AI Workshop"},{"id":"http://arxiv.org/abs/2407.00416v2","updated":"2024-12-13T03:15:51Z","published":"2024-06-29T11:50:16Z","title":"Too Late to Train, Too Early To Use? A Study on Necessity and Viability\n  of Low-Resource Bengali LLMs","summary":"  Each new generation of English-oriented Large Language Models (LLMs) exhibits\nenhanced cross-lingual transfer capabilities and significantly outperforms\nolder LLMs on low-resource languages. This prompts the question: Is there a\nneed for LLMs dedicated to a particular low-resource language? We aim to\nexplore this question for Bengali, a low-to-moderate resource Indo-Aryan\nlanguage native to the Bengal region of South Asia.\n  We compare the performance of open-weight and closed-source LLMs such as\nLLaMA-3 and GPT-4 against fine-tuned encoder-decoder models across a diverse\nset of Bengali downstream tasks, including translation, summarization,\nparaphrasing, question-answering, and natural language inference. Our findings\nreveal that while LLMs generally excel in reasoning tasks, their performance in\ntasks requiring Bengali script generation is inconsistent. Key challenges\ninclude inefficient tokenization of Bengali script by existing LLMs, leading to\nincreased computational costs and potential performance degradation.\nAdditionally, we highlight biases in machine-translated datasets commonly used\nfor Bengali NLP tasks. We conclude that there is a significant need for a\nBengali-oriented LLM, but the field currently lacks the high-quality\npretraining and instruction-tuning datasets necessary to develop a highly\neffective model.\n","authors":["Tamzeed Mahfuz","Satak Kumar Dey","Ruwad Naswan","Hasnaen Adil","Khondker Salman Sayeed","Haz Sameen Shahgir"],"pdf_url":"https://arxiv.org/pdf/2407.00416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09818v1","updated":"2024-12-13T03:15:05Z","published":"2024-12-13T03:15:05Z","title":"MERaLiON-AudioLLM: Technical Report","summary":"  We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning\nin One Network), the first speech-text model tailored for Singapore's\nmultilingual and multicultural landscape. Developed under the National Large\nLanguage Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates\nadvanced speech and text processing to address the diverse linguistic nuances\nof local accents and dialects, enhancing accessibility and usability in\ncomplex, multilingual environments. Our results demonstrate improvements in\nboth speech recognition and task-specific understanding, positioning\nMERaLiON-AudioLLM as a pioneering solution for region specific AI applications.\nWe envision this release to set a precedent for future models designed to\naddress localised linguistic and cultural contexts in a global framework.\n","authors":["Yingxu He","Zhuohan Liu","Shuo Sun","Bin Wang","Wenyu Zhang","Xunlong Zou","Nancy F. Chen","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2412.09818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09817v1","updated":"2024-12-13T03:13:44Z","published":"2024-12-13T03:13:44Z","title":"Enhancing Multimodal Large Language Models Complex Reason via Similarity\n  Computation","summary":"  Multimodal large language models have experienced rapid growth, and numerous\ndifferent models have emerged. The interpretability of LVLMs remains an\nunder-explored area. Especially when faced with more complex tasks such as\nchain-of-thought reasoning, its internal mechanisms still resemble a black box\nthat is difficult to decipher. By studying the interaction and information flow\nbetween images and text, we noticed that in models such as LLaVA1.5, image\ntokens that are semantically related to text are more likely to have\ninformation flow convergence in the LLM decoding layer, and these image tokens\nreceive higher attention scores. However, those image tokens that are less\nrelevant to the text do not have information flow convergence, and they only\nget very small attention scores. To efficiently utilize the image information,\nwe propose a new image token reduction method, Simignore, which aims to improve\nthe complex reasoning ability of LVLMs by computing the similarity between\nimage and text embeddings and ignoring image tokens that are irrelevant and\nunimportant to the text. Through extensive experiments, we demonstrate the\neffectiveness of our method for complex reasoning tasks. The paper's source\ncode can be accessed from \\url{https://github.com/FanshuoZeng/Simignore}.\n","authors":["Xiaofeng Zhang","Fanshuo Zeng","Yihao Quan","Zheng Hui","Jiawei Yao"],"pdf_url":"https://arxiv.org/pdf/2412.09817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09812v1","updated":"2024-12-13T03:00:48Z","published":"2024-12-13T03:00:48Z","title":"ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic\n  LayerReplace and Selective Rank Compression","summary":"  Offsite-tuning is a privacy-preserving method for tuning large language\nmodels (LLMs) by sharing a lossy compressed emulator from the LLM owners with\ndata owners for downstream task tuning. This approach protects the privacy of\nboth the model and data owners. However, current offsite tuning methods often\nsuffer from adaptation degradation, high computational costs, and limited\nprotection strength due to uniformly dropping LLM layers or relying on\nexpensive knowledge distillation. To address these issues, we propose ScaleOT,\na novel privacy-utility-scalable offsite-tuning framework that effectively\nbalances privacy and utility. ScaleOT introduces a novel layerwise lossy\ncompression algorithm that uses reinforcement learning to obtain the importance\nof each layer. It employs lightweight networks, termed harmonizers, to replace\nthe raw LLM layers. By combining important original LLM layers and harmonizers\nin different ratios, ScaleOT generates emulators tailored for optimal\nperformance with various model scales for enhanced privacy protection.\nAdditionally, we present a rank reduction method to further compress the\noriginal LLM layers, significantly enhancing privacy with negligible impact on\nutility. Comprehensive experiments show that ScaleOT can achieve nearly\nlossless offsite tuning performance compared with full fine-tuning while\nobtaining better model privacy.\n","authors":["Kai Yao","Zhaorui Tan","Tiandi Ye","Lichun Li","Yuan Zhao","Wenyan Liu","Wei Wang","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.09812v1.pdf","comment":"accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2408.09688v2","updated":"2024-12-13T02:48:44Z","published":"2024-08-19T03:53:48Z","title":"Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts","summary":"  Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task.\n","authors":["Jiaqing Liu","Chong Deng","Qinglin Zhang","Shilin Zhou","Qian Chen","Hai Yu","Wen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.09688v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.09807v1","updated":"2024-12-13T02:48:36Z","published":"2024-12-13T02:48:36Z","title":"LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering","summary":"  Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA.\n","authors":["Patrick Sutanto","Joan Santoso"],"pdf_url":"https://arxiv.org/pdf/2412.09807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03514v2","updated":"2024-12-13T02:45:14Z","published":"2024-04-04T15:21:22Z","title":"Embedding-Informed Adaptive Retrieval-Augmented Generation of Large\n  Language Models","summary":"  Retrieval-augmented large language models (LLMs) have been remarkably\ncompetent in various NLP tasks. However, it was observed by previous works that\nretrieval is not always helpful, especially when the LLM is already\nknowledgeable on the query to answer. Motivated by this, Adaptive\nRetrieval-Augmented Generation (ARAG) studies retrieving only when the\nknowledge asked by the query is absent in the LLM. Previous works of ARAG\neither require accessing the pre-training corpus or prompting with additional\nmodel inferences. Aiming to avoid such drawbacks, we propose to determine\nwhether the model is knowledgeable on a query via inspecting the\n(contextualized) pre-trained token embeddings of LLMs. We hypothesize that such\nembeddings capture rich information on the model's intrinsic knowledge base,\nwhich enables an efficient way of judging the necessity to retrieve from an\nexternal corpus. Extensive experiments demonstrate our ARAG approach's superior\nperformance across various benchmarks.\n","authors":["Chengkai Huang","Yu Xia","Rui Wang","Kaige Xie","Tong Yu","Julian McAuley","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2404.03514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09025v6","updated":"2024-12-13T02:44:34Z","published":"2024-02-14T09:01:13Z","title":"SLEB: Streamlining LLMs through Redundancy Verification and Elimination\n  of Transformer Blocks","summary":"  Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB outperforms\nprevious LLM pruning methods in accelerating LLM inference while also\nmaintaining superior perplexity and accuracy, making SLEB as a promising\ntechnique for enhancing the efficiency of LLMs. The code is available at:\nhttps://github.com/jiwonsong-dev/SLEB.\n","authors":["Jiwon Song","Kyungseok Oh","Taesu Kim","Hyungjun Kim","Yulhwa Kim","Jae-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2402.09025v6.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2412.09014v2","updated":"2024-12-13T02:34:05Z","published":"2024-12-12T07:24:16Z","title":"Improvement in Sign Language Translation Using Text CTC Alignment","summary":"  Current sign language translation (SLT) approaches often rely on gloss-based\nsupervision with Connectionist Temporal Classification (CTC), limiting their\nability to handle non-monotonic alignments between sign language video and\nspoken text. In this work, we propose a novel method combining joint\nCTC/Attention and transfer learning. The joint CTC/Attention introduces\nhierarchical encoding and integrates CTC with the attention mechanism during\ndecoding, effectively managing both monotonic and non-monotonic alignments.\nMeanwhile, transfer learning helps bridge the modality gap between vision and\nlanguage in SLT. Experimental results on two widely adopted benchmarks,\nRWTH-PHOENIX-Weather 2014 T and CSL-Daily, show that our method achieves\nresults comparable to state-of-the-art and outperforms the pure-attention\nbaseline. Additionally, this work opens a new door for future research into\ngloss-free SLT using text-based CTC alignment.\n","authors":["Sihan Tan","Taro Miyazaki","Nabeela Khan","Kazuhiro Nakadai"],"pdf_url":"https://arxiv.org/pdf/2412.09014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10188v6","updated":"2024-12-13T02:32:06Z","published":"2024-08-19T17:48:08Z","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","summary":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long video supervised fine-tuning. However, training on long\nvideo is computationally and memory intensive. We introduce the long-context\nMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes\nlong video training and inference, enabling 2M context length training on 256\nGPUs without any gradient checkpointing. LongVILA efficiently extends the\nnumber of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in\n6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g.\n65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring\nstyle sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid\ncontext and tensor parallelism. Moreover, it seamlessly integrates with Hugging\nFace Transformers.\n","authors":["Yukang Chen","Fuzhao Xue","Dacheng Li","Qinghao Hu","Ligeng Zhu","Xiuyu Li","Yunhao Fang","Haotian Tang","Shang Yang","Zhijian Liu","Ethan He","Hongxu Yin","Pavlo Molchanov","Jan Kautz","Linxi Fan","Yuke Zhu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2408.10188v6.pdf","comment":"Code and models are available at\n  https://github.com/NVlabs/VILA/tree/main/longvila"},{"id":"http://arxiv.org/abs/2412.09796v1","updated":"2024-12-13T02:27:34Z","published":"2024-12-13T02:27:34Z","title":"AutoPatent: A Multi-Agent Framework for Automatic Patent Generation","summary":"  As the capabilities of Large Language Models (LLMs) continue to advance, the\nfield of patent processing has garnered increased attention within the natural\nlanguage processing community. However, the majority of research has been\nconcentrated on classification tasks, such as patent categorization and\nexamination, or on short text generation tasks like patent summarization and\npatent quizzes. In this paper, we introduce a novel and practical task known as\nDraft2Patent, along with its corresponding D2P benchmark, which challenges LLMs\nto generate full-length patents averaging 17K tokens based on initial drafts.\nPatents present a significant challenge to LLMs due to their specialized\nnature, standardized terminology, and extensive length. We propose a\nmulti-agent framework called AutoPatent which leverages the LLM-based planner\nagent, writer agents, and examiner agent with PGTree and RRAG to generate\nlengthy, intricate, and high-quality complete patent documents. The\nexperimental results demonstrate that our AutoPatent framework significantly\nenhances the ability to generate comprehensive patents across various LLMs.\nFurthermore, we have discovered that patents generated solely with the\nAutoPatent framework based on the Qwen2.5-7B model outperform those produced by\nlarger and more powerful LLMs, such as GPT-4o, Qwen2.5-72B, and LLAMA3.1-70B,\nin both objective metrics and human evaluations. We will make the data and code\navailable upon acceptance at \\url{https://github.com/QiYao-Wang/AutoPatent}.\n","authors":["Qiyao Wang","Shiwen Ni","Huaren Liu","Shule Lu","Guhong Chen","Xi Feng","Chi Wei","Qiang Qu","Hamid Alinejad-Rokny","Yuan Lin","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09796v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.09784v1","updated":"2024-12-13T01:48:07Z","published":"2024-12-13T01:48:07Z","title":"Semi-IIN: Semi-supervised Intra-inter modal Interaction Learning Network\n  for Multimodal Sentiment Analysis","summary":"  Despite multimodal sentiment analysis being a fertile research ground that\nmerits further investigation, current approaches take up high annotation cost\nand suffer from label ambiguity, non-amicable to high-quality labeled data\nacquisition. Furthermore, choosing the right interactions is essential because\nthe significance of intra- or inter-modal interactions can differ among various\nsamples. To this end, we propose Semi-IIN, a Semi-supervised Intra-inter modal\nInteraction learning Network for multimodal sentiment analysis. Semi-IIN\nintegrates masked attention and gating mechanisms, enabling effective dynamic\nselection after independently capturing intra- and inter-modal interactive\ninformation. Combined with the self-training approach, Semi-IIN fully utilizes\nthe knowledge learned from unlabeled data. Experimental results on two public\ndatasets, MOSI and MOSEI, demonstrate the effectiveness of Semi-IIN,\nestablishing a new state-of-the-art on several metrics. Code is available at\nhttps://github.com/flow-ljh/Semi-IIN.\n","authors":["Jinhao Lin","Yifei Wang","Yanwu Xu","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09784v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.10373v1","updated":"2024-12-13T18:59:54Z","published":"2024-12-13T18:59:54Z","title":"GaussianWorld: Gaussian World Model for Streaming 3D Occupancy\n  Prediction","summary":"  3D occupancy prediction is important for autonomous driving due to its\ncomprehensive perception of the surroundings. To incorporate sequential inputs,\nmost existing methods fuse representations from previous frames to infer the\ncurrent 3D occupancy. However, they fail to consider the continuity of driving\nscenarios and ignore the strong prior provided by the evolution of 3D scenes\n(e.g., only dynamic objects move). In this paper, we propose a\nworld-model-based framework to exploit the scene evolution for perception. We\nreformulate 3D occupancy prediction as a 4D occupancy forecasting problem\nconditioned on the current sensor input. We decompose the scene evolution into\nthree factors: 1) ego motion alignment of static scenes; 2) local movements of\ndynamic objects; and 3) completion of newly-observed scenes. We then employ a\nGaussian world model (GaussianWorld) to explicitly exploit these priors and\ninfer the scene evolution in the 3D Gaussian space considering the current RGB\nobservation. We evaluate the effectiveness of our framework on the widely used\nnuScenes dataset. Our GaussianWorld improves the performance of the\nsingle-frame counterpart by over 2% in mIoU without introducing additional\ncomputations. Code: https://github.com/zuosc19/GaussianWorld.\n","authors":["Sicheng Zuo","Wenzhao Zheng","Yuanhui Huang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10373v1.pdf","comment":"Code is available at: https://github.com/zuosc19/GaussianWorld"},{"id":"http://arxiv.org/abs/2412.10372v1","updated":"2024-12-13T18:59:40Z","published":"2024-12-13T18:59:40Z","title":"UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for\n  Diverse Medical Imaging Modalities","summary":"  Vision-Language Models (VLMs) trained via contrastive learning have achieved\nnotable success in natural image tasks. However, their application in the\nmedical domain remains limited due to the scarcity of openly accessible,\nlarge-scale medical image-text datasets. Existing medical VLMs either train on\nclosed-source proprietary or relatively small open-source datasets that do not\ngeneralize well. Similarly, most models remain specific to a single or limited\nnumber of medical imaging domains, again restricting their applicability to\nother modalities. To address this gap, we introduce UniMed, a large-scale,\nopen-source multi-modal medical dataset comprising over 5.3 million image-text\npairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound,\nPathology, and Fundus. UniMed is developed using a data-collection framework\nthat leverages Large Language Models (LLMs) to transform modality-specific\nclassification datasets into image-text formats while incorporating existing\nimage-text data from the medical domain, facilitating scalable VLM pretraining.\nUsing UniMed, we trained UniMed-CLIP, a unified VLM for six modalities that\nsignificantly outperforms existing generalist VLMs and matches\nmodality-specific medical VLMs, achieving notable gains in zero-shot\nevaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained on\nproprietary data) by an absolute gain of +12.61, averaged over 21 datasets,\nwhile using 3x less training data. To facilitate future research, we release\nUniMed dataset, training codes, and models at\nhttps://github.com/mbzuai-oryx/UniMed-CLIP.\n","authors":["Muhammad Uzair Khattak","Shahina Kunhimon","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2412.10372v1.pdf","comment":"Code, models and demo available at\n  https://github.com/mbzuai-oryx/UniMed-CLIP"},{"id":"http://arxiv.org/abs/2412.10371v1","updated":"2024-12-13T18:59:30Z","published":"2024-12-13T18:59:30Z","title":"GaussianAD: Gaussian-Centric End-to-End Autonomous Driving","summary":"  Vision-based autonomous driving shows great potential due to its satisfactory\nperformance and low costs. Most existing methods adopt dense representations\n(e.g., bird's eye view) or sparse representations (e.g., instance boxes) for\ndecision-making, which suffer from the trade-off between comprehensiveness and\nefficiency. This paper explores a Gaussian-centric end-to-end autonomous\ndriving (GaussianAD) framework and exploits 3D semantic Gaussians to\nextensively yet sparsely describe the scene. We initialize the scene with\nuniform 3D Gaussians and use surrounding-view images to progressively refine\nthem to obtain the 3D Gaussian scene representation. We then use sparse\nconvolutions to efficiently perform 3D perception (e.g., 3D detection, semantic\nmap construction). We predict 3D flows for the Gaussians with dynamic semantics\nand plan the ego trajectory accordingly with an objective of future scene\nforecasting. Our GaussianAD can be trained in an end-to-end manner with\noptional perception labels when available. Extensive experiments on the widely\nused nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on\nvarious tasks including motion planning, 3D occupancy prediction, and 4D\noccupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.\n","authors":["Wenzhao Zheng","Junjie Wu","Yao Zheng","Sicheng Zuo","Zixun Xie","Longchao Yang","Yong Pan","Zhihui Hao","Peng Jia","Xianpeng Lang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10371v1.pdf","comment":"Code is available at: https://github.com/wzzheng/GaussianAD"},{"id":"http://arxiv.org/abs/2412.10369v1","updated":"2024-12-13T18:58:48Z","published":"2024-12-13T18:58:48Z","title":"A Grounded Typology of Word Classes","summary":"  We propose a grounded approach to meaning in language typology. We treat data\nfrom perceptual modalities, such as images, as a language-agnostic\nrepresentation of meaning. Hence, we can quantify the function--form\nrelationship between images and captions across languages. Inspired by\ninformation theory, we define \"groundedness\", an empirical measure of\ncontextual semantic contentfulness (formulated as a difference in surprisal)\nwhich can be computed with multilingual multimodal language models. As a proof\nof concept, we apply this measure to the typology of word classes. Our measure\ncaptures the contentfulness asymmetry between functional (grammatical) and\nlexical (content) classes across languages, but contradicts the view that\nfunctional classes do not convey content. Moreover, we find universal trends in\nthe hierarchy of groundedness (e.g., nouns > adjectives > verbs), and show that\nour measure partly correlates with psycholinguistic concreteness norms in\nEnglish. We release a dataset of groundedness scores for 30 languages. Our\nresults suggest that the grounded typology approach can provide quantitative\nevidence about semantic function in language.\n","authors":["Coleman Haley","Sharon Goldwater","Edoardo Ponti"],"pdf_url":"https://arxiv.org/pdf/2412.10369v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.10362v1","updated":"2024-12-13T18:55:19Z","published":"2024-12-13T18:55:19Z","title":"OP-LoRA: The Blessing of Dimensionality","summary":"  Low-rank adapters enable fine-tuning of large models with only a small number\nof parameters, thus reducing storage costs and minimizing the risk of\ncatastrophic forgetting. However, they often pose optimization challenges, with\npoor convergence. To overcome these challenges, we introduce an\nover-parameterized approach that accelerates training without increasing\ninference costs. This method reparameterizes low-rank adaptation by employing a\nseparate MLP and learned embedding for each layer. The learned embedding is\ninput to the MLP, which generates the adapter parameters. Such\noverparamaterization has been shown to implicitly function as an adaptive\nlearning rate and momentum, accelerating optimization. At inference time, the\nMLP can be discarded, leaving behind a standard low-rank adapter. To study the\neffect of MLP overparameterization on a small yet difficult proxy task, we\nimplement it for matrix factorization, and find it achieves faster convergence\nand lower final loss. Extending this approach to larger-scale tasks, we observe\nconsistent performance gains across domains. We achieve improvements in\nvision-language tasks and especially notable increases in image generation,\nwith CMMD scores improving by up to 15 points.\n","authors":["Piotr Teterwak","Kate Saenko","Bryan A. Plummer","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2412.10362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10360v1","updated":"2024-12-13T18:53:24Z","published":"2024-12-13T18:53:24Z","title":"Apollo: An Exploration of Video Understanding in Large Multimodal Models","summary":"  Despite the rapid integration of video perception capabilities into Large\nMultimodal Models (LMMs), the underlying mechanisms driving their video\nunderstanding remain poorly understood. Consequently, many design decisions in\nthis domain are made without proper justification or analysis. The high\ncomputational cost of training and evaluating such models, coupled with limited\nopen research, hinders the development of video-LMMs. To address this, we\npresent a comprehensive study that helps uncover what effectively drives video\nunderstanding in LMMs.\n  We begin by critically examining the primary contributors to the high\ncomputational requirements associated with video-LMM research and discover\nScaling Consistency, wherein design and training decisions made on smaller\nmodels and datasets (up to a critical size) effectively transfer to larger\nmodels. Leveraging these insights, we explored many video-specific aspects of\nvideo-LMMs, including video sampling, architectures, data composition, training\nschedules, and more. For example, we demonstrated that fps sampling during\ntraining is vastly preferable to uniform frame sampling and which vision\nencoders are the best for video representation.\n  Guided by these findings, we introduce Apollo, a state-of-the-art family of\nLMMs that achieve superior performance across different model sizes. Our models\ncan perceive hour-long videos efficiently, with Apollo-3B outperforming most\nexisting $7$B models with an impressive 55.1 on LongVideoBench. Apollo-7B is\nstate-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on\nVideo-MME.\n","authors":["Orr Zohar","Xiaohan Wang","Yann Dubois","Nikhil Mehta","Tong Xiao","Philippe Hansen-Estruch","Licheng Yu","Xiaofang Wang","Felix Juefei-Xu","Ning Zhang","Serena Yeung-Levy","Xide Xia"],"pdf_url":"https://arxiv.org/pdf/2412.10360v1.pdf","comment":"https://apollo-lmms.github.io"},{"id":"http://arxiv.org/abs/2412.10353v1","updated":"2024-12-13T18:49:25Z","published":"2024-12-13T18:49:25Z","title":"Robust image classification with multi-modal large language models","summary":"  Deep Neural Networks are vulnerable to adversarial examples, i.e., carefully\ncrafted input samples that can cause models to make incorrect predictions with\nhigh confidence. To mitigate these vulnerabilities, adversarial training and\ndetection-based defenses have been proposed to strengthen models in advance.\nHowever, most of these approaches focus on a single data modality, overlooking\nthe relationships between visual patterns and textual descriptions of the\ninput. In this paper, we propose a novel defense, Multi-Shield, designed to\ncombine and complement these defenses with multi-modal information to further\nenhance their robustness. Multi-Shield leverages multi-modal large language\nmodels to detect adversarial examples and abstain from uncertain\nclassifications when there is no alignment between textual and visual\nrepresentations of the input. Extensive evaluations on CIFAR-10 and ImageNet\ndatasets, using robust and non-robust image classification models, demonstrate\nthat Multi-Shield can be easily integrated to detect and reject adversarial\nexamples, outperforming the original defenses.\n","authors":["Francesco Villani","Igor Maljkovic","Dario Lazzaro","Angelo Sotgiu","Antonio Emanuele Cinà","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2412.10353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10351v1","updated":"2024-12-13T18:47:11Z","published":"2024-12-13T18:47:11Z","title":"VibrantVS: A high-resolution multi-task transformer for forest canopy\n  height estimation","summary":"  This paper explores the application of a novel multi-task vision transformer\n(ViT) model for the estimation of canopy height models (CHMs) using 4-band\nNational Agriculture Imagery Program (NAIP) imagery across the western United\nStates. We compare the effectiveness of this model in terms of accuracy and\nprecision aggregated across ecoregions and class heights versus three other\nbenchmark peer-reviewed models. Key findings suggest that, while other\nbenchmark models can provide high precision in localized areas, the VibrantVS\nmodel has substantial advantages across a broad reach of ecoregions in the\nwestern United States with higher accuracy, higher precision, the ability to\ngenerate updated inference at a cadence of three years or less, and high\nspatial resolution. The VibrantVS model provides significant value for\necological monitoring and land management decisions for wildfire mitigation.\n","authors":["Tony Chang","Kiarie Ndegwa","Andreas Gros","Vincent A. Landau","Luke J. Zachmann","Bogdan State","Mitchell A. Gritts","Colton W. Miller","Nathan E. Rutenbeck","Scott Conway","Guy Bayes"],"pdf_url":"https://arxiv.org/pdf/2412.10351v1.pdf","comment":"15 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.10349v1","updated":"2024-12-13T18:45:26Z","published":"2024-12-13T18:45:26Z","title":"Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit\n  Tactile Calibration","summary":"  In dynamic environments, robots often encounter constrained movement\ntrajectories when manipulating objects with specific properties, such as doors.\nTherefore, applying the appropriate force is crucial to prevent damage to both\nthe robots and the objects. However, current vision-guided robot state\ngeneration methods often falter in this regard, as they lack the integration of\ntactile perception. To tackle this issue, this paper introduces a novel state\ndiffusion framework termed SafeDiff. It generates a prospective state sequence\nfrom the current robot state and visual context observation while incorporating\nreal-time tactile feedback to refine the sequence. As far as we know, this is\nthe first study specifically focused on ensuring force safety in robotic\nmanipulation. It significantly enhances the rationality of state planning, and\nthe safe action trajectory is derived from inverse dynamics based on this\nrefined planning. In practice, unlike previous approaches that concatenate\nvisual and tactile data to generate future robot state sequences, our method\nemploys tactile data as a calibration signal to adjust the robot's state within\nthe state space implicitly. Additionally, we've developed a large-scale\nsimulation dataset called SafeDoorManip50k, offering extensive multimodal data\nto train and evaluate the proposed method. Extensive experiments show that our\nvisual-tactile model substantially mitigates the risk of harmful forces in the\ndoor opening, across both simulated and real-world settings.\n","authors":["Lai Wei","Jiahua Ma","Yibo Hu","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10348v1","updated":"2024-12-13T18:45:18Z","published":"2024-12-13T18:45:18Z","title":"A dual contrastive framework","summary":"  In current multimodal tasks, models typically freeze the encoder and decoder\nwhile adapting intermediate layers to task-specific goals, such as region\ncaptioning. Region-level visual understanding presents significant challenges\nfor large-scale vision-language models. While limited spatial awareness is a\nknown issue, coarse-grained pretraining, in particular, exacerbates the\ndifficulty of optimizing latent representations for effective encoder-decoder\nalignment. We propose AlignCap, a framework designed to enhance region-level\nunderstanding through fine-grained alignment of latent spaces. Our approach\nintroduces a novel latent feature refinement module that enhances conditioned\nlatent space representations to improve region-level captioning performance. We\nalso propose an innovative alignment strategy, the semantic space alignment\nmodule, which boosts the quality of multimodal representations. Additionally,\nwe incorporate contrastive learning in a novel manner within both modules to\nfurther enhance region-level captioning performance. To address spatial\nlimitations, we employ a General Object Detection (GOD) method as a data\npreprocessing pipeline that enhances spatial reasoning at the regional level.\nExtensive experiments demonstrate that our approach significantly improves\nregion-level captioning performance across various tasks\n","authors":["Yuan Sun","Zhao Zhang","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2412.10348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10342v1","updated":"2024-12-13T18:40:10Z","published":"2024-12-13T18:40:10Z","title":"Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining","summary":"  Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.\n","authors":["Zhiqi Ge","Juncheng Li","Xinglei Pang","Minghe Gao","Kaihang Pan","Wang Lin","Hao Fei","Wenqiao Zhang","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.10342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10339v1","updated":"2024-12-13T18:35:27Z","published":"2024-12-13T18:35:27Z","title":"A Universal Degradation-based Bridging Technique for Domain Adaptive\n  Semantic Segmentation","summary":"  Semantic segmentation often suffers from significant performance degradation\nwhen the trained network is applied to a different domain. To address this\nissue, unsupervised domain adaptation (UDA) has been extensively studied.\nExisting methods introduce the domain bridging techniques to mitigate\nsubstantial domain gap, which construct intermediate domains to facilitate the\ngradual transfer of knowledge across different domains. However, these\nstrategies often require dataset-specific designs and may generate unnatural\nintermediate distributions that lead to semantic shift. In this paper, we\npropose DiDA, a universal degradation-based bridging technique formalized as a\ndiffusion forward process. DiDA consists of two key modules: (1)\nDegradation-based Intermediate Domain Construction, which creates continuous\nintermediate domains through simple image degradation operations to encourage\nlearning domain-invariant features as domain differences gradually diminish;\n(2) Semantic Shift Compensation, which leverages a diffusion encoder to encode\nand compensate for semantic shift information with degraded time-steps,\npreserving discriminative representations in the intermediate domains. As a\nplug-and-play solution, DiDA supports various degradation operations and\nseamlessly integrates with existing UDA methods. Extensive experiments on\nprevalent synthetic-to-real semantic segmentation benchmarks demonstrate that\nDiDA consistently improves performance across different settings and achieves\nnew state-of-the-art results when combined with existing methods.\n","authors":["Wangkai Li","Rui Sun","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10338v1","updated":"2024-12-13T18:33:18Z","published":"2024-12-13T18:33:18Z","title":"XYScanNet: An Interpretable State Space Model for Perceptual Image\n  Deblurring","summary":"  Deep state-space models (SSMs), like recent Mamba architectures, are emerging\nas a promising alternative to CNN and Transformer networks. Existing\nMamba-based restoration methods process the visual data by leveraging a\nflatten-and-scan strategy that converts image patches into a 1D sequence before\nscanning. However, this scanning paradigm ignores local pixel dependencies and\nintroduces spatial misalignment by positioning distant pixels incorrectly\nadjacent, which reduces local noise-awareness and degrades image sharpness in\nlow-level vision tasks. To overcome these issues, we propose a novel\nslice-and-scan strategy that alternates scanning along intra- and inter-slices.\nWe further design a new Vision State Space Module (VSSM) for image deblurring,\nand tackle the inefficiency challenges of the current Mamba-based vision\nmodule. Building upon this, we develop XYScanNet, an SSM architecture\nintegrated with a lightweight feature fusion module for enhanced image\ndeblurring. XYScanNet, maintains competitive distortion metrics and\nsignificantly improves perceptual performance. Experimental results show that\nXYScanNet enhances KID by $17\\%$ compared to the nearest competitor. Our code\nwill be released soon.\n","authors":["Hanzhou Liu","Chengkai Liu","Jiacong Xu","Peng Jiang","Mi Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10316v1","updated":"2024-12-13T17:58:06Z","published":"2024-12-13T17:58:06Z","title":"BrushEdit: All-In-One Image Inpainting and Editing","summary":"  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n","authors":["Yaowei Li","Yuxuan Bian","Xuan Ju","Zhaoyang Zhang","Ying Shan","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10316v1.pdf","comment":"WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/"},{"id":"http://arxiv.org/abs/2412.10308v1","updated":"2024-12-13T17:42:53Z","published":"2024-12-13T17:42:53Z","title":"TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes","summary":"  We tackle the problem of localizing the traffic surveillance cameras in\ncooperative perception. To overcome the lack of large-scale real-world\nintersection datasets, we introduce Carla Intersection, a new simulated dataset\nwith 75 urban and rural intersections in Carla. Moreover, we introduce a novel\nneural network, TrafficLoc, localizing traffic cameras within a 3D reference\nmap. TrafficLoc employs a coarse-to-fine matching pipeline. For image-point\ncloud feature fusion, we propose a novel Geometry-guided Attention Loss to\naddress cross-modal viewpoint inconsistencies. During coarse matching, we\npropose an Inter-Intra Contrastive Learning to achieve precise alignment while\npreserving distinctiveness among local intra-features within image patch-point\ngroup pairs. Besides, we introduce Dense Training Alignment with a soft-argmax\noperator to consider additional features when regressing the final position.\nExtensive experiments show that our TrafficLoc improves the localization\naccuracy over the state-of-the-art Image-to-point cloud registration methods by\na large margin (up to 86%) on Carla Intersection and generalizes well to\nreal-world data. TrafficLoc also achieves new SOTA performance on KITTI and\nNuScenes datasets, demonstrating strong localization ability across both\nin-vehicle and traffic cameras. Our project page is publicly available at\nhttps://tum-luk.github.io/projects/trafficloc/.\n","authors":["Yan Xia","Yunxiang Lu","Rui Song","Oussema Dhaouadi","João F. Henriques","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2412.10308v1.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.10302v1","updated":"2024-12-13T17:37:48Z","published":"2024-12-13T17:37:48Z","title":"DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding","summary":"  We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.\n","authors":["Zhiyu Wu","Xiaokang Chen","Zizheng Pan","Xingchao Liu","Wen Liu","Damai Dai","Huazuo Gao","Yiyang Ma","Chengyue Wu","Bingxuan Wang","Zhenda Xie","Yu Wu","Kai Hu","Jiawei Wang","Yaofeng Sun","Yukun Li","Yishi Piao","Kang Guan","Aixin Liu","Xin Xie","Yuxiang You","Kai Dong","Xingkai Yu","Haowei Zhang","Liang Zhao","Yisong Wang","Chong Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.10302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10300v1","updated":"2024-12-13T17:35:42Z","published":"2024-12-13T17:35:42Z","title":"Iterating the Transient Light Transport Matrix for Non-Line-of-Sight\n  Imaging","summary":"  Active imaging systems sample the Transient Light Transport Matrix (TLTM) for\na scene by sequentially illuminating various positions in this scene using a\ncontrollable light source, and then measuring the resulting spatiotemporal\nlight transport with time of flight (ToF) sensors. Time-resolved\nNon-line-of-sight (NLOS) imaging employs an active imaging system that measures\npart of the TLTM of an intermediary relay surface, and uses the indirect\nreflections of light encoded within this TLTM to \"see around corners\". Such\nimaging systems have applications in diverse areas such as disaster response,\nremote surveillance, and autonomous navigation. While existing NLOS imaging\nsystems usually measure a subset of the full TLTM, development of customized\ngated Single Photon Avalanche Diode (SPAD) arrays\n\\cite{riccardo_fast-gated_2022} has made it feasible to probe the full\nmeasurement space. In this work, we demonstrate that the full TLTM on the relay\nsurface can be processed with efficient algorithms to computationally focus and\ndetect our illumination in different parts of the hidden scene, turning the\nrelay surface into a second-order active imaging system. These algorithms allow\nus to iterate on the measured, first-order TLTM, and extract a \\textbf{second\norder TLTM for surfaces in the hidden scene}. We showcase three applications of\nTLTMs in NLOS imaging: (1) Scene Relighting with novel illumination, (2)\nSeparation of direct and indirect components of light transport in the hidden\nscene, and (3) Dual Photography. Additionally, we empirically demonstrate that\nSPAD arrays enable parallel acquisition of photons, effectively mitigating long\nacquisition times.\n","authors":["Talha Sultan","Eric Brandt","Khadijeh Masumnia-Bisheh","Simone Riccardo","Pavel Polynkin","Alberto Tosi","Andreas Velten"],"pdf_url":"https://arxiv.org/pdf/2412.10300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10294v1","updated":"2024-12-13T17:26:45Z","published":"2024-12-13T17:26:45Z","title":"Coherent 3D Scene Diffusion From a Single RGB Image","summary":"  We present a novel diffusion-based approach for coherent 3D scene\nreconstruction from a single RGB image. Our method utilizes an\nimage-conditioned 3D scene diffusion model to simultaneously denoise the 3D\nposes and geometries of all objects within the scene. Motivated by the\nill-posed nature of the task and to obtain consistent scene reconstruction\nresults, we learn a generative scene prior by conditioning on all scene objects\nsimultaneously to capture the scene context and by allowing the model to learn\ninter-object relationships throughout the diffusion process. We further propose\nan efficient surface alignment loss to facilitate training even in the absence\nof full ground-truth annotation, which is common in publicly available\ndatasets. This loss leverages an expressive shape representation, which enables\ndirect point sampling from intermediate shape predictions. By framing the task\nof single RGB image 3D scene reconstruction as a conditional diffusion process,\nour approach surpasses current state-of-the-art methods, achieving a 12.04%\nimprovement in AP3D on SUN RGB-D and a 13.43% increase in F-Score on Pix3D.\n","authors":["Manuel Dahnert","Angela Dai","Norman Müller","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2412.10294v1.pdf","comment":"Project Page: https://www.manuel-dahnert.com/research/scene-diffusion\n  - Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.10292v1","updated":"2024-12-13T17:22:50Z","published":"2024-12-13T17:22:50Z","title":"Prompt-Guided Mask Proposal for Two-Stage Open-Vocabulary Segmentation","summary":"  We tackle the challenge of open-vocabulary segmentation, where we need to\nidentify objects from a wide range of categories in different environments,\nusing text prompts as our input. To overcome this challenge, existing methods\noften use multi-modal models like CLIP, which combine image and text features\nin a shared embedding space to bridge the gap between limited and extensive\nvocabulary recognition, resulting in a two-stage approach: In the first stage,\na mask generator takes an input image to generate mask proposals, and the in\nthe second stage the target mask is picked based on the query. However, the\nexpected target mask may not exist in the generated mask proposals, which leads\nto an unexpected output mask. In our work, we propose a novel approach named\nPrompt-guided Mask Proposal (PMP) where the mask generator takes the input text\nprompts and generates masks guided by these prompts. Compared with mask\nproposals generated without input prompts, masks generated by PMP are better\naligned with the input prompts. To realize PMP, we designed a cross-attention\nmechanism between text tokens and query tokens which is capable of generating\nprompt-guided mask proposals after each decoding. We combined our PMP with\nseveral existing works employing a query-based segmentation backbone and the\nexperiments on five benchmark datasets demonstrate the effectiveness of this\napproach, showcasing significant improvements over the current two-stage models\n(1% ~ 3% absolute performance gain in terms of mIOU). The steady improvement in\nperformance across these benchmarks indicates the effective generalization of\nour proposed lightweight prompt-aware method.\n","authors":["Yu-Jhe Li","Xinyang Zhang","Kun Wan","Lantao Yu","Ajinkya Kale","Xin Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10292v1.pdf","comment":"17 pages. Work done during 2023 summer and has been released"},{"id":"http://arxiv.org/abs/2412.08582v2","updated":"2024-12-13T17:11:38Z","published":"2024-12-11T17:57:25Z","title":"Utilizing Multi-step Loss for Single Image Reflection Removal","summary":"  Image reflection removal is crucial for restoring image quality. Distorted\nimages can negatively impact tasks like object detection and image\nsegmentation. In this paper, we present a novel approach for image reflection\nremoval using a single image. Instead of focusing on model architecture, we\nintroduce a new training technique that can be generalized to image-to-image\nproblems, with input and output being similar in nature. This technique is\nembodied in our multi-step loss mechanism, which has proven effective in the\nreflection removal task. Additionally, we address the scarcity of reflection\nremoval training data by synthesizing a high-quality, non-linear synthetic\ndataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances\nthe model's ability to learn better patterns for reflection removal. We also\nutilize a ranged depth map, extracted from the depth estimation of the ambient\nimage, as an auxiliary feature, leveraging its property of lacking depth\nestimations for reflections. Our approach demonstrates superior performance on\nthe SIR^2 benchmark and other real-world datasets, proving its effectiveness by\noutperforming other state-of-the-art models.\n","authors":["Abdelrahman Elnenaey","Marwan Torki"],"pdf_url":"https://arxiv.org/pdf/2412.08582v2.pdf","comment":"6 pages, 6 figures, IEEE AICCSA 2024"},{"id":"http://arxiv.org/abs/2412.04279v2","updated":"2024-12-13T16:59:08Z","published":"2024-12-05T16:00:55Z","title":"Targeted Hard Sample Synthesis Based on Estimated Pose and Occlusion\n  Error for Improved Object Pose Estimation","summary":"  6D Object pose estimation is a fundamental component in robotics enabling\nefficient interaction with the environment. It is particularly challenging in\nbin-picking applications, where objects may be textureless and in difficult\nposes, and occlusion between objects of the same type may cause confusion even\nin well-trained models. We propose a novel method of hard example synthesis\nthat is model-agnostic, using existing simulators and the modeling of pose\nerror in both the camera-to-object viewsphere and occlusion space. Through\nevaluation of the model performance with respect to the distribution of object\nposes and occlusions, we discover regions of high error and generate realistic\ntraining samples to specifically target these regions. With our training\napproach, we demonstrate an improvement in correct detection rate of up to 20%\nacross several ROBI-dataset objects using state-of-the-art pose estimation\nmodels.\n","authors":["Alan Li","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2412.04279v2.pdf","comment":"To be published in IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2410.01697v3","updated":"2024-12-13T16:55:33Z","published":"2024-10-02T16:05:03Z","title":"MOREL: Enhancing Adversarial Robustness through Multi-Objective\n  Representation Learning","summary":"  Extensive research has shown that deep neural networks (DNNs) are vulnerable\nto slight adversarial perturbations$-$small changes to the input data that\nappear insignificant but cause the model to produce drastically different\noutputs. In addition to augmenting training data with adversarial examples\ngenerated from a specific attack method, most of the current defense strategies\nnecessitate modifying the original model architecture components to improve\nrobustness or performing test-time data purification to handle adversarial\nattacks. In this work, we demonstrate that strong feature representation\nlearning during training can significantly enhance the original model's\nrobustness. We propose MOREL, a multi-objective feature representation learning\napproach, encouraging classification models to produce similar features for\ninputs within the same class, despite perturbations. Our training method\ninvolves an embedding space where cosine similarity loss and multi-positive\ncontrastive loss are used to align natural and adversarial features from the\nmodel encoder and ensure tight clustering. Concurrently, the classifier is\nmotivated to achieve accurate predictions. Through extensive experiments, we\ndemonstrate that our approach significantly enhances the robustness of DNNs\nagainst white-box and black-box adversarial attacks, outperforming other\nmethods that similarly require no architectural changes or test-time data\npurification. Our code is available at https://github.com/salomonhotegni/MOREL\n","authors":["Sedjro Salomon Hotegni","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2410.01697v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10275v1","updated":"2024-12-13T16:52:13Z","published":"2024-12-13T16:52:13Z","title":"TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to\n  Video Generation","summary":"  Text-driven Image to Video Generation (TI2V) aims to generate controllable\nvideo given the first frame and corresponding textual description. The primary\nchallenges of this task lie in two parts: (i) how to identify the target\nobjects and ensure the consistency between the movement trajectory and the\ntextual description. (ii) how to improve the subjective quality of generated\nvideos. To tackle the above challenges, we propose a new diffusion-based TI2V\nframework, termed TIV-Diffusion, via object-centric textual-visual alignment,\nintending to achieve precise control and high-quality video generation based on\ntextual-described motion for different objects. Concretely, we enable our\nTIV-Diffuion model to perceive the textual-described objects and their motion\ntrajectory by incorporating the fused textual and visual knowledge through\nscale-offset modulation. Moreover, to mitigate the problems of object\ndisappearance and misaligned objects and motion, we introduce an object-centric\ntextual-visual alignment module, which reduces the risk of misaligned\nobjects/motion by decoupling the objects in the reference image and aligning\ntextual features with each object individually. Based on the above innovations,\nour TIV-Diffusion achieves state-of-the-art high-quality video generation\ncompared with existing TI2V methods.\n","authors":["Xingrui Wang","Xin Li","Yaosi Hu","Hanxin Zhu","Chen Hou","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10273v1","updated":"2024-12-13T16:46:46Z","published":"2024-12-13T16:46:46Z","title":"Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry","summary":"  We introduce a hierarchical probabilistic approach to go from a 2D image to\nmultiview 3D: a diffusion \"prior\" models the unseen 3D geometry, which then\nconditions a diffusion \"decoder\" to generate novel views of the subject. We use\na pointmap-based geometric representation in a multiview image format to\ncoordinate the generation of multiple target views simultaneously. We\nfacilitate correspondence between views by assuming fixed target camera poses\nrelative to the source camera, and constructing a predictable distribution of\ngeometric features per target. Our modular, geometry-driven approach to\nnovel-view synthesis (called \"unPIC\") beats SoTA baselines such as CAT3D and\nOne-2-3-45 on held-out objects from ObjaverseXL, as well as real-world objects\nranging from Google Scanned Objects, Amazon Berkeley Objects, to the Digital\nTwin Catalog.\n","authors":["Rishabh Kabra","Drew A. Hudson","Sjoerd van Steenkiste","Joao Carreira","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.10273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09199v2","updated":"2024-12-13T16:44:42Z","published":"2024-12-12T11:49:18Z","title":"MVC-VPR: Mutual Learning of Viewpoint Classification and Visual Place\n  Recognition","summary":"  Visual Place Recognition (VPR) aims to robustly identify locations by\nleveraging image retrieval based on descriptors encoded from environmental\nimages. However, drastic appearance changes of images captured from different\nviewpoints at the same location pose incoherent supervision signals for\ndescriptor learning, which severely hinder the performance of VPR. Previous\nwork proposes classifying images based on manually defined rules or ground\ntruth labels for viewpoints, followed by descriptor training based on the\nclassification results. However, not all datasets have ground truth labels of\nviewpoints and manually defined rules may be suboptimal, leading to degraded\ndescriptor performance.To address these challenges, we introduce the mutual\nlearning of viewpoint self-classification and VPR. Starting from coarse\nclassification based on geographical coordinates, we progress to finer\nclassification of viewpoints using simple clustering techniques. The dataset is\npartitioned in an unsupervised manner while simultaneously training a\ndescriptor extractor for place recognition. Experimental results show that this\napproach almost perfectly partitions the dataset based on viewpoints, thus\nachieving mutually reinforcing effects. Our method even excels state-of-the-art\n(SOTA) methods that partition datasets using ground truth labels.\n","authors":["Qiwen Gu","Xufei Wang","Fenglin Zhang","Junqiao Zhao","Siyue Tao","Chen Ye","Tiantian Feng","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.09199v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.10261v1","updated":"2024-12-13T16:30:35Z","published":"2024-12-13T16:30:35Z","title":"MVQ:Towards Efficient DNN Compression and Acceleration with Masked\n  Vector Quantization","summary":"  Vector quantization(VQ) is a hardware-friendly DNN compression method that\ncan reduce the storage cost and weight-loading datawidth of hardware\naccelerators. However, conventional VQ techniques lead to significant accuracy\nloss because the important weights are not well preserved. To tackle this\nproblem, a novel approach called MVQ is proposed, which aims at better\napproximating important weights with a limited number of codewords. At the\nalgorithm level, our approach removes the less important weights through N:M\npruning and then minimizes the vector clustering error between the remaining\nweights and codewords by the masked k-means algorithm. Only distances between\nthe unpruned weights and the codewords are computed, which are then used to\nupdate the codewords. At the architecture level, our accelerator implements\nvector quantization on an EWS (Enhanced weight stationary) CNN accelerator and\nproposes a sparse systolic array design to maximize the benefits brought by\nmasked vector quantization.\\\\ Our algorithm is validated on various models for\nimage classification, object detection, and segmentation tasks. Experimental\nresults demonstrate that MVQ not only outperforms conventional vector\nquantization methods at comparable compression ratios but also reduces FLOPs.\nUnder ASIC evaluation, our MVQ accelerator boosts energy efficiency by\n2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared\nwith the base EWS accelerator. Compared to the previous sparse accelerators,\nMVQ achieves 1.73$\\times$ higher energy efficiency.\n","authors":["Shuaiting Li","Chengxuan Wang","Juncan Deng","Zeyu Wang","Zewen Ye","Zongsheng Wang","Haibin Shen","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.10261v1.pdf","comment":"Accepted by ASPLOS '25"},{"id":"http://arxiv.org/abs/2412.10258v1","updated":"2024-12-13T16:29:00Z","published":"2024-12-13T16:29:00Z","title":"Copy-Move Detection in Optical Microscopy: A Segmentation Network and A\n  Dataset","summary":"  With increasing revelations of academic fraud, detecting forged experimental\nimages in the biomedical field has become a public concern. The challenge lies\nin the fact that copy-move targets can include background tissue, small\nforeground objects, or both, which may be out of the training domain and\nsubject to unseen attacks, rendering standard object-detection-based approaches\nless effective. To address this, we reformulate the problem of detecting\nbiomedical copy-move forgery regions as an intra-image co-saliency detection\ntask and propose CMSeg-Net, a copy-move forgery segmentation network capable of\nidentifying unseen duplicated areas. Built on a multi-resolution\nencoder-decoder architecture, CMSeg-Net incorporates self-correlation and\ncorrelation-assisted spatial-attention modules to detect intra-image regional\nsimilarities within feature tensors at each observation scale. This design\nhelps distinguish even small copy-move targets in complex microscopic images\nfrom other similar objects. Furthermore, we created a copy-move forgery dataset\nof optical microscopic images, named FakeParaEgg, using open data from the ICIP\n2022 Challenge to support CMSeg-Net's development and verify its performance.\nExtensive experiments demonstrate that our approach outperforms previous\nstate-of-the-art methods on the FakeParaEgg dataset and other open copy-move\ndetection datasets, including CASIA-CMFD, CoMoFoD, and CMF. The FakeParaEgg\ndataset, our source code, and the CMF dataset with our manually defined\nsegmentation ground truths available at\n``https://github.com/YoursEver/FakeParaEgg''.\n","authors":["Hao-Chiang Shao","Yuan-Rong Liao","Tse-Yu Tseng","Yen-Liang Chuo","Fong-Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2412.10258v1.pdf","comment":"submitted to IEEE SPL"},{"id":"http://arxiv.org/abs/2412.10235v1","updated":"2024-12-13T16:06:46Z","published":"2024-12-13T16:06:46Z","title":"EnvPoser: Environment-aware Realistic Human Motion Estimation from\n  Sparse Observations with Uncertainty Modeling","summary":"  Estimating full-body motion using the tracking signals of head and hands from\nVR devices holds great potential for various applications. However, the\nsparsity and unique distribution of observations present a significant\nchallenge, resulting in an ill-posed problem with multiple feasible solutions\n(i.e., hypotheses). This amplifies uncertainty and ambiguity in full-body\nmotion estimation, especially for the lower-body joints. Therefore, we propose\na new method, EnvPoser, that employs a two-stage framework to perform full-body\nmotion estimation using sparse tracking signals and pre-scanned environment\nfrom VR devices. EnvPoser models the multi-hypothesis nature of human motion\nthrough an uncertainty-aware estimation module in the first stage. In the\nsecond stage, we refine these multi-hypothesis estimates by integrating\nsemantic and geometric environmental constraints, ensuring that the final\nmotion estimation aligns realistically with both the environmental context and\nphysical interactions. Qualitative and quantitative experiments on two public\ndatasets demonstrate that our method achieves state-of-the-art performance,\nhighlighting significant improvements in human motion estimation within\nmotion-environment interaction scenarios.\n","authors":["Songpengcheng Xia","Yu Zhang","Zhuo Su","Xiaozheng Zheng","Zheng Lv","Guidong Wang","Yongjie Zhang","Qi Wu","Lei Chu","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2412.10235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10231v1","updated":"2024-12-13T16:01:19Z","published":"2024-12-13T16:01:19Z","title":"SuperGSeg: Open-Vocabulary 3D Segmentation with Structured\n  Super-Gaussians","summary":"  3D Gaussian Splatting has recently gained traction for its efficient training\nand real-time rendering. While the vanilla Gaussian Splatting representation is\nmainly designed for view synthesis, more recent works investigated how to\nextend it with scene understanding and language features. However, existing\nmethods lack a detailed comprehension of scenes, limiting their ability to\nsegment and interpret complex structures. To this end, We introduce SuperGSeg,\na novel approach that fosters cohesive, context-aware scene representation by\ndisentangling segmentation and language field distillation. SuperGSeg first\nemploys neural Gaussians to learn instance and hierarchical segmentation\nfeatures from multi-view images with the aid of off-the-shelf 2D masks. These\nfeatures are then leveraged to create a sparse set of what we call\nSuper-Gaussians. Super-Gaussians facilitate the distillation of 2D language\nfeatures into 3D space. Through Super-Gaussians, our method enables\nhigh-dimensional language feature rendering without extreme increases in GPU\nmemory. Extensive experiments demonstrate that SuperGSeg outperforms prior\nworks on both open-vocabulary object localization and semantic segmentation\ntasks.\n","authors":["Siyun Liang","Sen Wang","Kunyi Li","Michael Niemeyer","Stefano Gasperini","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2412.10231v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.10224v1","updated":"2024-12-13T15:49:18Z","published":"2024-12-13T15:49:18Z","title":"SPT: Sequence Prompt Transformer for Interactive Image Segmentation","summary":"  Interactive segmentation aims to extract objects of interest from an image\nbased on user-provided clicks. In real-world applications, there is often a\nneed to segment a series of images featuring the same target object. However,\nexisting methods typically process one image at a time, failing to consider the\nsequential nature of the images. To overcome this limitation, we propose a\nnovel method called Sequence Prompt Transformer (SPT), the first to utilize\nsequential image information for interactive segmentation. Our model comprises\ntwo key components: (1) Sequence Prompt Transformer (SPT) for acquiring\ninformation from sequence of images, clicks and masks to improve accurate. (2)\nTop-k Prompt Selection (TPS) selects precise prompts for SPT to further enhance\nthe segmentation effect. Additionally, we create the ADE20K-Seq benchmark to\nbetter evaluate model performance. We evaluate our approach on multiple\nbenchmark datasets and show that our model surpasses state-of-the-art methods\nacross all datasets.\n","authors":["Senlin Cheng","Haopeng Sun"],"pdf_url":"https://arxiv.org/pdf/2412.10224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08490v3","updated":"2024-12-13T15:49:03Z","published":"2024-10-11T03:31:40Z","title":"CAS-GAN for Contrast-free Angiography Synthesis","summary":"  Iodinated contrast agents are widely utilized in numerous interventional\nprocedures, yet posing substantial health risks to patients. This paper\npresents CAS-GAN, a novel GAN framework that serves as a \"virtual contrast\nagent\" to synthesize X-ray angiographies via disentanglement representation\nlearning and vessel semantic guidance, thereby reducing the reliance on\niodinated contrast agents during interventional procedures. Specifically, our\napproach disentangles X-ray angiographies into background and vessel\ncomponents, leveraging medical prior knowledge. A specialized predictor then\nlearns to map the interrelationships between these components. Additionally, a\nvessel semantic-guided generator and a corresponding loss function are\nintroduced to enhance the visual fidelity of generated images. Experimental\nresults on the XCAD dataset demonstrate the state-of-the-art performance of our\nCAS-GAN, achieving a FID of 5.87 and a MMD of 0.016. These promising results\nhighlight CAS-GAN's potential for clinical applications.\n","authors":["De-Xing Huang","Xiao-Hu Zhou","Mei-Jiang Gui","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Hao Li","Tian-Yu Xiang","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2410.08490v3.pdf","comment":"IEEE Symposium Series on Computational Intelligence (SSCI 2025)"},{"id":"http://arxiv.org/abs/2412.10219v1","updated":"2024-12-13T15:41:08Z","published":"2024-12-13T15:41:08Z","title":"Learning Complex Non-Rigid Image Edits from Multimodal Conditioning","summary":"  In this paper we focus on inserting a given human (specifically, a single\nimage of a person) into a novel scene. Our method, which builds on top of\nStable Diffusion, yields natural looking images while being highly controllable\nwith text and pose. To accomplish this we need to train on pairs of images, the\nfirst a reference image with the person, the second a \"target image\" showing\nthe same person (with a different pose and possibly in a different background).\nAdditionally we require a text caption describing the new pose relative to that\nin the reference image. In this paper we present a novel dataset following this\ncriteria, which we create using pairs of frames from human-centric and\naction-rich videos and employing a multimodal LLM to automatically summarize\nthe difference in human pose for the text captions. We demonstrate that\nidentity preservation is a more challenging task in scenes \"in-the-wild\", and\nespecially scenes where there is an interaction between persons and objects.\nCombining the weak supervision from noisy captions, with robust 2D pose\nimproves the quality of person-object interactions.\n","authors":["Nikolai Warner","Jack Kolb","Meera Hahn","Vighnesh Birodkar","Jonathan Huang","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2412.10219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07889v2","updated":"2024-12-13T15:39:27Z","published":"2024-12-10T19:48:57Z","title":"Low-Latency Scalable Streaming for Event-Based Vision","summary":"  Recently, we have witnessed the rise of novel ``event-based'' camera sensors\nfor high-speed, low-power video capture. Rather than recording discrete image\nframes, these sensors output asynchronous ``event'' tuples with microsecond\nprecision, only when the brightness change of a given pixel exceeds a certain\nthreshold. Although these sensors have enabled compelling new computer vision\napplications, these applications often require expensive, power-hungry GPU\nsystems, rendering them incompatible for deployment on the low-power devices\nfor which event cameras are optimized. Whereas receiver-driven rate adaptation\nis a crucial feature of modern video streaming solutions, this topic is\nunderexplored in the realm of event-based vision systems. On a real-world event\ncamera dataset, we first demonstrate that a state-of-the-art object detection\napplication is resilient to dramatic data loss, and that this loss may be\nweighted towards the end of each temporal window. We then propose a scalable\nstreaming method for event-based data based on Media Over QUIC, prioritizing\nobject detection performance and low latency. The application server can\nreceive complementary event data across several streams simultaneously, and\ndrop streams as needed to maintain a certain latency. With a latency target of\n5 ms for end-to-end transmission across a small network, we observe an average\nreduction in detection mAP as low as 0.36. With a more relaxed latency target\nof 50 ms, we observe an average mAP reduction as low as 0.19.\n","authors":["Andrew Hamara","Benjamin Kilpatrick","Alex Baratta","Brendon Kofink","Andrew C. Freeman"],"pdf_url":"https://arxiv.org/pdf/2412.07889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10211v1","updated":"2024-12-13T15:34:34Z","published":"2024-12-13T15:34:34Z","title":"RAID-Database: human Responses to Affine Image Distortions","summary":"  Image quality databases are used to train models for predicting subjective\nhuman perception. However, most existing databases focus on distortions\ncommonly found in digital media and not in natural conditions. Affine\ntransformations are particularly relevant to study, as they are among the most\ncommonly encountered by human observers in everyday life. This Data Descriptor\npresents a set of human responses to suprathreshold affine image transforms\n(rotation, translation, scaling) and Gaussian noise as convenient reference to\ncompare with previously existing image quality databases. The responses were\nmeasured using well established psychophysics: the Maximum Likelihood\nDifference Scaling method. The set contains responses to 864 distorted images.\nThe experiments involved 105 observers and more than 20000 comparisons of\nquadruples of images. The quality of the dataset is ensured because (a) it\nreproduces the classical Pi\\'eron's law, (b) it reproduces classical absolute\ndetection thresholds, and (c) it is consistent with conventional image quality\ndatabases but improves them according to Group-MAD experiments.\n","authors":["Paula Daudén-Oliver","David Agost-Beltran","Emilio Sansano-Sansano","Valero Laparra","Jesús Malo","Marina Martínez-Garcia"],"pdf_url":"https://arxiv.org/pdf/2412.10211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10209v1","updated":"2024-12-13T15:31:22Z","published":"2024-12-13T15:31:22Z","title":"GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view\n  Diffusion","summary":"  We propose a novel approach for reconstructing animatable 3D Gaussian avatars\nfrom monocular videos captured by commodity devices like smartphones.\nPhotorealistic 3D head avatar reconstruction from such recordings is\nchallenging due to limited observations, which leaves unobserved regions\nunder-constrained and can lead to artifacts in novel views. To address this\nproblem, we introduce a multi-view head diffusion model, leveraging its priors\nto fill in missing regions and ensure view consistency in Gaussian splatting\nrenderings. To enable precise viewpoint control, we use normal maps rendered\nfrom FLAME-based head reconstruction, which provides pixel-aligned inductive\nbiases. We also condition the diffusion model on VAE features extracted from\nthe input image to preserve details of facial identity and appearance. For\nGaussian avatar reconstruction, we distill multi-view diffusion priors by using\niteratively denoised images as pseudo-ground truths, effectively mitigating\nover-saturation issues. To further improve photorealism, we apply latent\nupsampling to refine the denoised latent before decoding it into an image. We\nevaluate our method on the NeRSemble dataset, showing that GAF outperforms the\nprevious state-of-the-art methods in novel view synthesis by a 5.34\\% higher\nSSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions\nfrom monocular videos captured on commodity devices.\n","authors":["Jiapeng Tang","Davide Davoli","Tobias Kirschstein","Liam Schoneveld","Matthias Niessner"],"pdf_url":"https://arxiv.org/pdf/2412.10209v1.pdf","comment":"Paper Video: https://youtu.be/QuIYTljvhyg Project Page:\n  https://tangjiapeng.github.io/projects/GAF"},{"id":"http://arxiv.org/abs/2405.08909v2","updated":"2024-12-13T15:22:55Z","published":"2024-05-14T19:02:33Z","title":"ADA-Track++: End-to-End Multi-Camera 3D Multi-Object Tracking with\n  Alternating Detection and Association","summary":"  Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the\ntracking-by-attention paradigm, utilizing track queries for identity-consistent\ndetection and object queries for identity-agnostic track spawning.\nTracking-by-attention, however, entangles detection and tracking queries in one\nembedding for both the detection and tracking task, which is sub-optimal. Other\napproaches resemble the tracking-by-detection paradigm and detect objects using\ndecoupled track and detection queries followed by a subsequent association.\nThese methods, however, do not leverage synergies between the detection and\nassociation task. Combining the strengths of both paradigms, we introduce\nADA-Track++, a novel end-to-end framework for 3D MOT from multi-view cameras.\nWe introduce a learnable data association module based on edge-augmented\ncross-attention, leveraging appearance and geometric features. We also propose\nan auxiliary token in this attention-based association module, which helps\nmitigate disproportionately high attention to incorrect association targets\ncaused by attention normalization. Furthermore, we integrate this association\nmodule into the decoder layer of a DETR-based 3D detector, enabling\nsimultaneous DETR-like query-to-image cross-attention for detection and\nquery-to-query cross-attention for data association. By stacking these decoder\nlayers, queries are refined for the detection and association task alternately,\neffectively harnessing the task dependencies. We evaluate our method on the\nnuScenes dataset and demonstrate the advantage of our approach compared to the\ntwo previous paradigms.\n","authors":["Shuxiao Ding","Lukas Schneider","Marius Cordts","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2405.08909v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2305.05349v2","updated":"2024-12-13T15:17:28Z","published":"2023-05-09T11:20:11Z","title":"Towards the Characterization of Representations Learned via\n  Capsule-based Network Architectures","summary":"  Capsule Networks (CapsNets) have been re-introduced as a more compact and\ninterpretable alternative to standard deep neural networks. While recent\nefforts have proved their compression capabilities, to date, their\ninterpretability properties have not been fully assessed. Here, we conduct a\nsystematic and principled study towards assessing the interpretability of these\ntypes of networks. Moreover, we pay special attention towards analyzing the\nlevel to which part-whole relationships are indeed encoded within the learned\nrepresentation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebA\ndatasets suggest that the representations encoded in CapsNets might not be as\ndisentangled nor strictly related to parts-whole relationships as is commonly\nstated in the literature.\n","authors":["Saja Tawalbeh","José Oramas"],"pdf_url":"https://arxiv.org/pdf/2305.05349v2.pdf","comment":"This paper consist of 32 pages including 19 figures. This paper\n  concern about interpretation of capsule networks"},{"id":"http://arxiv.org/abs/2412.10184v1","updated":"2024-12-13T14:55:24Z","published":"2024-12-13T14:55:24Z","title":"Sims: An Interactive Tool for Geospatial Matching and Clustering","summary":"  Acquiring, processing, and visualizing geospatial data requires significant\ncomputing resources, especially for large spatio-temporal domains. This\nchallenge hinders the rapid discovery of predictive features, which is\nessential for advancing geospatial modeling. To address this, we developed\nSimilarity Search (Sims), a no-code web tool that allows users to visualize,\ncompare, cluster, and perform similarity search over defined regions of\ninterest using Google Earth Engine as a backend. Sims is designed to complement\nexisting modeling tools by focusing on feature exploration rather than model\ncreation. We demonstrate the utility of Sims through a case study analyzing\nsimulated maize yield data in Rwanda, where we evaluate how different\ncombinations of soil, weather, and agronomic features affect the clustering of\nyield response zones. Sims is open source and available at\nhttps://github.com/microsoft/Sims\n","authors":["Akram Zaytar","Girmaw Abebe Tadesse","Caleb Robinson","Eduardo G. Bendito","Medha Devare","Meklit Chernet","Gilles Q. Hacheme","Rahul Dodhia","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2412.10184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10182v1","updated":"2024-12-13T14:53:47Z","published":"2024-12-13T14:53:47Z","title":"Multi-Head Encoding for Extreme Label Classification","summary":"  The number of categories of instances in the real world is normally huge, and\neach instance may contain multiple labels. To distinguish these massive labels\nutilizing machine learning, eXtreme Label Classification (XLC) has been\nestablished. However, as the number of categories increases, the number of\nparameters and nonlinear operations in the classifier also rises. This results\nin a Classifier Computational Overload Problem (CCOP). To address this, we\npropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla\nclassifier with a multi-head classifier. During the training process, MHE\ndecomposes extreme labels into the product of multiple short local labels, with\neach head trained on these local labels. During testing, the predicted labels\ncan be directly calculated from the local predictions of each head. This\nreduces the computational load geometrically. Then, according to the\ncharacteristics of different XLC tasks, e.g., single-label, multi-label, and\nmodel pretraining tasks, three MHE-based implementations, i.e., Multi-Head\nProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more\neffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can\nachieve performance approximately equivalent to that of the vanilla classifier\nby generalizing the low-rank approximation problem from Frobenius-norm to\nCross-Entropy. Experimental results show that the proposed methods achieve\nstate-of-the-art performance while significantly streamlining the training and\ninference processes of XLC tasks. The source code has been made public at\nhttps://github.com/Anoise/MHE.\n","authors":["Daojun Liang","Haixia Zhang","Dongfeng Yuan","Minggao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10182v1.pdf","comment":"20 pages, 12 figs, Published in TPAMI"},{"id":"http://arxiv.org/abs/2412.10181v1","updated":"2024-12-13T14:53:07Z","published":"2024-12-13T14:53:07Z","title":"Ultra-High Resolution Segmentation via Boundary-Enhanced Patch-Merging\n  Transformer","summary":"  Segmentation of ultra-high resolution (UHR) images is a critical task with\nnumerous applications, yet it poses significant challenges due to high spatial\nresolution and rich fine details. Recent approaches adopt a dual-branch\narchitecture, where a global branch learns long-range contextual information\nand a local branch captures fine details. However, they struggle to handle the\nconflict between global and local information while adding significant extra\ncomputational cost. Inspired by the human visual system's ability to rapidly\norient attention to important areas with fine details and filter out irrelevant\ninformation, we propose a novel UHR segmentation method called\nBoundary-enhanced Patch-merging Transformer (BPT). BPT consists of two key\ncomponents: (1) Patch-Merging Transformer (PMT) for dynamically allocating\ntokens to informative regions to acquire global and local representations, and\n(2) Boundary-Enhanced Module (BEM) that leverages boundary information to\nenrich fine details. Extensive experiments on multiple UHR image segmentation\nbenchmarks demonstrate that our BPT outperforms previous state-of-the-art\nmethods without introducing extra computational overhead. Codes will be\nreleased to facilitate research.\n","authors":["Haopeng Sun"],"pdf_url":"https://arxiv.org/pdf/2412.10181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10178v1","updated":"2024-12-13T14:50:26Z","published":"2024-12-13T14:50:26Z","title":"SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models","summary":"  Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. While significant\nadvances have been made in image-based virtual try-ons, extending these\nsuccesses to video often results in frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequence. To address these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we introduce ShiftCaching, a novel technique\nthat maintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the \\dataname~dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments show\nthat our approach outperforms current baselines, particularly in terms of video\nconsistency and inference speed. Data and code are available at\nhttps://github.com/VinAIResearch/swift-try\n","authors":["Hung Nguyen","Quang Qui-Vinh Nguyen","Khoi Nguyen","Rang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2412.10178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10176v1","updated":"2024-12-13T14:45:11Z","published":"2024-12-13T14:45:11Z","title":"UN-DETR: Promoting Objectness Learning via Joint Supervision for Unknown\n  Object Detection","summary":"  Unknown Object Detection (UOD) aims to identify objects of unseen categories,\ndiffering from the traditional detection paradigm limited by the closed-world\nassumption. A key component of UOD is learning a generalized representation,\ni.e. objectness for both known and unknown categories to distinguish and\nlocalize objects from the background in a class-agnostic manner. However,\nprevious methods obtain supervision signals for learning objectness in\nisolation from either localization or classification information, leading to\npoor performance for UOD. To address this issue, we propose a transformer-based\nUOD framework, UN-DETR. Based on this, we craft Instance Presence Score (IPS)\nto represent the probability of an object's presence. For the purpose of\ninformation complementarity, IPS employs a strategy of joint supervised\nlearning, integrating attributes representing general objectness from the\npositional and the categorical latent space as supervision signals. To enhance\nIPS learning, we introduce a one-to-many assignment strategy to incorporate\nmore supervision. Then, we propose Unbiased Query Selection to provide premium\ninitial query vectors for the decoder. Additionally, we propose an IPS-guided\npost-process strategy to filter redundant boxes and correct classification\npredictions for known and unknown objects. Finally, we pretrain the entire\nUN-DETR in an unsupervised manner, in order to obtain objectness prior. Our\nUN-DETR is comprehensively evaluated on multiple UOD and known detection\nbenchmarks, demonstrating its effectiveness and achieving state-of-the-art\nperformance.\n","authors":["Haomiao Liu","Hao Xu","Chuhuai Yue","Bo Ma"],"pdf_url":"https://arxiv.org/pdf/2412.10176v1.pdf","comment":"Accepted by AAAI-2025;15 pages, 11figures"},{"id":"http://arxiv.org/abs/2412.10159v1","updated":"2024-12-13T14:20:43Z","published":"2024-12-13T14:20:43Z","title":"Arbitrary Reading Order Scene Text Spotter with Local Semantics Guidance","summary":"  Scene text spotting has attracted the enthusiasm of relative researchers in\nrecent years. Most existing scene text spotters follow the\ndetection-then-recognition paradigm, where the vanilla detection module hardly\ndetermines the reading order and leads to failure recognition. After rethinking\nthe auto-regressive scene text recognition method, we find that a well-trained\nrecognizer can implicitly perceive the local semantics of all characters in a\ncomplete word or a sentence without a character-level detection module. Local\nsemantic knowledge not only includes text content but also spatial information\nin the right reading order. Motivated by the above analysis, we propose the\nLocal Semantics Guided scene text Spotter (LSGSpotter), which auto-regressively\ndecodes the position and content of characters guided by the local semantics.\nSpecifically, two effective modules are proposed in LSGSpotter. On the one\nhand, we design a Start Point Localization Module (SPLM) for locating text\nstart points to determine the right reading order. On the other hand, a\nMulti-scale Adaptive Attention Module (MAAM) is proposed to adaptively\naggregate text features in a local area. In conclusion, LSGSpotter achieves the\narbitrary reading order spotting task without the limitation of sophisticated\ndetection, while alleviating the cost of computational resources with the grid\nsampling strategy. Extensive experiment results show LSGSpotter achieves\nstate-of-the-art performance on the InverseText benchmark. Moreover, our\nspotter demonstrates superior performance on English benchmarks for\narbitrary-shaped text, achieving improvements of 0.7\\% and 2.5\\% on Total-Text\nand SCUT-CTW1500, respectively. These results validate our text spotter is\neffective for scene texts in arbitrary reading order and shape.\n","authors":["Jiahao Lyu","Wei Wang","Dongbao Yang","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.10159v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.10155v1","updated":"2024-12-13T14:12:55Z","published":"2024-12-13T14:12:55Z","title":"WordVIS: A Color Worth A Thousand Words","summary":"  Document classification is considered a critical element in automated\ndocument processing systems. In recent years multi-modal approaches have become\nincreasingly popular for document classification. Despite their improvements,\nthese approaches are underutilized in the industry due to their requirement for\na tremendous volume of training data and extensive computational power. In this\npaper, we attempt to address these issues by embedding textual features\ndirectly into the visual space, allowing lightweight image-based classifiers to\nachieve state-of-the-art results using small-scale datasets in document\nclassification. To evaluate the efficacy of the visual features generated from\nour approach on limited data, we tested on the standard dataset Tobacco-3482.\nOur experiments show a tremendous improvement in image-based classifiers,\nachieving an improvement of 4.64% using ResNet50 with no document pre-training.\nIt also sets a new record for the best accuracy of the Tobacco-3482 dataset\nwith a score of 91.14% using the image-based DocXClassifier with no document\npre-training. The simplicity of the approach, its resource requirements, and\nsubsequent results provide a good prospect for its use in industrial use cases.\n","authors":["Umar Khan"," Saifullah","Stefan Agne","Andreas Dengel","Sheraz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2412.10155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10153v1","updated":"2024-12-13T14:11:42Z","published":"2024-12-13T14:11:42Z","title":"EVOS: Efficient Implicit Neural Training via EVOlutionary Selector","summary":"  We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.\n","authors":["Weixiang Zhang","Shuzhao Xie","Chengwei Ren","Siyi Xie","Chen Tang","Shijia Ge","Mingzi Wang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03551v3","updated":"2024-12-13T14:11:35Z","published":"2024-03-06T08:51:09Z","title":"Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers","summary":"  Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.\n","authors":["Tim Selig","Thomas März","Martin Storath","Andreas Weinmann"],"pdf_url":"https://arxiv.org/pdf/2403.03551v3.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.10151v1","updated":"2024-12-13T14:11:26Z","published":"2024-12-13T14:11:26Z","title":"VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval\n  Augmented Generation","summary":"  We propose the VLR-Bench, a visual question answering (VQA) benchmark for\nevaluating vision language models (VLMs) based on retrieval augmented\ngeneration (RAG). Unlike existing evaluation datasets for external\nknowledge-based VQA, the proposed VLR-Bench includes five input passages. This\nallows testing of the ability to determine which passage is useful for\nanswering a given query, a capability lacking in previous research. In this\ncontext, we constructed a dataset of 32,000 automatically generated\ninstruction-following examples, which we denote as VLR-IF. This dataset is\nspecifically designed to enhance the RAG capabilities of VLMs by enabling them\nto learn how to generate appropriate answers based on input passages. We\nevaluated the validity of the proposed benchmark and training data and verified\nits performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3\nmodel. The proposed VLR-Bench and VLR-IF datasets are publicly available\nonline.\n","authors":["Hyeonseok Lim","Dongjae Shin","Seohyun Song","Inho Won","Minjun Kim","Junghun Yuk","Haneol Jang","KyungTae Lim"],"pdf_url":"https://arxiv.org/pdf/2412.10151v1.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025), 19 pages"},{"id":"http://arxiv.org/abs/2412.10146v1","updated":"2024-12-13T14:02:41Z","published":"2024-12-13T14:02:41Z","title":"Investigating generalization capabilities of neural networks by means of\n  loss landscapes and Hessian analysis","summary":"  This paper studies generalization capabilities of neural networks (NNs) using\nnew and improved PyTorch library Loss Landscape Analysis (LLA). LLA facilitates\nvisualization and analysis of loss landscapes along with the properties of NN\nHessian. Different approaches to NN loss landscape plotting are discussed with\nparticular focus on normalization techniques showing that conventional methods\ncannot always ensure correct visualization when batch normalization layers are\npresent in NN architecture. The use of Hessian axes is shown to be able to\nmitigate this effect, and methods for choosing Hessian axes are proposed. In\naddition, spectra of Hessian eigendecomposition are studied and it is shown\nthat typical spectra exist for a wide range of NNs. This allows to propose\nquantitative criteria for Hessian analysis that can be applied to evaluate NN\nperformance and assess its generalization capabilities. Generalization\nexperiments are conducted using ImageNet-1K pre-trained models along with\nseveral models trained as part of this study. The experiment include training\nmodels on one dataset and testing on another one to maximize experiment\nsimilarity to model performance in the Wild. It is shown that when datasets\nchange, the changes in criteria correlate with the changes in accuracy, making\nthe proposed criteria a computationally efficient estimate of generalization\nability, which is especially useful for extremely large datasets.\n","authors":["Nikita Gabdullin"],"pdf_url":"https://arxiv.org/pdf/2412.10146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07009v2","updated":"2024-12-13T13:56:20Z","published":"2024-08-13T16:15:50Z","title":"Imagen 3","summary":"  We introduce Imagen 3, a latent diffusion model that generates high quality\nimages from text prompts. We describe our quality and responsibility\nevaluations. Imagen 3 is preferred over other state-of-the-art (SOTA) models at\nthe time of evaluation. In addition, we discuss issues around safety and\nrepresentation, as well as methods we used to minimize the potential harm of\nour models.\n","authors":[" Imagen-Team-Google"," :","Jason Baldridge","Jakob Bauer","Mukul Bhutani","Nicole Brichtova","Andrew Bunner","Lluis Castrejon","Kelvin Chan","Yichang Chen","Sander Dieleman","Yuqing Du","Zach Eaton-Rosen","Hongliang Fei","Nando de Freitas","Yilin Gao","Evgeny Gladchenko","Sergio Gómez Colmenarejo","Mandy Guo","Alex Haig","Will Hawkins","Hexiang Hu","Huilian Huang","Tobenna Peter Igwe","Christos Kaplanis","Siavash Khodadadeh","Yelin Kim","Ksenia Konyushkova","Karol Langner","Eric Lau","Shixin Luo","Soňa Mokrá","Henna Nandwani","Yasumasa Onoe","Aäron van den Oord","Zarana Parekh","Jordi Pont-Tuset","Hang Qi","Rui Qian","Deepak Ramachandran","Poorva Rane","Abdullah Rashwan","Ali Razavi","Robert Riachi","Hansa Srinivasan","Srivatsan Srinivasan","Robin Strudel","Benigno Uria","Oliver Wang","Su Wang","Austin Waters","Chris Wolff","Auriel Wright","Zhisheng Xiao","Hao Xiong","Keyang Xu","Marc van Zee","Junlin Zhang","Katie Zhang","Wenlei Zhou","Konrad Zolna","Ola Aboubakar","Canfer Akbulut","Oscar Akerlund","Isabela Albuquerque","Nina Anderson","Marco Andreetto","Lora Aroyo","Ben Bariach","David Barker","Sherry Ben","Dana Berman","Courtney Biles","Irina Blok","Pankil Botadra","Jenny Brennan","Karla Brown","John Buckley","Rudy Bunel","Elie Bursztein","Christina Butterfield","Ben Caine","Viral Carpenter","Norman Casagrande","Ming-Wei Chang","Solomon Chang","Shamik Chaudhuri","Tony Chen","John Choi","Dmitry Churbanau","Nathan Clement","Matan Cohen","Forrester Cole","Mikhail Dektiarev","Vincent Du","Praneet Dutta","Tom Eccles","Ndidi Elue","Ashley Feden","Shlomi Fruchter","Frankie Garcia","Roopal Garg","Weina Ge","Ahmed Ghazy","Bryant Gipson","Andrew Goodman","Dawid Górny","Sven Gowal","Khyatti Gupta","Yoni Halpern","Yena Han","Susan Hao","Jamie Hayes","Jonathan Heek","Amir Hertz","Ed Hirst","Emiel Hoogeboom","Tingbo Hou","Heidi Howard","Mohamed Ibrahim","Dirichi Ike-Njoku","Joana Iljazi","Vlad Ionescu","William Isaac","Reena Jana","Gemma Jennings","Donovon Jenson","Xuhui Jia","Kerry Jones","Xiaoen Ju","Ivana Kajic","Christos Kaplanis","Burcu Karagol Ayan","Jacob Kelly","Suraj Kothawade","Christina Kouridi","Ira Ktena","Jolanda Kumakaw","Dana Kurniawan","Dmitry Lagun","Lily Lavitas","Jason Lee","Tao Li","Marco Liang","Maggie Li-Calis","Yuchi Liu","Javier Lopez Alberca","Peggy Lu","Kristian Lum","Yukun Ma","Chase Malik","John Mellor","Thomas Mensink","Inbar Mosseri","Tom Murray","Aida Nematzadeh","Paul Nicholas","João Gabriel Oliveira","Guillermo Ortiz-Jimenez","Michela Paganini","Tom Le Paine","Roni Paiss","Alicia Parrish","Anne Peckham","Vikas Peswani","Igor Petrovski","Tobias Pfaff","Alex Pirozhenko","Ryan Poplin","Utsav Prabhu","Yuan Qi","Matthew Rahtz","Cyrus Rashtchian","Charvi Rastogi","Amit Raul","Ali Razavi","Sylvestre-Alvise Rebuffi","Susanna Ricco","Felix Riedel","Dirk Robinson","Pankaj Rohatgi","Bill Rosgen","Sarah Rumbley","Moonkyung Ryu","Anthony Salgado","Tim Salimans","Sahil Singla","Florian Schroff","Candice Schumann","Tanmay Shah","Brendan Shillingford","Kaushik Shivakumar","Dennis Shtatnov","Zach Singer","Evgeny Sluzhaev","Valerii Sokolov","Thibault Sottiaux","Florian Stimberg","Brad Stone","David Stutz","Yu-Chuan Su","Eric Tabellion","Shuai Tang","David Tao","Kurt Thomas","Gregory Thornton","Andeep Toor","Cristian Udrescu","Aayush Upadhyay","Cristina Vasconcelos","Alex Vasiloff","Andrey Voynov","Amanda Walker","Luyu Wang","Miaosen Wang","Simon Wang","Stanley Wang","Qifei Wang","Yuxiao Wang","Ágoston Weisz","Olivia Wiles","Chenxia Wu","Xingyu Federico Xu","Andrew Xue","Jianbo Yang","Luo Yu","Mete Yurtoglu","Ali Zand","Han Zhang","Jiageng Zhang","Catherine Zhao","Adilet Zhaxybay","Miao Zhou","Shengqi Zhu","Zhenkai Zhu","Dawn Bloxwich","Mahyar Bordbar","Luis C. Cobo","Eli Collins","Shengyang Dai","Tulsee Doshi","Anca Dragan","Douglas Eck","Demis Hassabis","Sissie Hsiao","Tom Hume","Koray Kavukcuoglu","Helen King","Jack Krawczyk","Yeqing Li","Kathy Meier-Hellstern","Andras Orban","Yury Pinsky","Amar Subramanya","Oriol Vinyals","Ting Yu","Yori Zwols"],"pdf_url":"https://arxiv.org/pdf/2408.07009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10137v1","updated":"2024-12-13T13:38:41Z","published":"2024-12-13T13:38:41Z","title":"Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments","summary":"  We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions.\n","authors":["Kehan Chen","Dong An","Yan Huang","Rongtao Xu","Yifei Su","Yonggen Ling","Ian Reid","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10122v1","updated":"2024-12-13T13:07:08Z","published":"2024-12-13T13:07:08Z","title":"The Art of Deception: Color Visual Illusions and Diffusion Models","summary":"  Visual illusions in humans arise when interpreting out-of-distribution\nstimuli: if the observer is adapted to certain statistics, perception of\noutliers deviates from reality. Recent studies have shown that artificial\nneural networks (ANNs) can also be deceived by visual illusions. This\nrevelation raises profound questions about the nature of visual information.\nWhy are two independent systems, both human brains and ANNs, susceptible to the\nsame illusions? Should any ANN be capable of perceiving visual illusions? Are\nthese perceptions a feature or a flaw? In this work, we study how visual\nillusions are encoded in diffusion models. Remarkably, we show that they\npresent human-like brightness/color shifts in their latent space. We use this\nfact to demonstrate that diffusion models can predict visual illusions.\nFurthermore, we also show how to generate new unseen visual illusions in\nrealistic images using text-to-image diffusion models. We validate this ability\nthrough psychophysical experiments that show how our model-generated illusions\nalso fool humans.\n","authors":["Alex Gomez-Villa","Kai Wang","Alejandro C. Parraga","Bartlomiej Twardowski","Jesus Malo","Javier Vazquez-Corral","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2412.10122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10116v1","updated":"2024-12-13T12:59:12Z","published":"2024-12-13T12:59:12Z","title":"HS-FPN: High Frequency and Spatial Perception FPN for Tiny Object\n  Detection","summary":"  The introduction of Feature Pyramid Network (FPN) has significantly improved\nobject detection performance. However, substantial challenges remain in\ndetecting tiny objects, as their features occupy only a very small proportion\nof the feature maps. Although FPN integrates multi-scale features, it does not\ndirectly enhance or enrich the features of tiny objects. Furthermore, FPN lacks\nspatial perception ability. To address these issues, we propose a novel High\nFrequency and Spatial Perception Feature Pyramid Network (HS-FPN) with two\ninnovative modules. First, we designed a high frequency perception module (HFP)\nthat generates high frequency responses through high pass filters. These high\nfrequency responses are used as mask weights from both spatial and channel\nperspectives to enrich and highlight the features of tiny objects in the\noriginal feature maps. Second, we developed a spatial dependency perception\nmodule (SDP) to capture the spatial dependencies that FPN lacks. Our\nexperiments demonstrate that detectors based on HS-FPN exhibit competitive\nadvantages over state-of-the-art models on the AI-TOD dataset for tiny object\ndetection.\n","authors":["Zican Shi","Jing Hu","Jie Ren","Hengkang Ye","Xuyang Yuan","Yan Ouyang","Jia He","Bo Ji","Junyu Guo"],"pdf_url":"https://arxiv.org/pdf/2412.10116v1.pdf","comment":"13 pages,12 figures,7 tables"},{"id":"http://arxiv.org/abs/2412.10115v1","updated":"2024-12-13T12:57:47Z","published":"2024-12-13T12:57:47Z","title":"Filter or Compensate: Towards Invariant Representation from Distribution\n  Shift for Anomaly Detection","summary":"  Recent Anomaly Detection (AD) methods have achieved great success with\nIn-Distribution (ID) data. However, real-world data often exhibits distribution\nshift, causing huge performance decay on traditional AD methods. From this\nperspective, few previous work has explored AD with distribution shift, and the\ndistribution-invariant normality learning has been proposed based on the\nReverse Distillation (RD) framework. However, we observe the misalignment issue\nbetween the teacher and the student network that causes detection failure,\nthereby propose FiCo, Filter or Compensate, to address the distribution shift\nissue in AD. FiCo firstly compensates the distribution-specific information to\nreduce the misalignment between the teacher and student network via the\nDistribution-Specific Compensation (DiSCo) module, and secondly filters all\nabnormal information to capture distribution-invariant normality with the\nDistribution-Invariant Filter (DiIFi) module. Extensive experiments on three\ndifferent AD benchmarks demonstrate the effectiveness of FiCo, which\noutperforms all existing state-of-the-art (SOTA) methods, and even achieves\nbetter results on the ID scenario compared with RD-based methods. Our code is\navailable at https://github.com/znchen666/FiCo.\n","authors":["Zining Chen","Xingshuang Luo","Weiqiu Wang","Zhicheng Zhao","Fei Su","Aidong Men"],"pdf_url":"https://arxiv.org/pdf/2412.10115v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.10106v1","updated":"2024-12-13T12:47:30Z","published":"2024-12-13T12:47:30Z","title":"A Cascaded Dilated Convolution Approach for Mpox Lesion Classification","summary":"  The global outbreak of Mpox virus, classified as a Public Health Emergency of\nInternational Concern by WHO, presents significant diagnostic challenges due to\nits visual similarity to other skin lesion diseases. Current clinical detection\ntechniques face limitations in accuracy and efficiency, necessitating improved\nautomated diagnostic solutions. This study introduces a novel Cascaded Atrous\nGroup Attention (CAGA) module, specifically designed to enhance multi-scale\nfeature representation while optimizing computational efficiency. By\nintegrating CAGA with EfficientViT-L1 as the backbone architecture, our\napproach achieves state-of-the-art performance with a score of 0.98% on the\nMCSI dataset, while reducing model parameters by 37.5% compared to the original\nEfficientViT-L1. This reduction in computational complexity maintains\ndiagnostic accuracy while enabling broader deployment across\nresource-constrained healthcare settings. Extensive validation across two other\nbenchmark datasets, including MSID and MSLD, demonstrate the model's\nrobustness, consistently outperforming existing approaches. Our findings\nsuggest that CAGA's efficient feature extraction mechanism could be adapted for\nother medical imaging tasks requiring fine-grained visual discrimination.\n","authors":["Ayush Deshmukh"],"pdf_url":"https://arxiv.org/pdf/2412.10106v1.pdf","comment":"(7 pages, 2 figures, 5 tables)"},{"id":"http://arxiv.org/abs/2412.07205v2","updated":"2024-12-13T12:38:04Z","published":"2024-12-10T05:50:50Z","title":"Crack-EdgeSAM Self-Prompting Crack Segmentation System for Edge Devices","summary":"  Structural health monitoring (SHM) is essential for the early detection of\ninfrastructure defects, such as cracks in concrete bridge pier. but often faces\nchallenges in efficiency and accuracy in complex environments. Although the\nSegment Anything Model (SAM) achieves excellent segmentation performance, its\ncomputational demands limit its suitability for real-time applications on edge\ndevices. To address these challenges, this paper proposes Crack-EdgeSAM, a\nself-prompting crack segmentation system that integrates YOLOv8 for generating\nprompt boxes and a fine-tuned EdgeSAM model for crack segmentation. To ensure\ncomputational efficiency, the method employs ConvLoRA, a Parameter-Efficient\nFine-Tuning (PEFT) technique, along with DiceFocalLoss to fine-tune the EdgeSAM\nmodel. Our experimental results on public datasets and the climbing robot\nautomatic inspections demonstrate that the system achieves high segmentation\naccuracy and significantly enhanced inference speed compared to the most recent\nmethods. Notably, the system processes 1024 x 1024 pixels images at 46 FPS on\nour PC and 8 FPS on Jetson Orin Nano.\n","authors":["Yingchu Wang","Ji He","Shijie Yu"],"pdf_url":"https://arxiv.org/pdf/2412.07205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09612v2","updated":"2024-12-13T12:27:52Z","published":"2024-12-12T18:59:40Z","title":"Olympus: A Universal Task Router for Computer Vision Tasks","summary":"  We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttp://yuanze-lin.me/Olympus_page/\n","authors":["Yuanze Lin","Yunsheng Li","Dongdong Chen","Weijian Xu","Ronald Clark","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2412.09612v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.10091v1","updated":"2024-12-13T12:27:47Z","published":"2024-12-13T12:27:47Z","title":"Data Pruning Can Do More: A Comprehensive Data Pruning Approach for\n  Object Re-identification","summary":"  Previous studies have demonstrated that not each sample in a dataset is of\nequal importance during training. Data pruning aims to remove less important or\ninformative samples while still achieving comparable results as training on the\noriginal (untruncated) dataset, thereby reducing storage and training costs.\nHowever, the majority of data pruning methods are applied to image\nclassification tasks. To our knowledge, this work is the first to explore the\nfeasibility of these pruning methods applied to object re-identification (ReID)\ntasks, while also presenting a more comprehensive data pruning approach. By\nfully leveraging the logit history during training, our approach offers a more\naccurate and comprehensive metric for quantifying sample importance, as well as\ncorrecting mislabeled samples and recognizing outliers. Furthermore, our\napproach is highly efficient, reducing the cost of importance score estimation\nby 10 times compared to existing methods. Our approach is a plug-and-play,\narchitecture-agnostic framework that can eliminate/reduce 35%, 30%, and 5% of\nsamples/training time on the VeRi, MSMT17 and Market1501 datasets,\nrespectively, with negligible loss in accuracy (< 0.1%). The lists of\nimportant, mislabeled, and outlier samples from these ReID datasets are\navailable at https://github.com/Zi-Y/data-pruning-reid.\n","authors":["Zi Yang","Haojin Yang","Soumajit Majumder","Jorge Cardoso","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2412.10091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10089v1","updated":"2024-12-13T12:25:16Z","published":"2024-12-13T12:25:16Z","title":"Guidance Not Obstruction: A Conjugate Consistent Enhanced Strategy for\n  Domain Generalization","summary":"  Domain generalization addresses domain shift in real-world applications. Most\napproaches adopt a domain angle, seeking invariant representation across\ndomains by aligning their marginal distributions, irrespective of individual\nclasses, naturally leading to insufficient exploration of discriminative\ninformation. Switching to a class angle, we find that multiple domain-related\npeaks or clusters within the same individual classes must emerge due to\ndistribution shift. In other words, marginal alignment does not guarantee\nconditional alignment, leading to suboptimal generalization. Therefore, we\nargue that acquiring discriminative generalization between classes within\ndomains is crucial. In contrast to seeking distribution alignment, we endeavor\nto safeguard domain-related between-class discrimination. To this end, we\ndevise a novel Conjugate Consistent Enhanced Module, namely Con2EM, based on a\ndistribution over domains, i.e., a meta-distribution. Specifically, we employ a\nnovel distribution-level Universum strategy to generate supplementary diverse\ndomain-related class-conditional distributions, thereby enhancing\ngeneralization. This allows us to resample from these generated distributions\nto provide feedback to the primordial instance-level classifier, further\nimproving its adaptability to the target-agnostic. To ensure generation\naccuracy, we establish an additional distribution-level classifier to\nregularize these conditional distributions. Extensive experiments have been\nconducted to demonstrate its effectiveness and low computational cost compared\nto SOTAs.\n","authors":["Meng Cao","Songcan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10084v1","updated":"2024-12-13T12:18:26Z","published":"2024-12-13T12:18:26Z","title":"ProbeSDF: Light Field Probes for Neural Surface Reconstruction","summary":"  SDF-based differential rendering frameworks have achieved state-of-the-art\nmultiview 3D shape reconstruction. In this work, we re-examine this family of\napproaches by minimally reformulating its core appearance model in a way that\nsimultaneously yields faster computation and increased performance. To this\ngoal, we exhibit a physically-inspired minimal radiance parametrization\ndecoupling angular and spatial contributions, by encoding them with a small\nnumber of features stored in two respective volumetric grids of different\nresolutions. Requiring as little as four parameters per voxel, and a tiny MLP\ncall inside a single fully fused kernel, our approach allows to enhance\nperformance with both surface and image (PSNR) metrics, while providing a\nsignificant training speedup and real-time rendering. We show this performance\nto be consistently achieved on real data over two widely different and popular\napplication fields, generic object and human subject shape reconstruction,\nusing four representative and challenging datasets.\n","authors":["Briac Toussaint","Diego Thomas","Jean-Sébastien Franco"],"pdf_url":"https://arxiv.org/pdf/2412.10084v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.10078v1","updated":"2024-12-13T12:10:53Z","published":"2024-12-13T12:10:53Z","title":"Toy-GS: Assembling Local Gaussians for Precisely Rendering Large-Scale\n  Free Camera Trajectories","summary":"  Currently, 3D rendering for large-scale free camera trajectories, namely,\narbitrary input camera trajectories, poses significant challenges: 1) The\ndistribution and observation angles of the cameras are irregular, and various\ntypes of scenes are included in the free trajectories; 2) Processing the entire\npoint cloud and all images at once for large-scale scenes requires a\nsubstantial amount of GPU memory. This paper presents a Toy-GS method for\naccurately rendering large-scale free camera trajectories. Specifically, we\npropose an adaptive spatial division approach for free trajectories to divide\ncameras and the sparse point cloud of the entire scene into various regions\naccording to camera poses. Training each local Gaussian in parallel for each\narea enables us to concentrate on texture details and minimize GPU memory\nusage. Next, we use the multi-view constraint and position-aware point adaptive\ncontrol (PPAC) to improve the rendering quality of texture details. In\naddition, our regional fusion approach combines local and global Gaussians to\nenhance rendering quality with an increasing number of divided areas. Extensive\nexperiments have been carried out to confirm the effectiveness and efficiency\nof Toy-GS, leading to state-of-the-art results on two public large-scale\ndatasets as well as our SCUTic dataset. Our proposal demonstrates an\nenhancement of 1.19 dB in PSNR and conserves 7 G of GPU memory when compared to\nvarious benchmarks.\n","authors":["Xiaohan Zhang","Zhenyu Sun","Yukui Qiu","Junyan Su","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.10078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03231v3","updated":"2024-12-13T12:09:48Z","published":"2023-03-06T15:48:33Z","title":"StyO: Stylize Your Face in Only One-shot","summary":"  This paper focuses on face stylization with a single artistic target.\nExisting works for this task often fail to retain the source content while\nachieving geometry variation. Here, we present a novel StyO model, ie. Stylize\nthe face in only One-shot, to solve the above problem. In particular, StyO\nexploits a disentanglement and recombination strategy. It first disentangles\nthe content and style of source and target images into identifiers, which are\nthen recombined in a cross manner to derive the stylized face image. In this\nway, StyO decomposes complex images into independent and specific attributes,\nand simplifies one-shot face stylization as the combination of different\nattributes from input images, thus producing results better matching face\ngeometry of target image and content of source one. StyO is implemented with\nlatent diffusion models (LDM) and composed of two key modules: 1) Identifier\nDisentanglement Learner (IDL) for disentanglement phase. It represents\nidentifiers as contrastive text prompts, ie. positive and negative\ndescriptions. And it introduces a novel triple reconstruction loss to fine-tune\nthe pre-trained LDM for encoding style and content into corresponding\nidentifiers; 2) Fine-grained Content Controller (FCC) for the recombination\nphase. It recombines disentangled identifiers from IDL to form an augmented\ntext prompt for generating stylized faces. In addition, FCC also constrains the\ncross-attention maps of latent and text features to preserve source face\ndetails in results. The extensive evaluation shows that StyO produces\nhigh-quality images on numerous paintings of various styles and outperforms the\ncurrent state-of-the-art.\n","authors":["Bonan Li","Zicheng Zhang","Xuecheng Nie","Congying Han","Yinhan Hu","Xinmin Qiu","Tiande Guo"],"pdf_url":"https://arxiv.org/pdf/2303.03231v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2408.12772v2","updated":"2024-12-13T12:01:44Z","published":"2024-08-23T00:15:43Z","title":"Symmetric masking strategy enhances the performance of Masked Image\n  Modeling","summary":"  Masked Image Modeling (MIM) is a technique in self-supervised learning that\nfocuses on acquiring detailed visual representations from unlabeled images by\nestimating the missing pixels in randomly masked sections. It has proven to be\na powerful tool for the preliminary training of Vision Transformers (ViTs),\nyielding impressive results across various tasks. Nevertheless, most MIM\nmethods heavily depend on the random masking strategy to formulate the pretext\ntask. This strategy necessitates numerous trials to ascertain the optimal\ndropping ratio, which can be resource-intensive, requiring the model to be\npre-trained for anywhere between 800 to 1600 epochs. Furthermore, this approach\nmay not be suitable for all datasets. In this work, we propose a new masking\nstrategy that effectively helps the model capture global and local features.\nBased on this masking strategy, SymMIM, our proposed training pipeline for MIM\nis introduced. SymMIM achieves a new SOTA accuracy of 85.9\\% on ImageNet using\nViT-Large and surpasses previous SOTA across downstream tasks such as image\nclassification, semantic segmentation, object detection, instance segmentation\ntasks, and so on.\n","authors":["Khanh-Binh Nguyen","Chae Jung Park"],"pdf_url":"https://arxiv.org/pdf/2408.12772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10061v1","updated":"2024-12-13T11:44:56Z","published":"2024-12-13T11:44:56Z","title":"Quaffure: Real-Time Quasi-Static Neural Hair Simulation","summary":"  Realistic hair motion is crucial for high-quality avatars, but it is often\nlimited by the computational resources available for real-time applications. To\naddress this challenge, we propose a novel neural approach to predict\nphysically plausible hair deformations that generalizes to various body poses,\nshapes, and hairstyles. Our model is trained using a self-supervised loss,\neliminating the need for expensive data generation and storage. We demonstrate\nour method's effectiveness through numerous results across a wide range of pose\nand shape variations, showcasing its robust generalization capabilities and\ntemporally smooth results. Our approach is highly suitable for real-time\napplications with an inference time of only a few milliseconds on consumer\nhardware and its ability to scale to predicting the drape of 1000 grooms in 0.3\nseconds.\n","authors":["Tuur Stuyck","Gene Wei-Chin Lin","Egor Larionov","Hsiao-yu Chen","Aljaz Bozic","Nikolaos Sarafianos","Doug Roble"],"pdf_url":"https://arxiv.org/pdf/2412.10061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17832v2","updated":"2024-12-13T11:40:57Z","published":"2024-11-26T19:13:38Z","title":"SVGDreamer++: Advancing Editability and Diversity in Text-Guided SVG\n  Generation","summary":"  Recently, text-guided scalable vector graphics (SVG) synthesis has\ndemonstrated significant potential in domains such as iconography and\nsketching. However, SVGs generated from existing Text-to-SVG methods often lack\neditability and exhibit deficiencies in visual quality and diversity. In this\npaper, we propose a novel text-guided vector graphics synthesis method to\naddress these limitations. To enhance the editability of output SVGs, we\nintroduce a Hierarchical Image VEctorization (HIVE) framework that operates at\nthe semantic object level and supervises the optimization of components within\nthe vector object. This approach facilitates the decoupling of vector graphics\ninto distinct objects and component levels. Our proposed HIVE algorithm,\ninformed by image segmentation priors, not only ensures a more precise\nrepresentation of vector graphics but also enables fine-grained editing\ncapabilities within vector objects. To improve the diversity of output SVGs, we\npresent a Vectorized Particle-based Score Distillation (VPSD) approach. VPSD\naddresses over-saturation issues in existing methods and enhances sample\ndiversity. A pre-trained reward model is incorporated to re-weight vector\nparticles, improving aesthetic appeal and enabling faster convergence.\nAdditionally, we design a novel adaptive vector primitives control strategy,\nwhich allows for the dynamic adjustment of the number of primitives, thereby\nenhancing the presentation of graphic details. Extensive experiments validate\nthe effectiveness of the proposed method, demonstrating its superiority over\nbaseline methods in terms of editability, visual quality, and diversity. We\nalso show that our new method supports up to six distinct vector styles,\ncapable of generating high-quality vector assets suitable for stylized vector\ndesign and poster design. Code and demo will be released at:\nhttp://ximinng.github.io/SVGDreamerV2Project/\n","authors":["Ximing Xing","Qian Yu","Chuang Wang","Haitao Zhou","Jing Zhang","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2411.17832v2.pdf","comment":"17 pages, 17 figures. Project Page:\n  http://ximinng.github.io/SVGDreamerV2Project/. arXiv admin note: text overlap\n  with arXiv:2312.16476"},{"id":"http://arxiv.org/abs/2412.10051v1","updated":"2024-12-13T11:26:38Z","published":"2024-12-13T11:26:38Z","title":"TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting\n  from Sparse Views","summary":"  Recent advances in Gaussian Splatting have significantly advanced the field,\nachieving both panoptic and interactive segmentation of 3D scenes. However,\nexisting methodologies often overlook the critical need for reconstructing\nspecified targets with complex structures from sparse views. To address this\nissue, we introduce TSGaussian, a novel framework that combines semantic\nconstraints with depth priors to avoid geometry degradation in challenging\nnovel view synthesis tasks. Our approach prioritizes computational resources on\ndesignated targets while minimizing background allocation. Bounding boxes from\nYOLOv9 serve as prompts for Segment Anything Model to generate 2D mask\npredictions, ensuring semantic accuracy and cost efficiency. TSGaussian\neffectively clusters 3D gaussians by introducing a compact identity encoding\nfor each Gaussian ellipsoid and incorporating 3D spatial consistency\nregularization. Leveraging these modules, we propose a pruning strategy to\neffectively reduce redundancy in 3D gaussians. Extensive experiments\ndemonstrate that TSGaussian outperforms state-of-the-art methods on three\nstandard datasets and a new challenging dataset we collected, achieving\nsuperior results in novel view synthesis of specific objects. Code is available\nat: https://github.com/leon2000-ai/TSGaussian.\n","authors":["Liang Zhao","Zehan Bao","Yi Xie","Hong Chen","Yaohui Chen","Weifu Li"],"pdf_url":"https://arxiv.org/pdf/2412.10051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10050v1","updated":"2024-12-13T11:22:01Z","published":"2024-12-13T11:22:01Z","title":"ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for\n  Articulated Object Manipulation?","summary":"  Visual actionable affordance has emerged as a transformative approach in\nrobotics, focusing on perceiving interaction areas prior to manipulation.\nTraditional methods rely on pixel sampling to identify successful interaction\nsamples or processing pointclouds for affordance mapping. However, these\napproaches are computationally intensive and struggle to adapt to diverse and\ndynamic environments. This paper introduces ManipGPT, a framework designed to\npredict optimal interaction areas for articulated objects using a large\npre-trained vision transformer (ViT). We created a dataset of 9.9k simulated\nand real images to bridge the sim-to-real gap and enhance real-world\napplicability. By fine-tuning the vision transformer on this small dataset, we\nsignificantly improved part-level affordance segmentation, adapting the model's\nin-context segmentation capabilities to robot manipulation scenarios. This\nenables effective manipulation across simulated and real-world environments by\ngenerating part-level affordance masks, paired with an impedance adaptation\npolicy, sufficiently eliminating the need for complex datasets or perception\nsystems.\n","authors":["Taewhan Kim","Hojin Bae","Zeming Li","Xiaoqi Li","Iaroslav Ponomarenko","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2412.10050v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.10049v1","updated":"2024-12-13T11:20:59Z","published":"2024-12-13T11:20:59Z","title":"SuperMark: Robust and Training-free Image Watermarking via\n  Diffusion-based Super-Resolution","summary":"  In today's digital landscape, the blending of AI-generated and authentic\ncontent has underscored the need for copyright protection and content\nauthentication. Watermarking has become a vital tool to address these\nchallenges, safeguarding both generated and real content. Effective\nwatermarking methods must withstand various distortions and attacks. Current\ndeep watermarking techniques often use an encoder-noise layer-decoder\narchitecture and include distortions to enhance robustness. However, they\nstruggle to balance robustness and fidelity and remain vulnerable to adaptive\nattacks, despite extensive training. To overcome these limitations, we propose\nSuperMark, a robust, training-free watermarking framework. Inspired by the\nparallels between watermark embedding/extraction in watermarking and the\ndenoising/noising processes in diffusion models, SuperMark embeds the watermark\ninto initial Gaussian noise using existing techniques. It then applies\npre-trained Super-Resolution (SR) models to denoise the watermarked noise,\nproducing the final watermarked image. For extraction, the process is reversed:\nthe watermarked image is inverted back to the initial watermarked noise via\nDDIM Inversion, from which the embedded watermark is extracted. This flexible\nframework supports various noise injection methods and diffusion-based SR\nmodels, enabling enhanced customization. The robustness of the DDIM Inversion\nprocess against perturbations allows SuperMark to achieve strong resilience to\ndistortions while maintaining high fidelity. Experiments demonstrate that\nSuperMark achieves fidelity comparable to existing methods while significantly\nimproving robustness. Under standard distortions, it achieves an average\nwatermark extraction accuracy of 99.46%, and 89.29% under adaptive attacks.\nMoreover, SuperMark shows strong transferability across datasets, SR models,\nembedding methods, and resolutions.\n","authors":["Runyi Hu","Jie Zhang","Yiming Li","Jiwei Li","Qing Guo","Han Qiu","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10049v1.pdf","comment":"robust image watermarking"},{"id":"http://arxiv.org/abs/2412.10040v1","updated":"2024-12-13T11:00:57Z","published":"2024-12-13T11:00:57Z","title":"RemDet: Rethinking Efficient Model Design for UAV Object Detection","summary":"  Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a\nfocal area of research, which presents two significant challenges: i) objects\nare typically small and dense within vast images; ii) computational resource\nconstraints render most models unsuitable for real-time deployment. Current\nreal-time object detectors are not optimized for UAV images, and complex\nmethods designed for small object detection often lack real-time capabilities.\nTo address these challenges, we propose a novel detector, RemDet (Reparameter\nefficient multiplication Detector). Our contributions are as follows: 1)\nRethinking the challenges of existing detectors for small and dense UAV images,\nand proposing information loss as a design guideline for efficient models. 2)\nWe introduce the ChannelC2f module to enhance small object detection\nperformance, demonstrating that high-dimensional representations can\neffectively mitigate information loss. 3) We design the GatedFFN module to\nprovide not only strong performance but also low latency, effectively\naddressing the challenges of real-time detection. Our research reveals that\nGatedFFN, through the use of multiplication, is more cost-effective than\nfeed-forward networks for high-dimensional representation. 4) We propose the\nCED module, which combines the advantages of ViT and CNN downsampling to\neffectively reduce information loss. It specifically enhances context\ninformation for small and dense objects. Extensive experiments on large UAV\ndatasets, Visdrone and UAVDT, validate the real-time efficiency and superior\nperformance of our methods. On the challenging UAV dataset VisDrone, our\nmethods not only provided state-of-the-art results, improving detection by more\nthan 3.4%, but also achieve 110 FPS on a single 4090.Codes are available at\n(this URL)(https://github.com/HZAI-ZJNU/RemDet).\n","authors":["Chen Li","Rui Zhao","Zeyu Wang","Huiying Xu","Xinzhong Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.10040v1.pdf","comment":"Accepted to AAAI25"},{"id":"http://arxiv.org/abs/2412.10033v1","updated":"2024-12-13T10:48:38Z","published":"2024-12-13T10:48:38Z","title":"Timealign: A multi-modal object detection method for time misalignment\n  fusing in autonomous driving","summary":"  The multi-modal perception methods are thriving in the autonomous driving\nfield due to their better usage of complementary data from different sensors.\nSuch methods depend on calibration and synchronization between sensors to get\naccurate environmental information. There have already been studies about\nspace-alignment robustness in autonomous driving object detection process,\nhowever, the research for time-alignment is relatively few. As in reality\nexperiments, LiDAR point clouds are more challenging for real-time data\ntransfer, our study used historical frames of LiDAR to better align features\nwhen the LiDAR data lags exist. We designed a Timealign module to predict and\ncombine LiDAR features with observation to tackle such time misalignment based\non SOTA GraphBEV framework.\n","authors":["Zhihang Song","Lihui Peng","Jianming Hu","Danya Yao","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10033v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.10032v1","updated":"2024-12-13T10:47:05Z","published":"2024-12-13T10:47:05Z","title":"Object-Focused Data Selection for Dense Prediction Tasks","summary":"  Dense prediction tasks such as object detection and segmentation require\nhigh-quality labels at pixel level, which are costly to obtain. Recent advances\nin foundation models have enabled the generation of autolabels, which we find\nto be competitive but not yet sufficient to fully replace human annotations,\nespecially for more complex datasets. Thus, we consider the challenge of\nselecting a representative subset of images for labeling from a large pool of\nunlabeled images under a constrained annotation budget. This task is further\ncomplicated by imbalanced class distributions, as rare classes are often\nunderrepresented in selected subsets. We propose object-focused data selection\n(OFDS) which leverages object-level representations to ensure that the selected\nimage subsets semantically cover the target classes, including rare ones. We\nvalidate OFDS on PASCAL VOC and Cityscapes for object detection and semantic\nsegmentation tasks. Our experiments demonstrate that prior methods which employ\nimage-level representations fail to consistently outperform random selection.\nIn contrast, OFDS consistently achieves state-of-the-art performance with\nsubstantial improvements over all baselines in scenarios with imbalanced class\ndistributions. Moreover, we demonstrate that pre-training with autolabels on\nthe full datasets before fine-tuning on human-labeled subsets selected by OFDS\nfurther enhances the final performance.\n","authors":["Niclas Popp","Dan Zhang","Jan Hendrik Metzen","Matthias Hein","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2412.10032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10031v1","updated":"2024-12-13T10:45:25Z","published":"2024-12-13T10:45:25Z","title":"FM2S: Self-Supervised Fluorescence Microscopy Denoising With Single\n  Noisy Image","summary":"  Fluorescence microscopy has significantly advanced biological research by\nvisualizing detailed cellular structures and biological processes. However,\nsuch image denoising task often faces challenges due to difficulty in precisely\nmodeling the inherent noise and acquiring clean images for training, which\nconstrains most existing methods. In this paper, we propose an efficient\nself-supervised denoiser Fluorescence Micrograph to Self (FM2S), enabling a\nhigh-quality denoised result with a single noisy image. Our method introduces\nan adaptive global-local Noise Addition module for data augmentation,\naddressing generalization problems caused by discrepancies between synthetic\nand real-world noise. We then train a two-layer neural network to learn the\nmapping from the noise-added image to the filtered image, achieving a balance\nbetween noise removal and computational efficiency. Experimental results\ndemonstrate that FM2S excels in various microscope types and noise levels in\nterms of denoising effects and time consumption, obtaining an average PSNR\nimprovement of around 6 dB over the original noisy image in a few seconds. The\ncode is available at https://github.com/Danielement321/FM2S.\n","authors":["Jizhihui Liu","Qixun Teng","Junjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.10031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10029v1","updated":"2024-12-13T10:39:31Z","published":"2024-12-13T10:39:31Z","title":"Enhancing Fine-Grained Vision-Language Pretraining with Negative\n  Augmented Samples","summary":"  Existing Vision-Language Pretraining (VLP) methods have achieved remarkable\nimprovements across a variety of vision-language tasks, confirming their\neffectiveness in capturing coarse-grained semantic correlations. However, their\ncapability for fine-grained understanding, which is critical for many nuanced\nvision-language applications, remains limited. Prevailing VLP models often\noverlook the intricate distinctions in expressing different modal features and\ntypically depend on the similarity of holistic features for cross-modal\ninteractions. Moreover, these models directly align and integrate features from\ndifferent modalities, focusing more on coarse-grained general representations,\nthus failing to capture the nuanced differences necessary for tasks demanding a\nmore detailed perception. In response to these limitations, we introduce\nNegative Augmented Samples(NAS), a refined vision-language pretraining model\nthat innovatively incorporates NAS to specifically address the challenge of\nfine-grained understanding. NAS utilizes a Visual Dictionary(VD) as a semantic\nbridge between visual and linguistic domains. Additionally, it employs a\nNegative Visual Augmentation(NVA) method based on the VD to generate\nchallenging negative image samples. These samples deviate from positive samples\nexclusively at the token level, thereby necessitating that the model discerns\nthe subtle disparities between positive and negative samples with greater\nprecision. Comprehensive experiments validate the efficacy of NAS components\nand underscore its potential to enhance fine-grained vision-language\ncomprehension.\n","authors":["Yeyuan Wang","Dehong Gao","Lei Yi","Linbo Jin","Jinxia Zhang","Libin Yang","Xiaoyan Cai"],"pdf_url":"https://arxiv.org/pdf/2412.10029v1.pdf","comment":"15pages, Accepted by AAAI2025, full paper"},{"id":"http://arxiv.org/abs/2412.10028v1","updated":"2024-12-13T10:39:27Z","published":"2024-12-13T10:39:27Z","title":"Mr. DETR: Instructive Multi-Route Training for Detection Transformers","summary":"  Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We enhance the training mechanism with a\nnovel instructive self-attention that dynamically and flexibly guides object\nqueries for one-to-many prediction. The auxiliary routes are removed during\ninference, ensuring no impact on model architecture or inference cost. We\nconduct extensive experiments on various baselines, achieving consistent\nimprovements as shown in Figure 1.\n","authors":["Chang-Bin Zhang","Yujie Zhong","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.10028v1.pdf","comment":"Tech. report"},{"id":"http://arxiv.org/abs/2407.07627v2","updated":"2024-12-13T10:09:55Z","published":"2024-07-10T13:07:39Z","title":"Synthetic to Authentic: Transferring Realism to 3D Face Renderings for\n  Boosting Face Recognition","summary":"  In this paper, we investigate the potential of image-to-image translation\n(I2I) techniques for transferring realism to 3D-rendered facial images in the\ncontext of Face Recognition (FR) systems. The primary motivation for using\n3D-rendered facial images lies in their ability to circumvent the challenges\nassociated with collecting large real face datasets for training FR systems.\nThese images are generated entirely by 3D rendering engines, facilitating the\ngeneration of synthetic identities. However, it has been observed that FR\nsystems trained on such synthetic datasets underperform when compared to those\ntrained on real datasets, on various FR benchmarks. In this work, we\ndemonstrate that by transferring the realism to 3D-rendered images (i.e.,\nmaking the 3D-rendered images look more real), we can boost the performance of\nFR systems trained on these more photorealistic images. This improvement is\nevident when these systems are evaluated against FR benchmarks utilizing\nreal-world data, thereby paving new pathways for employing synthetic data in\nreal-world applications.\n","authors":["Parsa Rahimi","Behrooz Razeghi","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2407.07627v2.pdf","comment":"ECCV24 Synthetic Data for Computer Vision (Oral)"},{"id":"http://arxiv.org/abs/2412.09602v2","updated":"2024-12-13T09:51:22Z","published":"2024-12-12T18:59:13Z","title":"Hidden Biases of End-to-End Driving Datasets","summary":"  End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.\n","authors":["Julian Zimmerlin","Jens Beißwenger","Bernhard Jaeger","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2412.09602v2.pdf","comment":"Technical report for the CVPR 2024 Workshop on Foundation Models for\n  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving\n  Challenge' in the 2024 Autonomous Grand Challenge\n  (https://opendrivelab.com/challenge2024/)"},{"id":"http://arxiv.org/abs/2412.08378v2","updated":"2024-12-13T09:49:53Z","published":"2024-12-11T13:41:21Z","title":"HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for\n  Vision-Language Models","summary":"  Recently, there has been growing interest in the capability of multimodal\nlarge language models (MLLMs) to process high-resolution images. A common\napproach currently involves dynamically cropping the original high-resolution\nimage into smaller sub-images, which are then fed into a vision encoder that\nwas pre-trained on lower-resolution images. However, this cropping approach\noften truncates objects and connected areas in the original image, causing\nsemantic breaks. To address this limitation, we introduce HyViLM, designed to\nprocess images of any resolution while retaining the overall context during\nencoding. Specifically, we: (i) Design a new visual encoder called Hybrid\nEncoder that not only encodes individual sub-images but also interacts with\ndetailed global visual features, significantly improving the model's ability to\nencode high-resolution images. (ii) Propose an optimal feature fusion strategy\nfor the dynamic cropping approach, effectively leveraging information from\ndifferent layers of the vision encoder. Compared with the state-of-the-art\nMLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out\nof ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance\non the TextVQA task and a 6.9% enhancement on the DocVQA task.\n","authors":["Shiding Zhu","Wenhui Dong","Jun Song","Yingbo Wang","Yanan Guo","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.08378v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.10004v1","updated":"2024-12-13T09:41:48Z","published":"2024-12-13T09:41:48Z","title":"NeRF-Texture: Synthesizing Neural Radiance Field Textures","summary":"  Texture synthesis is a fundamental problem in computer graphics that would\nbenefit various applications. Existing methods are effective in handling 2D\nimage textures. In contrast, many real-world textures contain meso-structure in\nthe 3D geometry space, such as grass, leaves, and fabrics, which cannot be\neffectively modeled using only 2D image textures. We propose a novel texture\nsynthesis method with Neural Radiance Fields (NeRF) to capture and synthesize\ntextures from given multi-view images. In the proposed NeRF texture\nrepresentation, a scene with fine geometric details is disentangled into the\nmeso-structure textures and the underlying base shape. This allows textures\nwith meso-structure to be effectively learned as latent features situated on\nthe base shape, which are fed into a NeRF decoder trained simultaneously to\nrepresent the rich view-dependent appearance. Using this implicit\nrepresentation, we can synthesize NeRF-based textures through patch matching of\nlatent features. However, inconsistencies between the metrics of the\nreconstructed content space and the latent feature space may compromise the\nsynthesis quality. To enhance matching performance, we further regularize the\ndistribution of latent features by incorporating a clustering constraint. In\naddition to generating NeRF textures over a planar domain, our method can also\nsynthesize NeRF textures over curved surfaces, which are practically useful.\nExperimental results and evaluations demonstrate the effectiveness of our\napproach.\n","authors":["Yi-Hua Huang","Yan-Pei Cao","Yu-Kun Lai","Ying Shan","Lin Gao"],"pdf_url":"https://arxiv.org/pdf/2412.10004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10002v1","updated":"2024-12-13T09:40:37Z","published":"2024-12-13T09:40:37Z","title":"NowYouSee Me: Context-Aware Automatic Audio Description","summary":"  Audio Description (AD) plays a pivotal role as an application system aimed at\nguaranteeing accessibility in multimedia content, which provides additional\nnarrations at suitable intervals to describe visual elements, catering\nspecifically to the needs of visually impaired audiences. In this paper, we\nintroduce $\\mathrm{CA^3D}$, the pioneering unified Context-Aware Automatic\nAudio Description system that provides AD event scripts with precise locations\nin the long cinematic content. Specifically, $\\mathrm{CA^3D}$ system consists\nof: 1) a Temporal Feature Enhancement Module to efficiently capture longer term\ndependencies, 2) an anchor-based AD event detector with feature suppression\nmodule that localizes the AD events and extracts discriminative feature for AD\ngeneration, and 3) a self-refinement module that leverages the generated output\nto tweak AD event boundaries from coarse to fine. Unlike conventional methods\nwhich rely on metadata and ground truth AD timestamp for AD detection and\ngeneration tasks, the proposed $\\mathrm{CA^3D}$ is the first end-to-end\ntrainable system that only uses visual cue. Extensive experiments demonstrate\nthat the proposed $\\mathrm{CA^3D}$ improves existing architectures for both AD\nevent detection and script generation metrics, establishing the new\nstate-of-the-art performances in the AD automation.\n","authors":["Seon-Ho Lee","Jue Wang","David Fan","Zhikang Zhang","Linda Liu","Xiang Hao","Vimal Bhat","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2412.10002v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2412.09998v1","updated":"2024-12-13T09:35:34Z","published":"2024-12-13T09:35:34Z","title":"Cycle-Consistent Bridge Diffusion Model for Accelerated MRI\n  Reconstruction","summary":"  Accelerated MRI reconstruction techniques aim to reduce examination time\nwhile maintaining high image fidelity, which is highly desirable in clinical\nsettings for improving patient comfort and hospital efficiency. Existing deep\nlearning methods typically reconstruct images from under-sampled data with\ntraditional reconstruction approaches, but they still struggle to provide\nhigh-fidelity results. Diffusion models show great potential to improve\nfidelity of generated images in recent years. However, their inference process\nstarting with a random Gaussian noise introduces instability into the results\nand usually requires thousands of sampling steps, resulting in sub-optimal\nreconstruction quality and low efficiency. To address these challenges, we\npropose Cycle-Consistent Bridge Diffusion Model (CBDM). CBDM employs two bridge\ndiffusion models to construct a cycle-consistent diffusion process with a\nconsistency loss, enhancing the fine-grained details of reconstructed images\nand reducing the number of diffusion steps. Moreover, CBDM incorporates a\nContourlet Decomposition Embedding Module (CDEM) which captures multi-scale\nstructural texture knowledge in images through frequency domain decomposition\npyramids and directional filter banks to improve structural fidelity. Extensive\nexperiments demonstrate the superiority of our model by higher reconstruction\nquality and fewer training iterations, achieving a new state of the art for\naccelerated MRI reconstruction in both fastMRI and IXI datasets.\n","authors":["Tao Song","Yicheng Wu","Minhao Hu","Xiangde Luo","Guoting Luo","Guotai Wang","Yi Guo","Feng Xu","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09997v1","updated":"2024-12-13T09:32:08Z","published":"2024-12-13T09:32:08Z","title":"GT23D-Bench: A Comprehensive General Text-to-3D Generation Benchmark","summary":"  Recent advances in General Text-to-3D (GT23D) have been significant. However,\nthe lack of a benchmark has hindered systematic evaluation and progress due to\nissues in datasets and metrics: 1) The largest 3D dataset Objaverse suffers\nfrom omitted annotations, disorganization, and low-quality. 2) Existing metrics\nonly evaluate textual-image alignment without considering the 3D-level quality.\nTo this end, we are the first to present a comprehensive benchmark for GT23D\ncalled GT23D-Bench consisting of: 1) a 400k high-fidelity and well-organized 3D\ndataset that curated issues in Objaverse through a systematical\nannotation-organize-filter pipeline; and 2) comprehensive 3D-aware evaluation\nmetrics which encompass 10 clearly defined metrics thoroughly accounting for\nmulti-dimension of GT23D. Notably, GT23D-Bench features three properties: 1)\nMultimodal Annotations. Our dataset annotates each 3D object with 64-view depth\nmaps, normal maps, rendered images, and coarse-to-fine captions. 2) Holistic\nEvaluation Dimensions. Our metrics are dissected into a) Textual-3D Alignment\nmeasures textual alignment with multi-granularity visual 3D representations;\nand b) 3D Visual Quality which considers texture fidelity, multi-view\nconsistency, and geometry correctness. 3) Valuable Insights. We delve into the\nperformance of current GT23D baselines across different evaluation dimensions\nand provide insightful analysis. Extensive experiments demonstrate that our\nannotations and metrics are aligned with human preferences.\n","authors":["Sitong Su","Xiao Cai","Lianli Gao","Pengpeng Zeng","Qinhong Du","Mengqi Li","Heng Tao Shen","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2412.09997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09991v1","updated":"2024-12-13T09:25:18Z","published":"2024-12-13T09:25:18Z","title":"Visual Object Tracking across Diverse Data Modalities: A Review","summary":"  Visual Object Tracking (VOT) is an attractive and significant research area\nin computer vision, which aims to recognize and track specific targets in video\nsequences where the target objects are arbitrary and class-agnostic. The VOT\ntechnology could be applied in various scenarios, processing data of diverse\nmodalities such as RGB, thermal infrared and point cloud. Besides, since no one\nsensor could handle all the dynamic and varying environments, multi-modal VOT\nis also investigated. This paper presents a comprehensive survey of the recent\nprogress of both single-modal and multi-modal VOT, especially the deep learning\nmethods. Specifically, we first review three types of mainstream single-modal\nVOT, including RGB, thermal infrared and point cloud tracking. In particular,\nwe conclude four widely-used single-modal frameworks, abstracting their schemas\nand categorizing the existing inheritors. Then we summarize four kinds of\nmulti-modal VOT, including RGB-Depth, RGB-Thermal, RGB-LiDAR and RGB-Language.\nMoreover, the comparison results in plenty of VOT benchmarks of the discussed\nmodalities are presented. Finally, we provide recommendations and insightful\nobservations, inspiring the future development of this fast-growing literature.\n","authors":["Mengmeng Wang","Teli Ma","Shuo Xin","Xiaojun Hou","Jiazheng Xing","Guang Dai","Jingdong Wang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09982v1","updated":"2024-12-13T09:09:14Z","published":"2024-12-13T09:09:14Z","title":"SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D\n  Gaussians from Monocular Video","summary":"  Synthesizing novel views from in-the-wild monocular videos is challenging due\nto scene dynamics and the lack of multi-view cues. To address this, we propose\nSplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for\nhigh-quality reconstruction and fast rendering from monocular videos. At its\ncore is a novel Motion-Adaptive Spline (MAS) method, which represents\ncontinuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a\nsmall number of control points. For MAS, we introduce a Motion-Adaptive Control\npoints Pruning (MACP) method to model the deformation of each dynamic 3D\nGaussian across varying motions, progressively pruning control points while\nmaintaining dynamic modeling integrity. Additionally, we present a joint\noptimization strategy for camera parameter estimation and 3D Gaussian\nattributes, leveraging photometric and geometric consistency. This eliminates\nthe need for Structure-from-Motion preprocessing and enhances SplineGS's\nrobustness in real-world conditions. Experiments show that SplineGS\nsignificantly outperforms state-of-the-art methods in novel view synthesis\nquality for dynamic scenes from monocular videos, achieving thousands times\nfaster rendering speed.\n","authors":["Jongmin Park","Minh-Quan Viet Bui","Juan Luis Gonzalez Bello","Jaeho Moon","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09982v1.pdf","comment":"The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work. Please\n  visit our project page at this https://kaist-viclab.github.io/splinegs-site/"},{"id":"http://arxiv.org/abs/2412.09981v1","updated":"2024-12-13T09:08:02Z","published":"2024-12-13T09:08:02Z","title":"SUMI-IFL: An Information-Theoretic Framework for Image Forgery\n  Localization with Sufficiency and Minimality Constraints","summary":"  Image forgery localization (IFL) is a crucial technique for preventing\ntampered image misuse and protecting social safety. However, due to the rapid\ndevelopment of image tampering technologies, extracting more comprehensive and\naccurate forgery clues remains an urgent challenge. To address these\nchallenges, we introduce a novel information-theoretic IFL framework named\nSUMI-IFL that imposes sufficiency-view and minimality-view constraints on\nforgery feature representation. First, grounded in the theoretical analysis of\nmutual information, the sufficiency-view constraint is enforced on the feature\nextraction network to ensure that the latent forgery feature contains\ncomprehensive forgery clues. Considering that forgery clues obtained from a\nsingle aspect alone may be incomplete, we construct the latent forgery feature\nby integrating several individual forgery features from multiple perspectives.\nSecond, based on the information bottleneck, the minimality-view constraint is\nimposed on the feature reasoning network to achieve an accurate and concise\nforgery feature representation that counters the interference of task-unrelated\nfeatures. Extensive experiments show the superior performance of SUMI-IFL to\nexisting state-of-the-art methods, not only on in-dataset comparisons but also\non cross-dataset comparisons.\n","authors":["Ziqi Sheng","Wei Lu","Xiangyang Luo","Jiantao Zhou","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2412.09981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06251v3","updated":"2024-12-13T09:02:22Z","published":"2024-02-09T08:59:37Z","title":"Single Channel EEG Based Insomnia Identification Without Sleep Stage\n  Annotations","summary":"  This paper proposes a new approach to identifying patients with insomnia\nusing a single EEG channel, without the need for sleep stage annotation. Data\npreprocessing, feature extraction, feature selection, and classification\ntechniques are used to automatically detect insomnia based on features\nextracted from spectral and temporal domains, including relative power in the\ndelta, sigma, beta and gamma bands, total power, absolute slow wave power,\npower ratios, mean, zero crossing rate, mobility, and complexity. A Pearson\ncorrelation coefficient, t-test, p-value, and two rules are used to select the\noptimal set of features for accurately classifying insomnia patients and\nrejecting negatively affecting features. Classification schemes including a\ngeneral artificial neural network, convolutional neural network, and support\nvector machine are applied to the optimal feature set to distinguish between\ninsomnia patients and healthy subjects. The performance of the model is\nvalidated using 50 insomnia patients and 50 healthy subjects, with the Fp2\nchannel and 1D-CNN classifier achieving the highest accuracy and Cohen's kappa\ncoefficient at 97.85% and 94.15%, respectively. The developed model has the\npotential to simplify current sleep monitoring systems and enable in-home\nambulatory monitoring.\n","authors":["Chan-Yun Yang","Nilantha Premakumara","Hooman Samani","Chinthaka Premachandra"],"pdf_url":"https://arxiv.org/pdf/2402.06251v3.pdf","comment":"30 Pages, 9 figures and 12 tables"},{"id":"http://arxiv.org/abs/2412.09966v1","updated":"2024-12-13T08:49:25Z","published":"2024-12-13T08:49:25Z","title":"EP-CFG: Energy-Preserving Classifier-Free Guidance","summary":"  Classifier-free guidance (CFG) is widely used in diffusion models but often\nintroduces over-contrast and over-saturation artifacts at higher guidance\nstrengths. We present EP-CFG (Energy-Preserving Classifier-Free Guidance),\nwhich addresses these issues by preserving the energy distribution of the\nconditional prediction during the guidance process. Our method simply rescales\nthe energy of the guided output to match that of the conditional prediction at\neach denoising step, with an optional robust variant for improved artifact\nsuppression. Through experiments, we show that EP-CFG maintains natural image\nquality and preserves details across guidance strengths while retaining CFG's\nsemantic alignment benefits, all with minimal computational overhead.\n","authors":["Kai Zhang","Fujun Luan","Sai Bi","Jianming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08050v2","updated":"2024-12-13T08:38:29Z","published":"2024-12-11T02:56:23Z","title":"BSAFusion: A Bidirectional Stepwise Feature Alignment Network for\n  Unaligned Medical Image Fusion","summary":"  If unaligned multimodal medical images can be simultaneously aligned and\nfused using a single-stage approach within a unified processing framework, it\nwill not only achieve mutual promotion of dual tasks but also help reduce the\ncomplexity of the model. However, the design of this model faces the challenge\nof incompatible requirements for feature fusion and alignment; specifically,\nfeature alignment requires consistency among corresponding features, whereas\nfeature fusion requires the features to be complementary to each other. To\naddress this challenge, this paper proposes an unaligned medical image fusion\nmethod called Bidirectional Stepwise Feature Alignment and Fusion (BSFA-F)\nstrategy. To reduce the negative impact of modality differences on cross-modal\nfeature matching, we incorporate the Modal Discrepancy-Free Feature\nRepresentation (MDF-FR) method into BSFA-F. MDF-FR utilizes a Modality Feature\nRepresentation Head (MFRH) to integrate the global information of the input\nimage. By injecting the information contained in MFRH of the current image into\nother modality images, it effectively reduces the impact of modality\ndifferences on feature alignment while preserving the complementary information\ncarried by different images. In terms of feature alignment, BSFA-F employs a\nbidirectional stepwise alignment deformation field prediction strategy based on\nthe path independence of vector displacement between two points. This strategy\nsolves the problem of large spans and inaccurate deformation field prediction\nin single-step alignment. Finally, Multi-Modal Feature Fusion block achieves\nthe fusion of aligned features. The experimental results across multiple\ndatasets demonstrate the effectiveness of our method. The source code is\navailable at https://github.com/slrl123/BSAFusion.\n","authors":["Huafeng Li","Dayong Su","Qing Cai","Yafei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08050v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.09960v1","updated":"2024-12-13T08:37:30Z","published":"2024-12-13T08:37:30Z","title":"END$^2$: Robust Dual-Decoder Watermarking Framework Against\n  Non-Differentiable Distortions","summary":"  DNN-based watermarking methods have rapidly advanced, with the\n``Encoder-Noise Layer-Decoder'' (END) framework being the most widely used. To\nensure end-to-end training, the noise layer in the framework must be\ndifferentiable. However, real-world distortions are often non-differentiable,\nleading to challenges in end-to-end training. Existing solutions only treat the\ndistortion perturbation as additive noise, which does not fully integrate the\neffect of distortion in training. To better incorporate non-differentiable\ndistortions into training, we propose a novel dual-decoder architecture\n(END$^2$). Unlike conventional END architecture, our method employs two\nstructurally identical decoders: the Teacher Decoder, processing pure\nwatermarked images, and the Student Decoder, handling distortion-perturbed\nimages. The gradient is backpropagated only through the Teacher Decoder branch\nto optimize the encoder thus bypassing the problem of non-differentiability. To\nensure resistance to arbitrary distortions, we enforce alignment of the two\ndecoders' feature representations by maximizing the cosine similarity between\ntheir intermediate vectors on a hypersphere. Extensive experiments demonstrate\nthat our scheme outperforms state-of-the-art algorithms under various\nnon-differentiable distortions. Moreover, even without the differentiability\nconstraint, our method surpasses baselines with a differentiable noise layer.\nOur approach is effective and easily implementable across all END\narchitectures, enhancing practicality and generalizability.\n","authors":["Nan Sun","Han Fang","Yuxing Lu","Chengxin Zhao","Hefei Ling"],"pdf_url":"https://arxiv.org/pdf/2412.09960v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.09959v1","updated":"2024-12-13T08:34:46Z","published":"2024-12-13T08:34:46Z","title":"Efficient Dataset Distillation via Diffusion-Driven Patch Selection for\n  Improved Generalization","summary":"  Dataset distillation offers an efficient way to reduce memory and\ncomputational costs by optimizing a smaller dataset with performance comparable\nto the full-scale original. However, for large datasets and complex deep\nnetworks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space\nlimits performance, reducing its practicality. Recent approaches employ\npre-trained diffusion models to generate informative images directly, avoiding\npixel-level optimization and achieving notable results. However, these methods\noften face challenges due to distribution shifts between pre-trained models and\ntarget datasets, along with the need for multiple distillation steps across\nvarying settings. To address these issues, we propose a novel framework\northogonal to existing diffusion-based distillation methods, leveraging\ndiffusion models for selection rather than generation. Our method starts by\npredicting noise generated by the diffusion model based on input images and\ntext prompts (with or without label text), then calculates the corresponding\nloss for each pair. With the loss differences, we identify distinctive regions\nof the original images. Additionally, we perform intra-class clustering and\nranking on selected patches to maintain diversity constraints. This streamlined\nframework enables a single-step distillation process, and extensive experiments\ndemonstrate that our approach outperforms state-of-the-art methods across\nvarious metrics.\n","authors":["Xinhao Zhong","Shuoyang Sun","Xulin Gu","Zhaoyang Xu","Yaowei Wang","Jianlong Wu","Bin Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09959v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2408.02752 by other authors"},{"id":"http://arxiv.org/abs/2412.09954v1","updated":"2024-12-13T08:24:12Z","published":"2024-12-13T08:24:12Z","title":"$\\textrm{A}^{\\textrm{2}}$RNet: Adversarial Attack Resilient Network for\n  Robust Infrared and Visible Image Fusion","summary":"  Infrared and visible image fusion (IVIF) is a crucial technique for enhancing\nvisual performance by integrating unique information from different modalities\ninto one fused image. Exiting methods pay more attention to conducting fusion\nwith undisturbed data, while overlooking the impact of deliberate interference\non the effectiveness of fusion results. To investigate the robustness of fusion\nmodels, in this paper, we propose a novel adversarial attack resilient network,\ncalled $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial\nparadigm with an anti-attack loss function to implement adversarial attacks and\ntraining. It is constructed based on the intrinsic nature of IVIF and provide a\nrobust foundation for future research advancements. We adopt a Unet as the\npipeline with a transformer-based defensive refinement module (DRM) under this\nparadigm, which guarantees fused image quality in a robust coarse-to-fine\nmanner. Compared to previous works, our method mitigates the adverse effects of\nadversarial perturbations, consistently maintaining high-fidelity fusion\nresults. Furthermore, the performance of downstream tasks can also be well\nmaintained under adversarial attacks. Code is available at\nhttps://github.com/lok-18/A2RNet.\n","authors":["Jiawei Li","Hongwei Yu","Jiansheng Chen","Xinlong Ding","Jinlong Wang","Jinyuan Liu","Bochao Zou","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.09954v1.pdf","comment":"9 pages, 8 figures, The 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.09951v1","updated":"2024-12-13T08:14:24Z","published":"2024-12-13T08:14:24Z","title":"WiseAD: Knowledge Augmented End-to-End Autonomous Driving with\n  Vision-Language Model","summary":"  The emergence of general human knowledge and impressive logical reasoning\ncapacity in rapidly progressed vision-language models (VLMs) have driven\nincreasing interest in applying VLMs to high-level autonomous driving tasks,\nsuch as scene understanding and decision-making. However, an in-depth study on\nthe relationship between knowledge proficiency, especially essential driving\nexpertise, and closed-loop autonomous driving performance requires further\nexploration. In this paper, we investigate the effects of the depth and breadth\nof fundamental driving knowledge on closed-loop trajectory planning and\nintroduce WiseAD, a specialized VLM tailored for end-to-end autonomous driving\ncapable of driving reasoning, action justification, object recognition, risk\nanalysis, driving suggestions, and trajectory planning across diverse\nscenarios. We employ joint training on driving knowledge and planning datasets,\nenabling the model to perform knowledge-aligned trajectory planning\naccordingly. Extensive experiments indicate that as the diversity of driving\nknowledge extends, critical accidents are notably reduced, contributing 11.9%\nand 12.4% improvements in the driving score and route completion on the Carla\nclosed-loop evaluations, achieving state-of-the-art performance. Moreover,\nWiseAD also demonstrates remarkable performance in knowledge evaluations on\nboth in-domain and out-of-domain datasets.\n","authors":["Songyan Zhang","Wenhui Huang","Zihui Gao","Hao Chen","Chen Lv"],"pdf_url":"https://arxiv.org/pdf/2412.09951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07689v3","updated":"2024-12-13T08:13:44Z","published":"2024-12-10T17:27:32Z","title":"DriveMM: All-in-One Large Multimodal Model for Autonomous Driving","summary":"  Large Multimodal Models (LMMs) have demonstrated exceptional comprehension\nand interpretation capabilities in Autonomous Driving (AD) by incorporating\nlarge language models. Despite the advancements, current data-driven AD\napproaches tend to concentrate on a single dataset and specific tasks,\nneglecting their overall capabilities and ability to generalize. To bridge\nthese gaps, we propose DriveMM, a general large multimodal model designed to\nprocess diverse data inputs, such as images and multi-view videos, while\nperforming a broad spectrum of AD tasks, including perception, prediction, and\nplanning. Initially, the model undergoes curriculum pre-training to process\nvaried visual signals and perform basic visual comprehension and perception\ntasks. Subsequently, we augment and standardize various AD-related datasets to\nfine-tune the model, resulting in an all-in-one LMM for autonomous driving. To\nassess the general capabilities and generalization ability, we conduct\nevaluations on six public benchmarks and undertake zero-shot transfer on an\nunseen dataset, where DriveMM achieves state-of-the-art performance across all\ntasks. We hope DriveMM as a promising solution for future end-to-end autonomous\ndriving applications in the real world. Project page with code:\nhttps://github.com/zhijian11/DriveMM.\n","authors":["Zhijian Huang","Chengjian Feng","Feng Yan","Baihui Xiao","Zequn Jie","Yujie Zhong","Xiaodan Liang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.07689v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09296v2","updated":"2024-12-13T08:11:13Z","published":"2024-12-12T14:12:07Z","title":"GoHD: Gaze-oriented and Highly Disentangled Portrait Animation with\n  Rhythmic Poses and Realistic Expression","summary":"  Audio-driven talking head generation necessitates seamless integration of\naudio and visual data amidst the challenges posed by diverse input portraits\nand intricate correlations between audio and facial motions. In response, we\npropose a robust framework GoHD designed to produce highly realistic,\nexpressive, and controllable portrait videos from any reference identity with\nany motion. GoHD innovates with three key modules: Firstly, an animation module\nutilizing latent navigation is introduced to improve the generalization ability\nacross unseen input styles. This module achieves high disentanglement of motion\nand identity, and it also incorporates gaze orientation to rectify unnatural\neye movements that were previously overlooked. Secondly, a conformer-structured\nconditional diffusion model is designed to guarantee head poses that are aware\nof prosody. Thirdly, to estimate lip-synchronized and realistic expressions\nfrom the input audio within limited training data, a two-stage training\nstrategy is devised to decouple frequent and frame-wise lip motion distillation\nfrom the generation of other more temporally dependent but less audio-related\nmotions, e.g., blinks and frowns. Extensive experiments validate GoHD's\nadvanced generalization capabilities, demonstrating its effectiveness in\ngenerating realistic talking face results on arbitrary subjects.\n","authors":["Ziqi Zhou","Weize Quan","Hailin Shi","Wei Li","Lili Wang","Dong-Ming Yan"],"pdf_url":"https://arxiv.org/pdf/2412.09296v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09945v1","updated":"2024-12-13T08:10:47Z","published":"2024-12-13T08:10:47Z","title":"Going Beyond Feature Similarity: Effective Dataset distillation based on\n  Class-aware Conditional Mutual Information","summary":"  Dataset distillation (DD) aims to minimize the time and memory consumption\nneeded for training deep neural networks on large datasets, by creating a\nsmaller synthetic dataset that has similar performance to that of the full real\ndataset. However, current dataset distillation methods often result in\nsynthetic datasets that are excessively difficult for networks to learn from,\ndue to the compression of a substantial amount of information from the original\ndata through metrics measuring feature similarity, e,g., distribution matching\n(DM). In this work, we introduce conditional mutual information (CMI) to assess\nthe class-aware complexity of a dataset and propose a novel method by\nminimizing CMI. Specifically, we minimize the distillation loss while\nconstraining the class-aware complexity of the synthetic dataset by minimizing\nits empirical CMI from the feature space of pre-trained networks,\nsimultaneously. Conducting on a thorough set of experiments, we show that our\nmethod can serve as a general regularization method to existing DD methods and\nimprove the performance and training efficiency.\n","authors":["Xinhao Zhong","Bin Chen","Hao Fang","Xulin Gu","Shu-Tao Xia","En-Hui Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09938v1","updated":"2024-12-13T07:57:31Z","published":"2024-12-13T07:57:31Z","title":"Pixel Intensity Tracking for Remote Respiratory Monitoring: A Study on\n  Indonesian Subject","summary":"  Respiratory rate is a vital sign indicating various health conditions.\nTraditional contact-based measurement methods are often uncomfortable, and\nalternatives like respiratory belts and smartwatches have limitations in cost\nand operability. Therefore, a non-contact method based on Pixel Intensity\nChanges (PIC) with RGB camera images is proposed. Experiments involved 3 sizes\nof bounding boxes, 3 filter options (Laplacian, Sobel, and no filter), and 2\ncorner detection algorithms (ShiTomasi and Harris), with tracking using the\nLukas-Kanade algorithm. Eighteen configurations were tested on 67 subjects in\nstatic and dynamic conditions. The best results in static conditions were\nachieved with the Medium Bounding box, Sobel Filter, and Harris Method (MAE:\n0.85, RMSE: 1.49). In dynamic conditions, the Large Bounding box with no filter\nand ShiTomasi, and Medium Bounding box with no filter and Harris, produced the\nlowest MAE (0.81) and RMSE (1.35)\n","authors":["Muhammad Yahya Ayyashy Mujahidan","Martin Clinton Tosima Manullang"],"pdf_url":"https://arxiv.org/pdf/2412.09938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09936v1","updated":"2024-12-13T07:51:32Z","published":"2024-12-13T07:51:32Z","title":"CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven\n  Visual Language Models","summary":"  The obesity phenomenon, known as the heavy issue, is a leading cause of\npreventable chronic diseases worldwide. Traditional calorie estimation tools\noften rely on specific data formats or complex pipelines, limiting their\npracticality in real-world scenarios. Recently, vision-language models (VLMs)\nhave excelled in understanding real-world contexts and enabling conversational\ninteractions, making them ideal for downstream tasks such as ingredient\nanalysis. However, applying VLMs to calorie estimation requires domain-specific\ndata and alignment strategies. To this end, we curated CalData, a 330K\nimage-text pair dataset tailored for ingredient recognition and calorie\nestimation, combining a large-scale recipe dataset with detailed nutritional\ninstructions for robust vision-language training. Built upon this dataset, we\npresent CaLoRAify, a novel VLM framework aligning ingredient recognition and\ncalorie estimation via training with visual-text pairs. During inference, users\nonly need a single monocular food image to estimate calories while retaining\nthe flexibility of agent-based conversational interaction. With Low-rank\nAdaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our\nsystem enhances the performance of foundational VLMs in the vertical domain of\ncalorie estimation. Our code and data are fully open-sourced at\nhttps://github.com/KennyYao2001/16824-CaLORAify.\n","authors":["Dongyu Yao","Keling Yao","Junhong Zhou","Yinghao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09936v1.pdf","comment":"Disclaimer: This work is part of a course project and reflects\n  ongoing exploration in the field of vision-language models and calorie\n  estimation. Findings and conclusions are subject to further validation and\n  refinement"},{"id":"http://arxiv.org/abs/2412.09927v1","updated":"2024-12-13T07:28:14Z","published":"2024-12-13T07:28:14Z","title":"Neural Vector Tomography for Reconstructing a Magnetization Vector Field","summary":"  Discretized techniques for vector tomographic reconstructions are prone to\nproducing artifacts in the reconstructions. The quality of these\nreconstructions may further deteriorate as the amount of noise increases. In\nthis work, we instead model the underlying vector fields using smooth neural\nfields. Owing to the fact that the activation functions in the neural network\nmay be chosen to be smooth and the domain is no longer pixelated, the model\nresults in high-quality reconstructions, even under presence of noise. In the\ncase where we have underlying global continuous symmetry, we find that the\nneural network substantially improves the accuracy of the reconstruction over\nthe existing techniques.\n","authors":["Giorgi Butbaia","Jiadong Zang"],"pdf_url":"https://arxiv.org/pdf/2412.09927v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.09066v3","updated":"2024-12-13T07:26:51Z","published":"2024-02-14T10:24:04Z","title":"Solid Waste Detection, Monitoring and Mapping in Remote Sensing Images:\n  A Survey","summary":"  The detection and characterization of illegal solid waste disposal sites are\nessential for environmental protection, particularly for mitigating pollution\nand health hazards. Improperly managed landfills contaminate soil and\ngroundwater via rainwater infiltration, posing threats to both animals and\nhumans. Traditional landfill identification approaches, such as on-site\ninspections, are time-consuming and expensive. Remote sensing is a\ncost-effective solution for the identification and monitoring of solid waste\ndisposal sites that enables broad coverage and repeated acquisitions over time.\nEarth Observation (EO) satellites, equipped with an array of sensors and\nimaging capabilities, have been providing high-resolution data for several\ndecades. Researchers proposed specialized techniques that leverage remote\nsensing imagery to perform a range of tasks such as waste site detection,\ndumping site monitoring, and assessment of suitable locations for new\nlandfills. This review aims to provide a detailed illustration of the most\nrelevant proposals for the detection and monitoring of solid waste sites by\ndescribing and comparing the approaches, the implemented techniques, and the\nemployed data. Furthermore, since the data sources are of the utmost importance\nfor developing an effective solid waste detection model, a comprehensive\noverview of the satellites and publicly available data sets is presented.\nFinally, this paper identifies the open issues in the state-of-the-art and\ndiscusses the relevant research directions for reducing the costs and improving\nthe effectiveness of novel solid waste detection methods.\n","authors":["Piero Fraternali","Luca Morandini","Sergio Luis Herrera González"],"pdf_url":"https://arxiv.org/pdf/2402.09066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09921v1","updated":"2024-12-13T07:20:35Z","published":"2024-12-13T07:20:35Z","title":"FaceShield: Defending Facial Image against Deepfake Threats","summary":"  The rising use of deepfakes in criminal activities presents a significant\nissue, inciting widespread controversy. While numerous studies have tackled\nthis problem, most primarily focus on deepfake detection. These reactive\nsolutions are insufficient as a fundamental approach for crimes where\nauthenticity verification is not critical. Existing proactive defenses also\nhave limitations, as they are effective only for deepfake models based on\nspecific Generative Adversarial Networks (GANs), making them less applicable in\nlight of recent advancements in diffusion-based models. In this paper, we\npropose a proactive defense method named FaceShield, which introduces novel\nattack strategies targeting deepfakes generated by Diffusion Models (DMs) and\nfacilitates attacks on various existing GAN-based deepfake models through\nfacial feature extractor manipulations. Our approach consists of three main\ncomponents: (i) manipulating the attention mechanism of DMs to exclude\nprotected facial features during the denoising process, (ii) targeting\nprominent facial feature extraction models to enhance the robustness of our\nadversarial perturbation, and (iii) employing Gaussian blur and low-pass\nfiltering techniques to improve imperceptibility while enhancing robustness\nagainst JPEG distortion. Experimental results on the CelebA-HQ and VGGFace2-HQ\ndatasets demonstrate that our method achieves state-of-the-art performance\nagainst the latest deepfake models based on DMs, while also exhibiting\napplicability to GANs and showcasing greater imperceptibility of noise along\nwith enhanced robustness.\n","authors":["Jaehwan Jeong","Sumin In","Sieun Kim","Hannie Shin","Jongheon Jeong","Sang Ho Yoon","Jaewook Chung","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09920v1","updated":"2024-12-13T07:15:52Z","published":"2024-12-13T07:15:52Z","title":"Precision-Enhanced Human-Object Contact Detection via Depth-Aware\n  Perspective Interaction and Object Texture Restoration","summary":"  Human-object contact (HOT) is designed to accurately identify the areas where\nhumans and objects come into contact. Current methods frequently fail to\naccount for scenarios where objects are frequently blocking the view, resulting\nin inaccurate identification of contact areas. To tackle this problem, we\nsuggest using a perspective interaction HOT detector called PIHOT, which\nutilizes a depth map generation model to offer depth information of humans and\nobjects related to the camera, thereby preventing false interaction detection.\nFurthermore, we use mask dilatation and object restoration techniques to\nrestore the texture details in covered areas, improve the boundaries between\nobjects, and enhance the perception of humans interacting with objects.\nMoreover, a spatial awareness perception is intended to concentrate on the\ncharacteristic features close to the points of contact. The experimental\nresults show that the PIHOT algorithm achieves state-of-the-art performance on\nthree benchmark datasets for HOT detection tasks. Compared to the most recent\nDHOT, our method enjoys an average improvement of 13%, 27.5%, 16%, and 18.5% on\nSC-Acc., C-Acc., mIoU, and wIoU metrics, respectively.\n","authors":["Yuxiao Wang","Wenpeng Neng","Zhenao Wei","Yu Lei","Weiying Xue","Nan Zhuang","Yanwu Xu","Xinyu Jiang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09920v1.pdf","comment":"Accepted by AAAl 2025"},{"id":"http://arxiv.org/abs/2412.09919v1","updated":"2024-12-13T07:13:40Z","published":"2024-12-13T07:13:40Z","title":"B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal\n  Tokens","summary":"  Recently, Vision Large Language Models (VLLMs) integrated with vision\nencoders have shown promising performance in vision understanding. The key of\nVLLMs is to encode visual content into sequences of visual tokens, enabling\nVLLMs to simultaneously process both visual and textual content. However,\nunderstanding videos, especially long videos, remain a challenge to VLLMs as\nthe number of visual tokens grows rapidly when encoding videos, resulting in\nthe risk of exceeding the context window of VLLMs and introducing heavy\ncomputation burden. To restrict the number of visual tokens, existing VLLMs\neither: (1) uniformly downsample videos into a fixed number of frames or (2)\nreducing the number of visual tokens encoded from each frame. We argue the\nformer solution neglects the rich temporal cue in videos and the later\noverlooks the spatial details in each frame. In this work, we present\nBalanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively\nleverage task relevant spatio-temporal cues while restricting the number of\nvisual tokens under the VLLM context window length. At the core of our method,\nwe devise a text-conditioned adaptive frame selection module to identify frames\nrelevant to the visual understanding task. The selected frames are then\nde-duplicated using a temporal frame token merging technique. The visual tokens\nof the selected frames are processed through a spatial token sampling module\nand an optional spatial token merging strategy to achieve precise control over\nthe token count. Experimental results show that B-VLLM is effective in\nbalancing the number of frames and visual tokens in video understanding,\nyielding superior performance on various video understanding benchmarks. Our\ncode is available at https://github.com/zhuqiangLu/B-VLLM.\n","authors":["Zhuqiang Lu","Zhenfei Yin","Mengwei He","Zhihui Wang","Zicheng Liu","Zhiyong Wang","Kun Hu"],"pdf_url":"https://arxiv.org/pdf/2412.09919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01500v2","updated":"2024-12-13T07:05:27Z","published":"2024-12-02T13:51:58Z","title":"SF-Loc: A Visual Mapping and Geo-Localization System based on Sparse\n  Visual Structure Frames","summary":"  For high-level geo-spatial applications and intelligent robotics, accurate\nglobal pose information is of crucial importance. Map-aided localization is a\nuniversal approach to overcome the limitations of global navigation satellite\nsystem (GNSS) in challenging environments. However, current solutions face\nchallenges in terms of mapping flexibility, storage burden and re-localization\nperformance. In this work, we present SF-Loc, a lightweight visual mapping and\nmap-aided localization system, whose core idea is the map representation based\non sparse frames with dense but compact depth, termed as visual structure\nframes. In the mapping phase, multi-sensor dense bundle adjustment (MS-DBA) is\napplied to construct geo-referenced visual structure frames. The local\nco-visbility is checked to keep the map sparsity and achieve incremental\nmapping. In the localization phase, coarse-to-fine vision-based localization is\nperformed, in which multi-frame information and the map distribution are fully\nintegrated. To be specific, the concept of spatially smoothed similarity (SSS)\nis proposed to overcome the place ambiguity, and pairwise frame matching is\napplied for efficient and robust pose estimation. Experimental results on the\ncross-season dataset verify the effectiveness of the system. In complex urban\nroad scenarios, the map size is down to 3 MB per kilometer and stable\ndecimeter-level re-localization can be achieved. The code will be made\nopen-source soon (https://github.com/GREAT-WHU/SF-Loc).\n","authors":["Yuxuan Zhou","Xingxing Li","Shengyu Li","Chunxi Xia","Xuanbin Wang","Shaoquan Feng"],"pdf_url":"https://arxiv.org/pdf/2412.01500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09912v1","updated":"2024-12-13T06:59:17Z","published":"2024-12-13T06:59:17Z","title":"All-in-One: Transferring Vision Foundation Models into Stereo Matching","summary":"  As a fundamental vision task, stereo matching has made remarkable progress.\nWhile recent iterative optimization-based methods have achieved promising\nperformance, their feature extraction capabilities still have room for\nimprovement. Inspired by the ability of vision foundation models (VFMs) to\nextract general representations, in this work, we propose AIO-Stereo which can\nflexibly select and transfer knowledge from multiple heterogeneous VFMs to a\nsingle stereo matching model. To better reconcile features between\nheterogeneous VFMs and the stereo matching model and fully exploit prior\nknowledge from VFMs, we proposed a dual-level feature utilization mechanism\nthat aligns heterogeneous features and transfers multi-level knowledge. Based\non the mechanism, a dual-level selective knowledge transfer module is designed\nto selectively transfer knowledge and integrate the advantages of multiple\nVFMs. Experimental results show that AIO-Stereo achieves start-of-the-art\nperformance on multiple datasets and ranks $1^{st}$ on the Middlebury dataset\nand outperforms all the published work on the ETH3D benchmark.\n","authors":["Jingyi Zhou","Haoyu Zhang","Jiakang Yuan","Peng Ye","Tao Chen","Hao Jiang","Meiya Chen","Yangyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09912v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.00477v3","updated":"2024-12-13T06:57:07Z","published":"2024-11-30T13:29:36Z","title":"LineGS : 3D Line Segment Representation on 3D Gaussian Splatting","summary":"  Abstract representations of 3D scenes play a crucial role in computer vision,\nenabling a wide range of applications such as mapping, localization, surface\nreconstruction, and even advanced tasks like SLAM and rendering. Among these\nrepresentations, line segments are widely used because of their ability to\nsuccinctly capture the structural features of a scene. However, existing 3D\nreconstruction methods often face significant challenges. Methods relying on 2D\nprojections suffer from instability caused by errors in multi-view matching and\nocclusions, while direct 3D approaches are hampered by noise and sparsity in 3D\npoint cloud data. This paper introduces LineGS, a novel method that combines\ngeometry-guided 3D line reconstruction with a 3D Gaussian splatting model to\naddress these challenges and improve representation ability. The method\nleverages the high-density Gaussian point distributions along the edge of the\nscene to refine and optimize initial line segments generated from traditional\ngeometric approaches. By aligning these segments with the underlying geometric\nfeatures of the scene, LineGS achieves a more precise and reliable\nrepresentation of 3D structures. The results show significant improvements in\nboth geometric accuracy and model compactness compared to baseline methods.\n","authors":["Chenggang Yang","Yuang Shi"],"pdf_url":"https://arxiv.org/pdf/2412.00477v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09910v1","updated":"2024-12-13T06:56:12Z","published":"2024-12-13T06:56:12Z","title":"Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on\n  Breast Ultrasound Images","summary":"  Deep neural networks (DNNs) offer significant promise for improving breast\ncancer diagnosis in medical imaging. However, these models are highly\nsusceptible to adversarial attacks--small, imperceptible changes that can\nmislead classifiers--raising critical concerns about their reliability and\nsecurity. Traditional attacks rely on fixed-norm perturbations, misaligning\nwith human perception. In contrast, diffusion-based attacks require pre-trained\nmodels, demanding substantial data when these models are unavailable, limiting\npractical use in data-scarce scenarios. In medical imaging, however, this is\noften unfeasible due to the limited availability of datasets. Building on\nrecent advancements in learnable prompts, we propose Prompt2Perturb (P2P), a\nnovel language-guided attack method capable of generating meaningful attack\nexamples driven by text instructions. During the prompt learning phase, our\napproach leverages learnable prompts within the text encoder to create subtle,\nyet impactful, perturbations that remain imperceptible while guiding the model\ntowards targeted outcomes. In contrast to current prompt learning-based\napproaches, our P2P stands out by directly updating text embeddings, avoiding\nthe need for retraining diffusion models. Further, we leverage the finding that\noptimizing only the early reverse diffusion steps boosts efficiency while\nensuring that the generated adversarial examples incorporate subtle noise, thus\npreserving ultrasound image quality without introducing noticeable artifacts.\nWe show that our method outperforms state-of-the-art attack techniques across\nthree breast ultrasound datasets in FID and LPIPS. Moreover, the generated\nimages are both more natural in appearance and more effective compared to\nexisting adversarial attacks. Our code will be publicly available\nhttps://github.com/yasamin-med/P2P.\n","authors":["Yasamin Medghalchi","Moein Heidari","Clayton Allard","Leonid Sigal","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2412.09910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11930v3","updated":"2024-12-13T06:54:04Z","published":"2024-11-18T11:54:58Z","title":"AtomThink: A Slow Thinking Framework for Multimodal Mathematical\n  Reasoning","summary":"  In this paper, we address the challenging task of multimodal mathematical\nreasoning by incorporating the ability of ``slow thinking\" into multimodal\nlarge language models (MLLMs). Contrary to existing methods that rely on direct\nor fast thinking, our key idea is to construct long chains of thought (CoT)\nconsisting of atomic actions in a step-by-step manner, guiding MLLMs to perform\ncomplex reasoning. To this end, we design a novel AtomThink framework composed\nof three key modules: (i) a CoT annotation engine that automatically generates\nhigh-quality CoT annotations to address the lack of high-quality visual\nmathematical data; (ii) an atomic step fine-tuning strategy that jointly\noptimizes an MLLM and a policy reward model (PRM) for step-wise reasoning; and\n(iii) four different search strategies that can be applied with the PRM to\ncomplete reasoning. Additionally, we propose AtomMATH, a large-scale multimodal\ndataset of long CoTs, and an atomic capability evaluation metric for\nmathematical tasks. Extensive experimental results show that the proposed\nAtomThink significantly improves the performance of baseline MLLMs, achieving\napproximately 50\\% relative accuracy gains on MathVista and 120\\% on MathVerse.\nTo support the advancement of multimodal slow-thinking models, we will make our\ncode and dataset publicly available on https://github.com/Quinn777/AtomThink.\n","authors":["Kun Xiang","Zhili Liu","Zihao Jiang","Yunshuang Nie","Runhui Huang","Haoxiang Fan","Hanhui Li","Weiran Huang","Yihan Zeng","Jianhua Han","Lanqing Hong","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2411.11930v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09907v1","updated":"2024-12-13T06:52:02Z","published":"2024-12-13T06:52:02Z","title":"IQViC: In-context, Question Adaptive Vision Compressor for Long-term\n  Video Understanding LMMs","summary":"  With the increasing complexity of video data and the need for more efficient\nlong-term temporal understanding, existing long-term video understanding\nmethods often fail to accurately capture and analyze extended video sequences.\nThese methods typically struggle to maintain performance over longer durations\nand to handle the intricate dependencies within the video content. To address\nthese limitations, we propose a simple yet effective large multi-modal model\nframework for long-term video understanding that incorporates a novel visual\ncompressor, the In-context, Question Adaptive Visual Compressor (IQViC). The\nkey idea, inspired by humans' selective attention and in-context memory\nmechanisms, is to introduce a novel visual compressor and incorporate efficient\nmemory management techniques to enhance long-term video question answering. Our\nframework utilizes IQViC, a transformer-based visual compressor, enabling\nquestion-conditioned in-context compression, unlike existing methods that rely\non full video visual features. This selectively extracts relevant information,\nsignificantly reducing memory token requirements. Through extensive experiments\non a new dataset based on InfiniBench for long-term video understanding, and\nstandard benchmarks used for existing methods' evaluation, we demonstrate the\neffectiveness of our proposed IQViC framework and its superiority over\nstate-of-the-art methods in terms of video understanding accuracy and memory\nefficiency.\n","authors":["Sosuke Yamao","Natsuki Miyahara","Yuki Harazono","Shun Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2412.09907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03015v2","updated":"2024-12-13T06:46:21Z","published":"2024-12-04T04:03:12Z","title":"Benchmarking Attention Mechanisms and Consistency Regularization\n  Semi-Supervised Learning for Post-Flood Building Damage Assessment in\n  Satellite Images","summary":"  Post-flood building damage assessment is critical for rapid response and\npost-disaster reconstruction planning. Current research fails to consider the\ndistinct requirements of disaster assessment (DA) from change detection (CD) in\nneural network design. This paper focuses on two key differences: 1) building\nchange features in DA satellite images are more subtle than in CD; 2) DA\ndatasets face more severe data scarcity and label imbalance. To address these\nissues, in terms of model architecture, the research explores the benchmark\nperformance of attention mechanisms in post-flood DA tasks and introduces\nSimple Prior Attention UNet (SPAUNet) to enhance the model's ability to\nrecognize subtle changes, in terms of semi-supervised learning (SSL)\nstrategies, the paper constructs four different combinations of image-level\nlabel category reference distributions for consistent training. Experimental\nresults on flood events of xBD dataset show that SPAUNet performs exceptionally\nwell in supervised learning experiments, achieving a recall of 79.10% and an F1\nscore of 71.32% for damaged classification, outperforming CD methods. The\nresults indicate the necessity of DA task-oriented model design. SSL\nexperiments demonstrate the positive impact of image-level consistency\nregularization on the model. Using pseudo-labels to form the reference\ndistribution for consistency training yields the best results, proving the\npotential of using the category distribution of a large amount of unlabeled\ndata for SSL. This paper clarifies the differences between DA and CD tasks. It\npreliminarily explores model design strategies utilizing prior attention\nmechanisms and image-level consistency regularization, establishing new\npost-flood DA task benchmark methods.\n","authors":["Jiaxi Yu","Tomohiro Fukuda","Nobuyoshi Yabuki"],"pdf_url":"https://arxiv.org/pdf/2412.03015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09901v1","updated":"2024-12-13T06:40:26Z","published":"2024-12-13T06:40:26Z","title":"MulSMo: Multimodal Stylized Motion Generation by Bidirectional Control\n  Flow","summary":"  Generating motion sequences conforming to a target style while adhering to\nthe given content prompts requires accommodating both the content and style. In\nexisting methods, the information usually only flows from style to content,\nwhich may cause conflict between the style and content, harming the\nintegration. Differently, in this work we build a bidirectional control flow\nbetween the style and the content, also adjusting the style towards the\ncontent, in which case the style-content collision is alleviated and the\ndynamics of the style is better preserved in the integration. Moreover, we\nextend the stylized motion generation from one modality, i.e. the style motion,\nto multiple modalities including texts and images through contrastive learning,\nleading to flexible style control on the motion generation. Extensive\nexperiments demonstrate that our method significantly outperforms previous\nmethods across different datasets, while also enabling multimodal signals\ncontrol. The code of our method will be made publicly available.\n","authors":["Zhe Li","Yisheng He","Lei Zhong","Weichao Shen","Qi Zuo","Lingteng Qiu","Zilong Dong","Laurence Tianruo Yang","Weihao Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.09901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09202v2","updated":"2024-12-13T06:38:45Z","published":"2024-12-12T11:56:24Z","title":"Temporal Action Localization with Cross Layer Task Decoupling and\n  Refinement","summary":"  Temporal action localization (TAL) involves dual tasks to classify and\nlocalize actions within untrimmed videos. However, the two tasks often have\nconflicting requirements for features. Existing methods typically employ\nseparate heads for classification and localization tasks but share the same\ninput feature, leading to suboptimal performance. To address this issue, we\npropose a novel TAL method with Cross Layer Task Decoupling and Refinement\n(CLTDR). Based on the feature pyramid of video, CLTDR strategy integrates\nsemantically strong features from higher pyramid layers and detailed\nboundary-aware boundary features from lower pyramid layers to effectively\ndisentangle the action classification and localization tasks. Moreover, the\nmultiple features from cross layers are also employed to refine and align the\ndisentangled classification and regression results. At last, a lightweight\nGated Multi-Granularity (GMG) module is proposed to comprehensively extract and\naggregate video features at instant, local, and global temporal granularities.\nBenefiting from the CLTDR and GMG modules, our method achieves state-of-the-art\nperformance on five challenging benchmarks: THUMOS14, MultiTHUMOS,\nEPIC-KITCHENS-100, ActivityNet-1.3, and HACS. Our code and pre-trained models\nare publicly available at: https://github.com/LiQiang0307/CLTDR-GMG.\n","authors":["Qiang Li","Di Liu","Jun Kong","Sen Li","Hui Xu","Jianzhong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09202v2.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2305.10769v5","updated":"2024-12-13T06:36:28Z","published":"2023-05-18T07:23:12Z","title":"Catch-Up Distillation: You Only Need to Train Once for Accelerating\n  Sampling","summary":"  Diffusion Probability Models (DPMs) have made impressive advancements in\nvarious machine learning domains. However, achieving high-quality synthetic\nsamples typically involves performing a large number of sampling steps, which\nimpedes the possibility of real-time sample synthesis. Traditional accelerated\nsampling algorithms via knowledge distillation rely on pre-trained model\nweights and discrete time step scenarios, necessitating additional training\nsessions to achieve their goals. To address these issues, we propose the\nCatch-Up Distillation (CUD), which encourages the current moment output of the\nvelocity estimation model ``catch up'' with its previous moment output.\nSpecifically, CUD adjusts the original Ordinary Differential Equation (ODE)\ntraining objective to align the current moment output with both the ground\ntruth label and the previous moment output, utilizing Runge-Kutta-based\nmulti-step alignment distillation for precise ODE estimation while preventing\nasynchronous updates. Furthermore, we investigate the design space for CUDs\nunder continuous time-step scenarios and analyze how to determine the suitable\nstrategies. To demonstrate CUD's effectiveness, we conduct thorough ablation\nand comparison experiments on CIFAR-10, MNIST, and ImageNet-64. On CIFAR-10, we\nobtain a FID of 2.80 by sampling in 15 steps under one-session training and the\nnew state-of-the-art FID of 3.37 by sampling in one step with additional\ntraining. This latter result necessitated only 620k iterations with a batch\nsize of 128, in contrast to Consistency Distillation, which demanded 2100k\niterations with a larger batch size of 256. Our code is released at\nhttps://anonymous.4open.science/r/Catch-Up-Distillation-E31F.\n","authors":["Shitong Shao","Xu Dai","Lujun Li","Huanran Chen","Yang Hu","Shouyi Yin"],"pdf_url":"https://arxiv.org/pdf/2305.10769v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09895v1","updated":"2024-12-13T06:30:52Z","published":"2024-12-13T06:30:52Z","title":"Building a Multi-modal Spatiotemporal Expert for Zero-shot Action\n  Recognition with CLIP","summary":"  Zero-shot action recognition (ZSAR) requires collaborative multi-modal\nspatiotemporal understanding. However, finetuning CLIP directly for ZSAR yields\nsuboptimal performance, given its inherent constraints in capturing essential\ntemporal dynamics from both vision and text perspectives, especially when\nencountering novel actions with fine-grained spatiotemporal discrepancies. In\nthis work, we propose Spatiotemporal Dynamic Duo (STDD), a novel CLIP-based\nframework to comprehend multi-modal spatiotemporal dynamics synergistically.\nFor the vision side, we propose an efficient Space-time Cross Attention, which\ncaptures spatiotemporal dynamics flexibly with simple yet effective operations\napplied before and after spatial attention, without adding additional\nparameters or increasing computational complexity. For the semantic side, we\nconduct spatiotemporal text augmentation by comprehensively constructing an\nAction Semantic Knowledge Graph (ASKG) to derive nuanced text prompts. The ASKG\nelaborates on static and dynamic concepts and their interrelations, based on\nthe idea of decomposing actions into spatial appearances and temporal motions.\nDuring the training phase, the frame-level video representations are\nmeticulously aligned with prompt-level nuanced text representations, which are\nconcurrently regulated by the video representations from the frozen CLIP to\nenhance generalizability. Extensive experiments validate the effectiveness of\nour approach, which consistently surpasses state-of-the-art approaches on\npopular video benchmarks (i.e., Kinetics-600, UCF101, and HMDB51) under\nchallenging ZSAR settings. Code is available at\nhttps://github.com/Mia-YatingYu/STDD.\n","authors":["Yating Yu","Congqi Cao","Yueran Zhang","Qinyi Lv","Lingtong Min","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09895v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.02734v2","updated":"2024-12-13T06:17:48Z","published":"2024-12-03T18:18:33Z","title":"MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual\n  Cues","summary":"  3D single object tracking is essential in autonomous driving and robotics.\nExisting methods often struggle with sparse and incomplete point cloud\nscenarios. To address these limitations, we propose a Multimodal-guided Virtual\nCues Projection (MVCP) scheme that generates virtual cues to enrich sparse\npoint clouds. Additionally, we introduce an enhanced tracker MVCTrack based on\nthe generated virtual cues. Specifically, the MVCP scheme seamlessly integrates\nRGB sensors into LiDAR-based systems, leveraging a set of 2D detections to\ncreate dense 3D virtual cues that significantly improve the sparsity of point\nclouds. These virtual cues can naturally integrate with existing LiDAR-based 3D\ntrackers, yielding substantial performance gains. Extensive experiments\ndemonstrate that our method achieves competitive performance on the NuScenes\ndataset.\n","authors":["Zhaofeng Hu","Sifan Zhou","Shibo Zhao","Zhihang Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.02734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03937v2","updated":"2024-12-13T06:15:54Z","published":"2024-12-05T07:35:19Z","title":"AIpparel: A Large Multimodal Generative Model for Digital Garments","summary":"  Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a large\nmultimodal model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. AIpparelachieves\nstate-of-the-art performance in single-modal tasks, including text-to-garment\nand image-to-garment prediction, and enables novel multimodal garment\ngeneration applications such as interactive garment editing. The project\nwebsite is at georgenakayama.github.io/AIpparel/.\n","authors":["Kiyohiro Nakayama","Jan Ackermann","Timur Levent Kesdogan","Yang Zheng","Maria Korosteleva","Olga Sorkine-Hornung","Leonidas J. Guibas","Guandao Yang","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2412.03937v2.pdf","comment":"The project website is at georgenakayama.github.io/AIpparel/"},{"id":"http://arxiv.org/abs/2412.09892v1","updated":"2024-12-13T06:14:57Z","published":"2024-12-13T06:14:57Z","title":"VQTalker: Towards Multilingual Talking Avatars through Facial Motion\n  Tokenization","summary":"  We present VQTalker, a Vector Quantization-based framework for multilingual\ntalking head generation that addresses the challenges of lip synchronization\nand natural motion across diverse languages. Our approach is grounded in the\nphonetic principle that human speech comprises a finite set of distinct sound\nunits (phonemes) and corresponding visual articulations (visemes), which often\nshare commonalities across languages. We introduce a facial motion tokenizer\nbased on Group Residual Finite Scalar Quantization (GRFSQ), which creates a\ndiscretized representation of facial features. This method enables\ncomprehensive capture of facial movements while improving generalization to\nmultiple languages, even with limited training data. Building on this quantized\nrepresentation, we implement a coarse-to-fine motion generation process that\nprogressively refines facial animations. Extensive experiments demonstrate that\nVQTalker achieves state-of-the-art performance in both video-driven and\nspeech-driven scenarios, particularly in multilingual settings. Notably, our\nmethod achieves high-quality results at a resolution of 512*512 pixels while\nmaintaining a lower bitrate of approximately 11 kbps. Our work opens new\npossibilities for cross-lingual talking face generation. Synthetic results can\nbe viewed at https://x-lance.github.io/VQTalker.\n","authors":["Tao Liu","Ziyang Ma","Qi Chen","Feilong Chen","Shuai Fan","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.09892v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2412.09886v1","updated":"2024-12-13T06:01:39Z","published":"2024-12-13T06:01:39Z","title":"T-GMSI: A transformer-based generative model for spatial interpolation\n  under sparse measurements","summary":"  Generating continuous environmental models from sparsely sampled data is a\ncritical challenge in spatial modeling, particularly for topography.\nTraditional spatial interpolation methods often struggle with handling sparse\nmeasurements. To address this, we propose a Transformer-based Generative Model\nfor Spatial Interpolation (T-GMSI) using a vision transformer (ViT)\narchitecture for digital elevation model (DEM) generation under sparse\nconditions. T-GMSI replaces traditional convolution-based methods with ViT for\nfeature extraction and DEM interpolation while incorporating a terrain\nfeature-aware loss function for enhanced accuracy. T-GMSI excels in producing\nhigh-quality elevation surfaces from datasets with over 70% sparsity and\ndemonstrates strong transferability across diverse landscapes without\nfine-tuning. Its performance is validated through extensive experiments,\noutperforming traditional methods such as ordinary Kriging (OK) and natural\nneighbor (NN) and a conditional generative adversarial network (CGAN)-based\nmodel (CEDGAN). Compared to OK and NN, T-GMSI reduces root mean square error\n(RMSE) by 40% and 25% on airborne lidar data and by 23% and 10% on spaceborne\nlidar data. Against CEDGAN, T-GMSI achieves a 20% RMSE improvement on provided\nDEM data, requiring no fine-tuning. The ability of model on generalizing to\nlarge, unseen terrains underscores its transferability and potential\napplicability beyond topographic modeling. This research establishes T-GMSI as\na state-of-the-art solution for spatial interpolation on sparse datasets and\nhighlights its broader utility for other sparse data interpolation challenges.\n","authors":["Xiangxi Tian","Jie Shan"],"pdf_url":"https://arxiv.org/pdf/2412.09886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09881v1","updated":"2024-12-13T05:51:03Z","published":"2024-12-13T05:51:03Z","title":"Sharpening Your Density Fields: Spiking Neuron Aided Fast Geometry\n  Learning","summary":"  Neural Radiance Fields (NeRF) have achieved remarkable progress in neural\nrendering. Extracting geometry from NeRF typically relies on the Marching Cubes\nalgorithm, which uses a hand-crafted threshold to define the level set.\nHowever, this threshold-based approach requires laborious and scenario-specific\ntuning, limiting its practicality for real-world applications. In this work, we\nseek to enhance the efficiency of this method during the training time. To this\nend, we introduce a spiking neuron mechanism that dynamically adjusts the\nthreshold, eliminating the need for manual selection. Despite its promise,\ndirectly training with the spiking neuron often results in model collapse and\nnoisy outputs. To overcome these challenges, we propose a round-robin strategy\nthat stabilizes the training process and enables the geometry network to\nachieve a sharper and more precise density distribution with minimal\ncomputational overhead. We validate our approach through extensive experiments\non both synthetic and real-world datasets. The results show that our method\nsignificantly improves the performance of threshold-based techniques, offering\na more robust and efficient solution for NeRF geometry extraction.\n","authors":["Yi Gu","Zhaorui Wang","Dongjun Ye","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2412.09881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09875v1","updated":"2024-12-13T05:40:50Z","published":"2024-12-13T05:40:50Z","title":"Selective State Space Memory for Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross a wide range of multimodal tasks. However, fine-tuning these models for\ndomain-specific applications remains a computationally intensive challenge.\nThis paper introduces State Space Memory Integration (SSMI), a novel approach\nfor efficient fine-tuning of LVLMs. By integrating lightweight Mamba-based\nstate space modules into the LVLM architecture, SSMI captures long-range\ndependencies and injects task-specific visual and sequential patterns\neffectively. Unlike traditional fine-tuning methods, SSMI requires only a\nfraction of the model's parameters to be updated, making it computationally\nefficient and scalable. Experiments on benchmark datasets, including COCO\nCaptioning, VQA, and Flickr30k, demonstrate that SSMI achieves state-of-the-art\nperformance while maintaining robustness and generalization capabilities.\nComprehensive analysis further validates the advantages of SSMI in terms of\nefficiency, adaptability, and interpretability, positioning it as a compelling\nsolution for fine-tuning large-scale vision-language models.\n","authors":["Chee Ng","Yuen Fung"],"pdf_url":"https://arxiv.org/pdf/2412.09875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09874v1","updated":"2024-12-13T05:40:20Z","published":"2024-12-13T05:40:20Z","title":"Can Students Beyond The Teacher? Distilling Knowledge from Teacher's\n  Bias","summary":"  Knowledge distillation (KD) is a model compression technique that transfers\nknowledge from a large teacher model to a smaller student model to enhance its\nperformance. Existing methods often assume that the student model is inherently\ninferior to the teacher model. However, we identify that the fundamental issue\naffecting student performance is the bias transferred by the teacher. Current\nKD frameworks transmit both right and wrong knowledge, introducing bias that\nmisleads the student model. To address this issue, we propose a novel strategy\nto rectify bias and greatly improve the student model's performance. Our\nstrategy involves three steps: First, we differentiate knowledge and design a\nbias elimination method to filter out biases, retaining only the right\nknowledge for the student model to learn. Next, we propose a bias rectification\nmethod to rectify the teacher model's wrong predictions, fundamentally\naddressing bias interference. The student model learns from both the right\nknowledge and the rectified biases, greatly improving its prediction accuracy.\nAdditionally, we introduce a dynamic learning approach with a loss function\nthat updates weights dynamically, allowing the student model to quickly learn\nright knowledge-based easy tasks initially and tackle hard tasks corresponding\nto biases later, greatly enhancing the student model's learning efficiency. To\nthe best of our knowledge, this is the first strategy enabling the student\nmodel to surpass the teacher model. Experiments demonstrate that our strategy,\nas a plug-and-play module, is versatile across various mainstream KD\nframeworks. We will release our code after the paper is accepted.\n","authors":["Jianhua Zhang","Yi Gao","Ruyu Liu","Xu Cheng","Houxiang Zhang","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09870v1","updated":"2024-12-13T05:29:37Z","published":"2024-12-13T05:29:37Z","title":"Dynamic Cross-Modal Alignment for Robust Semantic Location Prediction","summary":"  Semantic location prediction from multimodal social media posts is a critical\ntask with applications in personalized services and human mobility analysis.\nThis paper introduces \\textit{Contextualized Vision-Language Alignment\n(CoVLA)}, a discriminative framework designed to address the challenges of\ncontextual ambiguity and modality discrepancy inherent in this task. CoVLA\nleverages a Contextual Alignment Module (CAM) to enhance cross-modal feature\nalignment and a Cross-modal Fusion Module (CMF) to dynamically integrate\ntextual and visual information. Extensive experiments on a benchmark dataset\ndemonstrate that CoVLA significantly outperforms state-of-the-art methods,\nachieving improvements of 2.3\\% in accuracy and 2.5\\% in F1-score. Ablation\nstudies validate the contributions of CAM and CMF, while human evaluations\nhighlight the contextual relevance of the predictions. Additionally, robustness\nanalysis shows that CoVLA maintains high performance under noisy conditions,\nmaking it a reliable solution for real-world applications. These results\nunderscore the potential of CoVLA in advancing semantic location prediction\nresearch.\n","authors":["Liu Jing","Amirul Rahman"],"pdf_url":"https://arxiv.org/pdf/2412.09870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09868v1","updated":"2024-12-13T05:27:35Z","published":"2024-12-13T05:27:35Z","title":"RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian\n  Splatting","summary":"  3D Gaussian Splatting has emerged as a promising technique for high-quality\n3D rendering, leading to increasing interest in integrating 3DGS into realism\nSLAM systems. However, existing methods face challenges such as Gaussian\nprimitives redundancy, forgetting problem during continuous optimization, and\ndifficulty in initializing primitives in monocular case due to lack of depth\ninformation. In order to achieve efficient and photorealistic mapping, we\npropose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular\nand RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian\nprimitives optimization and consists of three key components. Firstly, we\npropose an efficient incremental mapping approach to achieve a compact and\naccurate representation of the scene through adaptive sampling and Gaussian\nprimitives filtering. Secondly, a dynamic window optimization method is\nproposed to mitigate the forgetting problem and improve map consistency.\nFinally, for the monocular case, a monocular keyframe initialization method\nbased on sparse point cloud is proposed to improve the initialization accuracy\nof Gaussian primitives, which provides a geometric basis for subsequent\noptimization. The results of numerous experiments demonstrate that RP-SLAM\nachieves state-of-the-art map rendering accuracy while ensuring real-time\nperformance and model compactness.\n","authors":["Lizhi Bai","Chunqi Tian","Jun Yang","Siyu Zhang","Masanori Suganuma","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2412.09868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08966v3","updated":"2024-12-13T04:59:10Z","published":"2024-02-14T06:20:48Z","title":"Pretraining Vision-Language Model for Difference Visual Question\n  Answering in Longitudinal Chest X-rays","summary":"  Difference visual question answering (diff-VQA) is a challenging task that\nrequires answering complex questions based on differences between a pair of\nimages. This task is particularly important in reading chest X-ray images\nbecause radiologists often compare multiple images of the same patient taken at\ndifferent times to track disease progression and changes in its severity in\ntheir clinical practice. However, previous works focused on designing specific\nnetwork architectures for the diff-VQA task, missing opportunities to enhance\nthe model's performance using a pretrained vision-language model (VLM). Here,\nwe introduce a novel VLM called PLURAL, which is pretrained on natural and\nlongitudinal chest X-ray data for the diff-VQA task. The model is developed\nusing a step-by-step approach, starting with being pretrained on natural images\nand texts, followed by being trained using longitudinal chest X-ray data. The\nlongitudinal data consist of pairs of X-ray images, along with question-answer\nsets and radiologist's reports that describe the changes in lung abnormalities\nand diseases over time. Our experimental results show that the PLURAL model\noutperforms state-of-the-art methods not only in diff-VQA for longitudinal\nX-rays but also in conventional VQA for a single X-ray image. Through extensive\nexperiments, we demonstrate the effectiveness of the proposed VLM architecture\nand pretraining method in improving the model's performance.\n","authors":["Yeongjae Cho","Taehee Kim","Heejun Shin","Sungzoon Cho","Dongmyung Shin"],"pdf_url":"https://arxiv.org/pdf/2402.08966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09616v2","updated":"2024-12-13T04:58:33Z","published":"2024-12-12T18:59:46Z","title":"V2PE: Improving Multimodal Long-Context Capability of Vision-Language\n  Models with Variable Visual Position Encoding","summary":"  Vision-Language Models (VLMs) have shown promising capabilities in handling\nvarious multimodal tasks, yet they struggle in long-context scenarios,\nparticularly in tasks involving videos, high-resolution images, or lengthy\nimage-text documents. In our work, we first conduct an empirical analysis of\nthe long-context capabilities of VLMs using our augmented long-context\nmultimodal datasets. Our findings reveal that directly applying the positional\nencoding mechanism used for textual tokens to visual tokens is suboptimal, and\nVLM performance degrades sharply when the position encoding exceeds the model's\ncontext window. To address this, we propose Variable Visual Position Encoding\n(V2PE), a novel positional encoding approach that employs variable and smaller\nincrements for visual tokens, enabling more efficient management of long\nmultimodal sequences. Our experiments demonstrate the effectiveness of V2PE to\nenhances VLMs' ability to effectively understand and reason over long\nmultimodal contexts. We further integrate V2PE with our augmented long-context\nmultimodal datasets to fine-tune the open-source VLM, InternVL2. The fine-tuned\nmodel achieves strong performance on both standard and long-context multimodal\ntasks. Notably, when the sequence length of the training dataset is increased\nto 256K tokens, the model is capable of processing multimodal sequences up to\n1M tokens, highlighting its potential for real-world long-context applications.\n","authors":["Junqi Ge","Ziyi Chen","Jintao Lin","Jinguo Zhu","Xihui Liu","Jifeng Dai","Xizhou Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.09616v2.pdf","comment":"The code and models will be available at\n  https://github.com/OpenGVLab/V2PE"},{"id":"http://arxiv.org/abs/2412.09856v1","updated":"2024-12-13T04:55:10Z","published":"2024-12-13T04:55:10Z","title":"LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation\n  with Linear Computational Complexity","summary":"  Text-to-video generation enhances content creation but is highly\ncomputationally intensive: The computational cost of Diffusion Transformers\n(DiTs) scales quadratically in the number of pixels. This makes minute-length\nvideo generation extremely expensive, limiting most existing models to\ngenerating videos of only 10-20 seconds length. We propose a Linear-complexity\ntext-to-video Generation (LinGen) framework whose cost scales linearly in the\nnumber of pixels. For the first time, LinGen enables high-resolution\nminute-length video generation on a single GPU without compromising quality. It\nreplaces the computationally-dominant and quadratic-complexity block,\nself-attention, with a linear-complexity block called MATE, which consists of\nan MA-branch and a TE-branch. The MA-branch targets short-to-long-range\ncorrelations, combining a bidirectional Mamba2 block with our token\nrearrangement method, Rotary Major Scan, and our review tokens developed for\nlong video generation. The TE-branch is a novel TEmporal Swin Attention block\nthat focuses on temporal correlations between adjacent tokens and medium-range\ntokens. The MATE block addresses the adjacency preservation issue of Mamba and\nimproves the consistency of generated videos significantly. Experimental\nresults show that LinGen outperforms DiT (with a 75.6% win rate) in video\nquality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction.\nFurthermore, both automatic metrics and human evaluation demonstrate our\nLinGen-4B yields comparable video quality to state-of-the-art models (with a\n50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling,\nrespectively). This paves the way to hour-length movie generation and real-time\ninteractive video generation. We provide 68s video generation results and more\nexamples in our project website: https://lineargen.github.io/.\n","authors":["Hongjie Wang","Chih-Yao Ma","Yen-Cheng Liu","Ji Hou","Tao Xu","Jialiang Wang","Felix Juefei-Xu","Yaqiao Luo","Peizhao Zhang","Tingbo Hou","Peter Vajda","Niraj K. Jha","Xiaoliang Dai"],"pdf_url":"https://arxiv.org/pdf/2412.09856v1.pdf","comment":"20 pages, 20 figures"},{"id":"http://arxiv.org/abs/2412.09324v2","updated":"2024-12-13T04:51:11Z","published":"2024-12-12T14:49:55Z","title":"Are Conditional Latent Diffusion Models Effective for Image Restoration?","summary":"  Recent advancements in image restoration increasingly employ conditional\nlatent diffusion models (CLDMs). While these models have demonstrated notable\nperformance improvements in recent years, this work questions their suitability\nfor IR tasks. CLDMs excel in capturing high-level semantic correlations, making\nthem effective for tasks like text-to-image generation with spatial\nconditioning. However, in IR, where the goal is to enhance image perceptual\nquality, these models face difficulty of modeling the relationship between\ndegraded images and ground truth images using a low-level representation. To\nsupport our claims, we compare state-of-the-art CLDMs with traditional image\nrestoration models through extensive experiments. Results reveal that despite\nthe scaling advantages of CLDMs, they suffer from high distortion and semantic\ndeviation, especially in cases with minimal degradation, where traditional\nmethods outperform them. Additionally, we perform empirical studies to examine\nthe impact of various CLDM design elements on their restoration performance. We\nhope this finding inspires a reexamination of current CLDM-based IR solutions,\nopening up more opportunities in this field.\n","authors":["Yunchen Yuan","Junyuan Xiao","Xinjie Li"],"pdf_url":"https://arxiv.org/pdf/2412.09324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13139v2","updated":"2024-12-13T04:45:39Z","published":"2024-10-17T01:51:58Z","title":"See Behind Walls in Real-time Using Aerial Drones and Augmented Reality","summary":"  This work presents ARD2, a framework that enables real-time through-wall\nsurveillance using two aerial drones and an augmented reality (AR) device. ARD2\nconsists of two main steps: target direction estimation and contour\nreconstruction. In the first stage, ARD2 leverages geometric relationships\nbetween the drones, the user, and the target to project the target's direction\nonto the user's AR display. In the second stage, images from the drones are\nsynthesized to reconstruct the target's contour, allowing the user to visualize\nthe target behind walls. Experimental results demonstrate the system's accuracy\nin both direction estimation and contour reconstruction.\n","authors":["Sikai Yang","Kang Yang","Yuning Chen","Fan Zhao","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2410.13139v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2409.19454v4","updated":"2024-12-13T04:44:31Z","published":"2024-09-28T20:40:18Z","title":"See Where You Read with Eye Gaze Tracking and Large Language Model","summary":"  Losing track of reading progress during line switching can be frustrating.\nEye gaze tracking technology offers a potential solution by highlighting read\nparagraphs, aiding users in avoiding wrong line switches. However, the gap\nbetween gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes\ndirect application impractical. Existing methods leverage the linear reading\npattern but fail during jump reading. This paper presents a reading tracking\nand highlighting system that supports both linear and jump reading. Based on\nexperimental insights from the gaze nature study of 16 users, two gaze error\nmodels are designed to enable both jump reading detection and relocation. The\nsystem further leverages the large language model's contextual perception\ncapability in aiding reading tracking. A reading tracking domain-specific\nline-gaze alignment opportunity is also exploited to enable dynamic and\nfrequent calibration of the gaze results. Controlled experiments demonstrate\nreliable linear reading tracking, as well as 84% accuracy in tracking jump\nreading. Furthermore, real field tests with 18 volunteers demonstrated the\nsystem's effectiveness in tracking and highlighting read paragraphs, improving\nreading efficiency, and enhancing user experience.\n","authors":["Sikai Yang","Gang Yan","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2409.19454v4.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2409.19554v4","updated":"2024-12-13T04:43:40Z","published":"2024-09-29T04:43:10Z","title":"Tri-Cam: Practical Eye Gaze Tracking via Camera Network","summary":"  As human eyes serve as conduits of rich information, unveiling emotions,\nintentions, and even aspects of an individual's health and overall well-being,\ngaze tracking also enables various human-computer interaction applications, as\nwell as insights in psychological and medical research. However, existing gaze\ntracking solutions fall short at handling free user movement, and also require\nlaborious user effort in system calibration. We introduce Tri-Cam, a practical\ndeep learning-based gaze tracking system using three affordable RGB webcams. It\nfeatures a split network structure for efficient training, as well as\ndesignated network designs to handle the separated gaze tracking tasks. Tri-Cam\nis also equipped with an implicit calibration module, which makes use of mouse\nclick opportunities to reduce calibration overhead on the user's end. We\nevaluate Tri-Cam against Tobii, the state-of-the-art commercial eye tracker,\nachieving comparable accuracy, while supporting a wider free movement area. In\nconclusion, Tri-Cam provides a user-friendly, affordable, and robust gaze\ntracking solution that could practically enable various applications.\n","authors":["Sikai Yang","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2409.19554v4.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.09846v1","updated":"2024-12-13T04:29:08Z","published":"2024-12-13T04:29:08Z","title":"A Single-Frame and Multi-Frame Cascaded Image Super-Resolution Method","summary":"  The objective of image super-resolution is to reconstruct a high-resolution\n(HR) image with the prior knowledge from one or several low-resolution (LR)\nimages. However, in the real world, due to the limited complementary\ninformation, the performance of both single-frame and multi-frame\nsuper-resolution reconstruction degrades rapidly as the magnification\nincreases. In this paper, we propose a novel two-step image super resolution\nmethod concatenating multi-frame super-resolution (MFSR) with single-frame\nsuper-resolution (SFSR), to progressively upsample images to the desired\nresolution. The proposed method consisting of an L0-norm constrained\nreconstruction scheme and an enhanced residual back-projection network,\nintegrating the flexibility of the variational modelbased method and the\nfeature learning capacity of the deep learning-based method. To verify the\neffectiveness of the proposed algorithm, extensive experiments with both\nsimulated and real world sequences were implemented. The experimental results\nshow that the proposed method yields superior performance in both objective and\nperceptual quality measurements. The average PSNRs of the cascade model in set5\nand set14 are 33.413 dB and 29.658 dB respectively, which are 0.76 dB and 0.621\ndB more than the baseline method. In addition, the experiment indicates that\nthis cascade model can be robustly applied to different SFSR and MFSR methods.\n","authors":["Jing Sun","Qiangqiang Yuan","Huanfeng Shen","Jie Li","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09846v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.09844v1","updated":"2024-12-13T04:27:08Z","published":"2024-12-13T04:27:08Z","title":"Real-time Identity Defenses against Malicious Personalization of\n  Diffusion Models","summary":"  Personalized diffusion models, capable of synthesizing highly realistic\nimages based on a few reference portraits, pose substantial social, ethical,\nand legal risks by enabling identity replication. Existing defense mechanisms\nrely on computationally intensive adversarial perturbations tailored to\nindividual images, rendering them impractical for real-world deployment. This\nstudy introduces Real-time Identity Defender (RID), a neural network designed\nto generate adversarial perturbations through a single forward pass, bypassing\nthe need for image-specific optimization. RID achieves unprecedented\nefficiency, with defense times as low as 0.12 seconds on a single GPU (4,400\ntimes faster than leading methods) and 1.1 seconds per image on a standard\nIntel i9 CPU, making it suitable for edge devices such as smartphones. Despite\nits efficiency, RID matches state-of-the-art performance across visual and\nquantitative benchmarks, effectively mitigating identity replication risks. Our\nanalysis reveals that RID's perturbations mimic the efficacy of traditional\ndefenses while exhibiting properties distinct from natural noise, such as\nGaussian perturbations. To enhance robustness, we extend RID into an ensemble\nframework that integrates multiple pre-trained text-to-image diffusion models,\nensuring resilience against black-box attacks and post-processing techniques,\nincluding JPEG compression and diffusion-based purification.\n","authors":["Hanzhong Guo","Shen Nie","Chao Du","Tianyu Pang","Hao Sun","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2412.09844v1.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.09842v1","updated":"2024-12-13T04:22:23Z","published":"2024-12-13T04:22:23Z","title":"Leveraging Programmatically Generated Synthetic Data for Differentially\n  Private Diffusion Training","summary":"  Programmatically generated synthetic data has been used in differential\nprivate training for classification to enhance performance without privacy\nleakage. However, as the synthetic data is generated from a random process, the\ndistribution of real data and the synthetic data are distinguishable and\ndifficult to transfer. Therefore, the model trained with the synthetic data\ngenerates unrealistic random images, raising challenges to adapt the synthetic\ndata for generative models. In this work, we propose DP-SynGen, which leverages\nprogrammatically generated synthetic data in diffusion models to address this\nchallenge. By exploiting the three stages of diffusion models(coarse, context,\nand cleaning) we identify stages where synthetic data can be effectively\nutilized. We theoretically and empirically verified that cleaning and coarse\nstages can be trained without private data, replacing them with synthetic data\nto reduce the privacy budget. The experimental results show that DP-SynGen\nimproves the quality of generative data by mitigating the negative impact of\nprivacy-induced noise on the generation process.\n","authors":["Yujin Choi","Jinseong Park","Junyoung Byun","Jaewook Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09841v1","updated":"2024-12-13T04:19:48Z","published":"2024-12-13T04:19:48Z","title":"Super-Resolution for Remote Sensing Imagery via the Coupling of a\n  Variational Model and Deep Learning","summary":"  Image super-resolution (SR) is an effective way to enhance the spatial\nresolution and detail information of remote sensing images, to obtain a\nsuperior visual quality. As SR is severely ill-conditioned, effective image\npriors are necessary to regularize the solution space and generate the\ncorresponding high-resolution (HR) image. In this paper, we propose a novel\ngradient-guided multi-frame super-resolution (MFSR) framework for remote\nsensing imagery reconstruction. The framework integrates a learned gradient\nprior as the regularization term into a model-based optimization method.\nSpecifically, the local gradient regularization (LGR) prior is derived from the\ndeep residual attention network (DRAN) through gradient profile transformation.\nThe non-local total variation (NLTV) prior is characterized using the spatial\nstructure similarity of the gradient patches with the maximum a posteriori\n(MAP) model. The modeled prior performs well in preserving edge smoothness and\nsuppressing visual artifacts, while the learned prior is effective in enhancing\nsharp edges and recovering fine structures. By incorporating the two\ncomplementary priors into an adaptive norm based reconstruction framework, the\nmixed L1 and L2 regularization minimization problem is optimized to achieve the\nrequired HR remote sensing image. Extensive experimental results on remote\nsensing data demonstrate that the proposed method can produce visually pleasant\nimages and is superior to several of the state-of-the-art SR algorithms in\nterms of the quantitative evaluation.\n","authors":["Jing Sun","Huanfeng Shen","Qiangqiang Yuan","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09841v1.pdf","comment":"18 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.07828v2","updated":"2024-12-13T04:12:25Z","published":"2024-06-12T02:48:52Z","title":"Spatial Annealing for Efficient Few-shot Neural Rendering","summary":"  Neural Radiance Fields (NeRF) with hybrid representations have shown\nimpressive capabilities for novel view synthesis, delivering high efficiency.\nNonetheless, their performance significantly drops with sparse input views.\nVarious regularization strategies have been devised to address these\nchallenges. However, these strategies either require additional rendering costs\nor involve complex pipeline designs, leading to a loss of training efficiency.\nAlthough FreeNeRF has introduced an efficient frequency annealing strategy, its\noperation on frequency positional encoding is incompatible with the efficient\nhybrid representations. In this paper, we introduce an accurate and efficient\nfew-shot neural rendering method named \\textbf{S}patial \\textbf{A}nnealing\nregularized \\textbf{NeRF} (\\textbf{SANeRF}), which adopts the pre-filtering\ndesign of a hybrid representation. We initially establish the analytical\nformulation of the frequency band limit for a hybrid architecture by deducing\nits filtering process. Based on this analysis, we propose a universal form of\nfrequency annealing in the spatial domain, which can be implemented by\nmodulating the sampling kernel to exponentially shrink from an initial one with\na narrow grid tangent kernel spectrum. This methodology is crucial for\nstabilizing the early stages of the training phase and significantly\ncontributes to enhancing the subsequent process of detail refinement. Our\nextensive experiments reveal that, by adding merely one line of code, SANeRF\ndelivers superior rendering quality and much faster reconstruction speed\ncompared to current few-shot neural rendering methods. Notably, SANeRF\noutperforms FreeNeRF on the Blender dataset, achieving 700$\\times$ faster\nreconstruction speed.\n","authors":["Yuru Xiao","Deming Zhai","Wenbo Zhao","Kui Jiang","Junjun Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07828v2.pdf","comment":"AAAI 2025, code available at https://github.com/pulangk97/SANeRF"},{"id":"http://arxiv.org/abs/2409.07040v3","updated":"2024-12-13T04:00:36Z","published":"2024-09-11T06:12:03Z","title":"Retinex-RAWMamba: Bridging Demosaicing and Denoising for Low-Light RAW\n  Image Enhancement","summary":"  Low-light image enhancement, particularly in cross-domain tasks such as\nmapping from the raw domain to the sRGB domain, remains a significant\nchallenge. Many deep learning-based methods have been developed to address this\nissue and have shown promising results in recent years. However, single-stage\nmethods, which attempt to unify the complex mapping across both domains,\nleading to limited denoising performance. In contrast, two-stage approaches\ntypically decompose a raw image with color filter arrays (CFA) into a\nfour-channel RGGB format before feeding it into a neural network. However, this\nstrategy overlooks the critical role of demosaicing within the Image Signal\nProcessing (ISP) pipeline, leading to color distortions under varying lighting\nconditions, especially in low-light scenarios. To address these issues, we\ndesign a novel Mamba scanning mechanism, called RAWMamba, to effectively handle\nraw images with different CFAs. Furthermore, we present a Retinex Decomposition\nModule (RDM) grounded in Retinex prior, which decouples illumination from\nreflectance to facilitate more effective denoising and automatic non-linear\nexposure correction. By bridging demosaicing and denoising, better raw image\nenhancement is achieved. Experimental evaluations conducted on public datasets\nSID and MCR demonstrate that our proposed RAWMamba achieves state-of-the-art\nperformance on cross-domain mapping.\n","authors":["Xianmin Chen","Peiliang Huang","Xiaoxu Feng","Dingwen Zhang","Longfei Han","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2409.07040v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09835v1","updated":"2024-12-13T03:56:40Z","published":"2024-12-13T03:56:40Z","title":"Which cycling environment appears safer? Learning cycling safety\n  perceptions from pairwise image comparisons","summary":"  Cycling is critical for cities to transition to more sustainable transport\nmodes. Yet, safety concerns remain a critical deterrent for individuals to\ncycle. If individuals perceive an environment as unsafe for cycling, it is\nlikely that they will prefer other means of transportation. Yet, capturing and\nunderstanding how individuals perceive cycling risk is complex and often slow,\nwith researchers defaulting to traditional surveys and in-loco interviews. In\nthis study, we tackle this problem. We base our approach on using pairwise\ncomparisons of real-world images, repeatedly presenting respondents with pairs\nof road environments and asking them to select the one they perceive as safer\nfor cycling, if any. Using the collected data, we train a siamese-convolutional\nneural network using a multi-loss framework that learns from individuals'\nresponses, learns preferences directly from images, and includes ties (often\ndiscarded in the literature). Effectively, this model learns to predict\nhuman-style perceptions, evaluating which cycling environments are perceived as\nsafer. Our model achieves good results, showcasing this approach has a\nreal-life impact, such as improving interventions' effectiveness. Furthermore,\nit facilitates the continuous assessment of changing cycling environments,\npermitting short-term evaluations of measures to enhance perceived cycling\nsafety. Finally, our method can be efficiently deployed in different locations\nwith a growing number of openly available street-view images.\n","authors":["Miguel Costa","Manuel Marques","Carlos Lima Azevedo","Felix Wilhelm Siebert","Filipe Moura"],"pdf_url":"https://arxiv.org/pdf/2412.09835v1.pdf","comment":"\\copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2411.16946v2","updated":"2024-12-13T03:42:00Z","published":"2024-11-25T21:35:42Z","title":"Lens Distortion Encoding System Version 1.0","summary":"  Lens Distortion Encoding System (LDES) allows for a distortion-accurate\nworkflow, with a seamless interchange of high quality motion picture images\nregardless of the lens source. This system is similar in a concept to the\nAcademy Color Encoding System (ACES), but for distortion. Presented solution is\nfully compatible with existing software/plug-in tools for STMapping found in\npopular production software like Adobe After Effects or DaVinci Resolve. LDES\nutilizes common distortion space and produces single high-quality, animatable\nSTMap used for direct transformation of one view to another, neglecting the\nneed of lens-swapping for each shoot. The LDES profile of a lens consist of two\nelements; View Map texture, and Footage Map texture, each labeled with the FOV\nvalue. Direct distortion mapping is produced by sampling of the Footage Map\nthrough the View Map. The result; animatable mapping texture, is then used to\nsample the footage to a desired distortion. While the Footage Map is specific\nto a footage, View Maps can be freely combined/transitioned and animated,\nallowing for effects like smooth shift from anamorphic to spherical distortion,\npreviously impossible to achieve in practice. Presented LDES Version 1.0 uses\ncommon 32-bit STMap format for encoding, supported by most compositing\nsoftware, directly or via plug-ins. The difference between standard STMap\nworkflow and LDES is that it encodes absolute pixel position in the spherical\nimage model. The main benefit of this approach is the ability to achieve a\nsimilar look of a highly expensive lens using some less expensive equipment in\nterms of distortion. It also provides greater artistic control and never seen\nbefore manipulation of footage.\n","authors":["Jakub Maksymilian Fober"],"pdf_url":"https://arxiv.org/pdf/2411.16946v2.pdf","comment":"8 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2412.09828v1","updated":"2024-12-13T03:39:09Z","published":"2024-12-13T03:39:09Z","title":"MSC: Multi-Scale Spatio-Temporal Causal Attention for Autoregressive\n  Video Diffusion","summary":"  Diffusion transformers enable flexible generative modeling for video.\nHowever, it is still technically challenging and computationally expensive to\ngenerate high-resolution videos with rich semantics and complex motion. Similar\nto languages, video data are also auto-regressive by nature, so it is\ncounter-intuitive to use attention mechanism with bi-directional dependency in\nthe model. Here we propose a Multi-Scale Causal (MSC) framework to address\nthese problems. Specifically, we introduce multiple resolutions in the spatial\ndimension and high-low frequencies in the temporal dimension to realize\nefficient attention calculation. Furthermore, attention blocks on multiple\nscales are combined in a controlled way to allow causal conditioning on noisy\nimage frames for diffusion training, based on the idea that noise destroys\ninformation at different rates on different resolutions. We theoretically show\nthat our approach can greatly reduce the computational complexity and enhance\nthe efficiency of training. The causal attention diffusion framework can also\nbe used for auto-regressive long video generation, without violating the\nnatural order of frame sequences.\n","authors":["Xunnong Xu","Mengying Cao"],"pdf_url":"https://arxiv.org/pdf/2412.09828v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.09827v1","updated":"2024-12-13T03:38:49Z","published":"2024-12-13T03:38:49Z","title":"Low-Rank Adaptation with Task-Relevant Feature Enhancement for\n  Fine-tuning Language Models","summary":"  Fine-tuning pre-trained large language models in a parameter-efficient manner\nis widely studied for its effectiveness and efficiency. LoRA is one of the most\nwidely used methods, which assumes that the optimization process is essentially\nlow dimensional. Although LoRA has demonstrated commendable performance, there\nremains a significant performance gap between LoRA and full fine-tuning when\nlearning new tasks. In this work, we propose Low-Rank Adaptation with\nTask-Relevant Feature Enhancement(LoRATRF) for enhancing task-relevant features\nfrom the perspective of editing neural network representations. To prioritize\ntask-relevant features, a task-aware filter that selectively extracts valuable\nknowledge from hidden representations for the target or current task is\ndesigned. As the experiments on a vareity of datasets including NLU,\ncommonsense reasoning and mathematical reasoning tasks demonstrates, our method\nreduces 33.71% parameters and achieves better performance on a variety of\ndatasets in comparison with SOTA low-rank methods.\n","authors":["Changqun Li","Chaofan Ding","Kexin Luan","Xinhan Di"],"pdf_url":"https://arxiv.org/pdf/2412.09827v1.pdf","comment":"6 Pages, 3 figures accepted by AAAI 2025 CoLoRAI - Connecting\n  Low-Rank Representations in AI Workshop"},{"id":"http://arxiv.org/abs/2412.09349v2","updated":"2024-12-13T03:30:44Z","published":"2024-12-12T15:15:59Z","title":"DisPose: Disentangling Pose Guidance for Controllable Human Image\n  Animation","summary":"  Controllable human image animation aims to generate videos from reference\nimages using driving videos. Due to the limited control signals provided by\nsparse guidance (e.g., skeleton pose), recent works have attempted to introduce\nadditional dense conditions (e.g., depth map) to ensure motion alignment.\nHowever, such strict dense guidance impairs the quality of the generated video\nwhen the body shape of the reference character differs significantly from that\nof the driving video. In this paper, we present DisPose to mine more\ngeneralizable and effective control signals without additional dense input,\nwhich disentangles the sparse skeleton pose in human image animation into\nmotion field guidance and keypoint correspondence. Specifically, we generate a\ndense motion field from a sparse motion field and the reference image, which\nprovides region-level dense guidance while maintaining the generalization of\nthe sparse pose control. We also extract diffusion features corresponding to\npose keypoints from the reference image, and then these point features are\ntransferred to the target pose to provide distinct identity information. To\nseamlessly integrate into existing models, we propose a plug-and-play hybrid\nControlNet that improves the quality and consistency of generated videos while\nfreezing the existing model parameters. Extensive qualitative and quantitative\nexperiments demonstrate the superiority of DisPose compared to current methods.\nCode:\n\\href{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.\n","authors":["Hongxiang Li","Yaowei Li","Yuhang Yang","Junjie Cao","Zhihong Zhu","Xuxin Cheng","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09822v1","updated":"2024-12-13T03:20:53Z","published":"2024-12-13T03:20:53Z","title":"Dynamic Try-On: Taming Video Virtual Try-on with Dynamic Attention\n  Mechanism","summary":"  Video try-on stands as a promising area for its tremendous real-world\npotential. Previous research on video try-on has primarily focused on\ntransferring product clothing images to videos with simple human poses, while\nperforming poorly with complex movements. To better preserve clothing details,\nthose approaches are armed with an additional garment encoder, resulting in\nhigher computational resource consumption. The primary challenges in this\ndomain are twofold: (1) leveraging the garment encoder's capabilities in video\ntry-on while lowering computational requirements; (2) ensuring temporal\nconsistency in the synthesis of human body parts, especially during rapid\nmovements. To tackle these issues, we propose a novel video try-on framework\nbased on Diffusion Transformer(DiT), named Dynamic Try-On.\n  To reduce computational overhead, we adopt a straightforward approach by\nutilizing the DiT backbone itself as the garment encoder and employing a\ndynamic feature fusion module to store and integrate garment features. To\nensure temporal consistency of human body parts, we introduce a limb-aware\ndynamic attention module that enforces the DiT backbone to focus on the regions\nof human limbs during the denoising process. Extensive experiments demonstrate\nthe superiority of Dynamic Try-On in generating stable and smooth try-on\nresults, even for videos featuring complicated human postures.\n","authors":["Jun Zheng","Jing Wang","Fuwei Zhao","Xujie Zhang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.09822v1.pdf","comment":"Project Page: https://zhengjun-ai.github.io/dynamic-tryon-page/"},{"id":"http://arxiv.org/abs/2408.03703v2","updated":"2024-12-13T03:19:24Z","published":"2024-08-07T11:33:46Z","title":"CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications","summary":"  Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we propose Convolutional Additive\nToken Mixer (CATM) employing underlying spatial and channel attention as novel\ninteraction forms. This module eliminates troublesome complex operations such\nas matrix multiplication and Softmax. We introduce Convolutional Additive\nSelf-attention(CAS) block hybrid architecture and utilize CATM for each block.\nAnd further, we build a family of lightweight networks, which can be easily\nextended to various downstream tasks. Finally, we evaluate CAS-ViT across a\nvariety of vision tasks, including image classification, object detection,\ninstance segmentation, and semantic segmentation. Our M and T model achieves\n83.0\\%/84.1\\% top-1 with only 12M/21M parameters on ImageNet-1K. Meanwhile,\nthroughput evaluations on GPUs, ONNX, and iPhones also demonstrate superior\nresults compared to other state-of-the-art backbones. Extensive experiments\ndemonstrate that our approach achieves a better balance of performance,\nefficient inference and easy-to-deploy. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}\n","authors":["Tianfang Zhang","Lei Li","Yang Zhou","Wentao Liu","Chen Qian","Jenq-Neng Hwang","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08580v2","updated":"2024-12-13T03:13:47Z","published":"2024-12-11T17:57:10Z","title":"LAION-SG: An Enhanced Large-Scale Dataset for Training Complex\n  Image-Text Models with Structural Annotations","summary":"  Recent advances in text-to-image (T2I) generation have shown remarkable\nsuccess in producing high-quality images from text. However, existing T2I\nmodels show decayed performance in compositional image generation involving\nmultiple objects and intricate relationships. We attribute this problem to\nlimitations in existing datasets of image-text pairs, which lack precise\ninter-object relationship annotations with prompts only. To address this\nproblem, we construct LAION-SG, a large-scale dataset with high-quality\nstructural annotations of scene graphs (SG), which precisely describe\nattributes and relationships of multiple objects, effectively representing the\nsemantic structure in complex scenes. Based on LAION-SG, we train a new\nfoundation model SDXL-SG to incorporate structural annotation information into\nthe generation process. Extensive experiments show advanced models trained on\nour LAION-SG boast significant performance improvements in complex scene\ngeneration over models on existing datasets. We also introduce CompSG-Bench, a\nbenchmark that evaluates models on compositional image generation, establishing\na new standard for this domain. Our annotations with the associated processing\ncode, the foundation model and the benchmark protocol are publicly available at\nhttps://github.com/mengcye/LAION-SG.\n","authors":["Zejian Li","Chenye Meng","Yize Li","Ling Yang","Shengyuan Zhang","Jiarui Ma","Jiayi Li","Guang Yang","Changyuan Yang","Zhiyuan Yang","Jinxiong Chang","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.08580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09817v1","updated":"2024-12-13T03:13:44Z","published":"2024-12-13T03:13:44Z","title":"Enhancing Multimodal Large Language Models Complex Reason via Similarity\n  Computation","summary":"  Multimodal large language models have experienced rapid growth, and numerous\ndifferent models have emerged. The interpretability of LVLMs remains an\nunder-explored area. Especially when faced with more complex tasks such as\nchain-of-thought reasoning, its internal mechanisms still resemble a black box\nthat is difficult to decipher. By studying the interaction and information flow\nbetween images and text, we noticed that in models such as LLaVA1.5, image\ntokens that are semantically related to text are more likely to have\ninformation flow convergence in the LLM decoding layer, and these image tokens\nreceive higher attention scores. However, those image tokens that are less\nrelevant to the text do not have information flow convergence, and they only\nget very small attention scores. To efficiently utilize the image information,\nwe propose a new image token reduction method, Simignore, which aims to improve\nthe complex reasoning ability of LVLMs by computing the similarity between\nimage and text embeddings and ignoring image tokens that are irrelevant and\nunimportant to the text. Through extensive experiments, we demonstrate the\neffectiveness of our method for complex reasoning tasks. The paper's source\ncode can be accessed from \\url{https://github.com/FanshuoZeng/Simignore}.\n","authors":["Xiaofeng Zhang","Fanshuo Zeng","Yihao Quan","Zheng Hui","Jiawei Yao"],"pdf_url":"https://arxiv.org/pdf/2412.09817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10210v4","updated":"2024-12-13T03:11:30Z","published":"2024-04-16T01:41:22Z","title":"MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and\n  Knowledge Distillation for Skeleton-based Action Recognition","summary":"  In recent years, multimodal Graph Convolutional Networks (GCNs) have achieved\nremarkable performance in skeleton-based action recognition. The reliance on\nhigh-energy-consuming continuous floating-point operations inherent in\nGCN-based methods poses significant challenges for deployment in\nenergy-constrained, battery-powered edge devices. To address these limitations,\nMK-SGN, a Spiking Graph Convolutional Network with Multimodal Fusion and\nKnowledge Distillation, is proposed to leverage the energy efficiency of\nSpiking Neural Networks (SNNs) for skeleton-based action recognition for the\nfirst time. By integrating the energy-saving properties of SNNs with the graph\nrepresentation capabilities of GCNs, MK-SGN achieves significant reductions in\nenergy consumption while maintaining competitive recognition accuracy. Firstly,\nwe formulate a Spiking Multimodal Fusion (SMF) module to effectively fuse\nmultimodal skeleton data represented as spike-form features. Secondly, we\npropose the Self-Attention Spiking Graph Convolution (SA-SGC) module and the\nSpiking Temporal Convolution (STC) module, to capture spatial relationships and\ntemporal dynamics of spike-form features. Finally, we propose an integrated\nknowledge distillation strategy to transfer information from the multimodal GCN\nto the SGN, incorporating both intermediate-layer distillation and soft-label\ndistillation to enhance the performance of the SGN. MK-SGN exhibits substantial\nadvantages, surpassing state-of-the-art GCN frameworks in energy efficiency and\noutperforming state-of-the-art SNN frameworks in recognition accuracy. The\nproposed method achieves a remarkable reduction in energy consumption,\nexceeding 98\\% compared to conventional GCN-based approaches. This research\nestablishes a robust baseline for developing high-performance, energy-efficient\nSNN-based models for skeleton-based action recognition\n","authors":["Naichuan Zheng","Hailun Xia","Zeyu Liang","Yuchen Du"],"pdf_url":"https://arxiv.org/pdf/2404.10210v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09063v2","updated":"2024-12-13T02:41:26Z","published":"2024-12-12T08:46:22Z","title":"An Efficient Framework for Enhancing Discriminative Models via Diffusion\n  Techniques","summary":"  Image classification serves as the cornerstone of computer vision,\ntraditionally achieved through discriminative models based on deep neural\nnetworks. Recent advancements have introduced classification methods derived\nfrom generative models, which offer the advantage of zero-shot classification.\nHowever, these methods suffer from two main drawbacks: high computational\noverhead and inferior performance compared to discriminative models. Inspired\nby the coordinated cognitive processes of rapid-slow pathway interactions in\nthe human brain during visual signal recognition, we propose the\nDiffusion-Based Discriminative Model Enhancement Framework (DBMEF). This\nframework seamlessly integrates discriminative and generative models in a\ntraining-free manner, leveraging discriminative models for initial predictions\nand endowing deep neural networks with rethinking capabilities via diffusion\nmodels. Consequently, DBMEF can effectively enhance the classification accuracy\nand generalization capability of discriminative models in a plug-and-play\nmanner. We have conducted extensive experiments across 17 prevalent deep model\narchitectures with different training methods, including both CNN-based models\nsuch as ResNet and Transformer-based models like ViT, to demonstrate the\neffectiveness of the proposed DBMEF. Specifically, the framework yields a\n1.51\\% performance improvement for ResNet-50 on the ImageNet dataset and 3.02\\%\non the ImageNet-A dataset. In conclusion, our research introduces a novel\nparadigm for image classification, demonstrating stable improvements across\ndifferent datasets and neural networks. The code is available at\nhttps://github.com/ChunXiaostudy/DBMEF.\n","authors":["Chunxiao Li","Xiaoxiao Wang","Boming Miao","Chuanlong Xie","Zizhe Wang","Yao Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.09063v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.09799v1","updated":"2024-12-13T02:36:29Z","published":"2024-12-13T02:36:29Z","title":"CP-DETR: Concept Prompt Guide DETR Toward Stronger Universal Object\n  Detection","summary":"  Recent research on universal object detection aims to introduce language in a\nSoTA closed-set detector and then generalize the open-set concepts by\nconstructing large-scale (text-region) datasets for training. However, these\nmethods face two main challenges: (i) how to efficiently use the prior\ninformation in the prompts to genericise objects and (ii) how to reduce\nalignment bias in the downstream tasks, both leading to sub-optimal performance\nin some scenarios beyond pre-training. To address these challenges, we propose\na strong universal detection foundation model called CP-DETR, which is\ncompetitive in almost all scenarios, with only one pre-training weight.\nSpecifically, we design an efficient prompt visual hybrid encoder that enhances\nthe information interaction between prompt and visual through scale-by-scale\nand multi-scale fusion modules. Then, the hybrid encoder is facilitated to\nfully utilize the prompted information by prompt multi-label loss and auxiliary\ndetection head. In addition to text prompts, we have designed two practical\nconcept prompt generation methods, visual prompt and optimized prompt, to\nextract abstract concepts through concrete visual examples and stably reduce\nalignment bias in downstream tasks. With these effective designs, CP-DETR\ndemonstrates superior universal detection performance in a broad spectrum of\nscenarios. For example, our Swin-T backbone model achieves 47.6 zero-shot AP on\nLVIS, and the Swin-L backbone model achieves 32.2 zero-shot AP on ODinW35.\nFurthermore, our visual prompt generation method achieves 68.4 AP on COCO val\nby interactive detection, and the optimized prompt achieves 73.1 fully-shot AP\non ODinW13.\n","authors":["Qibo Chen","Weizhong Jin","Jianyue Ge","Mengdi Liu","Yuchao Yan","Jian Jiang","Li Yu","Xuanjiang Guo","Shuchang Li","Jianzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09799v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2408.10188v6","updated":"2024-12-13T02:32:06Z","published":"2024-08-19T17:48:08Z","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","summary":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long video supervised fine-tuning. However, training on long\nvideo is computationally and memory intensive. We introduce the long-context\nMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes\nlong video training and inference, enabling 2M context length training on 256\nGPUs without any gradient checkpointing. LongVILA efficiently extends the\nnumber of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in\n6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g.\n65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring\nstyle sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid\ncontext and tensor parallelism. Moreover, it seamlessly integrates with Hugging\nFace Transformers.\n","authors":["Yukang Chen","Fuzhao Xue","Dacheng Li","Qinghao Hu","Ligeng Zhu","Xiuyu Li","Yunhao Fang","Haotian Tang","Shang Yang","Zhijian Liu","Ethan He","Hongxu Yin","Pavlo Molchanov","Jan Kautz","Linxi Fan","Yuke Zhu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2408.10188v6.pdf","comment":"Code and models are available at\n  https://github.com/NVlabs/VILA/tree/main/longvila"},{"id":"http://arxiv.org/abs/2412.09795v1","updated":"2024-12-13T02:26:58Z","published":"2024-12-13T02:26:58Z","title":"Is it the model or the metric -- On robustness measures of deeplearning\n  models","summary":"  Determining the robustness of deep learning models is an established and\nongoing challenge within automated decision-making systems. With the advent and\nsuccess of techniques that enable advanced deep learning (DL), these models are\nbeing used in widespread applications, including high-stake ones like\nhealthcare, education, border-control. Therefore, it is critical to understand\nthe limitations of these models and predict their regions of failures, in order\nto create the necessary guardrails for their successful and safe deployment. In\nthis work, we revisit robustness, specifically investigating the sufficiency of\nrobust accuracy (RA), within the context of deepfake detection. We present\nrobust ratio (RR) as a complementary metric, that can quantify the changes to\nthe normalized or probability outcomes under input perturbation. We present a\ncomparison of RA and RR and demonstrate that despite similar RA between models,\nthe models show varying RR under different tolerance (perturbation) levels.\n","authors":["Zhijin Lyu","Yutong Jin","Sneha Das"],"pdf_url":"https://arxiv.org/pdf/2412.09795v1.pdf","comment":"Extended abstract at Northern Lights Deep Learning (NLDL) Conference\n  2025"},{"id":"http://arxiv.org/abs/2409.00755v3","updated":"2024-12-13T02:10:35Z","published":"2024-09-01T15:48:20Z","title":"Trusted Unified Feature-Neighborhood Dynamics for Multi-View\n  Classification","summary":"  Multi-view classification (MVC) faces inherent challenges due to domain gaps\nand inconsistencies across different views, often resulting in uncertainties\nduring the fusion process. While Evidential Deep Learning (EDL) has been\neffective in addressing view uncertainty, existing methods predominantly rely\non the Dempster-Shafer combination rule, which is sensitive to conflicting\nevidence and often neglects the critical role of neighborhood structures within\nmulti-view data. To address these limitations, we propose a Trusted Unified\nFeature-NEighborhood Dynamics (TUNED) model for robust MVC. This method\neffectively integrates local and global feature-neighborhood (F-N) structures\nfor robust decision-making. Specifically, we begin by extracting local F-N\nstructures within each view. To further mitigate potential uncertainties and\nconflicts in multi-view fusion, we employ a selective Markov random field that\nadaptively manages cross-view neighborhood dependencies. Additionally, we\nemploy a shared parameterized evidence extractor that learns global consensus\nconditioned on local F-N structures, thereby enhancing the global integration\nof multi-view features. Experiments on benchmark datasets show that our method\nimproves accuracy and robustness over existing approaches, particularly in\nscenarios with high uncertainty and conflicting views. The code will be made\navailable at https://github.com/JethroJames/TUNED.\n","authors":["Haojian Huang","Chuanyu Qin","Zhe Liu","Kaijing Ma","Jin Chen","Han Fang","Chao Ban","Hao Sun","Zhongjiang He"],"pdf_url":"https://arxiv.org/pdf/2409.00755v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08410v2","updated":"2024-12-13T02:05:34Z","published":"2024-12-11T14:29:35Z","title":"Physical Informed Driving World Model","summary":"  Autonomous driving requires robust perception models trained on high-quality,\nlarge-scale multi-view driving videos for tasks like 3D object detection,\nsegmentation and trajectory prediction. While world models provide a\ncost-effective solution for generating realistic driving videos, challenges\nremain in ensuring these videos adhere to fundamental physical principles, such\nas relative and absolute motion, spatial relationship like occlusion and\nspatial consistency, and temporal consistency. To address these, we propose\nDrivePhysica, an innovative model designed to generate realistic multi-view\ndriving videos that accurately adhere to essential physical principles through\nthree key advancements: (1) a Coordinate System Aligner module that integrates\nrelative and absolute motion features to enhance motion interpretation, (2) an\nInstance Flow Guidance module that ensures precise temporal consistency via\nefficient 3D flow extraction, and (3) a Box Coordinate Guidance module that\nimproves spatial relationship understanding and accurately resolves occlusion\nhierarchies. Grounded in physical principles, we achieve state-of-the-art\nperformance in driving video generation quality (3.96 FID and 38.06 FVD on the\nNuscenes dataset) and downstream perception tasks. Our project homepage:\nhttps://metadrivescape.github.io/papers_project/DrivePhysica/page.html\n","authors":["Zhuoran Yang","Xi Guo","Chenjing Ding","Chiyu Wang","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2412.08410v2.pdf","comment":"project homepage:\n  https://metadrivescape.github.io/papers_project/DrivePhysica/page.html"},{"id":"http://arxiv.org/abs/2412.09782v1","updated":"2024-12-13T01:37:44Z","published":"2024-12-13T01:37:44Z","title":"EI-Drive: A Platform for Cooperative Perception with Realistic\n  Communication Models","summary":"  The growing interest in autonomous driving calls for realistic simulation\nplatforms capable of accurately simulating cooperative perception process in\nrealistic traffic scenarios. Existing studies for cooperative perception often\nhave not accounted for transmission latency and errors in real-world\nenvironments. To address this gap, we introduce EI-Drive, an edge-AI based\nautonomous driving simulation platform that integrates advanced cooperative\nperception with more realistic communication models. Built on the CARLA\nframework, EI-Drive features new modules for cooperative perception while\ntaking into account transmission latency and errors, providing a more realistic\nplatform for evaluating cooperative perception algorithms. In particular, the\nplatform enables vehicles to fuse data from multiple sources, improving\nsituational awareness and safety in complex environments. With its modular\ndesign, EI-Drive allows for detailed exploration of sensing, perception,\nplanning, and control in various cooperative driving scenarios. Experiments\nusing EI-Drive demonstrate significant improvements in vehicle safety and\nperformance, particularly in scenarios with complex traffic flow and network\nconditions. All code and documents are accessible on our GitHub page:\n\\url{https://ucd-dare.github.io/eidrive.github.io/}.\n","authors":["Hanchu Zhou","Edward Xie","Wei Shao","Dechen Gao","Michelle Dong","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09782v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.10313v1","updated":"2024-12-13T17:53:29Z","published":"2024-12-13T17:53:29Z","title":"MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation","summary":"  Regulatory documents are rich in nuanced terminology and specialized\nsemantics. FRAG systems: Frozen retrieval-augmented generators utilizing\npre-trained (or, frozen) components face consequent challenges with both\nretriever and answering performance. We present a system that adapts the\nretriever performance to the target domain using a multi-stage tuning (MST)\nstrategy. Our retrieval approach, called MST-R (a) first fine-tunes encoders\nused in vector stores using hard negative mining, (b) then uses a hybrid\nretriever, combining sparse and dense retrievers using reciprocal rank fusion,\nand then (c) adapts the cross-attention encoder by fine-tuning only the top-k\nretrieved results. We benchmark the system performance on the dataset released\nfor the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). We\nachieve significant performance gains obtaining a top rank on the RegNLP\nchallenge leaderboard. We also show that a trivial answering approach games the\nRePASs metric outscoring all baselines and a pre-trained Llama model. Analyzing\nthis anomaly, we present important takeaways for future research.\n","authors":["Yash Malviya","Karan Dhingra","Maneesh Singh"],"pdf_url":"https://arxiv.org/pdf/2412.10313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09983v1","updated":"2024-12-13T09:09:20Z","published":"2024-12-13T09:09:20Z","title":"Static Pruning in Dense Retrieval using Matrix Decomposition","summary":"  In the era of dense retrieval, document indexing and retrieval is largely\nbased on encoding models that transform text documents into embeddings. The\nefficiency of retrieval is directly proportional to the number of documents and\nthe size of the embeddings. Recent studies have shown that it is possible to\nreduce embedding size without sacrificing - and in some cases improving - the\nretrieval effectiveness. However, the methods introduced by these studies are\nquery-dependent, so they can't be applied offline and require additional\ncomputations during query processing, thus negatively impacting the retrieval\nefficiency. In this paper, we present a novel static pruning method for\nreducing the dimensionality of embeddings using Principal Components Analysis.\nThis approach is query-independent and can be executed offline, leading to a\nsignificant boost in dense retrieval efficiency with a negligible impact on the\nsystem effectiveness. Our experiments show that our proposed method reduces the\ndimensionality of document representations by over 50% with up to a 5%\nreduction in NDCG@10, for different dense retrieval models.\n","authors":["Federico Siciliano","Francesca Pezzuti","Nicola Tonellotto","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2412.09983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09950v1","updated":"2024-12-13T08:14:10Z","published":"2024-12-13T08:14:10Z","title":"Hesitation and Tolerance in Recommender Systems","summary":"  User interactions in recommender systems are inherently complex, often\ninvolving behaviors that go beyond simple acceptance or rejection. One\nparticularly common behavior is hesitation, where users deliberate over\nrecommended items, signaling uncertainty. Our large-scale surveys, with 6,644\nand 3,864 responses respectively, confirm that hesitation is not only\nwidespread but also has a profound impact on user experiences. When users spend\nadditional time engaging with content they are ultimately uninterested in, this\ncan lead to negative emotions, a phenomenon we term as tolerance. The surveys\nreveal that such tolerance behaviors often arise after hesitation and can erode\ntrust, satisfaction, and long-term loyalty to the platform. For instance, a\nclick might reflect a need for more information rather than genuine interest,\nand prolonged exposure to unsuitable content amplifies frustration. This\nmisalignment between user intent and system interpretation introduces noise\ninto recommendation training, resulting in suggestions that increase\nuncertainty and disengagement. To address these issues, we identified signals\nindicative of tolerance behavior and analyzed datasets from both e-commerce and\nshort-video platforms. The analysis shows a strong correlation between\nincreased tolerance behavior and decreased user activity. We integrated these\ninsights into the training process of a recommender system for a major\nshort-video platform. Results from four independent online A/B experiments\ndemonstrated significant improvements in user retention, achieved with minimal\nadditional computational costs. These findings underscore the importance of\nrecognizing hesitation as a ubiquitous user behavior and addressing tolerance\nto enhance satisfaction, build trust, and sustain long-term engagement in\nrecommender systems.\n","authors":["Kuan Zou","Aixin Sun","Xuemeng Jiang","Yitong Ji","Hao Zhang","Jing Wang","Ruijie Guo"],"pdf_url":"https://arxiv.org/pdf/2412.09950v1.pdf","comment":"30 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.00004v3","updated":"2024-12-13T06:41:02Z","published":"2024-05-12T04:15:05Z","title":"Navigating the Future of Federated Recommendation Systems with\n  Foundation Models","summary":"  In recent years, the integration of federated learning (FL) and\nrecommendation systems (RS), known as Federated Recommendation Systems (FRS),\nhas attracted attention for preserving user privacy by keeping private data on\nclient devices. However, FRS faces inherent limitations such as data\nheterogeneity and scarcity, due to the privacy requirements of FL and the\ntypical data sparsity issues of RSs. Models like ChatGPT are empowered by the\nconcept of transfer learning and self-supervised learning, so they can be\neasily applied to the downstream tasks after fine-tuning or prompting. These\nmodels, so-called Foundation Models (FM), fouce on understanding the human's\nintent and perform following their designed roles in the specific tasks, which\nare widely recognized for producing high-quality content in the image and\nlanguage domains. Thus, the achievements of FMs inspire the design of FRS and\nsuggest a promising research direction: integrating foundation models to\naddress the above limitations. In this study, we conduct a comprehensive review\nof FRSs with FMs. Specifically, we: 1) summarise the common approaches of\ncurrent FRSs and FMs; 2) review the challenges posed by FRSs and FMs; 3)\ndiscuss potential future research directions; and 4) introduce some common\nbenchmarks and evaluation metrics in the FRS field. We hope that this position\npaper provides the necessary background and guidance to explore this\ninteresting and emerging topic.\n","authors":["Zhiwei Li","Guodong Long","Chunxu Zhang","Honglei Zhang","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00004v3.pdf","comment":"20 pages, position paper, survey"},{"id":"http://arxiv.org/abs/2412.10595v1","updated":"2024-12-13T22:44:22Z","published":"2024-12-13T22:44:22Z","title":"Recommendation and Temptation","summary":"  Traditional recommender systems based on utility maximization and revealed\npreferences often fail to capture users' dual-self nature, where consumption\nchoices are driven by both long-term benefits (enrichment) and desire for\ninstant gratification (temptation). Consequently, these systems may generate\nrecommendations that fail to provide long-lasting satisfaction to users. To\naddress this issue, we propose a novel user model that accounts for this\ndual-self behavior and develop an optimal recommendation strategy to maximize\nenrichment from consumption. We highlight the limitations of historical\nconsumption data in implementing this strategy and present an estimation\nframework that makes minimal assumptions and leverages explicit user feedback\nand implicit choice data to overcome these constraints. We evaluate our\napproach through both synthetic simulations and simulations based on real-world\ndata from the MovieLens dataset. Results demonstrate that our proposed\nrecommender can deliver superior enrichment compared to several competitive\nbaseline algorithms that assume a single utility type and rely solely on\nrevealed preferences. Our work emphasizes the critical importance of optimizing\nfor enrichment in recommender systems, particularly in temptation-laden\nconsumption contexts. Our findings have significant implications for content\nplatforms, user experience design, and the development of responsible AI\nsystems, paving the way for more nuanced and user-centric recommendation\napproaches.\n","authors":["Md Sanzeed Anwar","Paramveer S. Dhillon","Grant Schoenebeck"],"pdf_url":"https://arxiv.org/pdf/2412.10595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10576v1","updated":"2024-12-13T21:37:37Z","published":"2024-12-13T21:37:37Z","title":"Agro-STAY : Collecte de données et analyse des informations en\n  agriculture alternative issues de YouTube","summary":"  To address the current crises (climatic, social, economic), the\nself-sufficiency -- a set of practices that combine energy sobriety,\nself-production of food and energy, and self-construction - arouses an\nincreasing interest. The CNRS STAY project (Savoirs Techniques pour\nl'Auto-suffisance, sur YouTube) explores this topic by analyzing techniques\nshared on YouTube. We present Agro-STAY, a platform designed for the\ncollection, processing, and visualization of data from YouTube videos and their\ncomments. We use Natural Language Processing (NLP) techniques and language\nmodels, which enable a fine-grained analysis of alternative agricultural\npractice described online.\n  --\n  Face aux crises actuelles (climatiques, sociales, \\'economiques),\nl'auto-suffisance -- ensemble de pratiques combinant sobri\\'et\\'e\n\\'energ\\'etique, autoproduction alimentaire et \\'energ\\'etique et\nautoconstruction - suscite un int\\'er\\^et croissant. Le projet CNRS STAY\n(Savoirs Techniques pour l'Auto-suffisance, sur YouTube) s'inscrit dans ce\ndomaine en analysant les savoirs techniques diffus\\'es sur YouTube. Nous\npr\\'esentons Agro-STAY, une plateforme d\\'edi\\'ee \\`a la collecte, au\ntraitement et \\`a la visualisation de donn\\'ees issues de vid\\'eos YouTube et\nde leurs commentaires. En mobilisant des techniques de traitement automatique\ndes langues (TAL) et des mod\\`eles de langues, ce travail permet une analyse\nfine des pratiques agricoles alternatives d\\'ecrites en ligne.\n","authors":["Laura Maxim","Julien Rabatel","Jean-Marc Douguet","Natalia Grabar","Roberto Interdonato","Sébastien Loustau","Mathieu Roche","Maguelonne Teisseire"],"pdf_url":"https://arxiv.org/pdf/2412.10576v1.pdf","comment":"8 pages, in Frnch language, 3 figures"},{"id":"http://arxiv.org/abs/2412.10571v1","updated":"2024-12-13T21:28:17Z","published":"2024-12-13T21:28:17Z","title":"Evidence Contextualization and Counterfactual Attribution for\n  Conversational QA over Heterogeneous Data with RAG Systems","summary":"  Retrieval Augmented Generation (RAG) works as a backbone for interacting with\nan enterprise's own data via Conversational Question Answering (ConvQA). In a\nRAG system, a retriever fetches passages from a collection in response to a\nquestion, which are then included in the prompt of a large language model (LLM)\nfor generating a natural language (NL) answer. However, several RAG systems\ntoday suffer from two shortcomings: (i) retrieved passages usually contain\ntheir raw text and lack appropriate document context, negatively impacting both\nretrieval and answering quality; and (ii) attribution strategies that explain\nanswer generation usually rely only on similarity between the answer and the\nretrieved passages, thereby only generating plausible but not causal\nexplanations. In this work, we demonstrate RAGONITE, a RAG system that remedies\nthe above concerns by: (i) contextualizing evidence with source metadata and\nsurrounding text; and (ii) computing counterfactual attribution, a causal\nexplanation approach where the contribution of an evidence to an answer is\ndetermined by the similarity of the original response to the answer obtained by\nremoving that evidence. To evaluate our proposals, we release a new benchmark\nConfQuestions, with 300 hand-created conversational questions, each in English\nand German, coupled with ground truth URLs, completed questions, and answers\nfrom 215 public Confluence pages, that are typical of enterprise wiki spaces\nwith heterogeneous elements. Experiments with RAGONITE on ConfQuestions show\nthe viability of our ideas: contextualization improves RAG performance, and\ncounterfactual attribution is effective at explaining RAG answers.\n","authors":["Rishiraj Saha Roy","Joel Schlotthauer","Chris Hinze","Andreas Foltyn","Luzian Hahn","Fabian Kuech"],"pdf_url":"https://arxiv.org/pdf/2412.10571v1.pdf","comment":"Extended version of demo paper accepted at WSDM 2025"},{"id":"http://arxiv.org/abs/2412.10543v1","updated":"2024-12-13T20:39:30Z","published":"2024-12-13T20:39:30Z","title":"RAGServe: Fast Quality-Aware RAG Systems with Configuration Adaptation","summary":"  RAG (Retrieval Augmented Generation) allows LLMs (large language models) to\ngenerate better responses with external knowledge, but using more external\nknowledge often improves generation quality at the expense of response delay.\nPrior work either reduces the response delay (through better scheduling of RAG\nqueries) or strives to maximize quality (which involves tuning the RAG\nworkflow), but they fall short in optimizing the tradeoff between the delay and\nquality of RAG responses. This paper presents RAGServe, the first RAG system\nthat jointly schedules queries and adapts the key RAG configurations of each\nquery, such as the number of retrieved text chunks and synthesis methods, in\norder to balance quality optimization and response delay reduction. Using 4\npopular RAG-QA datasets, we show that compared with the state-of-the-art RAG\noptimization schemes, RAGServe reduces the generation latency by\n$1.64-2.54\\times$ without sacrificing generation quality.\n","authors":["Siddhant Ray","Rui Pan","Zhuohan Gu","Kuntai Du","Ganesh Ananthanarayanan","Ravi Netravali","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.10543v1.pdf","comment":"17 pages, 18 figures"},{"id":"http://arxiv.org/abs/2412.10514v1","updated":"2024-12-13T19:16:38Z","published":"2024-12-13T19:16:38Z","title":"CRS Arena: Crowdsourced Benchmarking of Conversational Recommender\n  Systems","summary":"  We introduce CRS Arena, a research platform for scalable benchmarking of\nConversational Recommender Systems (CRS) based on human feedback. The platform\ndisplays pairwise battles between anonymous conversational recommender systems,\nwhere users interact with the systems one after the other before declaring\neither a winner or a draw. CRS Arena collects conversations and user feedback,\nproviding a foundation for reliable evaluation and ranking of CRSs. We conduct\nexperiments with CRS Arena on both open and closed crowdsourcing platforms,\nconfirming that both setups produce highly correlated rankings of CRSs and\nconversations with similar characteristics. We release CRSArena-Dial, a dataset\nof 474 conversations and their corresponding user feedback, along with a\npreliminary ranking of the systems based on the Elo rating system. The platform\nis accessible at https://iai-group-crsarena.hf.space/.\n","authors":["Nolwenn Bernard","Hideaki Joko","Faegheh Hasibi","Krisztian Balog"],"pdf_url":"https://arxiv.org/pdf/2412.10514v1.pdf","comment":"Proceedings of the Eighteenth ACM International Conference on Web\n  Search and Data Mining (WSDM '25), March 10--14, 2025, Hannover, Germany"},{"id":"http://arxiv.org/abs/2412.10576v1","updated":"2024-12-13T21:37:37Z","published":"2024-12-13T21:37:37Z","title":"Agro-STAY : Collecte de données et analyse des informations en\n  agriculture alternative issues de YouTube","summary":"  To address the current crises (climatic, social, economic), the\nself-sufficiency -- a set of practices that combine energy sobriety,\nself-production of food and energy, and self-construction - arouses an\nincreasing interest. The CNRS STAY project (Savoirs Techniques pour\nl'Auto-suffisance, sur YouTube) explores this topic by analyzing techniques\nshared on YouTube. We present Agro-STAY, a platform designed for the\ncollection, processing, and visualization of data from YouTube videos and their\ncomments. We use Natural Language Processing (NLP) techniques and language\nmodels, which enable a fine-grained analysis of alternative agricultural\npractice described online.\n  --\n  Face aux crises actuelles (climatiques, sociales, \\'economiques),\nl'auto-suffisance -- ensemble de pratiques combinant sobri\\'et\\'e\n\\'energ\\'etique, autoproduction alimentaire et \\'energ\\'etique et\nautoconstruction - suscite un int\\'er\\^et croissant. Le projet CNRS STAY\n(Savoirs Techniques pour l'Auto-suffisance, sur YouTube) s'inscrit dans ce\ndomaine en analysant les savoirs techniques diffus\\'es sur YouTube. Nous\npr\\'esentons Agro-STAY, une plateforme d\\'edi\\'ee \\`a la collecte, au\ntraitement et \\`a la visualisation de donn\\'ees issues de vid\\'eos YouTube et\nde leurs commentaires. En mobilisant des techniques de traitement automatique\ndes langues (TAL) et des mod\\`eles de langues, ce travail permet une analyse\nfine des pratiques agricoles alternatives d\\'ecrites en ligne.\n","authors":["Laura Maxim","Julien Rabatel","Jean-Marc Douguet","Natalia Grabar","Roberto Interdonato","Sébastien Loustau","Mathieu Roche","Maguelonne Teisseire"],"pdf_url":"https://arxiv.org/pdf/2412.10576v1.pdf","comment":"8 pages, in French language, 3 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2412.10373v1","updated":"2024-12-13T18:59:54Z","published":"2024-12-13T18:59:54Z","title":"GaussianWorld: Gaussian World Model for Streaming 3D Occupancy\n  Prediction","summary":"  3D occupancy prediction is important for autonomous driving due to its\ncomprehensive perception of the surroundings. To incorporate sequential inputs,\nmost existing methods fuse representations from previous frames to infer the\ncurrent 3D occupancy. However, they fail to consider the continuity of driving\nscenarios and ignore the strong prior provided by the evolution of 3D scenes\n(e.g., only dynamic objects move). In this paper, we propose a\nworld-model-based framework to exploit the scene evolution for perception. We\nreformulate 3D occupancy prediction as a 4D occupancy forecasting problem\nconditioned on the current sensor input. We decompose the scene evolution into\nthree factors: 1) ego motion alignment of static scenes; 2) local movements of\ndynamic objects; and 3) completion of newly-observed scenes. We then employ a\nGaussian world model (GaussianWorld) to explicitly exploit these priors and\ninfer the scene evolution in the 3D Gaussian space considering the current RGB\nobservation. We evaluate the effectiveness of our framework on the widely used\nnuScenes dataset. Our GaussianWorld improves the performance of the\nsingle-frame counterpart by over 2% in mIoU without introducing additional\ncomputations. Code: https://github.com/zuosc19/GaussianWorld.\n","authors":["Sicheng Zuo","Wenzhao Zheng","Yuanhui Huang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10373v1.pdf","comment":"Code is available at: https://github.com/zuosc19/GaussianWorld"},{"id":"http://arxiv.org/abs/2412.10371v1","updated":"2024-12-13T18:59:30Z","published":"2024-12-13T18:59:30Z","title":"GaussianAD: Gaussian-Centric End-to-End Autonomous Driving","summary":"  Vision-based autonomous driving shows great potential due to its satisfactory\nperformance and low costs. Most existing methods adopt dense representations\n(e.g., bird's eye view) or sparse representations (e.g., instance boxes) for\ndecision-making, which suffer from the trade-off between comprehensiveness and\nefficiency. This paper explores a Gaussian-centric end-to-end autonomous\ndriving (GaussianAD) framework and exploits 3D semantic Gaussians to\nextensively yet sparsely describe the scene. We initialize the scene with\nuniform 3D Gaussians and use surrounding-view images to progressively refine\nthem to obtain the 3D Gaussian scene representation. We then use sparse\nconvolutions to efficiently perform 3D perception (e.g., 3D detection, semantic\nmap construction). We predict 3D flows for the Gaussians with dynamic semantics\nand plan the ego trajectory accordingly with an objective of future scene\nforecasting. Our GaussianAD can be trained in an end-to-end manner with\noptional perception labels when available. Extensive experiments on the widely\nused nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on\nvarious tasks including motion planning, 3D occupancy prediction, and 4D\noccupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.\n","authors":["Wenzhao Zheng","Junjie Wu","Yao Zheng","Sicheng Zuo","Zixun Xie","Longchao Yang","Yong Pan","Zhihui Hao","Peng Jia","Xianpeng Lang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10371v1.pdf","comment":"Code is available at: https://github.com/wzzheng/GaussianAD"},{"id":"http://arxiv.org/abs/2402.01886v2","updated":"2024-12-13T18:59:14Z","published":"2024-02-02T20:21:09Z","title":"Inverse Reinforcement Learning by Estimating Expertise of Demonstrators","summary":"  In Imitation Learning (IL), utilizing suboptimal and heterogeneous\ndemonstrations presents a substantial challenge due to the varied nature of\nreal-world data. However, standard IL algorithms consider these datasets as\nhomogeneous, thereby inheriting the deficiencies of suboptimal demonstrators.\nPrevious approaches to this issue rely on impractical assumptions like\nhigh-quality data subsets, confidence rankings, or explicit environmental\nknowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by\nEstimating Expertise of Demonstrators, a novel framework that overcomes these\nhurdles without prior knowledge of demonstrator expertise. IRLEED enhances\nexisting Inverse Reinforcement Learning (IRL) algorithms by combining a general\nmodel for demonstrator suboptimality to address reward bias and action\nvariance, with a Maximum Entropy IRL framework to efficiently derive the\noptimal policy from diverse, suboptimal demonstrations. Experiments in both\nonline and offline IL settings, with simulated and human-generated data,\ndemonstrate IRLEED's adaptability and effectiveness, making it a versatile\nsolution for learning from suboptimal demonstrations.\n","authors":["Mark Beliaev","Ramtin Pedarsani"],"pdf_url":"https://arxiv.org/pdf/2402.01886v2.pdf","comment":"11 pages, 4 figures, extended version of AAAI publication"},{"id":"http://arxiv.org/abs/2412.10362v1","updated":"2024-12-13T18:55:19Z","published":"2024-12-13T18:55:19Z","title":"OP-LoRA: The Blessing of Dimensionality","summary":"  Low-rank adapters enable fine-tuning of large models with only a small number\nof parameters, thus reducing storage costs and minimizing the risk of\ncatastrophic forgetting. However, they often pose optimization challenges, with\npoor convergence. To overcome these challenges, we introduce an\nover-parameterized approach that accelerates training without increasing\ninference costs. This method reparameterizes low-rank adaptation by employing a\nseparate MLP and learned embedding for each layer. The learned embedding is\ninput to the MLP, which generates the adapter parameters. Such\noverparamaterization has been shown to implicitly function as an adaptive\nlearning rate and momentum, accelerating optimization. At inference time, the\nMLP can be discarded, leaving behind a standard low-rank adapter. To study the\neffect of MLP overparameterization on a small yet difficult proxy task, we\nimplement it for matrix factorization, and find it achieves faster convergence\nand lower final loss. Extending this approach to larger-scale tasks, we observe\nconsistent performance gains across domains. We achieve improvements in\nvision-language tasks and especially notable increases in image generation,\nwith CMMD scores improving by up to 15 points.\n","authors":["Piotr Teterwak","Kate Saenko","Bryan A. Plummer","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2412.10362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10357v1","updated":"2024-12-13T18:51:33Z","published":"2024-12-13T18:51:33Z","title":"The Correlated Gaussian Sparse Histogram Mechanism","summary":"  We consider the problem of releasing a sparse histogram under $(\\varepsilon,\n\\delta)$-differential privacy. The stability histogram independently adds noise\nfrom a Laplace or Gaussian distribution to the non-zero entries and removes\nthose noisy counts below a threshold.\n  Thereby, the introduction of new non-zero values between neighboring\nhistograms is only revealed with probability at most $\\delta$, and typically,\nthe value of the threshold dominates the error of the mechanism. We consider\nthe variant of the stability histogram with Gaussian noise.\n  Recent works ([Joseph and Yu, COLT '24] and [Lebeda, SOSA '25]) reduced the\nerror for private histograms using correlated Gaussian noise. However, these\ntechniques can not be directly applied in the very sparse setting. Instead, we\nadopt Lebeda's technique and show that adding correlated noise to the non-zero\ncounts only allows us to reduce the magnitude of noise when we have a sparsity\nbound. This, in turn, allows us to use a lower threshold by up to a factor of\n$1/2$ compared to the non-correlated noise mechanism. We then extend our\nmechanism to a setting without a known bound on sparsity. Additionally, we show\nthat correlated noise can give a similar improvement for the more practical\ndiscrete Gaussian mechanism.\n","authors":["Christian Janos Lebeda","Lukas Retschmeier"],"pdf_url":"https://arxiv.org/pdf/2412.10357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10354v1","updated":"2024-12-13T18:49:37Z","published":"2024-12-13T18:49:37Z","title":"A Library for Learning Neural Operators","summary":"  We present NeuralOperator, an open-source Python library for operator\nlearning. Neural operators generalize neural networks to maps between function\nspaces instead of finite-dimensional Euclidean spaces. They can be trained and\ninferenced on input and output functions given at various discretizations,\nsatisfying a discretization convergence properties. Built on top of PyTorch,\nNeuralOperator provides all the tools for training and deploying neural\noperator models, as well as developing new ones, in a high-quality, tested,\nopen-source package. It combines cutting-edge models and customizability with a\ngentle learning curve and simple user interface for newcomers.\n","authors":["Jean Kossaifi","Nikola Kovachki","Zongyi Li","Davit Pitt","Miguel Liu-Schiaffini","Robert Joseph George","Boris Bonev","Kamyar Azizzadenesheli","Julius Berner","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2412.10354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10353v1","updated":"2024-12-13T18:49:25Z","published":"2024-12-13T18:49:25Z","title":"Robust image classification with multi-modal large language models","summary":"  Deep Neural Networks are vulnerable to adversarial examples, i.e., carefully\ncrafted input samples that can cause models to make incorrect predictions with\nhigh confidence. To mitigate these vulnerabilities, adversarial training and\ndetection-based defenses have been proposed to strengthen models in advance.\nHowever, most of these approaches focus on a single data modality, overlooking\nthe relationships between visual patterns and textual descriptions of the\ninput. In this paper, we propose a novel defense, Multi-Shield, designed to\ncombine and complement these defenses with multi-modal information to further\nenhance their robustness. Multi-Shield leverages multi-modal large language\nmodels to detect adversarial examples and abstain from uncertain\nclassifications when there is no alignment between textual and visual\nrepresentations of the input. Extensive evaluations on CIFAR-10 and ImageNet\ndatasets, using robust and non-robust image classification models, demonstrate\nthat Multi-Shield can be easily integrated to detect and reject adversarial\nexamples, outperforming the original defenses.\n","authors":["Francesco Villani","Igor Maljkovic","Dario Lazzaro","Angelo Sotgiu","Antonio Emanuele Cinà","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2412.10353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10347v1","updated":"2024-12-13T18:42:00Z","published":"2024-12-13T18:42:00Z","title":"COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation\n  Tasks and Language Models","summary":"  As key elements within the central dogma, DNA, RNA, and proteins play crucial\nroles in maintaining life by guaranteeing accurate genetic expression and\nimplementation. Although research on these molecules has profoundly impacted\nfields like medicine, agriculture, and industry, the diversity of machine\nlearning approaches-from traditional statistical methods to deep learning\nmodels and large language models-poses challenges for researchers in choosing\nthe most suitable models for specific tasks, especially for cross-omics and\nmulti-omics tasks due to the lack of comprehensive benchmarks. To address this,\nwe introduce the first comprehensive multi-omics benchmark COMET (Benchmark for\nBiological COmprehensive Multi-omics Evaluation Tasks and Language Models),\ndesigned to evaluate models across single-omics, cross-omics, and multi-omics\ntasks. First, we curate and develop a diverse collection of downstream tasks\nand datasets covering key structural and functional aspects in DNA, RNA, and\nproteins, including tasks that span multiple omics levels. Then, we evaluate\nexisting foundational language models for DNA, RNA, and proteins, as well as\nthe newly proposed multi-omics method, offering valuable insights into their\nperformance in integrating and analyzing data from different biological\nmodalities. This benchmark aims to define critical issues in multi-omics\nresearch and guide future directions, ultimately promoting advancements in\nunderstanding biological processes through integrated and different omics data\nanalysis.\n","authors":["Yuchen Ren","Wenwei Han","Qianyuan Zhang","Yining Tang","Weiqiang Bai","Yuchen Cai","Lifeng Qiao","Hao Jiang","Dong Yuan","Tao Chen","Siqi Sun","Pan Tan","Wanli Ouyang","Nanqing Dong","Xinzhu Ma","Peng Ye"],"pdf_url":"https://arxiv.org/pdf/2412.10347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10341v1","updated":"2024-12-13T18:38:47Z","published":"2024-12-13T18:38:47Z","title":"Shape error prediction in 5-axis machining using graph neural networks","summary":"  This paper presents an innovative method for predicting shape errors in\n5-axis machining using graph neural networks. The graph structure is defined\nwith nodes representing workpiece surface points and edges denoting the\nneighboring relationships. The dataset encompasses data from a material removal\nsimulation, process data, and post-machining quality information. Experimental\nresults show that the presented approach can generalize the shape error\nprediction for the investigated workpiece geometry. Moreover, by modelling\nspatial and temporal connections within the workpiece, the approach handles a\nlow number of labels compared to non-graphical methods such as Support Vector\nMachines.\n","authors":["Julia Huuk","Abheek Dhingra","Eirini Ntoutsi","Bernd Denkena"],"pdf_url":"https://arxiv.org/pdf/2412.10341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10337v1","updated":"2024-12-13T18:32:21Z","published":"2024-12-13T18:32:21Z","title":"Generative AI in Medicine","summary":"  The increased capabilities of generative AI have dramatically expanded its\npossible use cases in medicine. We provide a comprehensive overview of\ngenerative AI use cases for clinicians, patients, clinical trial organizers,\nresearchers, and trainees. We then discuss the many challenges -- including\nmaintaining privacy and security, improving transparency and interpretability,\nupholding equity, and rigorously evaluating models -- which must be overcome to\nrealize this potential, and the open research directions they give rise to.\n","authors":["Divya Shanmugam","Monica Agrawal","Rajiv Movva","Irene Y. Chen","Marzyeh Ghassemi","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2412.10337v1.pdf","comment":"To appear in the Annual Review of Biomedical Data Science, August\n  2025"},{"id":"http://arxiv.org/abs/2406.06290v2","updated":"2024-12-13T18:12:02Z","published":"2024-06-10T14:12:33Z","title":"Geometric sparsification in recurrent neural networks","summary":"  A common technique for ameliorating the computational costs of running large\nneural models is sparsification, or the pruning of neural connections during\ntraining. Sparse models are capable of maintaining the high accuracy of state\nof the art models, while functioning at the cost of more parsimonious models.\nThe structures which underlie sparse architectures are, however, poorly\nunderstood and not consistent between differently trained models and\nsparsification schemes. In this paper, we propose a new technique for\nsparsification of recurrent neural nets (RNNs), called moduli regularization,\nin combination with magnitude pruning. Moduli regularization leverages the\ndynamical system induced by the recurrent structure to induce a geometric\nrelationship between neurons in the hidden state of the RNN. By making our\nregularizing term explicitly geometric, we provide the first, to our knowledge,\na priori description of the desired sparse architecture of our neural net, as\nwell as explicit end-to-end learning of RNN geometry. We verify the\neffectiveness of our scheme under diverse conditions, testing in navigation,\nnatural language processing, and addition RNNs. Navigation is a structurally\ngeometric task, for which there are known moduli spaces, and we show that\nregularization can be used to reach 90% sparsity while maintaining model\nperformance only when coefficients are chosen in accordance with a suitable\nmoduli space. Natural language processing and addition, however, have no known\nmoduli space in which computations are performed. Nevertheless, we show that\nmoduli regularization induces more stable recurrent neural nets, and achieves\nhigh fidelity models above 90% sparsity.\n","authors":["Wyatt Mackey","Ioannis Schizas","Jared Deighton","David L. Boothe, Jr.","Vasileios Maroulas"],"pdf_url":"https://arxiv.org/pdf/2406.06290v2.pdf","comment":"25 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.10321v1","updated":"2024-12-13T18:00:57Z","published":"2024-12-13T18:00:57Z","title":"AdvPrefix: An Objective for Nuanced LLM Jailbreaks","summary":"  Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks.\n","authors":["Sicheng Zhu","Brandon Amos","Yuandong Tian","Chuan Guo","Ivan Evtimov"],"pdf_url":"https://arxiv.org/pdf/2412.10321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10319v1","updated":"2024-12-13T17:59:52Z","published":"2024-12-13T17:59:52Z","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods","summary":"  Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.\n","authors":["Yucheng Li","Huiqiang Jiang","Qianhui Wu","Xufang Luo","Surin Ahn","Chengruidong Zhang","Amir H. Abdi","Dongsheng Li","Jianfeng Gao","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2412.10319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10313v1","updated":"2024-12-13T17:53:29Z","published":"2024-12-13T17:53:29Z","title":"MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation","summary":"  Regulatory documents are rich in nuanced terminology and specialized\nsemantics. FRAG systems: Frozen retrieval-augmented generators utilizing\npre-trained (or, frozen) components face consequent challenges with both\nretriever and answering performance. We present a system that adapts the\nretriever performance to the target domain using a multi-stage tuning (MST)\nstrategy. Our retrieval approach, called MST-R (a) first fine-tunes encoders\nused in vector stores using hard negative mining, (b) then uses a hybrid\nretriever, combining sparse and dense retrievers using reciprocal rank fusion,\nand then (c) adapts the cross-attention encoder by fine-tuning only the top-k\nretrieved results. We benchmark the system performance on the dataset released\nfor the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). We\nachieve significant performance gains obtaining a top rank on the RegNLP\nchallenge leaderboard. We also show that a trivial answering approach games the\nRePASs metric outscoring all baselines and a pre-trained Llama model. Analyzing\nthis anomaly, we present important takeaways for future research.\n","authors":["Yash Malviya","Karan Dhingra","Maneesh Singh"],"pdf_url":"https://arxiv.org/pdf/2412.10313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02820v2","updated":"2024-12-13T17:53:25Z","published":"2024-11-05T05:41:41Z","title":"DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving","summary":"  Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.\n","authors":["Yuhan Liu","Yuyang Huang","Jiayi Yao","Zhuohan Gu","Kuntai Du","Hanchen Li","Yihua Cheng","Junchen Jiang","Shan Lu","Madan Musuvathi","Esha Choukse"],"pdf_url":"https://arxiv.org/pdf/2411.02820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10312v1","updated":"2024-12-13T17:52:48Z","published":"2024-12-13T17:52:48Z","title":"Interlocking-free Selective Rationalization Through Genetic-based\n  Learning","summary":"  A popular end-to-end architecture for selective rationalization is the\nselect-then-predict pipeline, comprising a generator to extract highlights fed\nto a predictor. Such a cooperative system suffers from suboptimal equilibrium\nminima due to the dominance of one of the two modules, a phenomenon known as\ninterlocking. While several contributions aimed at addressing interlocking,\nthey only mitigate its effect, often by introducing feature-based heuristics,\nsampling, and ad-hoc regularizations. We present GenSPP, the first\ninterlocking-free architecture for selective rationalization that does not\nrequire any learning overhead, as the above-mentioned. GenSPP avoids\ninterlocking by performing disjoint training of the generator and predictor via\ngenetic global search. Experiments on a synthetic and a real-world benchmark\nshow that our model outperforms several state-of-the-art competitors.\n","authors":["Federico Ruggeri","Gaetano Signorelli"],"pdf_url":"https://arxiv.org/pdf/2412.10312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10298v1","updated":"2024-12-13T17:34:18Z","published":"2024-12-13T17:34:18Z","title":"Buzz to Broadcast: Predicting Sports Viewership Using Social Media\n  Engagement","summary":"  Accurately predicting sports viewership is crucial for optimizing ad sales\nand revenue forecasting. Social media platforms, such as Reddit, provide a\nwealth of user-generated content that reflects audience engagement and\ninterest. In this study, we propose a regression-based approach to predict\nsports viewership using social media metrics, including post counts, comments,\nscores, and sentiment analysis from TextBlob and VADER. Through iterative\nimprovements, such as focusing on major sports subreddits, incorporating\ncategorical features, and handling outliers by sport, the model achieved an\n$R^2$ of 0.99, a Mean Absolute Error (MAE) of 1.27 million viewers, and a Root\nMean Squared Error (RMSE) of 2.33 million viewers on the full dataset. These\nresults demonstrate the model's ability to accurately capture patterns in\naudience behavior, offering significant potential for pre-event revenue\nforecasting and targeted advertising strategies.\n","authors":["Anakin Trotter"],"pdf_url":"https://arxiv.org/pdf/2412.10298v1.pdf","comment":"17 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.10291v1","updated":"2024-12-13T17:21:29Z","published":"2024-12-13T17:21:29Z","title":"Still \"Talking About Large Language Models\": Some Clarifications","summary":"  My paper \"Talking About Large Language Models\" has more than once been\ninterpreted as advocating a reductionist stance towards large language models.\nBut the paper was not intended that way, and I do not endorse such positions.\nThis short note situates the paper in the context of a larger philosophical\nproject that is concerned with the (mis)use of words rather than metaphysics,\nin the spirit of Wittgenstein's later writing.\n","authors":["Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2412.10291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10288v1","updated":"2024-12-13T17:11:47Z","published":"2024-12-13T17:11:47Z","title":"Performance evaluation of predictive AI models to support medical\n  decisions: Overview and guidance","summary":"  A myriad of measures to illustrate performance of predictive artificial\nintelligence (AI) models have been proposed in the literature. Selecting\nappropriate performance measures is essential for predictive AI models that are\ndeveloped to be used in medical practice, because poorly performing models may\nharm patients and lead to increased costs. We aim to assess the merits of\nclassic and contemporary performance measures when validating predictive AI\nmodels for use in medical practice. We focus on models with a binary outcome.\nWe discuss 32 performance measures covering five performance domains\n(discrimination, calibration, overall, classification, and clinical utility)\nalong with accompanying graphical assessments. The first four domains cover\nstatistical performance, the fifth domain covers decision-analytic performance.\nWe explain why two key characteristics are important when selecting which\nperformance measures to assess: (1) whether the measure's expected value is\noptimized when it is calculated using the correct probabilities (i.e., a\n\"proper\" measure), and (2) whether they reflect either purely statistical\nperformance or decision-analytic performance by properly considering\nmisclassification costs. Seventeen measures exhibit both characteristics,\nfourteen measures exhibited one characteristic, and one measure possessed\nneither characteristic (the F1 measure). All classification measures (such as\nclassification accuracy and F1) are improper for clinically relevant decision\nthresholds other than 0.5 or the prevalence. We recommend the following\nmeasures and plots as essential to report: AUROC, calibration plot, a clinical\nutility measure such as net benefit with decision curve analysis, and a plot\nwith probability distributions per outcome category.\n","authors":["Ben Van Calster","Gary S. Collins","Andrew J. Vickers","Laure Wynants","Kathleen F. Kerr","Lasai Barreñada","Gael Varoquaux","Karandeep Singh","Karel G. M. Moons","Tina Hernandez-boussard","Dirk Timmerman","David J. Mclernon","Maarten Van Smeden","Ewout W. Steyerberg"],"pdf_url":"https://arxiv.org/pdf/2412.10288v1.pdf","comment":"60 pages, 8 tables, 11 figures, two supplementary appendices"},{"id":"http://arxiv.org/abs/2410.01697v3","updated":"2024-12-13T16:55:33Z","published":"2024-10-02T16:05:03Z","title":"MOREL: Enhancing Adversarial Robustness through Multi-Objective\n  Representation Learning","summary":"  Extensive research has shown that deep neural networks (DNNs) are vulnerable\nto slight adversarial perturbations$-$small changes to the input data that\nappear insignificant but cause the model to produce drastically different\noutputs. In addition to augmenting training data with adversarial examples\ngenerated from a specific attack method, most of the current defense strategies\nnecessitate modifying the original model architecture components to improve\nrobustness or performing test-time data purification to handle adversarial\nattacks. In this work, we demonstrate that strong feature representation\nlearning during training can significantly enhance the original model's\nrobustness. We propose MOREL, a multi-objective feature representation learning\napproach, encouraging classification models to produce similar features for\ninputs within the same class, despite perturbations. Our training method\ninvolves an embedding space where cosine similarity loss and multi-positive\ncontrastive loss are used to align natural and adversarial features from the\nmodel encoder and ensure tight clustering. Concurrently, the classifier is\nmotivated to achieve accurate predictions. Through extensive experiments, we\ndemonstrate that our approach significantly enhances the robustness of DNNs\nagainst white-box and black-box adversarial attacks, outperforming other\nmethods that similarly require no architectural changes or test-time data\npurification. Our code is available at https://github.com/salomonhotegni/MOREL\n","authors":["Sedjro Salomon Hotegni","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2410.01697v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10273v1","updated":"2024-12-13T16:46:46Z","published":"2024-12-13T16:46:46Z","title":"Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry","summary":"  We introduce a hierarchical probabilistic approach to go from a 2D image to\nmultiview 3D: a diffusion \"prior\" models the unseen 3D geometry, which then\nconditions a diffusion \"decoder\" to generate novel views of the subject. We use\na pointmap-based geometric representation in a multiview image format to\ncoordinate the generation of multiple target views simultaneously. We\nfacilitate correspondence between views by assuming fixed target camera poses\nrelative to the source camera, and constructing a predictable distribution of\ngeometric features per target. Our modular, geometry-driven approach to\nnovel-view synthesis (called \"unPIC\") beats SoTA baselines such as CAT3D and\nOne-2-3-45 on held-out objects from ObjaverseXL, as well as real-world objects\nranging from Google Scanned Objects, Amazon Berkeley Objects, to the Digital\nTwin Catalog.\n","authors":["Rishabh Kabra","Drew A. Hudson","Sjoerd van Steenkiste","Joao Carreira","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.10273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02702v4","updated":"2024-12-13T16:34:56Z","published":"2023-10-04T10:15:57Z","title":"On the Power of Adaptive Weighted Aggregation in Heterogeneous Federated\n  Learning and Beyond","summary":"  Federated averaging (FedAvg) is the most fundamental algorithm in Federated\nlearning (FL). Previous theoretical results assert that FedAvg convergence and\ngeneralization degenerate under heterogeneous clients. However, recent\nempirical results show that FedAvg can perform well in many real-world\nheterogeneous tasks. These results reveal an inconsistency between FL theory\nand practice that is not fully explained. In this paper, we show that common\nheterogeneity measures contribute to this inconsistency based on rigorous\nconvergence analysis. Furthermore, we introduce a new measure \\textit{client\nconsensus dynamics} and prove that \\textit{FedAvg can effectively handle client\nheterogeneity when an appropriate aggregation strategy is used}. Building on\nthis theoretical insight, we present a simple and effective FedAvg variant\ntermed FedAWARE. Extensive experiments on three datasets and two modern neural\nnetwork architectures demonstrate that FedAWARE ensures faster convergence and\nbetter generalization in heterogeneous client settings. Moreover, our results\nshow that FedAWARE can significantly enhance the generalization performance of\nadvanced FL algorithms when used as a plug-in module.\n","authors":["Dun Zeng","Zenglin Xu","Shiyu Liu","Yu Pan","Qifan Wang","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2310.02702v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10265v1","updated":"2024-12-13T16:33:36Z","published":"2024-12-13T16:33:36Z","title":"Adversarial Robustness of Bottleneck Injected Deep Neural Networks for\n  Task-Oriented Communication","summary":"  This paper investigates the adversarial robustness of Deep Neural Networks\n(DNNs) using Information Bottleneck (IB) objectives for task-oriented\ncommunication systems. We empirically demonstrate that while IB-based\napproaches provide baseline resilience against attacks targeting downstream\ntasks, the reliance on generative models for task-oriented communication\nintroduces new vulnerabilities. Through extensive experiments on several\ndatasets, we analyze how bottleneck depth and task complexity influence\nadversarial robustness. Our key findings show that Shallow Variational\nBottleneck Injection (SVBI) provides less adversarial robustness compared to\nDeep Variational Information Bottleneck (DVIB) approaches, with the gap\nwidening for more complex tasks. Additionally, we reveal that IB-based\nobjectives exhibit stronger robustness against attacks focusing on salient\npixels with high intensity compared to those perturbing many pixels with lower\nintensity. Lastly, we demonstrate that task-oriented communication systems that\nrely on generative models to extract and recover salient information have an\nincreased attack surface. The results highlight important security\nconsiderations for next-generation communication systems that leverage neural\nnetworks for goal-oriented compression.\n","authors":["Alireza Furutanpey","Pantelis A. Frangoudis","Patrik Szabo","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2412.10265v1.pdf","comment":"Submission to ICMLCN, 6 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.10251v1","updated":"2024-12-13T16:21:56Z","published":"2024-12-13T16:21:56Z","title":"Controlling dynamical systems into unseen target states using machine\n  learning","summary":"  We present a novel, model-free, and data-driven methodology for controlling\ncomplex dynamical systems into previously unseen target states, including those\nwith significantly different and complex dynamics. Leveraging a parameter-aware\nrealization of next-generation reservoir computing, our approach accurately\npredicts system behavior in unobserved parameter regimes, enabling control over\ntransitions to arbitrary target states. Crucially, this includes states with\ndynamics that differ fundamentally from known regimes, such as shifts from\nperiodic to intermittent or chaotic behavior. The method's parameter-awareness\nfacilitates non-stationary control, ensuring smooth transitions between states.\nBy extending the applicability of machine learning-based control mechanisms to\npreviously inaccessible target dynamics, this methodology opens the door to\ntransformative new applications while maintaining exceptional efficiency. Our\nresults highlight reservoir computing as a powerful alternative to traditional\nmethods for dynamic system control.\n","authors":["Daniel Köglmayr","Alexander Haluszczynski","Christoph Räth"],"pdf_url":"https://arxiv.org/pdf/2412.10251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10246v1","updated":"2024-12-13T16:14:49Z","published":"2024-12-13T16:14:49Z","title":"Detecting LLM Hallucination Through Layer-wise Information Deficiency:\n  Analysis of Unanswerable Questions and Ambiguous Prompts","summary":"  Large language models (LLMs) frequently generate confident yet inaccurate\nresponses, introducing significant risks for deployment in safety-critical\ndomains. We present a novel approach to detecting model hallucination through\nsystematic analysis of information flow across model layers when processing\ninputs with insufficient or ambiguous context. Our investigation reveals that\nhallucination manifests as usable information deficiencies in inter-layer\ntransmissions. While existing approaches primarily focus on final-layer output\nanalysis, we demonstrate that tracking cross-layer information dynamics\n($\\mathcal{L}$I) provides robust indicators of model reliability, accounting\nfor both information gain and loss during computation. $\\mathcal{L}$I improves\nmodel reliability by immediately integrating with universal LLMs without\nadditional training or architectural modifications.\n","authors":["Hazel Kim","Adel Bibi","Philip Torr","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2412.10246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10244v1","updated":"2024-12-13T16:13:35Z","published":"2024-12-13T16:13:35Z","title":"Efficient Continual Pre-training of LLMs for Low-resource Languages","summary":"  Open-source Large Language models (OsLLMs) propel the democratization of\nnatural language research by giving the flexibility to augment or update model\nparameters for performance improvement. Nevertheless, like proprietary LLMs,\nOs-LLMs offer poorer performance on low-resource languages (LRLs) than\nhigh-resource languages (HRLs), owing to smaller amounts of training data and\nunderrepresented vocabulary. On the other hand, continual pre-training (CPT)\nwith large amounts of language-specific data is a costly proposition in terms\nof data acquisition and computational resources. Our goal is to drastically\nreduce CPT cost. To that end, we first develop a new algorithm to select a\nsubset of texts from a larger corpus. We show the effectiveness of our\ntechnique using very little CPT data. In search of further improvement, we\ndesign a new algorithm to select tokens to include in the LLM vocabulary. We\nexperiment with the recent Llama-3 model and nine Indian languages with diverse\nscripts and extent of resource availability. For evaluation, we use\nIndicGenBench, a generation task benchmark dataset for Indic languages. We\nexperiment with various CPT corpora and augmented vocabulary size and offer\ninsights across language families.\n","authors":["Arijit Nag","Soumen Chakrabarti","Animesh Mukherjee","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2412.10244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15312v2","updated":"2024-12-13T15:48:04Z","published":"2024-05-24T07:53:27Z","title":"Multi-Feature Fusion and Compressed Bi-LSTM for Memory-Efficient\n  Heartbeat Classification on Wearable Devices","summary":"  In this article, we present a resource-efficient approach for\nelectrocardiogram (ECG) based heartbeat classification using multi-feature\nfusion and bidirectional long short-term memory (Bi-LSTM). The dataset\ncomprises five original classes from the MIT-BIH Arrhythmia Database: Normal\n(N), Left Bundle Branch Block (LBBB), Right Bundle Branch Block (RBBB),\nPremature Ventricular Contraction (PVC), and Paced Beat (PB). Preprocessing\nmethods including the discrete wavelet transform and dual moving average\nwindows are used to reduce noise and artifacts in the raw ECG signal, and\nextract the main points (PQRST) of the ECG waveform. Multi-feature fusion is\nachieved by utilizing time intervals and the proposed under-the-curve areas,\nwhich are inherently robust against noise, as input features. Simulations\ndemonstrated that incorporating under-the-curve area features improved the\nclassification accuracy for the challenging RBBB and LBBB classes from 31.4\\%\nto 84.3\\% for RBBB, and from 69.6\\% to 87.0\\% for LBBB. Using a Bi-LSTM\nnetwork, rather than a conventional LSTM network, resulted in higher accuracy\n(33.8\\% vs 21.8\\%) with a 28\\% reduction in required network parameters for the\nRBBB class. Multiple neural network models with varying parameter sizes,\nincluding tiny (84k), small (150k), medium (478k), and large (1.25M) models,\nare developed to achieve high accuracy \\textit{across all classes}, a more\ncrucial and challenging goal than overall classification accuracy.\n","authors":["Reza Nikandish","Jiayu He","Benyamin Haghi"],"pdf_url":"https://arxiv.org/pdf/2405.15312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02541v2","updated":"2024-12-13T15:45:48Z","published":"2024-10-03T14:45:23Z","title":"Fair Decentralized Learning","summary":"  Decentralized learning (DL) is an emerging approach that enables nodes to\ncollaboratively train a machine learning model without sharing raw data. In\nmany application domains, such as healthcare, this approach faces challenges\ndue to the high level of heterogeneity in the training data's feature space.\nSuch feature heterogeneity lowers model utility and negatively impacts\nfairness, particularly for nodes with under-represented training data. In this\npaper, we introduce \\textsc{Facade}, a clustering-based DL algorithm\nspecifically designed for fair model training when the training data exhibits\nseveral distinct features. The challenge of \\textsc{Facade} is to assign nodes\nto clusters, one for each feature, based on the similarity in the features of\ntheir local data, without requiring individual nodes to know apriori which\ncluster they belong to. \\textsc{Facade} (1) dynamically assigns nodes to their\nappropriate clusters over time, and (2) enables nodes to collaboratively train\na specialized model for each cluster in a fully decentralized manner. We\ntheoretically prove the convergence of \\textsc{Facade}, implement our\nalgorithm, and compare it against three state-of-the-art baselines. Our\nexperimental results on three datasets demonstrate the superiority of our\napproach in terms of model accuracy and fairness compared to all three\ncompetitors. Compared to the best-performing baseline, \\textsc{Facade} on the\nCIFAR-10 dataset also reduces communication costs by 32.3\\% to reach a target\naccuracy when cluster sizes are imbalanced.\n","authors":["Sayan Biswas","Anne-Marie Kermarrec","Rishi Sharma","Thibaud Trinca","Martijn de Vos"],"pdf_url":"https://arxiv.org/pdf/2410.02541v2.pdf","comment":"To appear in the proceedings of \"3rd IEEE Conference on Secure and\n  Trustworthy Machine Learning\" (SatML'25)"},{"id":"http://arxiv.org/abs/2412.10208v1","updated":"2024-12-13T15:31:17Z","published":"2024-12-13T15:31:17Z","title":"Efficient Generative Modeling with Residual Vector Quantization-Based\n  Tokens","summary":"  We explore the use of Residual Vector Quantization (RVQ) for high-fidelity\ngeneration in vector-quantized generative models. This quantization technique\nmaintains higher data fidelity by employing more in-depth tokens. However,\nincreasing the token number in generative models leads to slower inference\nspeeds. To this end, we introduce ResGen, an efficient RVQ-based discrete\ndiffusion model that generates high-fidelity samples without compromising\nsampling speed. Our key idea is a direct prediction of vector embedding of\ncollective tokens rather than individual ones. Moreover, we demonstrate that\nour proposed token masking and multi-token prediction method can be formulated\nwithin a principled probabilistic framework using a discrete diffusion process\nand variational inference. We validate the efficacy and generalizability of the\nproposed method on two challenging tasks across different modalities:\nconditional image generation} on ImageNet 256x256 and zero-shot text-to-speech\nsynthesis. Experimental results demonstrate that ResGen outperforms\nautoregressive counterparts in both tasks, delivering superior performance\nwithout compromising sampling speed. Furthermore, as we scale the depth of RVQ,\nour generative models exhibit enhanced generation fidelity or faster sampling\nspeeds compared to similarly sized baseline models. The project page can be\nfound at https://resgen-genai.github.io\n","authors":["Jaehyeon Kim","Taehong Moon","Keon Lee","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2412.10208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05349v2","updated":"2024-12-13T15:17:28Z","published":"2023-05-09T11:20:11Z","title":"Towards the Characterization of Representations Learned via\n  Capsule-based Network Architectures","summary":"  Capsule Networks (CapsNets) have been re-introduced as a more compact and\ninterpretable alternative to standard deep neural networks. While recent\nefforts have proved their compression capabilities, to date, their\ninterpretability properties have not been fully assessed. Here, we conduct a\nsystematic and principled study towards assessing the interpretability of these\ntypes of networks. Moreover, we pay special attention towards analyzing the\nlevel to which part-whole relationships are indeed encoded within the learned\nrepresentation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebA\ndatasets suggest that the representations encoded in CapsNets might not be as\ndisentangled nor strictly related to parts-whole relationships as is commonly\nstated in the literature.\n","authors":["Saja Tawalbeh","José Oramas"],"pdf_url":"https://arxiv.org/pdf/2305.05349v2.pdf","comment":"This paper consist of 32 pages including 19 figures. This paper\n  concern about interpretation of capsule networks"},{"id":"http://arxiv.org/abs/2412.10199v1","updated":"2024-12-13T15:17:23Z","published":"2024-12-13T15:17:23Z","title":"Integrative Analysis of Financial Market Sentiment Using CNN and GRU for\n  Risk Prediction and Alert Systems","summary":"  This document presents an in-depth examination of stock market sentiment\nthrough the integration of Convolutional Neural Networks (CNN) and Gated\nRecurrent Units (GRU), enabling precise risk alerts. The robust feature\nextraction capability of CNN is utilized to preprocess and analyze extensive\nnetwork text data, identifying local features and patterns. The extracted\nfeature sequences are then input into the GRU model to understand the\nprogression of emotional states over time and their potential impact on future\nmarket sentiment and risk. This approach addresses the order dependence and\nlong-term dependencies inherent in time series data, resulting in a detailed\nanalysis of stock market sentiment and effective early warnings of future\nrisks.\n","authors":["You Wu","Mengfang Sun","Hongye Zheng","Jinxin Hu","Yingbin Liang","Zhenghao Lin"],"pdf_url":"https://arxiv.org/pdf/2412.10199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17461v2","updated":"2024-12-13T15:08:32Z","published":"2024-11-26T14:28:25Z","title":"SoK: Decentralized AI (DeAI)","summary":"  The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research.\n","authors":["Zhipeng Wang","Rui Sun","Elizabeth Lui","Vatsal Shah","Xihan Xiong","Jiahao Sun","Davide Crapis","William Knottenbelt"],"pdf_url":"https://arxiv.org/pdf/2411.17461v2.pdf","comment":"This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI"},{"id":"http://arxiv.org/abs/2412.10193v1","updated":"2024-12-13T15:08:30Z","published":"2024-12-13T15:08:30Z","title":"Simple Guidance Mechanisms for Discrete Diffusion Models","summary":"  Diffusion models for continuous data gained widespread adoption owing to\ntheir high quality generation and control mechanisms. However, controllable\ndiffusion on discrete data faces challenges given that continuous guidance\nmethods do not directly apply to discrete diffusion. Here, we provide a\nstraightforward derivation of classifier-free and classifier-based guidance for\ndiscrete diffusion, as well as a new class of diffusion models that leverage\nuniform noise and that are more guidable because they can continuously edit\ntheir outputs. We improve the quality of these models with a novel\ncontinuous-time variational lower bound that yields state-of-the-art\nperformance, especially in settings involving guidance or fast generation.\nEmpirically, we demonstrate that our guidance mechanisms combined with uniform\nnoise diffusion improve controllable generation relative to autoregressive and\ndiffusion baselines on several discrete data domains, including genomic\nsequences, small molecule design, and discretized image generation.\n","authors":["Yair Schiff","Subham Sekhar Sahoo","Hao Phung","Guanghan Wang","Sam Boshar","Hugo Dalla-torre","Bernardo P. de Almeida","Alexander Rush","Thomas Pierrot","Volodymyr Kuleshov"],"pdf_url":"https://arxiv.org/pdf/2412.10193v1.pdf","comment":"Code to reproduce our experiments is available here:\n  https://github.com/kuleshov-group/discrete-diffusion-guidance"},{"id":"http://arxiv.org/abs/2409.15161v2","updated":"2024-12-13T15:04:30Z","published":"2024-09-23T16:11:43Z","title":"A Gated Residual Kolmogorov-Arnold Networks for Mixtures of Experts","summary":"  This paper introduces KAMoE, a novel Mixture of Experts (MoE) framework based\non Gated Residual Kolmogorov-Arnold Networks (GRKAN). We propose GRKAN as an\nalternative to the traditional gating function, aiming to enhance efficiency\nand interpretability in MoE modeling. Through extensive experiments on digital\nasset markets and real estate valuation, we demonstrate that KAMoE consistently\noutperforms traditional MoE architectures across various tasks and model types.\nOur results show that GRKAN exhibits superior performance compared to standard\nGating Residual Networks, particularly in LSTM-based models for sequential\ntasks. We also provide insights into the trade-offs between model complexity\nand performance gains in MoE and KAMoE architectures.\n","authors":["Hugo Inzirillo","Remi Genet"],"pdf_url":"https://arxiv.org/pdf/2409.15161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10186v1","updated":"2024-12-13T14:56:39Z","published":"2024-12-13T14:56:39Z","title":"BiCert: A Bilinear Mixed Integer Programming Formulation for Precise\n  Certified Bounds Against Data Poisoning Attacks","summary":"  Data poisoning attacks pose one of the biggest threats to modern AI systems,\nnecessitating robust defenses. While extensive efforts have been made to\ndevelop empirical defenses, attackers continue to evolve, creating\nsophisticated methods to circumvent these measures. To address this, we must\nmove beyond empirical defenses and establish provable certification methods\nthat guarantee robustness. This paper introduces a novel certification\napproach, BiCert, using Bilinear Mixed Integer Programming (BMIP) to compute\nsound deterministic bounds that provide such provable robustness. Using BMIP,\nwe compute the reachable set of parameters that could result from training with\npotentially manipulated data. A key element to make this computation feasible\nis to relax the reachable parameter set to a convex set between training\niterations. At test time, this parameter set allows us to predict all possible\noutcomes, guaranteeing robustness. BiCert is more precise than previous\nmethods, which rely solely on interval and polyhedral bounds. Crucially, our\napproach overcomes the fundamental limitation of prior approaches where\nparameter bounds could only grow, often uncontrollably. We show that BiCert's\ntighter bounds eliminate a key source of divergence issues, resulting in more\nstable training and higher certified accuracy.\n","authors":["Tobias Lorenz","Marta Kwiatkowska","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2412.10186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10185v1","updated":"2024-12-13T14:55:48Z","published":"2024-12-13T14:55:48Z","title":"Solving Robust Markov Decision Processes: Generic, Reliable, Efficient","summary":"  Markov decision processes (MDP) are a well-established model for sequential\ndecision-making in the presence of probabilities. In robust MDP (RMDP), every\naction is associated with an uncertainty set of probability distributions,\nmodelling that transition probabilities are not known precisely. Based on the\nknown theoretical connection to stochastic games, we provide a framework for\nsolving RMDPs that is generic, reliable, and efficient. It is *generic* both\nwith respect to the model, allowing for a wide range of uncertainty sets,\nincluding but not limited to intervals, $L^1$- or $L^2$-balls, and polytopes;\nand with respect to the objective, including long-run average reward,\nundiscounted total reward, and stochastic shortest path. It is *reliable*, as\nour approach not only converges in the limit, but provides precision guarantees\nat any time during the computation. It is *efficient* because -- in contrast to\nstate-of-the-art approaches -- it avoids explicitly constructing the underlying\nstochastic game. Consequently, our prototype implementation outperforms\nexisting tools by several orders of magnitude and can solve RMDPs with a\nmillion states in under a minute.\n","authors":["Tobias Meggendorfer","Maximilian Weininger","Patrick Wienhöft"],"pdf_url":"https://arxiv.org/pdf/2412.10185v1.pdf","comment":"Accepted for publication at AAAI'25. Extended version with full\n  appendix, 26 pages"},{"id":"http://arxiv.org/abs/2412.10184v1","updated":"2024-12-13T14:55:24Z","published":"2024-12-13T14:55:24Z","title":"Sims: An Interactive Tool for Geospatial Matching and Clustering","summary":"  Acquiring, processing, and visualizing geospatial data requires significant\ncomputing resources, especially for large spatio-temporal domains. This\nchallenge hinders the rapid discovery of predictive features, which is\nessential for advancing geospatial modeling. To address this, we developed\nSimilarity Search (Sims), a no-code web tool that allows users to visualize,\ncompare, cluster, and perform similarity search over defined regions of\ninterest using Google Earth Engine as a backend. Sims is designed to complement\nexisting modeling tools by focusing on feature exploration rather than model\ncreation. We demonstrate the utility of Sims through a case study analyzing\nsimulated maize yield data in Rwanda, where we evaluate how different\ncombinations of soil, weather, and agronomic features affect the clustering of\nyield response zones. Sims is open source and available at\nhttps://github.com/microsoft/Sims\n","authors":["Akram Zaytar","Girmaw Abebe Tadesse","Caleb Robinson","Eduardo G. Bendito","Medha Devare","Meklit Chernet","Gilles Q. Hacheme","Rahul Dodhia","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2412.10184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10182v1","updated":"2024-12-13T14:53:47Z","published":"2024-12-13T14:53:47Z","title":"Multi-Head Encoding for Extreme Label Classification","summary":"  The number of categories of instances in the real world is normally huge, and\neach instance may contain multiple labels. To distinguish these massive labels\nutilizing machine learning, eXtreme Label Classification (XLC) has been\nestablished. However, as the number of categories increases, the number of\nparameters and nonlinear operations in the classifier also rises. This results\nin a Classifier Computational Overload Problem (CCOP). To address this, we\npropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla\nclassifier with a multi-head classifier. During the training process, MHE\ndecomposes extreme labels into the product of multiple short local labels, with\neach head trained on these local labels. During testing, the predicted labels\ncan be directly calculated from the local predictions of each head. This\nreduces the computational load geometrically. Then, according to the\ncharacteristics of different XLC tasks, e.g., single-label, multi-label, and\nmodel pretraining tasks, three MHE-based implementations, i.e., Multi-Head\nProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more\neffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can\nachieve performance approximately equivalent to that of the vanilla classifier\nby generalizing the low-rank approximation problem from Frobenius-norm to\nCross-Entropy. Experimental results show that the proposed methods achieve\nstate-of-the-art performance while significantly streamlining the training and\ninference processes of XLC tasks. The source code has been made public at\nhttps://github.com/Anoise/MHE.\n","authors":["Daojun Liang","Haixia Zhang","Dongfeng Yuan","Minggao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10182v1.pdf","comment":"20 pages, 12 figs, Published in TPAMI"},{"id":"http://arxiv.org/abs/2412.10168v1","updated":"2024-12-13T14:33:50Z","published":"2024-12-13T14:33:50Z","title":"Learning payoffs while routing in skill-based queues","summary":"  Motivated by applications in service systems, we consider queueing systems\nwhere each customer must be handled by a server with the right skill set. We\nfocus on optimizing the routing of customers to servers in order to maximize\nthe total payoff of customer--server matches. In addition, customer--server\ndependent payoff parameters are assumed to be unknown a priori. We construct a\nmachine learning algorithm that adaptively learns the payoff parameters while\nmaximizing the total payoff and prove that it achieves polylogarithmic regret.\nMoreover, we show that the algorithm is asymptotically optimal up to\nlogarithmic terms by deriving a regret lower bound. The algorithm leverages the\nbasic feasible solutions of a static linear program as the action space. The\nregret analysis overcomes the complex interplay between queueing and learning\nby analyzing the convergence of the queue length process to its stationary\nbehavior. We also demonstrate the performance of the algorithm numerically, and\nhave included an experiment with time-varying parameters highlighting the\npotential of the algorithm in non-static environments.\n","authors":["Sanne van Kempen","Jaron Sanders","Fiona Sloothaak","Maarten G. Wolf"],"pdf_url":"https://arxiv.org/pdf/2412.10168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10163v1","updated":"2024-12-13T14:25:27Z","published":"2024-12-13T14:25:27Z","title":"Scaling Combinatorial Optimization Neural Improvement Heuristics with\n  Online Search and Adaptation","summary":"  We introduce Limited Rollout Beam Search (LRBS), a beam search strategy for\ndeep reinforcement learning (DRL) based combinatorial optimization improvement\nheuristics. Utilizing pre-trained models on the Euclidean Traveling Salesperson\nProblem, LRBS significantly enhances both in-distribution performance and\ngeneralization to larger problem instances, achieving optimality gaps that\noutperform existing improvement heuristics and narrowing the gap with\nstate-of-the-art constructive methods. We also extend our analysis to two\npickup and delivery TSP variants to validate our results. Finally, we employ\nour search strategy for offline and online adaptation of the pre-trained\nimprovement policy, leading to improved search performance and surpassing\nrecent adaptive methods for constructive heuristics.\n","authors":["Federico Julian Camerota Verdù","Lorenzo Castelli","Luca Bortolussi"],"pdf_url":"https://arxiv.org/pdf/2412.10163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10161v1","updated":"2024-12-13T14:24:52Z","published":"2024-12-13T14:24:52Z","title":"Data Integration with Fusion Searchlight: Classifying Brain States from\n  Resting-state fMRI","summary":"  Spontaneous neural activity observed in resting-state fMRI is characterized\nby complex spatio-temporal dynamics. Different measures related to local and\nglobal brain connectivity and fluctuations in low-frequency amplitudes can\nquantify individual aspects of these neural dynamics. Even though such measures\nare derived from the same functional signals, they are often evaluated\nseparately, neglecting their interrelations and potentially reducing the\nanalysis sensitivity. In our study, we present a fusion searchlight (FuSL)\nframework to combine the complementary information contained in different\nresting-state fMRI metrics and demonstrate how this can improve the decoding of\nbrain states. Moreover, we show how explainable AI allows us to reconstruct the\ndifferential impact of each metric on the decoding, which additionally\nincreases spatial specificity of searchlight analysis. In general, this\nframework can be adapted to combine information derived from different imaging\nmodalities or experimental conditions, offering a versatile and interpretable\ntool for data fusion in neuroimaging.\n","authors":["Simon Wein","Marco Riebel","Lisa-Marie Brunner","Caroline Nothdurfter","Rainer Rupprecht","Jens V. Schwarzbach"],"pdf_url":"https://arxiv.org/pdf/2412.10161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03551v3","updated":"2024-12-13T14:11:35Z","published":"2024-03-06T08:51:09Z","title":"Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers","summary":"  Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.\n","authors":["Tim Selig","Thomas März","Martin Storath","Andreas Weinmann"],"pdf_url":"https://arxiv.org/pdf/2403.03551v3.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.04402v2","updated":"2024-12-13T14:04:57Z","published":"2024-01-09T07:57:21Z","title":"IGNITE: Individualized GeNeration of Imputations in Time-series\n  Electronic health records","summary":"  Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.\n","authors":["Ghadeer O. Ghosheh","Jin Li","Tingting Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.04402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10146v1","updated":"2024-12-13T14:02:41Z","published":"2024-12-13T14:02:41Z","title":"Investigating generalization capabilities of neural networks by means of\n  loss landscapes and Hessian analysis","summary":"  This paper studies generalization capabilities of neural networks (NNs) using\nnew and improved PyTorch library Loss Landscape Analysis (LLA). LLA facilitates\nvisualization and analysis of loss landscapes along with the properties of NN\nHessian. Different approaches to NN loss landscape plotting are discussed with\nparticular focus on normalization techniques showing that conventional methods\ncannot always ensure correct visualization when batch normalization layers are\npresent in NN architecture. The use of Hessian axes is shown to be able to\nmitigate this effect, and methods for choosing Hessian axes are proposed. In\naddition, spectra of Hessian eigendecomposition are studied and it is shown\nthat typical spectra exist for a wide range of NNs. This allows to propose\nquantitative criteria for Hessian analysis that can be applied to evaluate NN\nperformance and assess its generalization capabilities. Generalization\nexperiments are conducted using ImageNet-1K pre-trained models along with\nseveral models trained as part of this study. The experiment include training\nmodels on one dataset and testing on another one to maximize experiment\nsimilarity to model performance in the Wild. It is shown that when datasets\nchange, the changes in criteria correlate with the changes in accuracy, making\nthe proposed criteria a computationally efficient estimate of generalization\nability, which is especially useful for extremely large datasets.\n","authors":["Nikita Gabdullin"],"pdf_url":"https://arxiv.org/pdf/2412.10146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06969v3","updated":"2024-12-13T13:56:04Z","published":"2024-04-10T12:29:05Z","title":"A Fixed-Point Approach for Causal Generative Modeling","summary":"  We propose a novel formalism for describing Structural Causal Models (SCMs)\nas fixed-point problems on causally ordered variables, eliminating the need for\nDirected Acyclic Graphs (DAGs), and establish the weakest known conditions for\ntheir unique recovery given the topological ordering (TO). Based on this, we\ndesign a two-stage causal generative model that first infers in a zero-shot\nmanner a valid TO from observations, and then learns the generative SCM on the\nordered variables. To infer TOs, we propose to amortize the learning of TOs on\nsynthetically generated datasets by sequentially predicting the leaves of\ngraphs seen during training. To learn SCMs, we design a transformer-based\narchitecture that exploits a new attention mechanism enabling the modeling of\ncausal structures, and show that this parameterization is consistent with our\nformalism. Finally, we conduct an extensive evaluation of each method\nindividually, and show that when combined, our model outperforms various\nbaselines on generated out-of-distribution problems. The code is available on\n\\href{https://github.com/microsoft/causica/tree/main/research_experiments/fip}{Github}.\n","authors":["Meyer Scetbon","Joel Jennings","Agrin Hilmkil","Cheng Zhang","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2404.06969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10136v1","updated":"2024-12-13T13:32:59Z","published":"2024-12-13T13:32:59Z","title":"Can LLMs Convert Graphs to Text-Attributed Graphs?","summary":"  Graphs are ubiquitous data structures found in numerous real-world\napplications, such as drug discovery, recommender systems, and social network\nanalysis. Graph neural networks (GNNs) have become a popular tool to learn node\nembeddings through message passing on these structures. However, a significant\nchallenge arises when applying GNNs to multiple graphs with different feature\nspaces, as existing GNN architectures are not designed for cross-graph feature\nalignment. To address this, recent approaches introduce text-attributed graphs,\nwhere each node is associated with a textual description, enabling the use of a\nshared textual encoder to project nodes from different graphs into a unified\nfeature space. While promising, this method relies heavily on the availability\nof text-attributed data, which can be difficult to obtain in practice. To\nbridge this gap, we propose a novel method named Topology-Aware Node\ndescription Synthesis (TANS), which leverages large language models (LLMs) to\nautomatically convert existing graphs into text-attributed graphs. The key idea\nis to integrate topological information with each node's properties, enhancing\nthe LLMs' ability to explain how graph topology influences node semantics. We\nevaluate our TANS on text-rich, text-limited, and text-free graphs,\ndemonstrating that it enables a single GNN to operate across diverse graphs.\nNotably, on text-free graphs, our method significantly outperforms existing\napproaches that manually design node features, showcasing the potential of LLMs\nfor preprocessing graph-structured data, even in the absence of textual\ninformation. The code and data are available at\nhttps://github.com/Zehong-Wang/TANS.\n","authors":["Zehong Wang","Sidney Liu","Zheyuan Zhang","Tianyi Ma","Chuxu Zhang","Yanfang Ye"],"pdf_url":"https://arxiv.org/pdf/2412.10136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10130v1","updated":"2024-12-13T13:22:39Z","published":"2024-12-13T13:22:39Z","title":"Optimal Bounds for Private Minimum Spanning Trees via Input Perturbation","summary":"  We study the problem of privately releasing an approximate minimum spanning\ntree (MST). Given a graph $G = (V, E, \\vec{W})$ where $V$ is a set of $n$\nvertices, $E$ is a set of $m$ undirected edges, and $ \\vec{W} \\in\n\\mathbb{R}^{|E|} $ is an edge-weight vector, our goal is to publish an\napproximate MST under edge-weight differential privacy, as introduced by\nSealfon in PODS 2016, where $V$ and $E$ are considered public and the weight\nvector is private. Our neighboring relation is $\\ell_\\infty$-distance on\nweights: for a sensitivity parameter $\\Delta_\\infty$, graphs $ G = (V, E,\n\\vec{W}) $ and $ G' = (V, E, \\vec{W}') $ are neighboring if\n$\\|\\vec{W}-\\vec{W}'\\|_\\infty \\leq \\Delta_\\infty$.\n  Existing private MST algorithms face a trade-off, sacrificing either\ncomputational efficiency or accuracy. We show that it is possible to get the\nbest of both worlds: With a suitable random perturbation of the input that does\nnot suffice to make the weight vector private, the result of any non-private\nMST algorithm will be private and achieves a state-of-the-art error guarantee.\n  Furthermore, by establishing a connection to Private Top-k Selection [Steinke\nand Ullman, FOCS '17], we give the first privacy-utility trade-off lower bound\nfor MST under approximate differential privacy, demonstrating that the error\nmagnitude, $\\tilde{O}(n^{3/2})$, is optimal up to logarithmic factors. That is,\nour approach matches the time complexity of any non-private MST algorithm and\nat the same time achieves optimal error. We complement our theoretical\ntreatment with experiments that confirm the practicality of our approach.\n","authors":["Rasmus Pagh","Lukas Retschmeier","Hao Wu","Hanwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10128v1","updated":"2024-12-13T13:20:10Z","published":"2024-12-13T13:20:10Z","title":"Feature Selection for Latent Factor Models","summary":"  Feature selection is crucial for pinpointing relevant features in\nhigh-dimensional datasets, mitigating the 'curse of dimensionality,' and\nenhancing machine learning performance. Traditional feature selection methods\nfor classification use data from all classes to select features for each class.\nThis paper explores feature selection methods that select features for each\nclass separately, using class models based on low-rank generative methods and\nintroducing a signal-to-noise ratio (SNR) feature selection criterion. This\nnovel approach has theoretical true feature recovery guarantees under certain\nassumptions and is shown to outperform some existing feature selection methods\non standard classification datasets.\n","authors":["Rittwika Kansabanik","Adrian Barbu"],"pdf_url":"https://arxiv.org/pdf/2412.10128v1.pdf","comment":"Submitted to the CVPR conference"},{"id":"http://arxiv.org/abs/2410.01774v2","updated":"2024-12-13T13:14:45Z","published":"2024-10-02T17:30:21Z","title":"Trained Transformer Classifiers Generalize and Exhibit Benign\n  Overfitting In-Context","summary":"  Transformers have the capacity to act as supervised learning algorithms: by\nproperly encoding a set of labeled training (\"in-context\") examples and an\nunlabeled test example into an input sequence of vectors of the same dimension,\nthe forward pass of the transformer can produce predictions for that unlabeled\ntest example. A line of recent work has shown that when linear transformers are\npre-trained on random instances for linear regression tasks, these trained\ntransformers make predictions using an algorithm similar to that of ordinary\nleast squares. In this work, we investigate the behavior of linear transformers\ntrained on random linear classification tasks. Via an analysis of the implicit\nregularization of gradient descent, we characterize how many pre-training tasks\nand in-context examples are needed for the trained transformer to generalize\nwell at test-time. We further show that in some settings, these trained\ntransformers can exhibit \"benign overfitting in-context\": when in-context\nexamples are corrupted by label flipping noise, the transformer memorizes all\nof its in-context examples (including those with noisy labels) yet still\ngeneralizes near-optimally for clean test examples.\n","authors":["Spencer Frei","Gal Vardi"],"pdf_url":"https://arxiv.org/pdf/2410.01774v2.pdf","comment":"36 pages; added experiments"},{"id":"http://arxiv.org/abs/2412.10119v1","updated":"2024-12-13T13:04:46Z","published":"2024-12-13T13:04:46Z","title":"AMUSE: Adaptive Model Updating using a Simulated Environment","summary":"  Prediction models frequently face the challenge of concept drift, in which\nthe underlying data distribution changes over time, weakening performance.\nExamples can include models which predict loan default, or those used in\nhealthcare contexts. Typical management strategies involve regular model\nupdates or updates triggered by concept drift detection. However, these simple\npolicies do not necessarily balance the cost of model updating with improved\nclassifier performance. We present AMUSE (Adaptive Model Updating using a\nSimulated Environment), a novel method leveraging reinforcement learning\ntrained within a simulated data generating environment, to determine update\ntimings for classifiers. The optimal updating policy depends on the current\ndata generating process and ongoing drift process. Our key idea is that we can\ntrain an arbitrarily complex model updating policy by creating a training\nenvironment in which possible episodes of drift are simulated by a parametric\nmodel, which represents expectations of possible drift patterns. As a result,\nAMUSE proactively recommends updates based on estimated performance\nimprovements, learning a policy that balances maintaining model performance\nwith minimizing update costs. Empirical results confirm the effectiveness of\nAMUSE in simulated data.\n","authors":["Louis Chislett","Catalina A. Vallejos","Timothy I. Cannings","James Liley"],"pdf_url":"https://arxiv.org/pdf/2412.10119v1.pdf","comment":"12 pages, 2 tables. Submitted to AIStats 2025 (under review)"},{"id":"http://arxiv.org/abs/2412.10117v1","updated":"2024-12-13T12:59:39Z","published":"2024-12-13T12:59:39Z","title":"CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language\n  Models","summary":"  In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2.\n","authors":["Zhihao Du","Yuxuan Wang","Qian Chen","Xian Shi","Xiang Lv","Tianyu Zhao","Zhifu Gao","Yexin Yang","Changfeng Gao","Hui Wang","Fan Yu","Huadai Liu","Zhengyan Sheng","Yue Gu","Chong Deng","Wen Wang","Shiliang Zhang","Zhijie Yan","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.10117v1.pdf","comment":"Tech report, work in progress"},{"id":"http://arxiv.org/abs/2412.10107v1","updated":"2024-12-13T12:48:15Z","published":"2024-12-13T12:48:15Z","title":"NetOrchLLM: Mastering Wireless Network Orchestration with Large Language\n  Models","summary":"  The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector.\n","authors":["Asmaa Abdallah","Abdullatif Albaseer","Abdulkadir Celik","Mohamed Abdallah","Ahmed M. Eltawil"],"pdf_url":"https://arxiv.org/pdf/2412.10107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07205v2","updated":"2024-12-13T12:38:04Z","published":"2024-12-10T05:50:50Z","title":"Crack-EdgeSAM Self-Prompting Crack Segmentation System for Edge Devices","summary":"  Structural health monitoring (SHM) is essential for the early detection of\ninfrastructure defects, such as cracks in concrete bridge pier. but often faces\nchallenges in efficiency and accuracy in complex environments. Although the\nSegment Anything Model (SAM) achieves excellent segmentation performance, its\ncomputational demands limit its suitability for real-time applications on edge\ndevices. To address these challenges, this paper proposes Crack-EdgeSAM, a\nself-prompting crack segmentation system that integrates YOLOv8 for generating\nprompt boxes and a fine-tuned EdgeSAM model for crack segmentation. To ensure\ncomputational efficiency, the method employs ConvLoRA, a Parameter-Efficient\nFine-Tuning (PEFT) technique, along with DiceFocalLoss to fine-tune the EdgeSAM\nmodel. Our experimental results on public datasets and the climbing robot\nautomatic inspections demonstrate that the system achieves high segmentation\naccuracy and significantly enhanced inference speed compared to the most recent\nmethods. Notably, the system processes 1024 x 1024 pixels images at 46 FPS on\nour PC and 8 FPS on Jetson Orin Nano.\n","authors":["Yingchu Wang","Ji He","Shijie Yu"],"pdf_url":"https://arxiv.org/pdf/2412.07205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10096v1","updated":"2024-12-13T12:32:53Z","published":"2024-12-13T12:32:53Z","title":"Reward Machine Inference for Robotic Manipulation","summary":"  Learning from Demonstrations (LfD) and Reinforcement Learning (RL) have\nenabled robot agents to accomplish complex tasks. Reward Machines (RMs) enhance\nRL's capability to train policies over extended time horizons by structuring\nhigh-level task information. In this work, we introduce a novel LfD approach\nfor learning RMs directly from visual demonstrations of robotic manipulation\ntasks. Unlike previous methods, our approach requires no predefined\npropositions or prior knowledge of the underlying sparse reward signals.\nInstead, it jointly learns the RM structure and identifies key high-level\nevents that drive transitions between RM states. We validate our method on\nvision-based manipulation tasks, showing that the inferred RM accurately\ncaptures task structure and enables an RL agent to effectively learn an optimal\npolicy.\n","authors":["Mattijs Baert","Sam Leroux","Pieter Simoens"],"pdf_url":"https://arxiv.org/pdf/2412.10096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10095v1","updated":"2024-12-13T12:31:06Z","published":"2024-12-13T12:31:06Z","title":"HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language\n  Transfer and Automatic Data Annotation","summary":"  In this paper we present our submission for the NorSID Shared Task as part of\nthe 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks:\nIntent Detection, Slot Filling and Dialect Identification, evaluated using data\nin different dialects of the Norwegian language. For Intent Detection and Slot\nFilling, we have fine-tuned a multitask model in a cross-lingual setting, to\nleverage the xSID dataset available in 17 languages. In the case of Dialect\nIdentification, our final submission consists of a model fine-tuned on the\nprovided development set, which has obtained the highest scores within our\nexperiments. Our final results on the test set show that our models do not drop\nin performance compared to the development set, likely due to the\ndomain-specificity of the dataset and the similar distribution of both subsets.\nFinally, we also report an in-depth analysis of the provided datasets and their\nartifacts, as well as other sets of experiments that have been carried out but\ndid not yield the best results. Additionally, we present an analysis on the\nreasons why some methods have been more successful than others; mainly the\nimpact of the combination of languages and domain-specificity of the training\ndata on the results.\n","authors":["Jaione Bengoetxea","Mikel Zubillaga","Ekhi Azurmendi","Maite Heredia","Julen Etxaniz","Markel Ferro","Jeremy Barnes"],"pdf_url":"https://arxiv.org/pdf/2412.10095v1.pdf","comment":"Vardial 2025 NorSID Shared Task"},{"id":"http://arxiv.org/abs/2412.10092v1","updated":"2024-12-13T12:30:09Z","published":"2024-12-13T12:30:09Z","title":"A Survey on Knowledge Graph Structure and Knowledge Graph Embeddings","summary":"  Knowledge Graphs (KGs) and their machine learning counterpart, Knowledge\nGraph Embedding Models (KGEMs), have seen ever-increasing use in a wide variety\nof academic and applied settings. In particular, KGEMs are typically applied to\nKGs to solve the link prediction task; i.e. to predict new facts in the domain\nof a KG based on existing, observed facts. While this approach has been shown\nsubstantial power in many end-use cases, it remains incompletely characterised\nin terms of how KGEMs react differently to KG structure. This is of particular\nconcern in light of recent studies showing that KG structure can be a\nsignificant source of bias as well as partially determinant of overall KGEM\nperformance. This paper seeks to address this gap in the state-of-the-art. This\npaper provides, to the authors' knowledge, the first comprehensive survey\nexploring established relationships of Knowledge Graph Embedding Models and\nGraph structure in the literature. It is the hope of the authors that this work\nwill inspire further studies in this area, and contribute to a more holistic\nunderstanding of KGs, KGEMs, and the link prediction task.\n","authors":["Jeffrey Sardina","John D. Kelleher","Declan O'Sullivan"],"pdf_url":"https://arxiv.org/pdf/2412.10092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00489v3","updated":"2024-12-13T12:11:01Z","published":"2024-06-01T16:38:43Z","title":"Efficient Sign-Based Optimization: Accelerating Convergence via Variance\n  Reduction","summary":"  Sign stochastic gradient descent (signSGD) is a communication-efficient\nmethod that transmits only the sign of stochastic gradients for parameter\nupdating. Existing literature has demonstrated that signSGD can achieve a\nconvergence rate of $\\mathcal{O}(d^{1/2}T^{-1/4})$, where $d$ represents the\ndimension and $T$ is the iteration number. In this paper, we improve this\nconvergence rate to $\\mathcal{O}(d^{1/2}T^{-1/3})$ by introducing the\nSign-based Stochastic Variance Reduction (SSVR) method, which employs variance\nreduction estimators to track gradients and leverages their signs to update.\nFor finite-sum problems, our method can be further enhanced to achieve a\nconvergence rate of $\\mathcal{O}(m^{1/4}d^{1/2}T^{-1/2})$, where $m$ denotes\nthe number of component functions. Furthermore, we investigate the\nheterogeneous majority vote in distributed settings and introduce two novel\nalgorithms that attain improved convergence rates of\n$\\mathcal{O}(d^{1/2}T^{-1/2} + dn^{-1/2})$ and $\\mathcal{O}(d^{1/4}T^{-1/4})$\nrespectively, outperforming the previous results of $\\mathcal{O}(dT^{-1/4} +\ndn^{-1/2})$ and $\\mathcal{O}(d^{3/8}T^{-1/8})$, where $n$ represents the number\nof nodes. Numerical experiments across different tasks validate the\neffectiveness of our proposed methods.\n","authors":["Wei Jiang","Sifan Yang","Wenhao Yang","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00489v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10064v1","updated":"2024-12-13T11:50:51Z","published":"2024-12-13T11:50:51Z","title":"Text2Cypher: Bridging Natural Language and Graph Databases","summary":"  Knowledge graphs use nodes, relationships, and properties to represent\narbitrarily complex data. When stored in a graph database, the Cypher query\nlanguage enables efficient modeling and querying of knowledge graphs. However,\nusing Cypher requires specialized knowledge, which can present a challenge for\nnon-expert users. Our work Text2Cypher aims to bridge this gap by translating\nnatural language queries into Cypher query language and extending the utility\nof knowledge graphs to non-technical expert users.\n  While large language models (LLMs) can be used for this purpose, they often\nstruggle to capture complex nuances, resulting in incomplete or incorrect\noutputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more\npromising approach, but the limited availability of high-quality, publicly\navailable Text2Cypher datasets makes this challenging. In this work, we show\nhow we combined, cleaned and organized several publicly available datasets into\na total of 44,387 instances, enabling effective fine-tuning and evaluation.\nModels fine-tuned on this dataset showed significant performance gains, with\nimprovements in Google-BLEU and Exact Match scores over baseline models,\nhighlighting the importance of high-quality datasets and fine-tuning in\nimproving Text2Cypher performance.\n","authors":["Makbule Gulcin Ozsoy","Leila Messallem","Jon Besga","Gianandrea Minneci"],"pdf_url":"https://arxiv.org/pdf/2412.10064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03389v2","updated":"2024-12-13T11:01:06Z","published":"2024-07-03T09:06:19Z","title":"A Deterministic Information Bottleneck Method for Clustering Mixed-Type\n  Data","summary":"  In this paper, we present an information-theoretic method for clustering\nmixed-type data, that is, data consisting of both continuous and categorical\nvariables. The proposed approach is built on the deterministic variant of the\nInformation Bottleneck algorithm, designed to optimally compress data while\npreserving its relevant structural information. We evaluate the performance of\nour method against four well-established clustering techniques for mixed-type\ndata -- KAMILA, K-Prototypes, Factor Analysis for Mixed Data with K-Means, and\nPartitioning Around Medoids using Gower's dissimilarity -- using both simulated\nand real-world datasets. The results highlight that the proposed approach\noffers a competitive alternative to traditional clustering techniques,\nparticularly under specific conditions where heterogeneity in data poses\nsignificant challenges.\n","authors":["Efthymios Costa","Ioanna Papatsouma","Angelos Markos"],"pdf_url":"https://arxiv.org/pdf/2407.03389v2.pdf","comment":"Submitted to Pattern Recognition"},{"id":"http://arxiv.org/abs/2401.16920v2","updated":"2024-12-13T10:20:13Z","published":"2024-01-30T11:42:52Z","title":"Sparse Portfolio Selection via Topological Data Analysis based\n  Clustering","summary":"  This paper uses topological data analysis (TDA) tools and introduces a\ndata-driven clustering-based stock selection strategy tailored for sparse\nportfolio construction. Our asset selection strategy exploits the topological\nfeatures of stock price movements to select a subset of topologically similar\n(different) assets for a sparse index tracking (Markowitz) portfolio. We\nintroduce new distance measures, which serve as an input to the clustering\nalgorithm, on the space of persistence diagrams and landscapes that consider\nthe time component of a time series. We conduct an empirical analysis on the\nS\\&P index from 2009 to 2022, including a study on the COVID-19 data to\nvalidate the robustness of our methodology. Our strategy to integrate TDA with\nthe clustering algorithm significantly enhanced the performance of sparse\nportfolios across various performance measures in diverse market scenarios.\n","authors":["Anubha Goel","Damir Filipović","Puneet Pasricha"],"pdf_url":"https://arxiv.org/pdf/2401.16920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06175v2","updated":"2024-12-13T10:11:31Z","published":"2024-11-09T13:17:39Z","title":"Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs","summary":"  This paper introduces a novel semi-supervised learning framework specifically\ndesigned for text classification tasks, effectively addressing the challenge of\nvast datasets with limited labeled examples. By integrating multi-level\nsimilarity based data augmentation techniques from Retrieval-Augmented\nGeneration (RAG) to Large Language Model (LLM) rewriting and traditional word\nsubstitution-we constructed an intelligent augmentation pipeline. This\nframework innovatively employs the selection of representative landmarks\nthrough clustering, which serve as intermediaries in the retrieval and\nrewriting processes, ensuring that the augmented data maintains a distribution\nsimilar to the original dataset. Empirical results show that even in complex\ntext document classification scenarios with over 100 categories, our method\nachieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and\nWeb of Science datasets, respectively. These findings highlight the\neffectiveness and broad applicability of our semi-supervised learning approach\nfor text classification tasks.\n","authors":["Shan Zhong","Jiahao Zeng","Yongxin Yu","Bohong Lin"],"pdf_url":"https://arxiv.org/pdf/2411.06175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04230v3","updated":"2024-12-13T10:03:10Z","published":"2024-05-07T11:50:25Z","title":"Unveiling the optimization process of Physics Informed Neural Networks:\n  How accurate and competitive can PINNs be?","summary":"  This study investigates the potential accuracy boundaries of physics-informed\nneural networks, contrasting their approach with previous similar works and\ntraditional numerical methods. We find that selecting improved optimization\nalgorithms significantly enhances the accuracy of the results. Simple\nmodifications to the loss function may also improve precision, offering an\nadditional avenue for enhancement. Despite optimization algorithms having a\ngreater impact on convergence than adjustments to the loss function, practical\nconsiderations often favor tweaking the latter due to ease of implementation.\nOn a global scale, the integration of an enhanced optimizer and a marginally\nadjusted loss function enables a reduction in the loss function by several\norders of magnitude across diverse physical problems. Consequently, our results\nobtained using compact networks (typically comprising 2 or 3 layers of 20-30\nneurons) achieve accuracies comparable to finite difference schemes employing\nthousands of grid points. This study encourages the continued advancement of\nPINNs and associated optimization techniques for broader applications across\nvarious fields.\n","authors":["Jorge F. Urbán","Petros Stefanou","José A. Pons"],"pdf_url":"https://arxiv.org/pdf/2405.04230v3.pdf","comment":"63 pages, 25 figures. This is the author-accepted manuscript of the\n  paper published in Journal of Computational Physics"},{"id":"http://arxiv.org/abs/2412.10011v1","updated":"2024-12-13T09:55:03Z","published":"2024-12-13T09:55:03Z","title":"Enhanced Speech Emotion Recognition with Efficient Channel Attention\n  Guided Deep CNN-BiLSTM Framework","summary":"  Speech emotion recognition (SER) is crucial for enhancing affective computing\nand enriching the domain of human-computer interaction. However, the main\nchallenge in SER lies in selecting relevant feature representations from speech\nsignals with lower computational costs. In this paper, we propose a lightweight\nSER architecture that integrates attention-based local feature blocks (ALFBs)\nto capture high-level relevant feature vectors from speech signals. We also\nincorporate a global feature block (GFB) technique to capture sequential,\nglobal information and long-term dependencies in speech signals. By aggregating\nattention-based local and global contextual feature vectors, our model\neffectively captures the internal correlation between salient features that\nreflect complex human emotional cues. To evaluate our approach, we extracted\nfour types of spectral features from speech audio samples: mel-frequency\ncepstral coefficients, mel-spectrogram, root mean square value, and\nzero-crossing rate. Through a 5-fold cross-validation strategy, we tested the\nproposed method on five multi-lingual standard benchmark datasets: TESS,\nRAVDESS, BanglaSER, SUBESCO, and Emo-DB, and obtained a mean accuracy of\n99.65%, 94.88%, 98.12%, 97.94%, and 97.19% respectively. The results indicate\nthat our model achieves state-of-the-art (SOTA) performance compared to most\nexisting methods.\n","authors":["Niloy Kumar Kundu","Sarah Kobir","Md. Rayhan Ahmed","Tahmina Aktar","Niloya Roy"],"pdf_url":"https://arxiv.org/pdf/2412.10011v1.pdf","comment":"42 pages,10 figures"},{"id":"http://arxiv.org/abs/2412.09602v2","updated":"2024-12-13T09:51:22Z","published":"2024-12-12T18:59:13Z","title":"Hidden Biases of End-to-End Driving Datasets","summary":"  End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.\n","authors":["Julian Zimmerlin","Jens Beißwenger","Bernhard Jaeger","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2412.09602v2.pdf","comment":"Technical report for the CVPR 2024 Workshop on Foundation Models for\n  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving\n  Challenge' in the 2024 Autonomous Grand Challenge\n  (https://opendrivelab.com/challenge2024/)"},{"id":"http://arxiv.org/abs/2412.10009v1","updated":"2024-12-13T09:49:40Z","published":"2024-12-13T09:49:40Z","title":"Class flipping for uplift modeling and Heterogeneous Treatment Effect\n  estimation on imbalanced RCT data","summary":"  Uplift modeling and Heterogeneous Treatment Effect (HTE) estimation aim at\npredicting the causal effect of an action, such as a medical treatment or a\nmarketing campaign on a specific individual. In this paper, we focus on data\nfrom Randomized Controlled Experiments which guarantee causal interpretation of\nthe outcomes. Class and treatment imbalance are important problems in uplift\nmodeling/HTE, but classical undersampling or oversampling based approaches are\nhard to apply in this case since they distort the predicted effect. Calibration\nmethods have been proposed in the past, however, they do not guarantee correct\npredictions. In this work, we propose an approach alternative to undersampling,\nbased on flipping the class value of selected records. We show that the\nproposed approach does not distort the predicted effect and does not require\ncalibration. The method is especially useful for models based on class variable\ntransformation (modified outcome models). We address those models separately,\ndesigning a transformation scheme which guarantees correct predictions and\naddresses also the problem of treatment imbalance which is especially important\nfor those models. Experiments fully confirm our theoretical results.\nAdditionally, we demonstrate that our method is a viable alternative also for\nstandard classification problems.\n","authors":["Krzysztof Rudaś","Szymon Jaroszewicz"],"pdf_url":"https://arxiv.org/pdf/2412.10009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07450v2","updated":"2024-12-13T09:47:04Z","published":"2024-07-10T08:07:55Z","title":"Using Low-Discrepancy Points for Data Compression in Machine Learning:\n  An Experimental Comparison","summary":"  Low-discrepancy points (also called Quasi-Monte Carlo points) are\ndeterministically and cleverly chosen point sets in the unit cube, which\nprovide an approximation of the uniform distribution. We explore two methods\nbased on such low-discrepancy points to reduce large data sets in order to\ntrain neural networks. The first one is the method of Dick and Feischl [4],\nwhich relies on digital nets and an averaging procedure. Motivated by our\nexperimental findings, we construct a second method, which again uses digital\nnets, but Voronoi clustering instead of averaging. Both methods are compared to\nthe supercompress approach of [14], which is a variant of the K-means\nclustering algorithm. The comparison is done in terms of the compression error\nfor different objective functions and the accuracy of the training of a neural\nnetwork.\n","authors":["Simone Göttlich","Jacob Heieck","Andreas Neuenkirch"],"pdf_url":"https://arxiv.org/pdf/2407.07450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10005v1","updated":"2024-12-13T09:42:42Z","published":"2024-12-13T09:42:42Z","title":"Matrix Completion via Residual Spectral Matching","summary":"  Noisy matrix completion has attracted significant attention due to its\napplications in recommendation systems, signal processing and image\nrestoration. Most existing works rely on (weighted) least squares methods under\nvarious low-rank constraints. However, minimizing the sum of squared residuals\nis not always efficient, as it may ignore the potential structural information\nin the residuals.In this study, we propose a novel residual spectral matching\ncriterion that incorporates not only the numerical but also locational\ninformation of residuals. This criterion is the first in noisy matrix\ncompletion to adopt the perspective of low-rank perturbation of random matrices\nand exploit the spectral properties of sparse random matrices. We derive\noptimal statistical properties by analyzing the spectral properties of sparse\nrandom matrices and bounding the effects of low-rank perturbations and partial\nobservations. Additionally, we propose algorithms that efficiently approximate\nsolutions by constructing easily computable pseudo-gradients. The iterative\nprocess of the proposed algorithms ensures convergence at a rate consistent\nwith the optimal statistical error bound. Our method and algorithms demonstrate\nimproved numerical performance in both simulated and real data examples,\nparticularly in environments with high noise levels.\n","authors":["Ziyuan Chen","Fang Yao"],"pdf_url":"https://arxiv.org/pdf/2412.10005v1.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.18380v3","updated":"2024-12-13T09:34:22Z","published":"2024-06-26T14:21:21Z","title":"KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning","summary":"  In recent years, Graph Neural Networks (GNNs) have become the de facto tool\nfor learning node and graph representations. Most GNNs typically consist of a\nsequence of neighborhood aggregation (a.k.a., message-passing) layers, within\nwhich the representation of each node is updated based on those of its\nneighbors. The most expressive message-passing GNNs can be obtained through the\nuse of the sum aggregator and of MLPs for feature transformation, thanks to\ntheir universal approximation capabilities. However, the limitations of MLPs\nrecently motivated the introduction of another family of universal\napproximators, called Kolmogorov-Arnold Networks (KANs) which rely on a\ndifferent representation theorem. In this work, we compare the performance of\nKANs against that of MLPs on graph learning tasks. We evaluate two different\nimplementations of KANs using two distinct base families of functions, namely\nB-splines and radial basis functions. We perform extensive experiments on node\nclassification, graph classification and graph regression datasets. Our results\nindicate that KANs are on-par with or better than MLPs on all studied tasks,\nmaking them viable alternatives, at the cost of some computational complexity.\nCode is available at https: //github.com/RomanBresson/KAGNN.\n","authors":["Roman Bresson","Giannis Nikolentzos","George Panagopoulos","Michail Chatzianastasis","Jun Pang","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2406.18380v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14347v2","updated":"2024-12-13T09:32:48Z","published":"2024-06-20T14:14:59Z","title":"$\\nabla^2$DFT: A Universal Quantum Chemistry Dataset of Drug-Like\n  Molecules and a Benchmark for Neural Network Potentials","summary":"  Methods of computational quantum chemistry provide accurate approximations of\nmolecular properties crucial for computer-aided drug discovery and other areas\nof chemical science. However, high computational complexity limits the\nscalability of their applications. Neural network potentials (NNPs) are a\npromising alternative to quantum chemistry methods, but they require large and\ndiverse datasets for training. This work presents a new dataset and benchmark\ncalled $\\nabla^2$DFT that is based on the nablaDFT. It contains twice as much\nmolecular structures, three times more conformations, new data types and tasks,\nand state-of-the-art models. The dataset includes energies, forces, 17\nmolecular properties, Hamiltonian and overlap matrices, and a wavefunction\nobject. All calculations were performed at the DFT level\n($\\omega$B97X-D/def2-SVP) for each conformation. Moreover, $\\nabla^2$DFT is the\nfirst dataset that contains relaxation trajectories for a substantial number of\ndrug-like molecules. We also introduce a novel benchmark for evaluating NNPs in\nmolecular property prediction, Hamiltonian prediction, and conformational\noptimization tasks. Finally, we propose an extendable framework for training\nNNPs and implement 10 models within it.\n","authors":["Kuzma Khrabrov","Anton Ber","Artem Tsypin","Konstantin Ushenin","Egor Rumiantsev","Alexander Telepov","Dmitry Protasov","Ilya Shenbin","Anton Alekseev","Mikhail Shirokikh","Sergey Nikolenko","Elena Tutubalina","Artur Kadurin"],"pdf_url":"https://arxiv.org/pdf/2406.14347v2.pdf","comment":"Published as a conference paper at NeurIPS2024 Track on Datasets and\n  Benchmarks (Poster)"},{"id":"http://arxiv.org/abs/2403.19895v2","updated":"2024-12-13T09:26:53Z","published":"2024-03-29T00:29:57Z","title":"An Information-Theoretic Framework for Out-of-Distribution\n  Generalization with Applications to Stochastic Gradient Langevin Dynamics","summary":"  We study the Out-of-Distribution (OOD) generalization in machine learning and\npropose a general framework that establishes information-theoretic\ngeneralization bounds. Our framework interpolates freely between Integral\nProbability Metric (IPM) and $f$-divergence, which naturally recovers some\nknown results (including Wasserstein- and KL-bounds), as well as yields new\ngeneralization bounds. Additionally, we show that our framework admits an\noptimal transport interpretation. When evaluated in two concrete examples, the\nproposed bounds either strictly improve upon existing bounds in some cases or\nmatch the best existing OOD generalization bounds. Moreover, by focusing on\n$f$-divergence and combining it with the Conditional Mutual Information (CMI)\nmethods, we derive a family of CMI-based generalization bounds, which include\nthe state-of-the-art ICIMI bound as a special instance. Finally, leveraging\nthese findings, we analyze the generalization of the Stochastic Gradient\nLangevin Dynamics (SGLD) algorithm, showing that our derived generalization\nbounds outperform existing information-theoretic generalization bounds in\ncertain scenarios.\n","authors":["Wenliang Liu","Guanding Yu","Lele Wang","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2403.19895v2.pdf","comment":"This work was accepted in part at the 2024 IEEE International\n  Symposium on Information Theory and the 2024 Canadian Workshop on Information\n  Theory. This work was submitted to IEEE Transactions on Information Theory"},{"id":"http://arxiv.org/abs/2412.09989v1","updated":"2024-12-13T09:21:02Z","published":"2024-12-13T09:21:02Z","title":"One Filter to Deploy Them All: Robust Safety for Quadrupedal Navigation\n  in Unknown Environments","summary":"  As learning-based methods for legged robots rapidly grow in popularity, it is\nimportant that we can provide safety assurances efficiently across different\ncontrollers and environments. Existing works either rely on a priori knowledge\nof the environment and safety constraints to ensure system safety or provide\nassurances for a specific locomotion policy. To address these limitations, we\npropose an observation-conditioned reachability-based (OCR) safety-filter\nframework. Our key idea is to use an OCR value network (OCR-VN) that predicts\nthe optimal control-theoretic safety value function for new failure regions and\ndynamic uncertainty during deployment time. Specifically, the OCR-VN\nfacilitates rapid safety adaptation through two key components: a LiDAR-based\ninput that allows the dynamic construction of safe regions in light of new\nobstacles and a disturbance estimation module that accounts for dynamics\nuncertainty in the wild. The predicted safety value function is used to\nconstruct an adaptive safety filter that overrides the nominal quadruped\ncontroller when necessary to maintain safety. Through simulation studies and\nhardware experiments on a Unitree Go1 quadruped, we demonstrate that the\nproposed framework can automatically safeguard a wide range of hierarchical\nquadruped controllers, adapts to novel environments, and is robust to unmodeled\ndynamics without a priori access to the controllers or environments - hence,\n\"One Filter to Deploy Them All\". The experiment videos can be found on the\nproject website.\n","authors":["Albert Lin","Shuang Peng","Somil Bansal"],"pdf_url":"https://arxiv.org/pdf/2412.09989v1.pdf","comment":"Project website:\n  https://sia-lab-git.github.io/One_Filter_to_Deploy_Them_All/"},{"id":"http://arxiv.org/abs/2412.09980v1","updated":"2024-12-13T09:07:41Z","published":"2024-12-13T09:07:41Z","title":"Real-Time Fall Detection Using Smartphone Accelerometers and WiFi\n  Channel State Information","summary":"  In recent years, as the population ages, falls have increasingly posed a\nsignificant threat to the health of the elderly. We propose a real-time fall\ndetection system that integrates the inertial measurement unit (IMU) of a\nsmartphone with optimized Wi-Fi channel state information (CSI) for secondary\nvalidation. Initially, the IMU distinguishes falls from routine daily\nactivities with minimal computational demand. Subsequently, the CSI is employed\nfor further assessment, which includes evaluating the individual's post-fall\nmobility. This methodology not only achieves high accuracy but also reduces\nenergy consumption in the smartphone platform. An Android application developed\nspecifically for the purpose issues an emergency alert if the user experiences\na fall and is unable to move. Experimental results indicate that the CSI model,\nbased on convolutional neural networks (CNN), achieves a detection accuracy of\n99%, \\revised{surpassing comparable IMU-only models, and demonstrating\nsignificant resilience in distinguishing between falls and non-fall activities.\n","authors":["Lingyun Wang","Deqi Su","Aohua Zhang","Yujun Zhu","Weiwei Jiang","Xin He","Panlong Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09972v1","updated":"2024-12-13T08:59:18Z","published":"2024-12-13T08:59:18Z","title":"Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial\n  Data Management Perspective","summary":"  Road traffic forecasting is crucial in real-world intelligent transportation\nscenarios like traffic dispatching and path planning in city management and\npersonal traveling. Spatio-temporal graph neural networks (STGNNs) stand out as\nthe mainstream solution in this task. Nevertheless, the quadratic complexity of\nremarkable dynamic spatial modeling-based STGNNs has become the bottleneck over\nlarge-scale traffic data. From the spatial data management perspective, we\npresent a novel Transformer framework called PatchSTG to efficiently and\ndynamically model spatial dependencies for large-scale traffic forecasting with\ninterpretability and fidelity. Specifically, we design a novel irregular\nspatial patching to reduce the number of points involved in the dynamic\ncalculation of Transformer. The irregular spatial patching first utilizes the\nleaf K-dimensional tree (KDTree) to recursively partition irregularly\ndistributed traffic points into leaf nodes with a small capacity, and then\nmerges leaf nodes belonging to the same subtree into occupancy-equaled and\nnon-overlapped patches through padding and backtracking. Based on the patched\ndata, depth and breadth attention are used interchangeably in the encoder to\ndynamically learn local and global spatial knowledge from points in a patch and\npoints with the same index of patches. Experimental results on four real world\nlarge-scale traffic datasets show that our PatchSTG achieves train speed and\nmemory utilization improvements up to $10\\times$ and $4\\times$ with the\nstate-of-the-art performance.\n","authors":["Yuchen Fang","Yuxuan Liang","Bo Hui","Zezhi Shao","Liwei Deng","Xu Liu","Xinke Jiang","Kai Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.09972v1.pdf","comment":"Accepted by SIGKDD 2025"},{"id":"http://arxiv.org/abs/2412.09968v1","updated":"2024-12-13T08:55:02Z","published":"2024-12-13T08:55:02Z","title":"GraSP: Simple yet Effective Graph Similarity Predictions","summary":"  Graph similarity computation (GSC) is to calculate the similarity between one\npair of graphs, which is a fundamental problem with fruitful applications in\nthe graph community. In GSC, graph edit distance (GED) and maximum common\nsubgraph (MCS) are two important similarity metrics, both of which are NP-hard\nto compute. Instead of calculating the exact values, recent solutions resort to\nleveraging graph neural networks (GNNs) to learn data-driven models for the\nestimation of GED and MCS. Most of them are built on components involving\nnode-level interactions crossing graphs, which engender vast computation\noverhead but are of little avail in effectiveness. In the paper, we present\nGraSP, a simple yet effective GSC approach for GED and MCS prediction. GraSP\nachieves high result efficacy through several key instruments: enhanced node\nfeatures via positional encoding and a GNN model augmented by a gating\nmechanism, residual connections, as well as multi-scale pooling. Theoretically,\nGraSP can surpass the 1-WL test, indicating its high expressiveness.\nEmpirically, extensive experiments comparing GraSP against 10 competitors on\nmultiple widely adopted benchmark datasets showcase the superiority of GraSP\nover prior arts in terms of both effectiveness and efficiency. The code is\navailable at https://github.com/HaoranZ99/GraSP.\n","authors":["Haoran Zheng","Jieming Shi","Renchi Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09968v1.pdf","comment":"Accepted by AAAI2025. 13 pages, 14 figures. The code is available at\n  https://github.com/HaoranZ99/GraSP"},{"id":"http://arxiv.org/abs/2412.09966v1","updated":"2024-12-13T08:49:25Z","published":"2024-12-13T08:49:25Z","title":"EP-CFG: Energy-Preserving Classifier-Free Guidance","summary":"  Classifier-free guidance (CFG) is widely used in diffusion models but often\nintroduces over-contrast and over-saturation artifacts at higher guidance\nstrengths. We present EP-CFG (Energy-Preserving Classifier-Free Guidance),\nwhich addresses these issues by preserving the energy distribution of the\nconditional prediction during the guidance process. Our method simply rescales\nthe energy of the guided output to match that of the conditional prediction at\neach denoising step, with an optional robust variant for improved artifact\nsuppression. Through experiments, we show that EP-CFG maintains natural image\nquality and preserves details across guidance strengths while retaining CFG's\nsemantic alignment benefits, all with minimal computational overhead.\n","authors":["Kai Zhang","Fujun Luan","Sai Bi","Jianming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09265v2","updated":"2024-12-13T08:44:16Z","published":"2024-12-12T13:22:02Z","title":"Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation","summary":"  Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.\n","authors":["Bofang Jia","Pengxiang Ding","Can Cui","Mingyang Sun","Pengfang Qian","Siteng Huang","Zhaoxin Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09265v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2412.09961v1","updated":"2024-12-13T08:42:19Z","published":"2024-12-13T08:42:19Z","title":"What constitutes a Deep Fake? The blurry line between legitimate\n  processing and manipulation under the EU AI Act","summary":"  When does a digital image resemble reality? The relevance of this question\nincreases as the generation of synthetic images -- so called deep fakes --\nbecomes increasingly popular. Deep fakes have gained much attention for a\nnumber of reasons -- among others, due to their potential to disrupt the\npolitical climate. In order to mitigate these threats, the EU AI Act implements\nspecific transparency regulations for generating synthetic content or\nmanipulating existing content. However, the distinction between real and\nsynthetic images is -- even from a computer vision perspective -- far from\ntrivial. We argue that the current definition of deep fakes in the AI act and\nthe corresponding obligations are not sufficiently specified to tackle the\nchallenges posed by deep fakes. By analyzing the life cycle of a digital photo\nfrom the camera sensor to the digital editing features, we find that: (1.) Deep\nfakes are ill-defined in the EU AI Act. The definition leaves too much scope\nfor what a deep fake is. (2.) It is unclear how editing functions like Google's\n``best take'' feature can be considered as an exception to transparency\nobligations. (3.) The exception for substantially edited images raises\nquestions about what constitutes substantial editing of content and whether or\nnot this editing must be perceptible by a natural person. Our results\ndemonstrate that complying with the current AI Act transparency obligations is\ndifficult for providers and deployers. As a consequence of the unclear\nprovisions, there is a risk that exceptions may be either too broad or too\nlimited. We intend our analysis to foster the discussion on what constitutes a\ndeep fake and to raise awareness about the pitfalls in the current AI Act\ntransparency obligations.\n","authors":["Kristof Meding","Christoph Sorge"],"pdf_url":"https://arxiv.org/pdf/2412.09961v1.pdf","comment":"Preprint. Accepted at ACM CS&Law '25"},{"id":"http://arxiv.org/abs/2412.08050v2","updated":"2024-12-13T08:38:29Z","published":"2024-12-11T02:56:23Z","title":"BSAFusion: A Bidirectional Stepwise Feature Alignment Network for\n  Unaligned Medical Image Fusion","summary":"  If unaligned multimodal medical images can be simultaneously aligned and\nfused using a single-stage approach within a unified processing framework, it\nwill not only achieve mutual promotion of dual tasks but also help reduce the\ncomplexity of the model. However, the design of this model faces the challenge\nof incompatible requirements for feature fusion and alignment; specifically,\nfeature alignment requires consistency among corresponding features, whereas\nfeature fusion requires the features to be complementary to each other. To\naddress this challenge, this paper proposes an unaligned medical image fusion\nmethod called Bidirectional Stepwise Feature Alignment and Fusion (BSFA-F)\nstrategy. To reduce the negative impact of modality differences on cross-modal\nfeature matching, we incorporate the Modal Discrepancy-Free Feature\nRepresentation (MDF-FR) method into BSFA-F. MDF-FR utilizes a Modality Feature\nRepresentation Head (MFRH) to integrate the global information of the input\nimage. By injecting the information contained in MFRH of the current image into\nother modality images, it effectively reduces the impact of modality\ndifferences on feature alignment while preserving the complementary information\ncarried by different images. In terms of feature alignment, BSFA-F employs a\nbidirectional stepwise alignment deformation field prediction strategy based on\nthe path independence of vector displacement between two points. This strategy\nsolves the problem of large spans and inaccurate deformation field prediction\nin single-step alignment. Finally, Multi-Modal Feature Fusion block achieves\nthe fusion of aligned features. The experimental results across multiple\ndatasets demonstrate the effectiveness of our method. The source code is\navailable at https://github.com/slrl123/BSAFusion.\n","authors":["Huafeng Li","Dayong Su","Qing Cai","Yafei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08050v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2411.07853v2","updated":"2024-12-13T08:36:29Z","published":"2024-11-12T15:06:04Z","title":"Evidential time-to-event prediction with calibrated uncertainty\n  quantification","summary":"  Time-to-event analysis provides insights into clinical prognosis and\ntreatment recommendations. However, this task is more challenging than standard\nregression problems due to the presence of censored observations. Additionally,\nthe lack of confidence assessment, model robustness, and prediction calibration\nraises concerns about the reliability of predictions. To address these\nchallenges, we propose an evidential regression model specifically designed for\ntime-to-event prediction. The proposed model quantifies both epistemic and\naleatory uncertainties using Gaussian Random Fuzzy Numbers and belief\nfunctions, providing clinicians with uncertainty-aware survival time\npredictions. The model is trained by minimizing a generalized negative\nlog-likelihood function accounting for data censoring. Experimental evaluations\nusing simulated datasets with different data distributions and censoring\nconditions, as well as real-world datasets across diverse clinical\napplications, demonstrate that our model delivers both accurate and reliable\nperformance, outperforming state-of-the-art methods. These results highlight\nthe potential of our approach for enhancing clinical decision-making in\nsurvival analysis.\n","authors":["Ling Huang","Yucheng Xing","Swapnil Mishra","Thierry Denoeux","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2411.07853v2.pdf","comment":"Preprint submitted to International Journal of Approximate Reasoning"},{"id":"http://arxiv.org/abs/2412.09952v1","updated":"2024-12-13T08:22:19Z","published":"2024-12-13T08:22:19Z","title":"Llama 3 Meets MoE: Efficient Upcycling","summary":"  Scaling large language models (LLMs) significantly improves performance but\ncomes with prohibitive computational costs. Mixture-of-Experts (MoE) models\noffer an efficient alternative, increasing capacity without a proportional rise\nin compute requirements. However, training MoE models from scratch poses\nchallenges like overfitting and routing instability. We present an efficient\ntraining recipe leveraging pre-trained dense checkpoints, training an 8-Expert\nTop-2 MoE model from Llama 3-8B with less than $1\\%$ of typical pre-training\ncompute. Our approach enhances downstream performance on academic benchmarks,\nachieving a $\\textbf{2%}$ improvement in 0-shot accuracy on MMLU, while\nreaching a Model FLOPs Utilization (MFU) of $\\textbf{46.8%}$ during training\nusing our framework. We also integrate online upcycling in NeMo for seamless\nuse of pre-trained weights, enabling cost-effective development of\nhigh-capacity MoE models.\n","authors":["Aditya Vavre","Ethan He","Dennis Liu","Zijie Yan","June Yang","Nima Tajbakhsh","Ashwath Aithal"],"pdf_url":"https://arxiv.org/pdf/2412.09952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09947v1","updated":"2024-12-13T08:11:40Z","published":"2024-12-13T08:11:40Z","title":"Towards Fair Graph Neural Networks via Graph Counterfactual without\n  Sensitive Attributes","summary":"  Graph-structured data is ubiquitous in today's connected world, driving\nextensive research in graph analysis. Graph Neural Networks (GNNs) have shown\ngreat success in this field, leading to growing interest in developing fair\nGNNs for critical applications. However, most existing fair GNNs focus on\nstatistical fairness notions, which may be insufficient when dealing with\nstatistical anomalies. Hence, motivated by causal theory, there has been\ngrowing attention to mitigating root causes of unfairness utilizing graph\ncounterfactuals. Unfortunately, existing methods for generating graph\ncounterfactuals invariably require the sensitive attribute. Nevertheless, in\nmany real-world applications, it is usually infeasible to obtain sensitive\nattributes due to privacy or legal issues, which challenge existing methods. In\nthis paper, we propose a framework named Fairwos (improving Fairness without\nsensitive attributes). In particular, we first propose a mechanism to generate\npseudo-sensitive attributes to remedy the problem of missing sensitive\nattributes, and then design a strategy for finding graph counterfactuals from\nthe real dataset. To train fair GNNs, we propose a method to ensure that the\nembeddings from the original data are consistent with those from the graph\ncounterfactuals, and dynamically adjust the weight of each pseudo-sensitive\nattribute to balance its contribution to fairness and utility. Furthermore, we\ntheoretically demonstrate that minimizing the relation between these\npseudo-sensitive attributes and the prediction can enable the fairness of GNNs.\nExperimental results on six real-world datasets show that our approach\noutperforms state-of-the-art methods in balancing utility and fairness.\n","authors":["Xuemin Wang","Tianlong Gu","Xuguang Bao","Liang Chang"],"pdf_url":"https://arxiv.org/pdf/2412.09947v1.pdf","comment":"ICDE 2025"},{"id":"http://arxiv.org/abs/2412.09942v1","updated":"2024-12-13T08:04:21Z","published":"2024-12-13T08:04:21Z","title":"Latent feedback control of distributed systems in multiple scenarios\n  through deep learning-based reduced order models","summary":"  Continuous monitoring and real-time control of high-dimensional distributed\nsystems are often crucial in applications to ensure a desired physical\nbehavior, without degrading stability and system performances. Traditional\nfeedback control design that relies on full-order models, such as\nhigh-dimensional state-space representations or partial differential equations,\nfails to meet these requirements due to the delay in the control computation,\nwhich requires multiple expensive simulations of the physical system. The\ncomputational bottleneck is even more severe when considering parametrized\nsystems, as new strategies have to be determined for every new scenario. To\naddress these challenges, we propose a real-time closed-loop control strategy\nenhanced by nonlinear non-intrusive Deep Learning-based Reduced Order Models\n(DL-ROMs). Specifically, in the offline phase, (i) full-order state-control\npairs are generated for different scenarios through the adjoint method, (ii)\nthe essential features relevant for control design are extracted from the\nsnapshots through a combination of Proper Orthogonal Decomposition (POD) and\ndeep autoencoders, and (iii) the low-dimensional policy bridging latent control\nand state spaces is approximated with a feedforward neural network. After data\ngeneration and neural networks training, the optimal control actions are\nretrieved in real-time for any observed state and scenario. In addition, the\ndynamics may be approximated through a cheap surrogate model in order to close\nthe loop at the latent level, thus continuously controlling the system in\nreal-time even when full-order state measurements are missing. The\neffectiveness of the proposed method, in terms of computational speed,\naccuracy, and robustness against noisy data, is finally assessed on two\ndifferent high-dimensional optimal transport problems, one of which also\ninvolving an underlying fluid flow.\n","authors":["Matteo Tomasetto","Francesco Braghin","Andrea Manzoni"],"pdf_url":"https://arxiv.org/pdf/2412.09942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09940v1","updated":"2024-12-13T08:03:57Z","published":"2024-12-13T08:03:57Z","title":"Predictive Query-based Pipeline for Graph Data","summary":"  Graphs face challenges when dealing with massive datasets. They are essential\ntools for modeling interconnected data and often become computationally\nexpensive. Graph embedding techniques, on the other hand, provide an efficient\napproach. By projecting complex graphs into a lower-dimensional space, these\ntechniques simplify the analysis and processing of large-scale graphs. By\ntransforming graphs into vectors, it simplifies the analysis and processing of\nlarge-scale datasets. Several approaches, such as GraphSAGE, Node2Vec, and\nFastRP, offer efficient methods for generating graph embeddings. By storing\nembeddings as node properties, it is possible to compare different embedding\ntechniques and evaluate their effectiveness for specific tasks. This\nflexibilityallows for dynamic updates to embeddings and facilitates\nexperimentation with different approaches. By analyzing these embeddings, one\ncan extract valuable insights into the relationships between nodes and their\nsimilarities within the embedding space\n","authors":["Plácido A Souza Neto"],"pdf_url":"https://arxiv.org/pdf/2412.09940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08160v2","updated":"2024-12-13T08:00:55Z","published":"2024-12-11T07:32:38Z","title":"DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with\n  Selective State Space Models","summary":"  Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns,\nwidely existing in the real world. Nevertheless, the structure incompleteness,\nnoise, and redundancy result in poor robustness for Dynamic Graph Neural\nNetworks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising\nway to optimize graph structures. However, aside from encountering unacceptable\nquadratic complexity, it overly relies on heuristic priors, making it hard to\ndiscover underlying predictive patterns. How to efficiently refine the dynamic\nstructures, capture intrinsic dependencies, and learn robust representations,\nremains under-explored. In this work, we propose the novel DG-Mamba, a robust\nand efficient Dynamic Graph structure learning framework with the Selective\nState Space Models (Mamba). To accelerate the spatio-temporal structure\nlearning, we propose a kernelized dynamic message-passing operator that reduces\nthe quadratic time complexity to linear. To capture global intrinsic dynamics,\nwe establish the dynamic graph as a self-contained system with State Space\nModel. By discretizing the system states with the cross-snapshot graph\nadjacency, we enable the long-distance dependencies capturing with the\nselective snapshot scan. To endow learned dynamic structures more expressive\nwith informativeness, we propose the self-supervised Principle of Relevant\nInformation for DGSL to regularize the most relevant yet least redundant\ninformation, enhancing global robustness. Extensive experiments demonstrate the\nsuperiority of the robustness and efficiency of our DG-Mamba compared with the\nstate-of-the-art baselines against adversarial attacks.\n","authors":["Haonan Yuan","Qingyun Sun","Zhaonan Wang","Xingcheng Fu","Cheng Ji","Yongjian Wang","Bo Jin","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2412.08160v2.pdf","comment":"Accepted by the Main Technical Track of the 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2412.09399v2","updated":"2024-12-13T07:40:02Z","published":"2024-12-12T16:05:39Z","title":"A Geometry-Aware Message Passing Neural Network for Modeling\n  Aerodynamics over Airfoils","summary":"  Computational modeling of aerodynamics is a key problem in aerospace\nengineering, often involving flows interacting with solid objects such as\nairfoils. Deep surrogate models have emerged as purely data-driven approaches\nthat learn direct mappings from simulation conditions to solutions based on\neither simulation or experimental data. Here, we consider modeling of\nincompressible flows over solid objects, wherein geometric structures are a key\nfactor in determining aerodynamics. To effectively incorporate geometries, we\npropose a message passing scheme that efficiently and expressively integrates\nthe airfoil shape with the mesh representation. Under this framework, we first\nobtain a representation of the geometry in the form of a latent graph on the\nairfoil surface. We subsequently propagate this representation to all\ncollocation points through message passing on a directed, bipartite graph. We\ndemonstrate that this framework supports efficient training by downsampling the\nsolution mesh while avoiding distribution shifts at test time when evaluated on\nthe full mesh. To enable our model to be able to distinguish between distinct\nspatial regimes of dynamics relative to the airfoil, we represent mesh points\nin both a leading edge and trailing edge coordinate system. We further enhance\nthe expressiveness of our coordinate system representations by embedding our\nhybrid Polar-Cartesian coordinates using sinusoidal and spherical harmonics\nbases. We additionally find that a change of basis to canonicalize input\nrepresentations with respect to inlet velocity substantially improves\ngeneralization. Altogether, these design choices lead to a purely data-driven\nmachine learning framework known as GeoMPNN, which won the Best Student\nSubmission award at the NeurIPS 2024 ML4CFD Competition, placing 4th overall.\nOur code is publicly available as part of the AIRS library\n(https://github.com/divelab/AIRS).\n","authors":["Jacob Helwig","Xuan Zhang","Haiyang Yu","Shuiwang Ji"],"pdf_url":"https://arxiv.org/pdf/2412.09399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09925v1","updated":"2024-12-13T07:27:42Z","published":"2024-12-13T07:27:42Z","title":"Simulating Hard Attention Using Soft Attention","summary":"  We study conditions under which transformers using soft attention can\nsimulate hard attention, that is, effectively focus all attention on a subset\nof positions. First, we examine several variants of linear temporal logic,\nwhose formulas have been previously been shown to be computable using hard\nattention transformers. We demonstrate how soft attention transformers can\ncompute formulas of these logics using unbounded positional embeddings or\ntemperature scaling. Second, we demonstrate how temperature scaling allows\nsoftmax transformers to simulate a large subclass of average-hard attention\ntransformers, those that have what we call the uniform-tieless property.\n","authors":["Andy Yang","Lena Strobl","David Chiang","Dana Angluin"],"pdf_url":"https://arxiv.org/pdf/2412.09925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09066v3","updated":"2024-12-13T07:26:51Z","published":"2024-02-14T10:24:04Z","title":"Solid Waste Detection, Monitoring and Mapping in Remote Sensing Images:\n  A Survey","summary":"  The detection and characterization of illegal solid waste disposal sites are\nessential for environmental protection, particularly for mitigating pollution\nand health hazards. Improperly managed landfills contaminate soil and\ngroundwater via rainwater infiltration, posing threats to both animals and\nhumans. Traditional landfill identification approaches, such as on-site\ninspections, are time-consuming and expensive. Remote sensing is a\ncost-effective solution for the identification and monitoring of solid waste\ndisposal sites that enables broad coverage and repeated acquisitions over time.\nEarth Observation (EO) satellites, equipped with an array of sensors and\nimaging capabilities, have been providing high-resolution data for several\ndecades. Researchers proposed specialized techniques that leverage remote\nsensing imagery to perform a range of tasks such as waste site detection,\ndumping site monitoring, and assessment of suitable locations for new\nlandfills. This review aims to provide a detailed illustration of the most\nrelevant proposals for the detection and monitoring of solid waste sites by\ndescribing and comparing the approaches, the implemented techniques, and the\nemployed data. Furthermore, since the data sources are of the utmost importance\nfor developing an effective solid waste detection model, a comprehensive\noverview of the satellites and publicly available data sets is presented.\nFinally, this paper identifies the open issues in the state-of-the-art and\ndiscusses the relevant research directions for reducing the costs and improving\nthe effectiveness of novel solid waste detection methods.\n","authors":["Piero Fraternali","Luca Morandini","Sergio Luis Herrera González"],"pdf_url":"https://arxiv.org/pdf/2402.09066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17053v2","updated":"2024-12-13T07:21:58Z","published":"2024-08-30T07:23:59Z","title":"Estimating Conditional Average Treatment Effects via Sufficient\n  Representation Learning","summary":"  Estimating the conditional average treatment effects (CATE) is very important\nin causal inference and has a wide range of applications across many fields. In\nthe estimation process of CATE, the unconfoundedness assumption is typically\nrequired to ensure the identifiability of the regression problems. When\nestimating CATE using high-dimensional data, there have been many variable\nselection methods and neural network approaches based on representation\nlearning, while these methods do not provide a way to verify whether the subset\nof variables after dimensionality reduction or the learned representations\nstill satisfy the unconfoundedness assumption during the estimation process,\nwhich can lead to ineffective estimates of the treatment effects. Additionally,\nthese methods typically use data from only the treatment or control group when\nestimating the regression functions for each group. This paper proposes a novel\nneural network approach named \\textbf{CrossNet} to learn a sufficient\nrepresentation for the features, based on which we then estimate the CATE,\nwhere cross indicates that in estimating the regression functions, we used data\nfrom their own group as well as cross-utilized data from another group.\nNumerical simulations and empirical results demonstrate that our method\noutperforms the competitive approaches.\n","authors":["Pengfei Shi","Wei Zhong","Xinyu Zhang","Ningtao Wang","Xing Fu","Weiqiang Wang","Yin Jin"],"pdf_url":"https://arxiv.org/pdf/2408.17053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16283v2","updated":"2024-12-13T07:01:16Z","published":"2024-04-25T01:56:00Z","title":"Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text\n  Streaming Services","summary":"  Large language models (LLMs) are now at the core of conversational AI\nservices such as real-time translation and chatbots, which provide live user\ninteraction by incrementally streaming text to the user. However, existing LLM\nserving systems fail to provide good user experience because their optimization\nmetrics are not always aligned with user experience.\n  In this paper, we first introduce and define the notion of\nQuality-of-Experience (QoE) for text streaming services by considering each\nuser's end-to-end interaction timeline. Based on this, we propose Andes, a\nQoE-aware LLM serving system that enhances user experience by ensuring that\nusers receive the first token promptly and subsequent tokens at a smooth,\ndigestible pace, even during surge periods. This is enabled by Andes's\npreemptive request scheduler that dynamically prioritizes requests at the token\ngranularity based on each request's expected QoE gain and GPU resource usage.\nOur evaluations demonstrate that, compared to state-of-the-art LLM serving\nsystems, Andes improves the average QoE by up to $4.7\\times$ given the same GPU\nresource, or saves up to 61% GPU resources while maintaining the same high QoE.\n","authors":["Jiachen Liu","Jae-Won Chung","Zhiyu Wu","Fan Lai","Myungjin Lee","Mosharaf Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2404.16283v2.pdf","comment":"16 pages, 21 figures"},{"id":"http://arxiv.org/abs/2405.19225v4","updated":"2024-12-13T06:58:47Z","published":"2024-05-29T16:05:57Z","title":"Synthetic Potential Outcomes and Causal Mixture Identifiability","summary":"  Heterogeneous data from multiple populations, sub-groups, or sources is often\nrepresented as a ``mixture model'' with a single latent class influencing all\nof the observed covariates. Heterogeneity can be resolved at multiple levels by\ngrouping populations according to different notions of similarity. This paper\nproposes grouping with respect to the causal response of an intervention or\nperturbation on the system. This definition is distinct from previous notions,\nsuch as similar covariate values (e.g. clustering) or similar correlations\nbetween covariates (e.g. Gaussian mixture models). To solve the problem, we\n``synthetically sample'' from a counterfactual distribution using higher-order\nmulti-linear moments of the observable data. To understand how these ``causal\nmixtures'' fit in with more classical notions, we develop a hierarchy of\nmixture identifiability.\n","authors":["Bijan Mazaheri","Chandler Squires","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2405.19225v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13748v2","updated":"2024-12-13T06:55:46Z","published":"2024-06-19T18:01:08Z","title":"Learn and Unlearn in Multilingual LLMs","summary":"  This paper investigates the propagation of harmful information in\nmultilingual large language models (LLMs) and evaluates the efficacy of various\nunlearning methods. We demonstrate that fake information, regardless of the\nlanguage it is in, once introduced into these models through training data, can\nspread across different languages, compromising the integrity and reliability\nof the generated content. Our findings reveal that standard unlearning\ntechniques, which typically focus on English data, are insufficient in\nmitigating the spread of harmful content in multilingual contexts and could\ninadvertently reinforce harmful content across languages. We show that only by\naddressing harmful responses in both English and the original language of the\nharmful data can we effectively eliminate generations for all languages. This\nunderscores the critical need for comprehensive unlearning strategies that\nconsider the multilingual nature of modern LLMs to enhance their safety and\nreliability across diverse linguistic landscapes.\n","authors":["Taiming Lu","Philipp Koehn"],"pdf_url":"https://arxiv.org/pdf/2406.13748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14535v3","updated":"2024-12-13T06:48:46Z","published":"2024-10-18T15:23:29Z","title":"Comparing Differentiable and Dynamic Ray Tracing: Introducing the\n  Multipath Lifetime Map","summary":"  With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle\ncommunications, radio propagation modeling tools must adapt to the rapidly\nchanging nature of the radio channel. Recently, both Differentiable and Dynamic\nRay Tracing frameworks have emerged to address these challenges. However, there\nis often confusion about how these approaches differ and which one should be\nused in specific contexts. In this paper, we provide an overview of these two\ntechniques and a comparative analysis against two state-of-the-art tools:\n3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise\ncharacterization of the scope of these methods, we introduce a novel\nsimulation-based metric, the Multipath Lifetime Map, which enables the\nevaluation of spatial and temporal coherence in radio channels only based on\nthe geometrical description of the environment. Finally, our metrics are\nevaluated on a classic urban street canyon scenario, yielding similar results\nto those obtained from measurement campaigns.\n","authors":["Jérome Eertmans","Enrico Maria Vittuci","Vittorio Degli-Esposti","Laurent Jacques","Claude Oestges"],"pdf_url":"https://arxiv.org/pdf/2410.14535v3.pdf","comment":"5 pages, 5 figures, 1 table, accepted at EuCAP 2025"},{"id":"http://arxiv.org/abs/2409.13213v2","updated":"2024-12-13T06:42:48Z","published":"2024-09-20T04:50:49Z","title":"MalMixer: Few-Shot Malware Classification with Retrieval-Augmented\n  Semi-Supervised Learning","summary":"  Recent growth and proliferation of malware has tested practitioners' ability\nto promptly classify new samples according to malware families. In contrast to\nlabor-intensive reverse engineering efforts, machine learning approaches have\ndemonstrated increased speed and accuracy. However, most existing deep-learning\nmalware family classifiers must be calibrated using a large number of samples\nthat are painstakingly manually analyzed before training. Furthermore, as novel\nmalware samples arise that are beyond the scope of the training set, additional\nreverse engineering effort must be employed to update the training set. The\nsheer volume of new samples found in the wild creates substantial pressure on\npractitioners' ability to reverse engineer enough malware to adequately train\nmodern classifiers. In this paper, we present MalMixer, a malware family\nclassifier using semi-supervised learning that achieves high accuracy with\nsparse training data. We present a novel domain-knowledge-aware technique for\naugmenting malware feature representations, enhancing few-shot performance of\nsemi-supervised malware family classification. We show that MalMixer achieves\nstate-of-the-art performance in few-shot malware family classification\nsettings. Our research confirms the feasibility and effectiveness of\nlightweight, domain-knowledge-aware feature augmentation methods and highlights\nthe capabilities of similar semi-supervised classifiers in addressing malware\nclassification issues.\n","authors":["Jiliang Li","Yifan Zhang","Yu Huang","Kevin Leach"],"pdf_url":"https://arxiv.org/pdf/2409.13213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09902v1","updated":"2024-12-13T06:42:36Z","published":"2024-12-13T06:42:36Z","title":"One Node One Model: Featuring the Missing-Half for Graph Clustering","summary":"  Most existing graph clustering methods primarily focus on exploiting\ntopological structure, often neglecting the ``missing-half\" node feature\ninformation, especially how these features can enhance clustering performance.\nThis issue is further compounded by the challenges associated with\nhigh-dimensional features. Feature selection in graph clustering is\nparticularly difficult because it requires simultaneously discovering clusters\nand identifying the relevant features for these clusters. To address this gap,\nwe introduce a novel paradigm called ``one node one model\", which builds an\nexclusive model for each node and defines the node label as a combination of\npredictions for node groups. Specifically, the proposed ``Feature Personalized\nGraph Clustering (FPGC)\" method identifies cluster-relevant features for each\nnode using a squeeze-and-excitation block, integrating these features into each\nmodel to form the final representations. Additionally, the concept of feature\ncross is developed as a data augmentation technique to learn low-order feature\ninteractions. Extensive experimental results demonstrate that FPGC outperforms\nstate-of-the-art clustering methods. Moreover, the plug-and-play nature of our\nmethod provides a versatile solution to enhance GNN-based models from a feature\nperspective.\n","authors":["Xuanting Xie","Bingheng Li","Erlin Pan","Zhaochen Guo","Zhao Kang","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00004v3","updated":"2024-12-13T06:41:02Z","published":"2024-05-12T04:15:05Z","title":"Navigating the Future of Federated Recommendation Systems with\n  Foundation Models","summary":"  In recent years, the integration of federated learning (FL) and\nrecommendation systems (RS), known as Federated Recommendation Systems (FRS),\nhas attracted attention for preserving user privacy by keeping private data on\nclient devices. However, FRS faces inherent limitations such as data\nheterogeneity and scarcity, due to the privacy requirements of FL and the\ntypical data sparsity issues of RSs. Models like ChatGPT are empowered by the\nconcept of transfer learning and self-supervised learning, so they can be\neasily applied to the downstream tasks after fine-tuning or prompting. These\nmodels, so-called Foundation Models (FM), fouce on understanding the human's\nintent and perform following their designed roles in the specific tasks, which\nare widely recognized for producing high-quality content in the image and\nlanguage domains. Thus, the achievements of FMs inspire the design of FRS and\nsuggest a promising research direction: integrating foundation models to\naddress the above limitations. In this study, we conduct a comprehensive review\nof FRSs with FMs. Specifically, we: 1) summarise the common approaches of\ncurrent FRSs and FMs; 2) review the challenges posed by FRSs and FMs; 3)\ndiscuss potential future research directions; and 4) introduce some common\nbenchmarks and evaluation metrics in the FRS field. We hope that this position\npaper provides the necessary background and guidance to explore this\ninteresting and emerging topic.\n","authors":["Zhiwei Li","Guodong Long","Chunxu Zhang","Honglei Zhang","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00004v3.pdf","comment":"20 pages, position paper, survey"},{"id":"http://arxiv.org/abs/2412.08038v2","updated":"2024-12-13T06:39:00Z","published":"2024-12-11T02:37:32Z","title":"Bootstrapping Heterogeneous Graph Representation Learning via Large\n  Language Models: A Generalized Approach","summary":"  Graph representation learning methods are highly effective in handling\ncomplex non-Euclidean data by capturing intricate relationships and features\nwithin graph structures. However, traditional methods face challenges when\ndealing with heterogeneous graphs that contain various types of nodes and edges\ndue to the diverse sources and complex nature of the data. Existing\nHeterogeneous Graph Neural Networks (HGNNs) have shown promising results but\nrequire prior knowledge of node and edge types and unified node feature\nformats, which limits their applicability. Recent advancements in graph\nrepresentation learning using Large Language Models (LLMs) offer new solutions\nby integrating LLMs' data processing capabilities, enabling the alignment of\nvarious graph representations. Nevertheless, these methods often overlook\nheterogeneous graph data and require extensive preprocessing. To address these\nlimitations, we propose a novel method that leverages the strengths of both LLM\nand GNN, allowing for the processing of graph data with any format and type of\nnodes and edges without the need for type information or special preprocessing.\nOur method employs LLM to automatically summarize and classify different data\nformats and types, aligns node features, and uses a specialized GNN for\ntargeted learning, thus obtaining effective graph representations for\ndownstream tasks. Theoretical analysis and experimental validation have\ndemonstrated the effectiveness of our method.\n","authors":["Hang Gao","Chenhao Zhang","Fengge Wu","Junsuo Zhao","Changwen Zheng","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.08038v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2210.05102v4","updated":"2024-12-13T06:37:17Z","published":"2022-10-11T02:39:06Z","title":"Pre-Training Representations of Binary Code Using Contrastive Learning","summary":"  Binary code analysis and comprehension is critical to applications in reverse\nengineering and computer security tasks where source code is not available.\nUnfortunately, unlike source code, binary code lacks semantics and is more\ndifficult for human engineers to understand and analyze. In this paper, we\npresent ContraBin, a contrastive learning technique that integrates source code\nand comment information along with binaries to create an embedding capable of\naiding binary analysis and comprehension tasks. Specifically, we present three\ncomponents in ContraBin: (1) a primary contrastive learning method for initial\npre-training, (2) a simplex interpolation method to integrate source code,\ncomments, and binary code, and (3) an intermediate representation learning\nalgorithm to train a binary code embedding. We further analyze the impact of\nhuman-written and synthetic comments on binary code comprehension tasks,\nrevealing a significant performance disparity. While synthetic comments provide\nsubstantial benefits, human-written comments are found to introduce noise, even\nresulting in performance drops compared to using no comments. These findings\nreshape the narrative around the role of comment types in binary code analysis.\nWe evaluate the effectiveness of ContraBin through four indicative downstream\ntasks related to binary code: algorithmic functionality classification,\nfunction name recovery, code summarization, and reverse engineering. The\nresults show that ContraBin considerably improves performance on all four\ntasks, measured by accuracy, mean of average precision, and BLEU scores as\nappropriate. ContraBin is the first language representation model to\nincorporate source code, binary code, and comments into contrastive code\nrepresentation learning and is intended to contribute to the field of binary\ncode analysis. The dataset used in this study is available for further\nresearch.\n","authors":["Yifan Zhang","Chen Huang","Yueke Zhang","Kevin Cao","Scott Thomas Andersen","Huajie Shao","Kevin Leach","Yu Huang"],"pdf_url":"https://arxiv.org/pdf/2210.05102v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10769v5","updated":"2024-12-13T06:36:28Z","published":"2023-05-18T07:23:12Z","title":"Catch-Up Distillation: You Only Need to Train Once for Accelerating\n  Sampling","summary":"  Diffusion Probability Models (DPMs) have made impressive advancements in\nvarious machine learning domains. However, achieving high-quality synthetic\nsamples typically involves performing a large number of sampling steps, which\nimpedes the possibility of real-time sample synthesis. Traditional accelerated\nsampling algorithms via knowledge distillation rely on pre-trained model\nweights and discrete time step scenarios, necessitating additional training\nsessions to achieve their goals. To address these issues, we propose the\nCatch-Up Distillation (CUD), which encourages the current moment output of the\nvelocity estimation model ``catch up'' with its previous moment output.\nSpecifically, CUD adjusts the original Ordinary Differential Equation (ODE)\ntraining objective to align the current moment output with both the ground\ntruth label and the previous moment output, utilizing Runge-Kutta-based\nmulti-step alignment distillation for precise ODE estimation while preventing\nasynchronous updates. Furthermore, we investigate the design space for CUDs\nunder continuous time-step scenarios and analyze how to determine the suitable\nstrategies. To demonstrate CUD's effectiveness, we conduct thorough ablation\nand comparison experiments on CIFAR-10, MNIST, and ImageNet-64. On CIFAR-10, we\nobtain a FID of 2.80 by sampling in 15 steps under one-session training and the\nnew state-of-the-art FID of 3.37 by sampling in one step with additional\ntraining. This latter result necessitated only 620k iterations with a batch\nsize of 128, in contrast to Consistency Distillation, which demanded 2100k\niterations with a larger batch size of 256. Our code is released at\nhttps://anonymous.4open.science/r/Catch-Up-Distillation-E31F.\n","authors":["Shitong Shao","Xu Dai","Lujun Li","Huanran Chen","Yang Hu","Shouyi Yin"],"pdf_url":"https://arxiv.org/pdf/2305.10769v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09900v1","updated":"2024-12-13T06:35:55Z","published":"2024-12-13T06:35:55Z","title":"Analyzing Fairness of Computer Vision and Natural Language Processing\n  Models","summary":"  Machine learning (ML) algorithms play a crucial role in decision making\nacross diverse fields such as healthcare, finance, education, and law\nenforcement. Despite their widespread adoption, these systems raise ethical and\nsocial concerns due to potential biases and fairness issues. This study focuses\non evaluating and improving the fairness of Computer Vision and Natural\nLanguage Processing (NLP) models applied to unstructured datasets, emphasizing\nhow biased predictions can reinforce existing systemic inequalities. A publicly\navailable dataset from Kaggle was utilized to simulate a practical scenario for\nexamining fairness in ML workflows. To address and mitigate biases, the study\nemployed two leading fairness libraries: Fairlearn by Microsoft, and AIF360 by\nIBM. These tools offer comprehensive frameworks for fairness analysis,\nincluding metrics evaluation, result visualization, and bias mitigation\ntechniques. The research aims to measure bias levels in ML models, compare the\neffectiveness of these fairness libraries, and provide actionable\nrecommendations for practitioners. The results demonstrate that each library\npossesses distinct strengths and limitations in evaluating and mitigating\nfairness. By systematically analyzing these tools, the study contributes\nvaluable insights to the growing field of ML fairness, offering practical\nguidance for integrating fairness solutions into real world applications. This\nresearch underscores the importance of building more equitable and responsible\nmachine learning systems.\n","authors":["Ahmed Rashed","Abdelkrim Kallich","Mohamed Eltayeb"],"pdf_url":"https://arxiv.org/pdf/2412.09900v1.pdf","comment":"16 pages, 1 table, 4 figures"},{"id":"http://arxiv.org/abs/2412.09899v1","updated":"2024-12-13T06:34:59Z","published":"2024-12-13T06:34:59Z","title":"TTAQ: Towards Stable Post-training Quantization in Continuous Domain\n  Adaptation","summary":"  Post-training quantization (PTQ) reduces excessive hardware cost by\nquantizing full-precision models into lower bit representations on a tiny\ncalibration set, without retraining. Despite the remarkable progress made\nthrough recent efforts, traditional PTQ methods typically encounter failure in\ndynamic and ever-changing real-world scenarios, involving unpredictable data\nstreams and continual domain shifts, which poses greater challenges. In this\npaper, we propose a novel and stable quantization process for test-time\nadaptation (TTA), dubbed TTAQ, to address the performance degradation of\ntraditional PTQ in dynamically evolving test domains. To tackle domain shifts\nin quantizer, TTAQ proposes the Perturbation Error Mitigation (PEM) and\nPerturbation Consistency Reconstruction (PCR). Specifically, PEM analyzes the\nerror propagation and devises a weight regularization scheme to mitigate the\nimpact of input perturbations. On the other hand, PCR introduces consistency\nlearning to ensure that quantized models provide stable predictions for same\nsample. Furthermore, we introduce Adaptive Balanced Loss (ABL) to adjust the\nlogits by taking advantage of the frequency and complexity of the class, which\ncan effectively address the class imbalance caused by unpredictable data\nstreams during optimization. Extensive experiments are conducted on multiple\ndatasets with generic TTA methods, proving that TTAQ can outperform existing\nbaselines and encouragingly improve the accuracy of low bit PTQ models in\ncontinually changing test domains. For instance, TTAQ decreases the mean error\nof 2-bit models on ImageNet-C dataset by an impressive 10.1\\%.\n","authors":["Junrui Xiao","Zhikai Li","Lianwei Yang","Yiduo Mei","Qingyi Gu"],"pdf_url":"https://arxiv.org/pdf/2412.09899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09896v1","updated":"2024-12-13T06:31:09Z","published":"2024-12-13T06:31:09Z","title":"Analyzing Fairness of Classification Machine Learning Model with\n  Structured Dataset","summary":"  Machine learning (ML) algorithms have become integral to decision making in\nvarious domains, including healthcare, finance, education, and law enforcement.\nHowever, concerns about fairness and bias in these systems pose significant\nethical and social challenges. This study investigates the fairness of ML\nmodels applied to structured datasets in classification tasks, highlighting the\npotential for biased predictions to perpetuate systemic inequalities. A\npublicly available dataset from Kaggle was selected for analysis, offering a\nrealistic scenario for evaluating fairness in machine learning workflows.\n  To assess and mitigate biases, three prominent fairness libraries; Fairlearn\nby Microsoft, AIF360 by IBM, and the What If Tool by Google were employed.\nThese libraries provide robust frameworks for analyzing fairness, offering\ntools to evaluate metrics, visualize results, and implement bias mitigation\nstrategies. The research aims to assess the extent of bias in the ML models,\ncompare the effectiveness of these libraries, and derive actionable insights\nfor practitioners.\n  The findings reveal that each library has unique strengths and limitations in\nfairness evaluation and mitigation. By systematically comparing their\ncapabilities, this study contributes to the growing field of ML fairness by\nproviding practical guidance for integrating fairness tools into real world\napplications. These insights are intended to support the development of more\nequitable machine learning systems.\n","authors":["Ahmed Rashed","Abdelkrim Kallich","Mohamed Eltayeb"],"pdf_url":"https://arxiv.org/pdf/2412.09896v1.pdf","comment":"12 pages, 3 tables"},{"id":"http://arxiv.org/abs/2405.18944v2","updated":"2024-12-13T06:23:03Z","published":"2024-05-29T09:56:00Z","title":"Predicting Many Crystal Properties via an Adaptive Transformer-based\n  Framework","summary":"  Machine learning has revolutionized many fields, including materials science.\nHowever, predicting properties of crystalline materials using machine learning\nfaces challenges in input encoding, output versatility, and interpretability.\nWe introduce CrystalBERT, an adaptable transformer-based framework integrating\nspace group, elemental, and unit cell information. This novel structure can\nseamlessly combine diverse features and accurately predict various physical\nproperties, including topological properties, superconducting transition\ntemperatures, dielectric constants, and more. CrystalBERT provides insightful\ninterpretations of features influencing target properties. Our results indicate\nthat space group and elemental information are crucial for predicting\ntopological and superconducting properties, underscoring their intricate\nnature. By incorporating these features, we achieve 91\\% accuracy in\ntopological classification, surpassing prior studies and identifying previously\nmisclassified materials. This research demonstrates that integrating diverse\nmaterial information enhances the prediction of complex material properties,\npaving the way for more accurate and interpretable machine learning models in\nmaterials science.\n","authors":["Haosheng Xu","Dongheng Qian","Jing Wang"],"pdf_url":"https://arxiv.org/pdf/2405.18944v2.pdf","comment":"38+20 pages, 5+12 figures. The codes are available upon reasonable\n  request"},{"id":"http://arxiv.org/abs/2408.01129v5","updated":"2024-12-13T06:16:06Z","published":"2024-08-02T09:18:41Z","title":"A Survey of Mamba","summary":"  As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.\n","authors":["Haohao Qu","Liangbo Ning","Rui An","Wenqi Fan","Tyler Derr","Hui Liu","Xin Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01129v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09889v1","updated":"2024-12-13T06:06:49Z","published":"2024-12-13T06:06:49Z","title":"Semi-Periodic Activation for Time Series Classification","summary":"  This paper investigates the lack of research on activation functions for\nneural network models in time series tasks. It highlights the need to identify\nessential properties of these activations to improve their effectiveness in\nspecific domains. To this end, the study comprehensively analyzes properties,\nsuch as bounded, monotonic, nonlinearity, and periodicity, for activation in\ntime series neural networks. We propose a new activation that maximizes the\ncoverage of these properties, called LeakySineLU. We empirically evaluate the\nLeakySineLU against commonly used activations in the literature using 112\nbenchmark datasets for time series classification, obtaining the best average\nranking in all comparative scenarios.\n","authors":["José Gilberto Barbosa de Medeiros Júnior","Andre Guarnier de Mitri","Diego Furtado Silva"],"pdf_url":"https://arxiv.org/pdf/2412.09889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09880v1","updated":"2024-12-13T05:51:00Z","published":"2024-12-13T05:51:00Z","title":"Financial Fine-tuning a Large Time Series Model","summary":"  Large models have shown unprecedented capabilities in natural language\nprocessing, image generation, and most recently, time series forecasting. This\nleads us to ask the question: treating market prices as a time series, can\nlarge models be used to predict the market? In this paper, we answer this by\nevaluating the performance of the latest time series foundation model TimesFM\non price prediction. We find that due to the irregular nature of price data,\ndirectly applying TimesFM gives unsatisfactory results and propose to fine-tune\nTimeFM on financial data for the task of price prediction. This is done by\ncontinual pre-training of the latest time series foundation model TimesFM on\nprice data containing 100 million time points, spanning a range of financial\ninstruments spanning hourly and daily granularities. The fine-tuned model\ndemonstrates higher price prediction accuracy than the baseline model. We\nconduct mock trading for our model in various financial markets and show that\nit outperforms various benchmarks in terms of returns, sharpe ratio, max\ndrawdown and trading cost.\n","authors":["Xinghong Fu","Masanori Hirano","Kentaro Imajo"],"pdf_url":"https://arxiv.org/pdf/2412.09880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09861v1","updated":"2024-12-13T05:02:16Z","published":"2024-12-13T05:02:16Z","title":"Data-Driven Transfer Learning Framework for Estimating Turning Movement\n  Counts","summary":"  Urban transportation networks are vital for the efficient movement of people\nand goods, necessitating effective traffic management and planning. An integral\npart of traffic management is understanding the turning movement counts (TMCs)\nat intersections, Accurate TMCs at intersections are crucial for traffic signal\ncontrol, congestion mitigation, and road safety. In general, TMCs are obtained\nusing physical sensors installed at intersections, but this approach can be\ncost-prohibitive and technically challenging, especially for cities with\nextensive road networks. Recent advancements in machine learning and\ndata-driven approaches have offered promising alternatives for estimating TMCs.\nTraffic patterns can vary significantly across different intersections due to\nfactors such as road geometry, traffic signal settings, and local driver\nbehaviors. This domain discrepancy limits the generalizability and accuracy of\nmachine learning models when applied to new or unseen intersections. In\nresponse to these limitations, this research proposes a novel framework\nleveraging transfer learning (TL) to estimate TMCs at intersections by using\ntraffic controller event-based data, road infrastructure data, and\npoint-of-interest (POI) data. Evaluated on 30 intersections in Tucson, Arizona,\nthe performance of the proposed TL model was compared with eight\nstate-of-the-art regression models and achieved the lowest values in terms of\nMean Absolute Error and Root Mean Square Error.\n","authors":["Xiaobo Ma","Hyunsoo Noh","Ryan Hatch","James Tokishi","Zepu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09860v1","updated":"2024-12-13T05:00:57Z","published":"2024-12-13T05:00:57Z","title":"Brain-inspired Chaotic Graph Backpropagation for Large-scale\n  Combinatorial Optimization","summary":"  Graph neural networks (GNNs) with unsupervised learning can solve large-scale\ncombinatorial optimization problems (COPs) with efficient time complexity,\nmaking them versatile for various applications. However, since this method maps\nthe combinatorial optimization problem to the training process of a graph\nneural network, and the current mainstream backpropagation-based training\nalgorithms are prone to fall into local minima, the optimization performance is\nstill inferior to the current state-of-the-art (SOTA) COP methods. To address\nthis issue, inspired by possibly chaotic dynamics of real brain learning, we\nintroduce a chaotic training algorithm, i.e. chaotic graph backpropagation\n(CGBP), which introduces a local loss function in GNN that makes the training\nprocess not only chaotic but also highly efficient. Different from existing\nmethods, we show that the global ergodicity and pseudo-randomness of such\nchaotic dynamics enable CGBP to learn each optimal GNN effectively and\nglobally, thus solving the COP efficiently. We have applied CGBP to solve\nvarious COPs, such as the maximum independent set, maximum cut, and graph\ncoloring. Results on several large-scale benchmark datasets showcase that CGBP\ncan outperform not only existing GNN algorithms but also SOTA methods. In\naddition to solving large-scale COPs, CGBP as a universal learning algorithm\nfor GNNs, i.e. as a plug-in unit, can be easily integrated into any existing\nmethod for improving the performance.\n","authors":["Peng Tao","Kazuyuki Aihara","Luonan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09859v1","updated":"2024-12-13T04:59:50Z","published":"2024-12-13T04:59:50Z","title":"Financial Sentiment Analysis: Leveraging Actual and Synthetic Data for\n  Supervised Fine-tuning","summary":"  The Efficient Market Hypothesis (EMH) highlights the essence of financial\nnews in stock price movement. Financial news comes in the form of corporate\nannouncements, news titles, and other forms of digital text. The generation of\ninsights from financial news can be done with sentiment analysis.\nGeneral-purpose language models are too general for sentiment analysis in\nfinance. Curated labeled data for fine-tuning general-purpose language models\nare scare, and existing fine-tuned models for sentiment analysis in finance do\nnot capture the maximum context width. We hypothesize that using actual and\nsynthetic data can improve performance. We introduce BertNSP-finance to\nconcatenate shorter financial sentences into longer financial sentences, and\nfinbert-lc to determine sentiment from digital text. The results show improved\nperformance on the accuracy and the f1 score for the financial phrasebank data\nwith $50\\%$ and $100\\%$ agreement levels.\n","authors":["Abraham Atsiwo"],"pdf_url":"https://arxiv.org/pdf/2412.09859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09858v1","updated":"2024-12-13T04:57:55Z","published":"2024-12-13T04:57:55Z","title":"RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning","summary":"  Recent advances in robotic foundation models have enabled the development of\ngeneralist policies that can adapt to diverse tasks. While these models show\nimpressive flexibility, their performance heavily depends on the quality of\ntheir training data. In this work, we propose Reinforcement Learning Distilled\nGeneralists (RLDG), a method that leverages reinforcement learning to generate\nhigh-quality training data for finetuning generalist policies. Through\nextensive real-world experiments on precise manipulation tasks like connector\ninsertion and assembly, we demonstrate that generalist policies trained with\nRL-generated data consistently outperform those trained with human\ndemonstrations, achieving up to 40% higher success rates while generalizing\nbetter to new tasks. We also provide a detailed analysis that reveals this\nperformance gain stems from both optimized action distributions and improved\nstate coverage. Our results suggest that combining task-specific RL with\ngeneralist policy distillation offers a promising approach for developing more\ncapable and efficient robotic manipulation systems that maintain the\nflexibility of foundation models while achieving the performance of specialized\ncontrollers. Videos and code can be found on our project website\nhttps://generalist-distillation.github.io\n","authors":["Charles Xu","Qiyang Li","Jianlan Luo","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2412.09858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09856v1","updated":"2024-12-13T04:55:10Z","published":"2024-12-13T04:55:10Z","title":"LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation\n  with Linear Computational Complexity","summary":"  Text-to-video generation enhances content creation but is highly\ncomputationally intensive: The computational cost of Diffusion Transformers\n(DiTs) scales quadratically in the number of pixels. This makes minute-length\nvideo generation extremely expensive, limiting most existing models to\ngenerating videos of only 10-20 seconds length. We propose a Linear-complexity\ntext-to-video Generation (LinGen) framework whose cost scales linearly in the\nnumber of pixels. For the first time, LinGen enables high-resolution\nminute-length video generation on a single GPU without compromising quality. It\nreplaces the computationally-dominant and quadratic-complexity block,\nself-attention, with a linear-complexity block called MATE, which consists of\nan MA-branch and a TE-branch. The MA-branch targets short-to-long-range\ncorrelations, combining a bidirectional Mamba2 block with our token\nrearrangement method, Rotary Major Scan, and our review tokens developed for\nlong video generation. The TE-branch is a novel TEmporal Swin Attention block\nthat focuses on temporal correlations between adjacent tokens and medium-range\ntokens. The MATE block addresses the adjacency preservation issue of Mamba and\nimproves the consistency of generated videos significantly. Experimental\nresults show that LinGen outperforms DiT (with a 75.6% win rate) in video\nquality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction.\nFurthermore, both automatic metrics and human evaluation demonstrate our\nLinGen-4B yields comparable video quality to state-of-the-art models (with a\n50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling,\nrespectively). This paves the way to hour-length movie generation and real-time\ninteractive video generation. We provide 68s video generation results and more\nexamples in our project website: https://lineargen.github.io/.\n","authors":["Hongjie Wang","Chih-Yao Ma","Yen-Cheng Liu","Ji Hou","Tao Xu","Jialiang Wang","Felix Juefei-Xu","Yaqiao Luo","Peizhao Zhang","Tingbo Hou","Peter Vajda","Niraj K. Jha","Xiaoliang Dai"],"pdf_url":"https://arxiv.org/pdf/2412.09856v1.pdf","comment":"20 pages, 20 figures"},{"id":"http://arxiv.org/abs/2409.07089v2","updated":"2024-12-13T04:48:20Z","published":"2024-09-11T08:20:30Z","title":"TrialSynth: Generation of Synthetic Sequential Clinical Trial Data","summary":"  Analyzing data from past clinical trials is part of the ongoing effort to\noptimize the design, implementation, and execution of new clinical trials and\nmore efficiently bring life-saving interventions to market. While there have\nbeen recent advances in the generation of static context synthetic clinical\ntrial data, due to both limited patient availability and constraints imposed by\npatient privacy needs, the generation of fine-grained synthetic time-sequential\nclinical trial data has been challenging. Given that patient trajectories over\nan entire clinical trial are of high importance for optimizing trial design and\nefforts to prevent harmful adverse events, there is a significant need for the\ngeneration of high-fidelity time-sequence clinical trial data. Here we\nintroduce TrialSynth, a Variational Autoencoder (VAE) designed to address the\nspecific challenges of generating synthetic time-sequence clinical trial data.\nDistinct from related clinical data VAE methods, the core of our method\nleverages Hawkes Processes (HP), which are particularly well-suited for\nmodeling event-type and time gap prediction needed to capture the structure of\nsequential clinical trial data. Our experiments demonstrate that TrialSynth\nsurpasses the performance of other comparable methods that can generate\nsequential clinical trial data at varying levels of fidelity / privacy\ntradeoff, enabling the generation of highly accurate event sequences across\nmultiple real-world sequential event datasets with small patient source\npopulations. Notably, our empirical findings highlight that TrialSynth not only\noutperforms existing clinical sequence-generating methods but also produces\ndata with superior utility while empirically preserving patient privacy.\n","authors":["Chufan Gao","Mandis Beigi","Afrah Shafquat","Jacob Aptekar","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2409.07089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09853v1","updated":"2024-12-13T04:46:17Z","published":"2024-12-13T04:46:17Z","title":"Understand the Effectiveness of Shortcuts through the Lens of DCA","summary":"  Difference-of-Convex Algorithm (DCA) is a well-known nonconvex optimization\nalgorithm for minimizing a nonconvex function that can be expressed as the\ndifference of two convex ones. Many famous existing optimization algorithms,\nsuch as SGD and proximal point methods, can be viewed as special DCAs with\nspecific DC decompositions, making it a powerful framework for optimization. On\nthe other hand, shortcuts are a key architectural feature in modern deep neural\nnetworks, facilitating both training and optimization. We showed that the\nshortcut neural network gradient can be obtained by applying DCA to vanilla\nneural networks, networks without shortcut connections. Therefore, from the\nperspective of DCA, we can better understand the effectiveness of networks with\nshortcuts. Moreover, we proposed a new architecture called NegNet that does not\nfit the previous interpretation but performs on par with ResNet and can be\nincluded in the DCA framework.\n","authors":["Youran Sun","Yihua Liu","Yi-Shuai Niu"],"pdf_url":"https://arxiv.org/pdf/2412.09853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03350v2","updated":"2024-12-13T04:32:21Z","published":"2024-01-07T00:58:33Z","title":"Accurate and Scalable Estimation of Epistemic Uncertainty for Graph\n  Neural Networks","summary":"  While graph neural networks (GNNs) are widely used for node and graph\nrepresentation learning tasks, the reliability of GNN uncertainty estimates\nunder distribution shifts remains relatively under-explored. Indeed, while\npost-hoc calibration strategies can be used to improve in-distribution\ncalibration, they need not also improve calibration under distribution shift.\nHowever, techniques which produce GNNs with better intrinsic uncertainty\nestimates are particularly valuable, as they can always be combined with\npost-hoc strategies later. Therefore, in this work, we propose G-$\\Delta$UQ, a\nnovel training framework designed to improve intrinsic GNN uncertainty\nestimates. Our framework adapts the principle of stochastic data centering to\ngraph data through novel graph anchoring strategies, and is able to support\npartially stochastic GNNs. While, the prevalent wisdom is that fully stochastic\nnetworks are necessary to obtain reliable estimates, we find that the\nfunctional diversity induced by our anchoring strategies when sampling\nhypotheses renders this unnecessary and allows us to support G-$\\Delta$UQ on\npretrained models. Indeed, through extensive evaluation under covariate,\nconcept and graph size shifts, we show that G-$\\Delta$UQ leads to better\ncalibrated GNNs for node and graph classification. Further, it also improves\nperformance on the uncertainty-based tasks of out-of-distribution detection and\ngeneralization gap estimation. Overall, our work provides insights into\nuncertainty estimation for GNNs, and demonstrates the utility of G-$\\Delta$UQ\nin obtaining reliable estimates.\n","authors":["Puja Trivedi","Mark Heimann","Rushil Anirudh","Danai Koutra","Jayaraman J. Thiagarajan"],"pdf_url":"https://arxiv.org/pdf/2401.03350v2.pdf","comment":"Published at ICLR 2024; Project page:\n  https://pujacomputes.github.io/gduq/"},{"id":"http://arxiv.org/abs/2412.09843v1","updated":"2024-12-13T04:25:56Z","published":"2024-12-13T04:25:56Z","title":"Learning Structural Causal Models from Ordering: Identifiable Flow\n  Models","summary":"  In this study, we address causal inference when only observational data and a\nvalid causal ordering from the causal graph are available. We introduce a set\nof flow models that can recover component-wise, invertible transformation of\nexogenous variables. Our flow-based methods offer flexible model design while\nmaintaining causal consistency regardless of the number of discretization\nsteps. We propose design improvements that enable simultaneous learning of all\ncausal mechanisms and reduce abduction and prediction complexity to linear O(n)\nrelative to the number of layers, independent of the number of causal\nvariables. Empirically, we demonstrate that our method outperforms previous\nstate-of-the-art approaches and delivers consistent performance across a wide\nrange of structural causal models in answering observational, interventional,\nand counterfactual questions. Additionally, our method achieves a significant\nreduction in computational time compared to existing diffusion-based\ntechniques, making it practical for large structural causal models.\n","authors":["Minh Khoa Le","Kien Do","Truyen Tran"],"pdf_url":"https://arxiv.org/pdf/2412.09843v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09842v1","updated":"2024-12-13T04:22:23Z","published":"2024-12-13T04:22:23Z","title":"Leveraging Programmatically Generated Synthetic Data for Differentially\n  Private Diffusion Training","summary":"  Programmatically generated synthetic data has been used in differential\nprivate training for classification to enhance performance without privacy\nleakage. However, as the synthetic data is generated from a random process, the\ndistribution of real data and the synthetic data are distinguishable and\ndifficult to transfer. Therefore, the model trained with the synthetic data\ngenerates unrealistic random images, raising challenges to adapt the synthetic\ndata for generative models. In this work, we propose DP-SynGen, which leverages\nprogrammatically generated synthetic data in diffusion models to address this\nchallenge. By exploiting the three stages of diffusion models(coarse, context,\nand cleaning) we identify stages where synthetic data can be effectively\nutilized. We theoretically and empirically verified that cleaning and coarse\nstages can be trained without private data, replacing them with synthetic data\nto reduce the privacy budget. The experimental results show that DP-SynGen\nimproves the quality of generative data by mitigating the negative impact of\nprivacy-induced noise on the generation process.\n","authors":["Yujin Choi","Jinseong Park","Junyoung Byun","Jaewook Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08894v2","updated":"2024-12-13T04:03:14Z","published":"2024-12-12T03:14:50Z","title":"SMMF: Square-Matricized Momentum Factorization for Memory-Efficient\n  Optimization","summary":"  We propose SMMF (Square-Matricized Momentum Factorization), a\nmemory-efficient optimizer that reduces the memory requirement of the widely\nused adaptive learning rate optimizers, such as Adam, by up to 96%. SMMF\nenables flexible and efficient factorization of an arbitrary rank (shape) of\nthe first and second momentum tensors during optimization, based on the\nproposed square-matricization and one-time single matrix factorization. From\nthis, it becomes effectively applicable to any rank (shape) of momentum\ntensors, i.e., bias, matrix, and any rank-d tensors, prevalent in various deep\nmodel architectures, such as CNNs (high rank) and Transformers (low rank), in\ncontrast to existing memory-efficient optimizers that applies only to a\nparticular (rank-2) momentum tensor, e.g., linear layers. We conduct a regret\nbound analysis of SMMF, which shows that it converges similarly to\nnon-memory-efficient adaptive learning rate optimizers, such as AdamNC,\nproviding a theoretical basis for its competitive optimization capability. In\nour experiment, SMMF takes up to 96% less memory compared to state-of-the-art\nmemory efficient optimizers, e.g., Adafactor, CAME, and SM3, while achieving\ncomparable model performance on various CNN and Transformer tasks.\n","authors":["Kwangryeol Park","Seulki Lee"],"pdf_url":"https://arxiv.org/pdf/2412.08894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09832v1","updated":"2024-12-13T03:51:39Z","published":"2024-12-13T03:51:39Z","title":"Multivariate Time Series Clustering for Environmental State\n  Characterization of Ground-Based Gravitational-Wave Detectors","summary":"  Gravitational-wave observatories like LIGO are large-scale, terrestrial\ninstruments housed in infrastructure that spans a multi-kilometer geographic\narea and which must be actively controlled to maintain operational stability\nfor long observation periods. Despite exquisite seismic isolation, they remain\nsusceptible to seismic noise and other terrestrial disturbances that can couple\nundesirable vibrations into the instrumental infrastructure, potentially\nleading to control instabilities or noise artifacts in the detector output. It\nis, therefore, critical to characterize the seismic state of these\nobservatories to identify a set of temporal patterns that can inform the\ndetector operators in day-to-day monitoring and diagnostics. On a day-to-day\nbasis, the operators monitor several seismically relevant data streams to\ndiagnose operational instabilities and sources of noise using some simple\nempirically-determined thresholds. It can be untenable for a human operator to\nmonitor multiple data streams in this manual fashion and thus a distillation of\nthese data-streams into a more human-friendly format is sought. In this paper,\nwe present an end-to-end machine learning pipeline for features-based\nmultivariate time series clustering to achieve this goal and to provide\nactionable insights to the detector operators by correlating found clusters\nwith events of interest in the detector.\n","authors":["Rutuja Gurav","Isaac Kelly","Pooyan Goodarzi","Anamaria Effler","Barry Barish","Evangelos Papalexakis","Jonathan Richardson"],"pdf_url":"https://arxiv.org/pdf/2412.09832v1.pdf","comment":"8 pages, 6 figures, Accepted to The 5th International Workshop on Big\n  Data & AI Tools, Methods, and Use Cases for Innovative Scientific Discovery\n  (BTSD 2024)"},{"id":"http://arxiv.org/abs/2412.03506v2","updated":"2024-12-13T03:46:39Z","published":"2024-12-04T17:48:38Z","title":"Self-test loss functions for learning weak-form operators and gradient\n  flows","summary":"  The construction of loss functions presents a major challenge in data-driven\nmodeling involving weak-form operators in PDEs and gradient flows, particularly\ndue to the need to select test functions appropriately. We address this\nchallenge by introducing self-test loss functions, which employ test functions\nthat depend on the unknown parameters, specifically for cases where the\noperator depends linearly on the unknowns. The proposed self-test loss function\nconserves energy for gradient flows and coincides with the expected\nlog-likelihood ratio for stochastic differential equations. Importantly, it is\nquadratic, facilitating theoretical analysis of identifiability and\nwell-posedness of the inverse problem, while also leading to efficient\nparametric or nonparametric regression algorithms. It is computationally\nsimple, requiring only low-order derivatives or even being entirely\nderivative-free, and numerical experiments demonstrate its robustness against\nnoisy and discrete data.\n","authors":["Yuan Gao","Quanjun Lang","Fei Lu"],"pdf_url":"https://arxiv.org/pdf/2412.03506v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.05996v3","updated":"2024-12-13T03:38:34Z","published":"2022-02-12T06:04:13Z","title":"Mixture of Online and Offline Experts for Non-stationary Time Series","summary":"  We consider a general and realistic scenario involving non-stationary time\nseries, consisting of several offline intervals with different distributions\nwithin a fixed offline time horizon, and an online interval that continuously\nreceives new samples. For non-stationary time series, the data distribution in\nthe current online interval may have appeared in previous offline intervals. We\ntheoretically explore the feasibility of applying knowledge from offline\nintervals to the current online interval. To this end, we propose the Mixture\nof Online and Offline Experts (MOOE). MOOE learns static offline experts from\noffline intervals and maintains a dynamic online expert for the current online\ninterval. It then adaptively combines the offline and online experts using a\nmeta expert to make predictions for the samples received in the online\ninterval. Specifically, we focus on theoretical analysis, deriving parameter\nconvergence, regret bounds, and generalization error bounds to prove the\neffectiveness of the algorithm.\n","authors":["Zhilin Zhao","Longbing Cao","Yuanyu Wan"],"pdf_url":"https://arxiv.org/pdf/2202.05996v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09826v1","updated":"2024-12-13T03:36:23Z","published":"2024-12-13T03:36:23Z","title":"Precise Antigen-Antibody Structure Predictions Enhance Antibody\n  Development with HelixFold-Multimer","summary":"  The accurate prediction of antigen-antibody structures is essential for\nadvancing immunology and therapeutic development, as it helps elucidate\nmolecular interactions that underlie immune responses. Despite recent progress\nwith deep learning models like AlphaFold and RoseTTAFold, accurately modeling\nantigen-antibody complexes remains a challenge due to their unique evolutionary\ncharacteristics. HelixFold-Multimer, a specialized model developed for this\npurpose, builds on the framework of AlphaFold-Multimer and demonstrates\nimproved precision for antigen-antibody structures. HelixFold-Multimer not only\nsurpasses other models in accuracy but also provides essential insights into\nantibody development, enabling more precise identification of binding sites,\nimproved interaction prediction, and enhanced design of therapeutic antibodies.\nThese advances underscore HelixFold-Multimer's potential in supporting antibody\nresearch and therapeutic innovation.\n","authors":["Jie Gao","Jing Hu","Lihang Liu","Yang Xue","Kunrui Zhu","Xiaonan Zhang","Xiaomin Fang"],"pdf_url":"https://arxiv.org/pdf/2412.09826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00979v2","updated":"2024-12-13T03:31:51Z","published":"2024-12-01T22:02:07Z","title":"Hierarchical Prompt Decision Transformer: Improving Few-Shot Policy\n  Generalization with Global and Adaptive Guidance","summary":"  Decision transformers recast reinforcement learning as a conditional sequence\ngeneration problem, offering a simple but effective alternative to traditional\nvalue or policy-based methods. A recent key development in this area is the\nintegration of prompting in decision transformers to facilitate few-shot policy\ngeneralization. However, current methods mainly use static prompt segments to\nguide rollouts, limiting their ability to provide context-specific guidance.\nAddressing this, we introduce a hierarchical prompting approach enabled by\nretrieval augmentation. Our method learns two layers of soft tokens as guiding\nprompts: (1) global tokens encapsulating task-level information about\ntrajectories, and (2) adaptive tokens that deliver focused, timestep-specific\ninstructions. The adaptive tokens are dynamically retrieved from a curated set\nof demonstration segments, ensuring context-aware guidance. Experiments across\nseven benchmark tasks in the MuJoCo and MetaWorld environments demonstrate the\nproposed approach consistently outperforms all baseline methods, suggesting\nthat hierarchical prompting for decision transformers is an effective strategy\nto enable few-shot policy generalization.\n","authors":["Zhe Wang","Haozhu Wang","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2412.00979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09819v1","updated":"2024-12-13T03:16:14Z","published":"2024-12-13T03:16:14Z","title":"FDM-Bench: A Comprehensive Benchmark for Evaluating Large Language\n  Models in Additive Manufacturing Tasks","summary":"  Fused Deposition Modeling (FDM) is a widely used additive manufacturing (AM)\ntechnique valued for its flexibility and cost-efficiency, with applications in\na variety of industries including healthcare and aerospace. Recent developments\nhave made affordable FDM machines accessible and encouraged adoption among\ndiverse users. However, the design, planning, and production process in FDM\nrequire specialized interdisciplinary knowledge. Managing the complex\nparameters and resolving print defects in FDM remain challenging. These\ntechnical complexities form the most critical barrier preventing individuals\nwithout technical backgrounds and even professional engineers without training\nin other domains from participating in AM design and manufacturing. Large\nLanguage Models (LLMs), with their advanced capabilities in text and code\nprocessing, offer the potential for addressing these challenges in FDM.\nHowever, existing research on LLM applications in this field is limited,\ntypically focusing on specific use cases without providing comprehensive\nevaluations across multiple models and tasks. To this end, we introduce\nFDM-Bench, a benchmark dataset designed to evaluate LLMs on FDM-specific tasks.\nFDM-Bench enables a thorough assessment by including user queries across\nvarious experience levels and G-code samples that represent a range of\nanomalies. We evaluate two closed-source models (GPT-4o and Claude 3.5 Sonnet)\nand two open-source models (Llama-3.1-70B and Llama-3.1-405B) on FDM-Bench. A\npanel of FDM experts assess the models' responses to user queries in detail.\nResults indicate that closed-source models generally outperform open-source\nmodels in G-code anomaly detection, whereas Llama-3.1-405B demonstrates a\nslight advantage over other models in responding to user queries. These\nfindings underscore FDM-Bench's potential as a foundational tool for advancing\nresearch on LLM capabilities in FDM.\n","authors":["Ahmadreza Eslaminia","Adrian Jackson","Beitong Tian","Avi Stern","Hallie Gordon","Rajiv Malhotra","Klara Nahrstedt","Chenhui Shao"],"pdf_url":"https://arxiv.org/pdf/2412.09819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09814v1","updated":"2024-12-13T03:09:35Z","published":"2024-12-13T03:09:35Z","title":"Temporal Causal Discovery in Dynamic Bayesian Networks Using Federated\n  Learning","summary":"  Traditionally, learning the structure of a Dynamic Bayesian Network has been\ncentralized, with all data pooled in one location. However, in real-world\nscenarios, data are often dispersed among multiple parties (e.g., companies,\ndevices) that aim to collaboratively learn a Dynamic Bayesian Network while\npreserving their data privacy and security. In this study, we introduce a\nfederated learning approach for estimating the structure of a Dynamic Bayesian\nNetwork from data distributed horizontally across different parties. We propose\na distributed structure learning method that leverages continuous optimization\nso that only model parameters are exchanged during optimization. Experimental\nresults on synthetic and real datasets reveal that our method outperforms other\nstate-of-the-art techniques, particularly when there are many clients with\nlimited individual sample sizes.\n","authors":["Jianhong Chen","Ying Ma","Xubo Yue"],"pdf_url":"https://arxiv.org/pdf/2412.09814v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2412.09810v1","updated":"2024-12-13T02:57:59Z","published":"2024-12-13T02:57:59Z","title":"The Complexity Dynamics of Grokking","summary":"  We investigate the phenomenon of generalization through the lens of\ncompression. In particular, we study the complexity dynamics of neural networks\nto explain grokking, where networks suddenly transition from memorizing to\ngeneralizing solutions long after over-fitting the training data. To this end\nwe introduce a new measure of intrinsic complexity for neural networks based on\nthe theory of Kolmogorov complexity. Tracking this metric throughout network\ntraining, we find a consistent pattern in training dynamics, consisting of a\nrise and fall in complexity. We demonstrate that this corresponds to\nmemorization followed by generalization. Based on insights from\nrate--distortion theory and the minimum description length principle, we lay\nout a principled approach to lossy compression of neural networks, and connect\nour complexity measure to explicit generalization bounds. Based on a careful\nanalysis of information capacity in neural networks, we propose a new\nregularization method which encourages networks towards low-rank\nrepresentations by penalizing their spectral entropy, and find that our\nregularizer outperforms baselines in total compression of the dataset.\n","authors":["Branton DeMoss","Silvia Sapora","Jakob Foerster","Nick Hawes","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2412.09810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08901v2","updated":"2024-12-13T02:55:30Z","published":"2024-12-12T03:25:13Z","title":"Radiology Report Generation via Multi-objective Preference Optimization","summary":"  Automatic Radiology Report Generation (RRG) is an important topic for\nalleviating the substantial workload of radiologists. Existing RRG approaches\nrely on supervised regression based on different architectures or additional\nknowledge injection,while the generated report may not align optimally with\nradiologists' preferences. Especially, since the preferences of radiologists\nare inherently heterogeneous and multidimensional, e.g., some may prioritize\nreport fluency, while others emphasize clinical accuracy. To address this\nproblem,we propose a new RRG method via Multi-objective Preference Optimization\n(MPO) to align the pre-trained RRG model with multiple human preferences, which\ncan be formulated by multi-dimensional reward functions and optimized by\nmulti-objective reinforcement learning (RL). Specifically, we use a preference\nvector to represent the weight of preferences and use it as a condition for the\nRRG model. Then, a linearly weighed reward is obtained via a dot product\nbetween the preference vector and multi-dimensional reward. Next,the RRG model\nis optimized to align with the preference vector by optimizing such a reward\nvia RL. In the training stage,we randomly sample diverse preference vectors\nfrom the preference space and align the model by optimizing the weighted\nmulti-objective rewards, which leads to an optimal policy on the entire\npreference space. When inference,our model can generate reports aligned with\nspecific preferences without further fine-tuning. Extensive experiments on two\npublic datasets show the proposed method can generate reports that cater to\ndifferent preferences in a single model and achieve state-of-the-art\nperformance.\n","authors":["Ting Xiao","Lei Shi","Peng Liu","Zhe Wang","Chenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2412.08901v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09805v1","updated":"2024-12-13T02:44:47Z","published":"2024-12-13T02:44:47Z","title":"Universal Inceptive GNNs by Eliminating the Smoothness-generalization\n  Dilemma","summary":"  Graph Neural Networks (GNNs) have demonstrated remarkable success in various\ndomains, such as transaction and social net-works. However, their application\nis often hindered by the varyinghomophily levels across different orders of\nneighboring nodes, ne-cessitating separate model designs for homophilic and\nheterophilicgraphs. In this paper, we aim to develop a unified framework\nca-pable of handling neighborhoods of various orders and homophilylevels.\nThrough theoretical exploration, we identify a previouslyoverlooked\narchitectural aspect in multi-hop learning: the cascadedependency, which leads\nto asmoothness-generalization dilemma.This dilemma significantly affects the\nlearning process, especiallyin the context of high-order neighborhoods and\nheterophilic graphs.To resolve this issue, we propose an Inceptive Graph Neural\nNet-work (IGNN), a universal message-passing framework that replacesthe cascade\ndependency with an inceptive architecture. IGNN pro-vides independent\nrepresentations for each hop, allowing personal-ized generalization\ncapabilities, and captures neighborhood-wiserelationships to select appropriate\nreceptive fields. Extensive ex-periments show that our IGNN outperforms 23\nbaseline methods,demonstrating superior performance on both homophilic and\nhet-erophilic graphs, while also scaling efficiently to large graphs.\n","authors":["Ming Gu","Zhuonan Zheng","Sheng Zhou","Meihan Liu","Jiawei Chen","Tanyu Qiao","Liangcheng Li","Jiajun Bu"],"pdf_url":"https://arxiv.org/pdf/2412.09805v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2402.09025v6","updated":"2024-12-13T02:44:34Z","published":"2024-02-14T09:01:13Z","title":"SLEB: Streamlining LLMs through Redundancy Verification and Elimination\n  of Transformer Blocks","summary":"  Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB outperforms\nprevious LLM pruning methods in accelerating LLM inference while also\nmaintaining superior perplexity and accuracy, making SLEB as a promising\ntechnique for enhancing the efficiency of LLMs. The code is available at:\nhttps://github.com/jiwonsong-dev/SLEB.\n","authors":["Jiwon Song","Kyungseok Oh","Taesu Kim","Hyungjun Kim","Yulhwa Kim","Jae-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2402.09025v6.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2412.09803v1","updated":"2024-12-13T02:42:56Z","published":"2024-12-13T02:42:56Z","title":"deepNoC: A deep learning system to assign the number of contributors to\n  a short tandem repeat DNA profile","summary":"  A common task in forensic biology is to interpret and evaluate short tandem\nrepeat DNA profiles. The first step in these interpretations is to assign a\nnumber of contributors to the profiles, a task that is most often performed\nmanually by a scientist using their knowledge of DNA profile behaviour. Studies\nusing constructed DNA profiles have shown that as DNA profiles become more\ncomplex, and the number of DNA-donating individuals increases, the ability for\nscientists to assign the target number. There have been a number of machine\nlearning algorithms developed that seek to assign the number of contributors to\na DNA profile, however due to practical limitations in being able to generate\nDNA profiles in a laboratory, the algorithms have been based on summaries of\nthe available information. In this work we develop an analysis pipeline that\nsimulates the electrophoretic signal of an STR profile, allowing virtually\nunlimited, pre-labelled training material to be generated. We show that by\nsimulating 100 000 profiles and training a number of contributors estimation\ntool using a deep neural network architecture (in an algorithm named deepNoC)\nthat a high level of performance is achieved (89% for 1 to 10 contributors).\nThe trained network can then have fine-tuning training performed with only a\nfew hundred profiles in order to achieve the same accuracy within a specific\nlaboratory. We also build into deepNoC secondary outputs that provide a level\nof explainability to a user of algorithm, and show how they can be displayed in\nan intuitive manner.\n","authors":["Duncan Taylor","Melissa A. Humphries"],"pdf_url":"https://arxiv.org/pdf/2412.09803v1.pdf","comment":"29 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.09800v1","updated":"2024-12-13T02:39:04Z","published":"2024-12-13T02:39:04Z","title":"Infinite-dimensional next-generation reservoir computing","summary":"  Next-generation reservoir computing (NG-RC) has attracted much attention due\nto its excellent performance in spatio-temporal forecasting of complex systems\nand its ease of implementation. This paper shows that NG-RC can be encoded as a\nkernel ridge regression that makes training efficient and feasible even when\nthe space of chosen polynomial features is very large. Additionally, an\nextension to an infinite number of covariates is possible, which makes the\nmethodology agnostic with respect to the lags into the past that are considered\nas explanatory factors, as well as with respect to the number of polynomial\ncovariates, an important hyperparameter in traditional NG-RC. We show that this\napproach has solid theoretical backing and good behavior based on kernel\nuniversality properties previously established in the literature. Various\nnumerical illustrations show that these generalizations of NG-RC outperform the\ntraditional approach in several forecasting applications.\n","authors":["Lyudmila Grigoryeva","Hannah Lim Jing Ting","Juan-Pablo Ortega"],"pdf_url":"https://arxiv.org/pdf/2412.09800v1.pdf","comment":"13 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.09795v1","updated":"2024-12-13T02:26:58Z","published":"2024-12-13T02:26:58Z","title":"Is it the model or the metric -- On robustness measures of deeplearning\n  models","summary":"  Determining the robustness of deep learning models is an established and\nongoing challenge within automated decision-making systems. With the advent and\nsuccess of techniques that enable advanced deep learning (DL), these models are\nbeing used in widespread applications, including high-stake ones like\nhealthcare, education, border-control. Therefore, it is critical to understand\nthe limitations of these models and predict their regions of failures, in order\nto create the necessary guardrails for their successful and safe deployment. In\nthis work, we revisit robustness, specifically investigating the sufficiency of\nrobust accuracy (RA), within the context of deepfake detection. We present\nrobust ratio (RR) as a complementary metric, that can quantify the changes to\nthe normalized or probability outcomes under input perturbation. We present a\ncomparison of RA and RR and demonstrate that despite similar RA between models,\nthe models show varying RR under different tolerance (perturbation) levels.\n","authors":["Zhijin Lyu","Yutong Jin","Sneha Das"],"pdf_url":"https://arxiv.org/pdf/2412.09795v1.pdf","comment":"Extended abstract at Northern Lights Deep Learning (NLDL) Conference\n  2025"},{"id":"http://arxiv.org/abs/2412.08128v2","updated":"2024-12-13T02:12:40Z","published":"2024-12-11T06:31:06Z","title":"Why Does Dropping Edges Usually Outperform Adding Edges in Graph\n  Contrastive Learning?","summary":"  Graph contrastive learning (GCL) has been widely used as an effective\nself-supervised learning method for graph representation learning. However, how\nto apply adequate and stable graph augmentation to generating proper views for\ncontrastive learning remains an essential problem. Dropping edges is a primary\naugmentation in GCL while adding edges is not a common method due to its\nunstable performance. To our best knowledge, there is no theoretical analysis\nto study why dropping edges usually outperforms adding edges. To answer this\nquestion, we introduce a new metric, namely Error Passing Rate (EPR), to\nquantify how a graph fits the network. Inspired by the theoretical conclusions,\nwe propose a novel GCL algorithm, Error-PAssing-based Graph Contrastive\nLearning (EPAGCL), which uses both edge adding and edge dropping as its\naugmentation. To be specific, we generate views by adding and dropping edges\naccording to the weights derived from EPR. Extensive experiments on various\nreal-world datasets are conducted to validate the correctness of our\ntheoretical analysis and the effectiveness of our proposed algorithm.\n","authors":["Yanchen Xu","Siqi Huang","Hongyuan Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2412.08128v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2112.06657v3","updated":"2024-12-13T02:08:47Z","published":"2021-12-09T12:23:06Z","title":"You Can Wash Hands Better: Accurate Daily Handwashing Assessment with\n  Smartwatches","summary":"  Hand hygiene is among the most effective daily practices for preventing\ninfectious diseases such as influenza, malaria, and skin infections. While\nprofessional guidelines emphasize proper handwashing to reduce the risk of\nviral infections, surveys reveal that adherence to these recommendations\nremains low. To address this gap, we propose UWash, a wearable solution\nleveraging smartwatches to evaluate handwashing procedures, aiming to raise\nawareness and cultivate high-quality handwashing habits. We frame the task of\nhandwashing assessment as an action segmentation problem, similar to those in\ncomputer vision, and introduce a simple yet efficient two-stream UNet-like\nnetwork to achieve this goal. Experiments involving 51 subjects demonstrate\nthat UWash achieves 92.27% accuracy in handwashing gesture recognition, an\nerror of <0.5 seconds in onset/offset detection, and an error of <5 points in\ngesture scoring under user-dependent settings. The system also performs\nrobustly in user-independent and user-independent-location-independent\nevaluations. Remarkably, UWash maintains high performance in real-world tests,\nincluding evaluations with 10 random passersby at a hospital 9 months later and\n10 passersby in an in-the-wild test conducted 2 years later. UWash is the first\nsystem to score handwashing quality based on gesture sequences, offering\nactionable guidance for improving daily hand hygiene. The code and dataset are\npublicly available at \\url{https://github.com/aiotgroup/UWash}.\n","authors":["Fei Wang","Xilei Wu","Tingting Zhang","Xin Wang","Pengcheng Wang","Han Ding","Jingang Shi","Jinsong Han","Dong Huang"],"pdf_url":"https://arxiv.org/pdf/2112.06657v3.pdf","comment":"Under review. 13 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.04458v3","updated":"2024-12-13T01:58:52Z","published":"2024-10-06T12:15:00Z","title":"A Comprehensive Framework for Analyzing the Convergence of Adam:\n  Bridging the Gap with SGD","summary":"  Adaptive Moment Estimation (Adam) is a cornerstone optimization algorithm in\ndeep learning, widely recognized for its flexibility with adaptive learning\nrates and efficiency in handling large-scale data. However, despite its\npractical success, the theoretical understanding of Adam's convergence has been\nconstrained by stringent assumptions, such as almost surely bounded stochastic\ngradients or uniformly bounded gradients, which are more restrictive than those\ntypically required for analyzing stochastic gradient descent (SGD).\n  In this paper, we introduce a novel and comprehensive framework for analyzing\nthe convergence properties of Adam. This framework offers a versatile approach\nto establishing Adam's convergence. Specifically, we prove that Adam achieves\nasymptotic (last iterate sense) convergence in both the almost sure sense and\nthe \\(L_1\\) sense under the relaxed assumptions typically used for SGD, namely\n\\(L\\)-smoothness and the ABC inequality. Meanwhile, under the same assumptions,\nwe show that Adam attains non-asymptotic sample complexity bounds similar to\nthose of SGD.\n","authors":["Ruinan Jin","Xiao Li","Yaoliang Yu","Baoxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.04458v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.04932v2","updated":"2024-12-13T01:46:46Z","published":"2023-04-11T02:09:13Z","title":"Robust Dequantization of the Quantum Singular value Transformation and\n  Quantum Machine Learning Algorithms","summary":"  Several quantum algorithms for linear algebra problems, and in particular\nquantum machine learning problems, have been \"dequantized\" in the past few\nyears. These dequantization results typically hold when classical algorithms\ncan access the data via length-squared sampling. In this work we investigate\nhow robust these dequantization results are. We introduce the notion of\napproximate length-squared sampling, where classical algorithms are only able\nto sample from a distribution close to the ideal distribution in total\nvariation distance. While quantum algorithms are natively robust against small\nperturbations, current techniques in dequantization are not. Our main technical\ncontribution is showing how many techniques from randomized linear algebra can\nbe adapted to work under this weaker assumption as well. We then use these\ntechniques to show that the recent low-rank dequantization framework by Chia,\nGily\\'en, Li, Lin, Tang and Wang (JACM 2022) and the dequantization framework\nfor sparse matrices by Gharibian and Le Gall (STOC 2022), which are both based\non the Quantum Singular Value Transformation, can be generalized to the case of\napproximate length-squared sampling access to the input. We also apply these\nresults to obtain a robust dequantization of many quantum machine learning\nalgorithms, including quantum algorithms for recommendation systems, supervised\nclustering and low-rank matrix inversion.\n","authors":["François Le Gall"],"pdf_url":"https://arxiv.org/pdf/2304.04932v2.pdf","comment":"56 pages; v2: minor changes (final journal version)"},{"id":"http://arxiv.org/abs/2412.09779v1","updated":"2024-12-13T01:15:17Z","published":"2024-12-13T01:15:17Z","title":"A Statistical Analysis for Supervised Deep Learning with Exponential\n  Families for Intrinsically Low-dimensional Data","summary":"  Recent advances have revealed that the rate of convergence of the expected\ntest error in deep supervised learning decays as a function of the intrinsic\ndimension and not the dimension $d$ of the input space. Existing literature\ndefines this intrinsic dimension as the Minkowski dimension or the manifold\ndimension of the support of the underlying probability measures, which often\nresults in sub-optimal rates and unrealistic assumptions. In this paper, we\nconsider supervised deep learning when the response given the explanatory\nvariable is distributed according to an exponential family with a\n$\\beta$-H\\\"older smooth mean function. We consider an entropic notion of the\nintrinsic data-dimension and demonstrate that with $n$ independent and\nidentically distributed samples, the test error scales as\n$\\tilde{\\mathcal{O}}\\left(n^{-\\frac{2\\beta}{2\\beta +\n\\bar{d}_{2\\beta}(\\lambda)}}\\right)$, where $\\bar{d}_{2\\beta}(\\lambda)$ is the\n$2\\beta$-entropic dimension of $\\lambda$, the distribution of the explanatory\nvariables. This improves on the best-known rates. Furthermore, under the\nassumption of an upper-bounded density of the explanatory variables, we\ncharacterize the rate of convergence as $\\tilde{\\mathcal{O}}\\left(\nd^{\\frac{2\\lfloor\\beta\\rfloor(\\beta + d)}{2\\beta + d}}n^{-\\frac{2\\beta}{2\\beta\n+ d}}\\right)$, establishing that the dependence on $d$ is not exponential but\nat most polynomial. We also demonstrate that when the explanatory variable has\na lower bounded density, this rate in terms of the number of data samples, is\nnearly optimal for learning the dependence structure for exponential families.\n","authors":["Saptarshi Chakraborty","Peter L. Bartlett"],"pdf_url":"https://arxiv.org/pdf/2412.09779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14856v4","updated":"2024-12-13T00:46:38Z","published":"2024-06-21T04:02:19Z","title":"Accessible, At-Home Detection of Parkinson's Disease via Multi-task\n  Video Analysis","summary":"  Limited accessibility to neurological care leads to underdiagnosed\nParkinson's Disease (PD), preventing early intervention. Existing AI-based PD\ndetection methods primarily focus on unimodal analysis of motor or speech\ntasks, overlooking the multifaceted nature of the disease. To address this, we\nintroduce a large-scale, multi-task video dataset consisting of 1102 sessions\n(each containing videos of finger tapping, facial expression, and speech tasks\ncaptured via webcam) from 845 participants (272 with PD). We propose a novel\nUncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal\ndata to enhance diagnostic accuracy. UFNet employs independent task-specific\nnetworks, trained with Monte Carlo Dropout for uncertainty quantification,\nfollowed by self-attended fusion of features, with attention weights\ndynamically adjusted based on task-specific uncertainties. To ensure\npatient-centered evaluation, the participants were randomly split into three\nsets: 60% for training, 20% for model selection, and 20% for final performance\nevaluation. UFNet significantly outperformed single-task models in terms of\naccuracy, area under the ROC curve (AUROC), and sensitivity while maintaining\nnon-inferior specificity. Withholding uncertain predictions further boosted the\nperformance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9%\nsensitivity, and 92.6+-0.3% specificity, at the expense of not being able to\npredict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further\nanalysis suggests that the trained model does not exhibit any detectable bias\nacross sex and ethnic subgroups and is most effective for individuals aged\nbetween 50 and 80. Requiring only a webcam and microphone, our approach\nfacilitates accessible home-based PD screening, especially in regions with\nlimited healthcare resources.\n","authors":["Md Saiful Islam","Tariq Adnan","Jan Freyberg","Sangwu Lee","Abdelrahman Abdelkader","Meghan Pawlik","Cathe Schwartz","Karen Jaffe","Ruth B. Schneider","E Ray Dorsey","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2406.14856v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06291v3","updated":"2024-12-13T00:38:11Z","published":"2023-06-09T22:48:13Z","title":"Optimal Multitask Linear Regression and Contextual Bandits under Sparse\n  Heterogeneity","summary":"  Large and complex datasets are often collected from several, possibly\nheterogeneous sources. Multitask learning methods improve efficiency by\nleveraging commonalities across datasets while accounting for possible\ndifferences among them. Here, we study multitask linear regression and\ncontextual bandits under sparse heterogeneity, where the source/task-associated\nparameters are equal to a global parameter plus a sparse task-specific term. We\npropose a novel two-stage estimator called MOLAR that leverages this structure\nby first constructing a covariate-wise weighted median of the task-wise linear\nregression estimates and then shrinking the task-wise estimates towards the\nweighted median. Compared to task-wise least squares estimates, MOLAR improves\nthe dependence of the estimation error on the data dimension. Extensions of\nMOLAR to generalized linear models and constructing confidence intervals are\ndiscussed in the paper. We then apply MOLAR to develop methods for sparsely\nheterogeneous multitask contextual bandits, obtaining improved regret\nguarantees over single-task bandit methods. We further show that our methods\nare minimax optimal by providing a number of lower bounds. Finally, we support\nthe efficiency of our methods by performing experiments on both synthetic data\nand the PISA dataset on student educational outcomes from heterogeneous\ncountries.\n","authors":["Xinmeng Huang","Kan Xu","Donghwan Lee","Hamed Hassani","Hamsa Bastani","Edgar Dobriban"],"pdf_url":"https://arxiv.org/pdf/2306.06291v3.pdf","comment":"Journal of the American Statistical Association, 2024"},{"id":"http://arxiv.org/abs/2412.09769v1","updated":"2024-12-13T00:26:36Z","published":"2024-12-13T00:26:36Z","title":"A Novel Methodology in Credit Spread Prediction Based on Ensemble\n  Learning and Feature Selection","summary":"  The credit spread is a key indicator in bond investments, offering valuable\ninsights for fixed-income investors to devise effective trading strategies.\nThis study proposes a novel credit spread forecasting model leveraging ensemble\nlearning techniques. To enhance predictive accuracy, a feature selection method\nbased on mutual information is incorporated. Empirical results demonstrate that\nthe proposed methodology delivers superior accuracy in credit spread\npredictions. Additionally, we present a forecast of future credit spread trends\nusing current data, providing actionable insights for investment\ndecision-making.\n","authors":["Yu Shao","Jiawen Bai","Yingze Hou","Xia'an Zhou","Zhanhao Pan"],"pdf_url":"https://arxiv.org/pdf/2412.09769v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.10041v2","updated":"2024-12-13T00:23:09Z","published":"2024-10-13T23:05:37Z","title":"WormKAN: Are KAN Effective for Identifying and Tracking Concept Drift in\n  Time Series?","summary":"  Dynamic concepts in time series are crucial for understanding complex systems\nsuch as financial markets, healthcare, and online activity logs. These concepts\nhelp reveal structures and behaviors in sequential data for better\ndecision-making and forecasting. However, existing models often struggle to\ndetect and track concept drift due to limitations in interpretability and\nadaptability. To address this challenge, inspired by the flexibility of the\nrecent Kolmogorov-Arnold Network (KAN), we propose WormKAN, a concept-aware\nKAN-based model to address concept drift in co-evolving time series. WormKAN\nconsists of three key components: Patch Normalization, Temporal Representation\nModule, and Concept Dynamics. Patch normalization processes co-evolving time\nseries into patches, treating them as fundamental modeling units to capture\nlocal dependencies while ensuring consistent scaling. The temporal\nrepresentation module learns robust latent representations by leveraging a\nKAN-based autoencoder, complemented by a smoothness constraint, to uncover\ninter-patch correlations. Concept dynamics identifies and tracks dynamic\ntransitions, revealing structural shifts in the time series through concept\nidentification and drift detection. These transitions, akin to passing through\na \\textit{wormhole}, are identified by abrupt changes in the latent space.\nExperiments show that KAN and KAN-based models (WormKAN) effectively segment\ntime series into meaningful concepts, enhancing the identification and tracking\nof concept drift.\n","authors":["Kunpeng Xu","Lifei Chen","Shengrui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10041v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2412.07889v2","updated":"2024-12-13T15:39:27Z","published":"2024-12-10T19:48:57Z","title":"Low-Latency Scalable Streaming for Event-Based Vision","summary":"  Recently, we have witnessed the rise of novel ``event-based'' camera sensors\nfor high-speed, low-power video capture. Rather than recording discrete image\nframes, these sensors output asynchronous ``event'' tuples with microsecond\nprecision, only when the brightness change of a given pixel exceeds a certain\nthreshold. Although these sensors have enabled compelling new computer vision\napplications, these applications often require expensive, power-hungry GPU\nsystems, rendering them incompatible for deployment on the low-power devices\nfor which event cameras are optimized. Whereas receiver-driven rate adaptation\nis a crucial feature of modern video streaming solutions, this topic is\nunderexplored in the realm of event-based vision systems. On a real-world event\ncamera dataset, we first demonstrate that a state-of-the-art object detection\napplication is resilient to dramatic data loss, and that this loss may be\nweighted towards the end of each temporal window. We then propose a scalable\nstreaming method for event-based data based on Media Over QUIC, prioritizing\nobject detection performance and low latency. The application server can\nreceive complementary event data across several streams simultaneously, and\ndrop streams as needed to maintain a certain latency. With a latency target of\n5 ms for end-to-end transmission across a small network, we observe an average\nreduction in detection mAP as low as 0.36. With a more relaxed latency target\nof 50 ms, we observe an average mAP reduction as low as 0.19.\n","authors":["Andrew Hamara","Benjamin Kilpatrick","Alex Baratta","Brendon Kofink","Andrew C. Freeman"],"pdf_url":"https://arxiv.org/pdf/2412.07889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10153v1","updated":"2024-12-13T14:11:42Z","published":"2024-12-13T14:11:42Z","title":"EVOS: Efficient Implicit Neural Training via EVOlutionary Selector","summary":"  We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.\n","authors":["Weixiang Zhang","Shuzhao Xie","Chengwei Ren","Siyi Xie","Chen Tang","Shijia Ge","Mingzi Wang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07689v3","updated":"2024-12-13T08:13:44Z","published":"2024-12-10T17:27:32Z","title":"DriveMM: All-in-One Large Multimodal Model for Autonomous Driving","summary":"  Large Multimodal Models (LMMs) have demonstrated exceptional comprehension\nand interpretation capabilities in Autonomous Driving (AD) by incorporating\nlarge language models. Despite the advancements, current data-driven AD\napproaches tend to concentrate on a single dataset and specific tasks,\nneglecting their overall capabilities and ability to generalize. To bridge\nthese gaps, we propose DriveMM, a general large multimodal model designed to\nprocess diverse data inputs, such as images and multi-view videos, while\nperforming a broad spectrum of AD tasks, including perception, prediction, and\nplanning. Initially, the model undergoes curriculum pre-training to process\nvaried visual signals and perform basic visual comprehension and perception\ntasks. Subsequently, we augment and standardize various AD-related datasets to\nfine-tune the model, resulting in an all-in-one LMM for autonomous driving. To\nassess the general capabilities and generalization ability, we conduct\nevaluations on six public benchmarks and undertake zero-shot transfer on an\nunseen dataset, where DriveMM achieves state-of-the-art performance across all\ntasks. We hope DriveMM as a promising solution for future end-to-end autonomous\ndriving applications in the real world. Project page with code:\nhttps://github.com/zhijian11/DriveMM.\n","authors":["Zhijian Huang","Chengjian Feng","Feng Yan","Baihui Xiao","Zequn Jie","Yujie Zhong","Xiaodan Liang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.07689v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16946v2","updated":"2024-12-13T03:42:00Z","published":"2024-11-25T21:35:42Z","title":"Lens Distortion Encoding System Version 1.0","summary":"  Lens Distortion Encoding System (LDES) allows for a distortion-accurate\nworkflow, with a seamless interchange of high quality motion picture images\nregardless of the lens source. This system is similar in a concept to the\nAcademy Color Encoding System (ACES), but for distortion. Presented solution is\nfully compatible with existing software/plug-in tools for STMapping found in\npopular production software like Adobe After Effects or DaVinci Resolve. LDES\nutilizes common distortion space and produces single high-quality, animatable\nSTMap used for direct transformation of one view to another, neglecting the\nneed of lens-swapping for each shoot. The LDES profile of a lens consist of two\nelements; View Map texture, and Footage Map texture, each labeled with the FOV\nvalue. Direct distortion mapping is produced by sampling of the Footage Map\nthrough the View Map. The result; animatable mapping texture, is then used to\nsample the footage to a desired distortion. While the Footage Map is specific\nto a footage, View Maps can be freely combined/transitioned and animated,\nallowing for effects like smooth shift from anamorphic to spherical distortion,\npreviously impossible to achieve in practice. Presented LDES Version 1.0 uses\ncommon 32-bit STMap format for encoding, supported by most compositing\nsoftware, directly or via plug-ins. The difference between standard STMap\nworkflow and LDES is that it encodes absolute pixel position in the spherical\nimage model. The main benefit of this approach is the ability to achieve a\nsimilar look of a highly expensive lens using some less expensive equipment in\nterms of distortion. It also provides greater artistic control and never seen\nbefore manipulation of footage.\n","authors":["Jakub Maksymilian Fober"],"pdf_url":"https://arxiv.org/pdf/2411.16946v2.pdf","comment":"8 pages, 1 figure, 2 tables"}],"Database":[{"id":"http://arxiv.org/abs/2412.10253v1","updated":"2024-12-13T16:22:58Z","published":"2024-12-13T16:22:58Z","title":"A Novel Framework Using Deep Reinforcement Learning for Join Order\n  Selection","summary":"  Join order selection is a sub-field of query optimization that aims to find\nthe optimal join order for an SQL query with the minimum cost. The challenge\nlies in the exponentially growing search space as the number of tables\nincreases, making exhaustive enumeration impractical. Traditional optimizers\nuse static heuristics to prune the search space, but they often fail to adapt\nto changes or improve based on feedback from the DBMS. Recent research\naddresses these limitations with Deep Reinforcement Learning (DRL), allowing\nmodels to use feedback to dynamically search for better join orders and enhance\nperformance over time. Existing research primarily focuses on capturing join\norder sequences and their representations at various levels, with limited\ncomparative analysis of reinforcement learning methods. In this paper, we\npropose GTDD, a novel framework that integrates Graph Neural Networks (GNN),\nTreestructured Long Short-Term Memory (Tree LSTM), and DuelingDQN. We conduct a\nseries of experiments that demonstrate a clear advantage of GTDD over state-of\nthe-art techniques.\n","authors":["Chang Liu","Amin Kamali","Verena Kantere","Calisto Zuzarte","Vincent Corvinelli"],"pdf_url":"https://arxiv.org/pdf/2412.10253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09940v1","updated":"2024-12-13T08:03:57Z","published":"2024-12-13T08:03:57Z","title":"Predictive Query-based Pipeline for Graph Data","summary":"  Graphs face challenges when dealing with massive datasets. They are essential\ntools for modeling interconnected data and often become computationally\nexpensive. Graph embedding techniques, on the other hand, provide an efficient\napproach. By projecting complex graphs into a lower-dimensional space, these\ntechniques simplify the analysis and processing of large-scale graphs. By\ntransforming graphs into vectors, it simplifies the analysis and processing of\nlarge-scale datasets. Several approaches, such as GraphSAGE, Node2Vec, and\nFastRP, offer efficient methods for generating graph embeddings. By storing\nembeddings as node properties, it is possible to compare different embedding\ntechniques and evaluate their effectiveness for specific tasks. This\nflexibilityallows for dynamic updates to embeddings and facilitates\nexperimentation with different approaches. By analyzing these embeddings, one\ncan extract valuable insights into the relationships between nodes and their\nsimilarities within the embedding space\n","authors":["Plácido A Souza Neto"],"pdf_url":"https://arxiv.org/pdf/2412.09940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07214v2","updated":"2024-12-13T07:08:54Z","published":"2024-12-10T06:11:23Z","title":"Towards Automated Cross-domain Exploratory Data Analysis through Large\n  Language Models","summary":"  Exploratory data analysis (EDA), coupled with SQL, is essential for data\nanalysts involved in data exploration and analysis. However, data analysts\noften encounter two primary challenges: (1) the need to craft SQL queries\nskillfully, and (2) the requirement to generate suitable visualization types\nthat enhance the interpretation of query results. Due to its significance,\nsubstantial research efforts have been made to explore different approaches to\naddress these challenges, including leveraging large language models (LLMs).\nHowever, existing methods fail to meet real-world data exploration requirements\nprimarily due to (1) complex database schema; (2) unclear user intent; (3)\nlimited cross-domain generalization capability; and (4) insufficient end-to-end\ntext-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain\nexploratory data analysis system. First, we propose hierarchical data context\n(i.e., HDC), which leverages LLMs to summarize the contexts related to the\ndatabase schema, which is crucial for open-world EDA systems to generalize\nacross data domains. Second, the EDA system is divided into four components\n(i.e., stages): HDC generation, question clarification and decomposition,\ntext-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart).\nFinally, we implemented an end-to-end EDA system with a user-friendly GUI\ninterface in the production environment at PingCAP. We have also open-sourced\nall APIs of TiInsight to facilitate research within the EDA community. Through\nextensive evaluations by a real-world user study, we demonstrate that TiInsight\noffers remarkable performance compared to human experts. Specifically, TiSQL\nachieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It\nalso demonstrates state-of-the-art performance on the Bird dataset.\n","authors":["Jun-Peng Zhu","Boyan Niu","Peng Cai","Zheming Ni","Jianwei Wan","Kai Xu","Jiajun Huang","Shengbo Ma","Bing Wang","Xuan Zhou","Guanglei Bao","Donghui Zhang","Liu Tang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.07214v2.pdf","comment":"14 pages, 10 figures. Submitted to SIGMOD 2025"},{"id":"http://arxiv.org/abs/2412.09337v2","updated":"2024-12-13T05:24:56Z","published":"2024-12-12T15:04:03Z","title":"RTCUDB: Building Databases with RT Processors","summary":"  A spectrum of new hardware has been studied to accelerate database systems in\nthe past decade. Specifically, CUDA cores are known to benefit from the fast\ndevelopment of GPUs and make notable performance improvements. The\nstate-of-the-art GPU-based implementation, i.e., Crystal, can achieve up to 61\ntimes higher performance than CPU-based implementations. However, experiments\nshow that the approach has already saturated almost all GPU memory bandwidth,\nwhich means there is little room left for further performance improvements. We\nintroduce RTCUDB, the first query engine that leverages ray tracing (RT) cores\nin GPUs to accelerate database query processing. RTCUDB efficiently transforms\nthe evaluation of a query into a ray-tracing job in a three-dimensional space.\nBy dramatically reducing the amount of accessed data and optimizing the data\naccess pattern with the ray tracing mechanism, the performance of RTCUDB is no\nlonger limited by the memory bandwidth as in CUDA-based implementations.\nExperimental results show that RTCUDB outperforms the state-of-the-art\nGPU-based query engine by up to 18.3 times while the memory bandwidth usage\ndrops to only 36.7% on average.\n","authors":["Xuri Shi","Kai Zhang","X. Sean Wang","Xiaodong Zhang","Rubao Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09337v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06724v2","updated":"2024-12-13T03:43:35Z","published":"2024-12-09T18:13:27Z","title":"AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and\n  Benchmark","summary":"  We investigate the reasoning capabilities of large language models (LLMs) for\nautomatically generating data-cleaning workflows. To evaluate LLMs' ability to\ncomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data\nCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations\nto repair three types of data quality issues: duplicates, missing values, and\ninconsistent data formats. Given a dirty table and a purpose (expressed as a\nquery), this pipeline generates a minimal, clean table sufficient to address\nthe purpose and the data cleaning workflow used to produce the table. The\nplanning process involves three main LLM-driven components: (1) Select Target\nColumns: Identifies a set of target columns related to the purpose. (2) Inspect\nColumn Quality: Assesses the data quality for each target column and generates\na Data Quality Report as operation objectives. (3) Generate Operation &\nArguments: Predicts the next operation and arguments based on the data quality\nreport results. Additionally, we propose a data cleaning benchmark to evaluate\nthe capability of LLM agents to automatically generate workflows that address\ndata cleaning purposes of varying difficulty levels. The benchmark comprises\nthe annotated datasets as a collection of purpose, raw table, clean table, data\ncleaning workflow, and answer set. In our experiments, we evaluated three LLMs\nthat auto-generate purpose-driven data cleaning workflows. The results indicate\nthat LLMs perform well in planning and generating data-cleaning workflows\nwithout the need for fine-tuning.\n","authors":["Lan Li","Liri Fang","Vetle I. Torvik"],"pdf_url":"https://arxiv.org/pdf/2412.06724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04349v2","updated":"2024-12-13T02:37:00Z","published":"2024-10-06T03:59:54Z","title":"HyperBlocker: Accelerating Rule-based Blocking in Entity Resolution\n  using GPUs","summary":"  This paper studies rule-based blocking in Entity Resolution (ER). We propose\nHyperBlocker, a GPU-accelerated system for blocking in ER. As opposed to\nprevious blocking algorithms and parallel blocking solvers, HyperBlocker\nemploys a pipelined architecture to overlap data transfer and GPU operations.\nIt generates a dataaware and rule-aware execution plan on CPUs, for specifying\nhow rules are evaluated, and develops a number of hardware-aware optimizations\nto achieve massive parallelism on GPUs. Using reallife datasets, we show that\nHyperBlocker is at least 6.8x and 9.1x faster than prior CPU-powered\ndistributed systems and GPU-based ER solvers, respectively. Better still, by\ncombining HyperBlocker with the state-of-the-art ER matcher, we can speed up\nthe overall ER process by at least 30% with comparable accuracy.\n","authors":["Xiaoke Zhu","Min Xie","Ting Deng","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.04349v2.pdf","comment":"In PVLDB 2025"},{"id":"http://arxiv.org/abs/2412.09788v1","updated":"2024-12-13T02:04:53Z","published":"2024-12-13T02:04:53Z","title":"OpenForge: Probabilistic Metadata Integration","summary":"  Modern data stores increasingly rely on metadata for enabling diverse\nactivities such as data cataloging and search. However, metadata curation\nremains a labor-intensive task, and the broader challenge of metadata\nmaintenance -- ensuring its consistency, usefulness, and freshness -- has been\nlargely overlooked. In this work, we tackle the problem of resolving\nrelationships among metadata concepts from disparate sources. These\nrelationships are critical for creating clean, consistent, and up-to-date\nmetadata repositories, and a central challenge for metadata integration.\n  We propose OpenForge, a two-stage prior-posterior framework for metadata\nintegration. In the first stage, OpenForge exploits multiple methods including\nfine-tuned large language models to obtain prior beliefs about concept\nrelationships. In the second stage, OpenForge refines these predictions by\nleveraging Markov Random Field, a probabilistic graphical model. We formalize\nmetadata integration as an optimization problem, where the objective is to\nidentify the relationship assignments that maximize the joint probability of\nassignments. The MRF formulation allows OpenForge to capture prior beliefs\nwhile encoding critical relationship properties, such as transitivity, in\nprobabilistic inference. Experiments on real-world datasets demonstrate the\neffectiveness and efficiency of OpenForge. On a use case of matching two\nmetadata vocabularies, OpenForge outperforms GPT-4, the second-best method, by\n25 F1-score points.\n","authors":["Tianji Cong","Fatemeh Nargesian","Junjie Xing","H. V. Jagadish"],"pdf_url":"https://arxiv.org/pdf/2412.09788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07856v3","updated":"2024-12-13T23:11:18Z","published":"2023-09-14T17:00:53Z","title":"SMARTFEAT: Efficient Feature Construction through Feature-Level\n  Foundation Model Interactions","summary":"  Before applying data analytics or machine learning to a data set, a vital\nstep is usually the construction of an informative set of features from the\ndata. In this paper, we present SMARTFEAT, an efficient automated feature\nengineering tool to assist data users, even non-experts, in constructing useful\nfeatures. Leveraging the power of Foundation Models (FMs), our approach enables\nthe creation of new features from the data, based on contextual information and\nopen-world knowledge. Our method incorporates an intelligent operator selector\nthat discerns a subset of operators, effectively avoiding exhaustive\ncombinations of original features, as is typically observed in traditional\nautomated feature engineering tools. Moreover, we address the limitations of\nperforming data tasks through row-level interactions with FMs, which could lead\nto significant delays and costs due to excessive API calls. We introduce a\nfunction generator that facilitates the acquisition of efficient data\ntransformations, such as dataframe built-in methods or lambda functions,\nensuring the applicability of SMARTFEAT to generate new features for large\ndatasets. Code repo with prompt details and datasets:\n(https://github.com/niceIrene/SMARTFEAT).\n","authors":["Yin Lin","Bolin Ding","H. V. Jagadish","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2309.07856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10546v1","updated":"2024-12-13T20:49:01Z","published":"2024-12-13T20:49:01Z","title":"EmpireDB: Data System to Accelerate Computational Sciences","summary":"  The emerging discipline of Computational Science is concerned with using\ncomputers to simulate or solve scientific problems. These problems span the\nnatural, political, and social sciences. The discipline has exploded over the\npast decade due to the emergence of larger amounts of observational data and\nlarge-scale simulations that were previously unavailable or unfeasible.\nHowever, there are still significant challenges with managing the large amounts\nof data and simulations.\n  The database management systems community has always been at the forefront of\nthe development of the theory and practice of techniques for formalizing and\nactualizing systems that access or query large datasets. In this paper, we\npresent EmpireDB, a vision for a data management system to accelerate\ncomputational sciences. In addition, we identify challenges and opportunities\nfor the database community to further the fledgling field of computational\nsciences. Finally, we present preliminary evidence showing that the optimized\ncomponents in EmpireDB could lead to improvements in performance compared to\ncontemporary implementations.\n","authors":["Daniel Alabi","Eugene Wu"],"pdf_url":"https://arxiv.org/pdf/2412.10546v1.pdf","comment":null}],"Performance":[{"id":"http://arxiv.org/abs/2412.10494v1","updated":"2024-12-13T18:59:56Z","published":"2024-12-13T18:59:56Z","title":"SnapGen-V: Generating a Five-Second Video within Five Seconds on a\n  Mobile Device","summary":"  We have witnessed the unprecedented success of diffusion-based video\ngeneration over the past year. Recently proposed models from the community have\nwielded the power to generate cinematic and high-resolution videos with smooth\nmotions from arbitrary input prompts. However, as a supertask of image\ngeneration, video generation models require more computation and are thus\nhosted mostly on cloud servers, limiting broader adoption among content\ncreators. In this work, we propose a comprehensive acceleration framework to\nbring the power of the large-scale video diffusion model to the hands of edge\nusers. From the network architecture scope, we initialize from a compact image\nbackbone and search out the design and arrangement of temporal layers to\nmaximize hardware efficiency. In addition, we propose a dedicated adversarial\nfine-tuning algorithm for our efficient model and reduce the denoising steps to\n4. Our model, with only 0.6B parameters, can generate a 5-second video on an\niPhone 16 PM within 5 seconds. Compared to server-side models that take minutes\non powerful GPUs to generate a single video, we accelerate the generation by\nmagnitudes while delivering on-par quality.\n","authors":["Yushu Wu","Zhixing Zhang","Yanyu Li","Yanwu Xu","Anil Kag","Yang Sui","Huseyin Coskun","Ke Ma","Aleksei Lebedev","Ju Hu","Dimitris Metaxas","Yanzhi Wang","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2412.10494v1.pdf","comment":"https://snap-research.github.io/snapgen-v/"}]},"2024-12-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2412.12094v1","updated":"2024-12-16T18:58:57Z","published":"2024-12-16T18:58:57Z","title":"SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator","summary":"  Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.\n","authors":["Guoxuan Chen","Han Shi","Jiawei Li","Yihang Gao","Xiaozhe Ren","Yimeng Chen","Xin Jiang","Zhenguo Li","Weiyang Liu","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17241v3","updated":"2024-12-16T18:54:05Z","published":"2024-06-25T03:09:53Z","title":"Understanding Language Model Circuits through Knowledge Editing","summary":"  Recent advances in language model interpretability have identified circuits,\ncritical subnetworks that replicate model behaviors, yet how knowledge is\nstructured within these crucial subnetworks remains opaque. To gain an\nunderstanding toward the knowledge in the circuits, we conduct systematic\nknowledge editing experiments on the circuits of the GPT-2 language model. Our\nanalysis reveals intriguing patterns in how circuits respond to editing\nattempts, the extent of knowledge distribution across network components, and\nthe architectural composition of knowledge-bearing circuits. These findings\noffer insights into the complex relationship between model circuits and\nknowledge representation, deepening the understanding of how information is\norganized within language models. Our findings offer novel insights into the\n``meanings'' of the circuits, and introduce directions for further\ninterpretability and safety research of language models.\n","authors":["Huaizhi Ge","Frank Rudzicz","Zining Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.17241v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12072v1","updated":"2024-12-16T18:46:12Z","published":"2024-12-16T18:46:12Z","title":"Making FETCH! Happen: Finding Emergent Dog Whistles Through Common\n  Habitats","summary":"  WARNING: This paper contains content that maybe upsetting or offensive to\nsome readers. Dog whistles are coded expressions with dual meanings: one\nintended for the general public (outgroup) and another that conveys a specific\nmessage to an intended audience (ingroup). Often, these expressions are used to\nconvey controversial political opinions while maintaining plausible deniability\nand slip by content moderation filters. Identification of dog whistles relies\non curated lexicons, which have trouble keeping up to date. We introduce\n\\textbf{FETCH!}, a task for finding novel dog whistles in massive social media\ncorpora. We find that state-of-the-art systems fail to achieve meaningful\nresults across three distinct social media case studies. We present\n\\textbf{EarShot}, a novel system that combines the strengths of vector\ndatabases and Large Language Models (LLMs) to efficiently and effectively\nidentify new dog whistles.\n","authors":["Kuleen Sasse","Carlos Aguirre","Isabel Cachola","Sharon Levy","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2412.12072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12062v1","updated":"2024-12-16T18:35:58Z","published":"2024-12-16T18:35:58Z","title":"Semi-automated analysis of audio-recorded lessons: The case of teachers'\n  engaging messages","summary":"  Engaging messages delivered by teachers are a key aspect of the classroom\ndiscourse that influences student outcomes. However, improving this\ncommunication is challenging due to difficulties in obtaining observations.\nThis study presents a methodology for efficiently extracting actual\nobservations of engaging messages from audio-recorded lessons. We collected\n2,477 audio-recorded lessons from 75 teachers over two academic years. Using\nautomatic transcription and keyword-based filtering analysis, we identified and\nclassified engaging messages. This method reduced the information to be\nanalysed by 90%, optimising the time and resources required compared to\ntraditional manual coding. Subsequent descriptive analysis revealed that the\nmost used messages emphasised the future benefits of participating in school\nactivities. In addition, the use of engaging messages decreased as the academic\nyear progressed. This study offers insights for researchers seeking to extract\ninformation from teachers' discourse in naturalistic settings and provides\nuseful information for designing interventions to improve teachers'\ncommunication strategies.\n","authors":["Samuel Falcon","Carmen Alvarez-Alvarez","Jaime Leon"],"pdf_url":"https://arxiv.org/pdf/2412.12062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12061v1","updated":"2024-12-16T18:34:32Z","published":"2024-12-16T18:34:32Z","title":"Virtual Agent-Based Communication Skills Training to Facilitate Health\n  Persuasion Among Peers","summary":"  Many laypeople are motivated to improve the health behavior of their family\nor friends but do not know where to start, especially if the health behavior is\npotentially stigmatizing or controversial. We present an approach that uses\nvirtual agents to coach community-based volunteers in health counseling\ntechniques, such as motivational interviewing, and allows them to practice\nthese skills in role-playing scenarios. We use this approach in a virtual\nagent-based system to increase COVID-19 vaccination by empowering users to\ninfluence their social network. In a between-subjects comparative design study,\nwe test the effects of agent system interactivity and role-playing\nfunctionality on counseling outcomes, with participants evaluated by\nstandardized patients and objective judges. We find that all versions are\neffective at producing peer counselors who score adequately on a standardized\nmeasure of counseling competence, and that participants were significantly more\nsatisfied with interactive virtual agents compared to passive viewing of the\ntraining material. We discuss design implications for interpersonal skills\ntraining systems based on our findings.\n","authors":["Farnaz Nouraei","Keith Rebello","Mina Fallah","Prasanth Murali","Haley Matuszak","Valerie Jap","Andrea Parker","Michael Paasche-Orlow","Timothy Bickmore"],"pdf_url":"https://arxiv.org/pdf/2412.12061v1.pdf","comment":"Accepted at CSCW '24"},{"id":"http://arxiv.org/abs/2403.10799v4","updated":"2024-12-16T18:31:27Z","published":"2024-03-16T04:12:50Z","title":"Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment","summary":"  Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82\\% in\naccuracy across seven downstream tasks when pruning LLaMA-7B by 50\\%.\n","authors":["Jun Liu","Zhenglun Kong","Pu Zhao","Changdi Yang","Hao Tang","Xuan Shen","Geng Yuan","Wei Niu","Wenbin Zhang","Xue Lin","Dong Huang","Yanzhi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10799v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11848v2","updated":"2024-12-16T18:25:26Z","published":"2024-08-13T01:30:03Z","title":"MGH Radiology Llama: A Llama 3 70B Model for Radiology","summary":"  In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs.\n","authors":["Yucheng Shi","Peng Shu","Zhengliang Liu","Zihao Wu","Quanzheng Li","Tianming Liu","Ninghao Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2408.11848v2.pdf","comment":"11 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2411.12720v2","updated":"2024-12-16T18:24:53Z","published":"2024-11-19T18:38:01Z","title":"Scaling laws for nonlinear dynamical models of articulatory control","summary":"  Dynamical theories of speech use computational models of articulatory control\nto generate quantitative predictions and advance understanding of speech\ndynamics. The addition of a nonlinear restoring force to task dynamic models is\na significant improvement over linear models, but nonlinearity introduces\nchallenges with parameterization and interpretability. We illustrate these\nproblems through numerical simulations and introduce solutions in the form of\nscaling laws. We apply the scaling laws to a cubic model and show how they\nfacilitate interpretable simulations of articulatory dynamics, and can be\ntheoretically interpreted as imposing physical and cognitive constraints on\nmodels of speech movement dynamics.\n","authors":["Sam Kirkham"],"pdf_url":"https://arxiv.org/pdf/2411.12720v2.pdf","comment":"Updated title and minor changes to text after first round of reviews"},{"id":"http://arxiv.org/abs/2412.12040v1","updated":"2024-12-16T18:08:22Z","published":"2024-12-16T18:08:22Z","title":"How Private are Language Models in Abstractive Summarization?","summary":"  Language models (LMs) have shown outstanding performance in text\nsummarization including sensitive domains such as medicine and law. In these\nsettings, it is important that personally identifying information (PII)\nincluded in the source document should not leak in the summary. Prior efforts\nhave mostly focused on studying how LMs may inadvertently elicit PII from\ntraining data. However, to what extent LMs can provide privacy-preserving\nsummaries given a non-private source document remains under-explored. In this\npaper, we perform a comprehensive study across two closed- and three\nopen-weight LMs of different sizes and families. We experiment with prompting\nand fine-tuning strategies for privacy-preservation across a range of\nsummarization datasets across three domains. Our extensive quantitative and\nqualitative analysis including human evaluation shows that LMs often cannot\nprevent PII leakage on their summaries and that current widely-used metrics\ncannot capture context dependent privacy risks.\n","authors":["Anthony Hughes","Nikolaos Aletras","Ning Ma"],"pdf_url":"https://arxiv.org/pdf/2412.12040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12039v1","updated":"2024-12-16T18:08:14Z","published":"2024-12-16T18:08:14Z","title":"Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability\n  Detection","summary":"  Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on applied tasks such as vulnerability detection. We\ninvestigate various prompting strategies for vulnerability detection and, as\npart of this exploration, propose a prompting strategy that integrates natural\nlanguage descriptions of vulnerabilities with a contrastive chain-of-thought\nreasoning approach, augmented using contrastive samples from a synthetic\ndataset. Our study highlights the potential of LLMs to detect vulnerabilities\nby integrating natural language descriptions, contrastive reasoning, and\nsynthetic examples into a comprehensive prompting framework. Our results show\nthat this approach can enhance LLM understanding of vulnerabilities. On a\nhigh-quality vulnerability detection dataset such as SVEN, our prompting\nstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,\n11%, and 14%, respectively.\n","authors":["Ira Ceka","Feitong Qiao","Anik Dey","Aastha Valechia","Gail Kaiser","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2412.12039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00986v3","updated":"2024-12-16T18:00:10Z","published":"2024-03-01T21:16:29Z","title":"Merging Text Transformer Models from Different Initializations","summary":"  Recent work on permutation-based model merging has shown impressive low- or\nzero-barrier mode connectivity between models from completely different\ninitializations. However, this line of work has not yet extended to the\nTransformer architecture, despite its dominant popularity in the language\ndomain. Therefore, in this work, we investigate the extent to which separate\nTransformer minima learn similar features, and propose a model merging\ntechnique to investigate the relationship between these minima in the loss\nlandscape. The specifics of the architecture, like its residual connections,\nmulti-headed attention, and discrete, sequential input, require specific\ninterventions in order to compute model permutations that remain within the\nsame functional equivalence class. In merging these models with our method, we\nconsistently find lower loss barriers between minima compared to model\naveraging, across models trained on a masked-language modeling task or\nfine-tuned on a language understanding benchmark. Our results show that the\nminima of these models are less sharp and isolated than previously understood,\nand provide a basis for future work on merging separately trained Transformer\nmodels.\n","authors":["Neha Verma","Maha Elbayad"],"pdf_url":"https://arxiv.org/pdf/2403.00986v3.pdf","comment":"TMLR, November 2024"},{"id":"http://arxiv.org/abs/2403.05530v5","updated":"2024-12-16T17:39:39Z","published":"2024-03-08T18:54:20Z","title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context","summary":"  In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.\n","authors":[" Gemini Team","Petko Georgiev","Ving Ian Lei","Ryan Burnell","Libin Bai","Anmol Gulati","Garrett Tanzer","Damien Vincent","Zhufeng Pan","Shibo Wang","Soroosh Mariooryad","Yifan Ding","Xinyang Geng","Fred Alcober","Roy Frostig","Mark Omernick","Lexi Walker","Cosmin Paduraru","Christina Sorokin","Andrea Tacchetti","Colin Gaffney","Samira Daruki","Olcan Sercinoglu","Zach Gleicher","Juliette Love","Paul Voigtlaender","Rohan Jain","Gabriela Surita","Kareem Mohamed","Rory Blevins","Junwhan Ahn","Tao Zhu","Kornraphop Kawintiranon","Orhan Firat","Yiming Gu","Yujing Zhang","Matthew Rahtz","Manaal Faruqui","Natalie Clay","Justin Gilmer","JD Co-Reyes","Ivo Penchev","Rui Zhu","Nobuyuki Morioka","Kevin Hui","Krishna Haridasan","Victor Campos","Mahdis Mahdieh","Mandy Guo","Samer Hassan","Kevin Kilgour","Arpi Vezer","Heng-Tze Cheng","Raoul de Liedekerke","Siddharth Goyal","Paul Barham","DJ Strouse","Seb Noury","Jonas Adler","Mukund Sundararajan","Sharad Vikram","Dmitry Lepikhin","Michela Paganini","Xavier Garcia","Fan Yang","Dasha Valter","Maja Trebacz","Kiran Vodrahalli","Chulayuth Asawaroengchai","Roman Ring","Norbert Kalb","Livio Baldini Soares","Siddhartha Brahma","David Steiner","Tianhe Yu","Fabian Mentzer","Antoine He","Lucas Gonzalez","Bibo Xu","Raphael Lopez Kaufman","Laurent El Shafey","Junhyuk Oh","Tom Hennigan","George van den Driessche","Seth Odoom","Mario Lucic","Becca Roelofs","Sid Lall","Amit Marathe","Betty Chan","Santiago Ontanon","Luheng He","Denis Teplyashin","Jonathan Lai","Phil Crone","Bogdan Damoc","Lewis Ho","Sebastian Riedel","Karel Lenc","Chih-Kuan Yeh","Aakanksha Chowdhery","Yang Xu","Mehran Kazemi","Ehsan Amid","Anastasia Petrushkina","Kevin Swersky","Ali Khodaei","Gowoon Chen","Chris Larkin","Mario Pinto","Geng Yan","Adria Puigdomenech Badia","Piyush Patil","Steven Hansen","Dave Orr","Sebastien M. R. Arnold","Jordan Grimstad","Andrew Dai","Sholto Douglas","Rishika Sinha","Vikas Yadav","Xi Chen","Elena Gribovskaya","Jacob Austin","Jeffrey Zhao","Kaushal Patel","Paul Komarek","Sophia Austin","Sebastian Borgeaud","Linda Friso","Abhimanyu Goyal","Ben Caine","Kris Cao","Da-Woon Chung","Matthew Lamm","Gabe Barth-Maron","Thais Kagohara","Kate Olszewska","Mia Chen","Kaushik Shivakumar","Rishabh Agarwal","Harshal Godhia","Ravi Rajwar","Javier Snaider","Xerxes Dotiwalla","Yuan Liu","Aditya Barua","Victor Ungureanu","Yuan Zhang","Bat-Orgil Batsaikhan","Mateo Wirth","James Qin","Ivo Danihelka","Tulsee Doshi","Martin Chadwick","Jilin Chen","Sanil Jain","Quoc Le","Arjun Kar","Madhu Gurumurthy","Cheng Li","Ruoxin Sang","Fangyu Liu","Lampros Lamprou","Rich Munoz","Nathan Lintz","Harsh Mehta","Heidi Howard","Malcolm Reynolds","Lora Aroyo","Quan Wang","Lorenzo Blanco","Albin Cassirer","Jordan Griffith","Dipanjan Das","Stephan Lee","Jakub Sygnowski","Zach Fisher","James Besley","Richard Powell","Zafarali Ahmed","Dominik Paulus","David Reitter","Zalan Borsos","Rishabh Joshi","Aedan Pope","Steven Hand","Vittorio Selo","Vihan Jain","Nikhil Sethi","Megha Goel","Takaki Makino","Rhys May","Zhen Yang","Johan Schalkwyk","Christina Butterfield","Anja Hauth","Alex Goldin","Will Hawkins","Evan Senter","Sergey Brin","Oliver Woodman","Marvin Ritter","Eric Noland","Minh Giang","Vijay Bolina","Lisa Lee","Tim Blyth","Ian Mackinnon","Machel Reid","Obaid Sarvana","David Silver","Alexander Chen","Lily Wang","Loren Maggiore","Oscar Chang","Nithya Attaluri","Gregory Thornton","Chung-Cheng Chiu","Oskar Bunyan","Nir Levine","Timothy Chung","Evgenii Eltyshev","Xiance Si","Timothy Lillicrap","Demetra Brady","Vaibhav Aggarwal","Boxi Wu","Yuanzhong Xu","Ross McIlroy","Kartikeya Badola","Paramjit Sandhu","Erica Moreira","Wojciech Stokowiec","Ross Hemsley","Dong Li","Alex Tudor","Pranav Shyam","Elahe Rahimtoroghi","Salem Haykal","Pablo Sprechmann","Xiang Zhou","Diana Mincu","Yujia Li","Ravi Addanki","Kalpesh Krishna","Xiao Wu","Alexandre Frechette","Matan Eyal","Allan Dafoe","Dave Lacey","Jay Whang","Thi Avrahami","Ye Zhang","Emanuel Taropa","Hanzhao Lin","Daniel Toyama","Eliza Rutherford","Motoki Sano","HyunJeong Choe","Alex Tomala","Chalence Safranek-Shrader","Nora Kassner","Mantas Pajarskas","Matt Harvey","Sean Sechrist","Meire Fortunato","Christina Lyu","Gamaleldin Elsayed","Chenkai Kuang","James Lottes","Eric Chu","Chao Jia","Chih-Wei Chen","Peter Humphreys","Kate Baumli","Connie Tao","Rajkumar Samuel","Cicero Nogueira dos Santos","Anders Andreassen","Nemanja Rakićević","Dominik Grewe","Aviral Kumar","Stephanie Winkler","Jonathan Caton","Andrew Brock","Sid Dalmia","Hannah Sheahan","Iain Barr","Yingjie Miao","Paul Natsev","Jacob Devlin","Feryal Behbahani","Flavien Prost","Yanhua Sun","Artiom Myaskovsky","Thanumalayan Sankaranarayana Pillai","Dan Hurt","Angeliki Lazaridou","Xi Xiong","Ce Zheng","Fabio Pardo","Xiaowei Li","Dan Horgan","Joe Stanton","Moran Ambar","Fei Xia","Alejandro Lince","Mingqiu Wang","Basil Mustafa","Albert Webson","Hyo Lee","Rohan Anil","Martin Wicke","Timothy Dozat","Abhishek Sinha","Enrique Piqueras","Elahe Dabir","Shyam Upadhyay","Anudhyan Boral","Lisa Anne Hendricks","Corey Fry","Josip Djolonga","Yi Su","Jake Walker","Jane Labanowski","Ronny Huang","Vedant Misra","Jeremy Chen","RJ Skerry-Ryan","Avi Singh","Shruti Rijhwani","Dian Yu","Alex Castro-Ros","Beer Changpinyo","Romina Datta","Sumit Bagri","Arnar Mar Hrafnkelsson","Marcello Maggioni","Daniel Zheng","Yury Sulsky","Shaobo Hou","Tom Le Paine","Antoine Yang","Jason Riesa","Dominika Rogozinska","Dror Marcus","Dalia El Badawy","Qiao Zhang","Luyu Wang","Helen Miller","Jeremy Greer","Lars Lowe Sjos","Azade Nova","Heiga Zen","Rahma Chaabouni","Mihaela Rosca","Jiepu Jiang","Charlie Chen","Ruibo Liu","Tara Sainath","Maxim Krikun","Alex Polozov","Jean-Baptiste Lespiau","Josh Newlan","Zeyncep Cankara","Soo Kwak","Yunhan Xu","Phil Chen","Andy Coenen","Clemens Meyer","Katerina Tsihlas","Ada Ma","Juraj Gottweis","Jinwei Xing","Chenjie Gu","Jin Miao","Christian Frank","Zeynep Cankara","Sanjay Ganapathy","Ishita Dasgupta","Steph Hughes-Fitt","Heng Chen","David Reid","Keran Rong","Hongmin Fan","Joost van Amersfoort","Vincent Zhuang","Aaron Cohen","Shixiang Shane Gu","Anhad Mohananey","Anastasija Ilic","Taylor Tobin","John Wieting","Anna Bortsova","Phoebe Thacker","Emma Wang","Emily Caveness","Justin Chiu","Eren Sezener","Alex Kaskasoli","Steven Baker","Katie Millican","Mohamed Elhawaty","Kostas Aisopos","Carl Lebsack","Nathan Byrd","Hanjun Dai","Wenhao Jia","Matthew Wiethoff","Elnaz Davoodi","Albert Weston","Lakshman Yagati","Arun Ahuja","Isabel Gao","Golan Pundak","Susan Zhang","Michael Azzam","Khe Chai Sim","Sergi Caelles","James Keeling","Abhanshu Sharma","Andy Swing","YaGuang Li","Chenxi Liu","Carrie Grimes Bostock","Yamini Bansal","Zachary Nado","Ankesh Anand","Josh Lipschultz","Abhijit Karmarkar","Lev Proleev","Abe Ittycheriah","Soheil Hassas Yeganeh","George Polovets","Aleksandra Faust","Jiao Sun","Alban Rrustemi","Pen Li","Rakesh Shivanna","Jeremiah Liu","Chris Welty","Federico Lebron","Anirudh Baddepudi","Sebastian Krause","Emilio Parisotto","Radu Soricut","Zheng Xu","Dawn Bloxwich","Melvin Johnson","Behnam Neyshabur","Justin Mao-Jones","Renshen Wang","Vinay Ramasesh","Zaheer Abbas","Arthur Guez","Constant Segal","Duc Dung Nguyen","James Svensson","Le Hou","Sarah York","Kieran Milan","Sophie Bridgers","Wiktor Gworek","Marco Tagliasacchi","James Lee-Thorp","Michael Chang","Alexey Guseynov","Ale Jakse Hartman","Michael Kwong","Ruizhe Zhao","Sheleem Kashem","Elizabeth Cole","Antoine Miech","Richard Tanburn","Mary Phuong","Filip Pavetic","Sebastien Cevey","Ramona Comanescu","Richard Ives","Sherry Yang","Cosmo Du","Bo Li","Zizhao Zhang","Mariko Iinuma","Clara Huiyi Hu","Aurko Roy","Shaan Bijwadia","Zhenkai Zhu","Danilo Martins","Rachel Saputro","Anita Gergely","Steven Zheng","Dawei Jia","Ioannis Antonoglou","Adam Sadovsky","Shane Gu","Yingying Bi","Alek Andreev","Sina Samangooei","Mina Khan","Tomas Kocisky","Angelos Filos","Chintu Kumar","Colton Bishop","Adams Yu","Sarah Hodkinson","Sid Mittal","Premal Shah","Alexandre Moufarek","Yong Cheng","Adam Bloniarz","Jaehoon Lee","Pedram Pejman","Paul Michel","Stephen Spencer","Vladimir Feinberg","Xuehan Xiong","Nikolay Savinov","Charlotte Smith","Siamak Shakeri","Dustin Tran","Mary Chesus","Bernd Bohnet","George Tucker","Tamara von Glehn","Carrie Muir","Yiran Mao","Hideto Kazawa","Ambrose Slone","Kedar Soparkar","Disha Shrivastava","James Cobon-Kerr","Michael Sharman","Jay Pavagadhi","Carlos Araya","Karolis Misiunas","Nimesh Ghelani","Michael Laskin","David Barker","Qiujia Li","Anton Briukhov","Neil Houlsby","Mia Glaese","Balaji Lakshminarayanan","Nathan Schucher","Yunhao Tang","Eli Collins","Hyeontaek Lim","Fangxiaoyu Feng","Adria Recasens","Guangda Lai","Alberto Magni","Nicola De Cao","Aditya Siddhant","Zoe Ashwood","Jordi Orbay","Mostafa Dehghani","Jenny Brennan","Yifan He","Kelvin Xu","Yang Gao","Carl Saroufim","James Molloy","Xinyi Wu","Seb Arnold","Solomon Chang","Julian Schrittwieser","Elena Buchatskaya","Soroush Radpour","Martin Polacek","Skye Giordano","Ankur Bapna","Simon Tokumine","Vincent Hellendoorn","Thibault Sottiaux","Sarah Cogan","Aliaksei Severyn","Mohammad Saleh","Shantanu Thakoor","Laurent Shefey","Siyuan Qiao","Meenu Gaba","Shuo-yiin Chang","Craig Swanson","Biao Zhang","Benjamin Lee","Paul Kishan Rubenstein","Gan Song","Tom Kwiatkowski","Anna Koop","Ajay Kannan","David Kao","Parker Schuh","Axel Stjerngren","Golnaz Ghiasi","Gena Gibson","Luke Vilnis","Ye Yuan","Felipe Tiengo Ferreira","Aishwarya Kamath","Ted Klimenko","Ken Franko","Kefan Xiao","Indro Bhattacharya","Miteyan Patel","Rui Wang","Alex Morris","Robin Strudel","Vivek Sharma","Peter Choy","Sayed Hadi Hashemi","Jessica Landon","Mara Finkelstein","Priya Jhakra","Justin Frye","Megan Barnes","Matthew Mauger","Dennis Daun","Khuslen Baatarsukh","Matthew Tung","Wael Farhan","Henryk Michalewski","Fabio Viola","Felix de Chaumont Quitry","Charline Le Lan","Tom Hudson","Qingze Wang","Felix Fischer","Ivy Zheng","Elspeth White","Anca Dragan","Jean-baptiste Alayrac","Eric Ni","Alexander Pritzel","Adam Iwanicki","Michael Isard","Anna Bulanova","Lukas Zilka","Ethan Dyer","Devendra Sachan","Srivatsan Srinivasan","Hannah Muckenhirn","Honglong Cai","Amol Mandhane","Mukarram Tariq","Jack W. Rae","Gary Wang","Kareem Ayoub","Nicholas FitzGerald","Yao Zhao","Woohyun Han","Chris Alberti","Dan Garrette","Kashyap Krishnakumar","Mai Gimenez","Anselm Levskaya","Daniel Sohn","Josip Matak","Inaki Iturrate","Michael B. Chang","Jackie Xiang","Yuan Cao","Nishant Ranka","Geoff Brown","Adrian Hutter","Vahab Mirrokni","Nanxin Chen","Kaisheng Yao","Zoltan Egyed","Francois Galilee","Tyler Liechty","Praveen Kallakuri","Evan Palmer","Sanjay Ghemawat","Jasmine Liu","David Tao","Chloe Thornton","Tim Green","Mimi Jasarevic","Sharon Lin","Victor Cotruta","Yi-Xuan Tan","Noah Fiedel","Hongkun Yu","Ed Chi","Alexander Neitz","Jens Heitkaemper","Anu Sinha","Denny Zhou","Yi Sun","Charbel Kaed","Brice Hulse","Swaroop Mishra","Maria Georgaki","Sneha Kudugunta","Clement Farabet","Izhak Shafran","Daniel Vlasic","Anton Tsitsulin","Rajagopal Ananthanarayanan","Alen Carin","Guolong Su","Pei Sun","Shashank V","Gabriel Carvajal","Josef Broder","Iulia Comsa","Alena Repina","William Wong","Warren Weilun Chen","Peter Hawkins","Egor Filonov","Lucia Loher","Christoph Hirnschall","Weiyi Wang","Jingchen Ye","Andrea Burns","Hardie Cate","Diana Gage Wright","Federico Piccinini","Lei Zhang","Chu-Cheng Lin","Ionel Gog","Yana Kulizhskaya","Ashwin Sreevatsa","Shuang Song","Luis C. Cobo","Anand Iyer","Chetan Tekur","Guillermo Garrido","Zhuyun Xiao","Rupert Kemp","Huaixiu Steven Zheng","Hui Li","Ananth Agarwal","Christel Ngani","Kati Goshvadi","Rebeca Santamaria-Fernandez","Wojciech Fica","Xinyun Chen","Chris Gorgolewski","Sean Sun","Roopal Garg","Xinyu Ye","S. M. Ali Eslami","Nan Hua","Jon Simon","Pratik Joshi","Yelin Kim","Ian Tenney","Sahitya Potluri","Lam Nguyen Thiet","Quan Yuan","Florian Luisier","Alexandra Chronopoulou","Salvatore Scellato","Praveen Srinivasan","Minmin Chen","Vinod Koverkathu","Valentin Dalibard","Yaming Xu","Brennan Saeta","Keith Anderson","Thibault Sellam","Nick Fernando","Fantine Huot","Junehyuk Jung","Mani Varadarajan","Michael Quinn","Amit Raul","Maigo Le","Ruslan Habalov","Jon Clark","Komal Jalan","Kalesha Bullard","Achintya Singhal","Thang Luong","Boyu Wang","Sujeevan Rajayogam","Julian Eisenschlos","Johnson Jia","Daniel Finchelstein","Alex Yakubovich","Daniel Balle","Michael Fink","Sameer Agarwal","Jing Li","Dj Dvijotham","Shalini Pal","Kai Kang","Jaclyn Konzelmann","Jennifer Beattie","Olivier Dousse","Diane Wu","Remi Crocker","Chen Elkind","Siddhartha Reddy Jonnalagadda","Jong Lee","Dan Holtmann-Rice","Krystal Kallarackal","Rosanne Liu","Denis Vnukov","Neera Vats","Luca Invernizzi","Mohsen Jafari","Huanjie Zhou","Lilly Taylor","Jennifer Prendki","Marcus Wu","Tom Eccles","Tianqi Liu","Kavya Kopparapu","Francoise Beaufays","Christof Angermueller","Andreea Marzoca","Shourya Sarcar","Hilal Dib","Jeff Stanway","Frank Perbet","Nejc Trdin","Rachel Sterneck","Andrey Khorlin","Dinghua Li","Xihui Wu","Sonam Goenka","David Madras","Sasha Goldshtein","Willi Gierke","Tong Zhou","Yaxin Liu","Yannie Liang","Anais White","Yunjie Li","Shreya Singh","Sanaz Bahargam","Mark Epstein","Sujoy Basu","Li Lao","Adnan Ozturel","Carl Crous","Alex Zhai","Han Lu","Zora Tung","Neeraj Gaur","Alanna Walton","Lucas Dixon","Ming Zhang","Amir Globerson","Grant Uy","Andrew Bolt","Olivia Wiles","Milad Nasr","Ilia Shumailov","Marco Selvi","Francesco Piccinno","Ricardo Aguilar","Sara McCarthy","Misha Khalman","Mrinal Shukla","Vlado Galic","John Carpenter","Kevin Villela","Haibin Zhang","Harry Richardson","James Martens","Matko Bosnjak","Shreyas Rammohan Belle","Jeff Seibert","Mahmoud Alnahlawi","Brian McWilliams","Sankalp Singh","Annie Louis","Wen Ding","Dan Popovici","Lenin Simicich","Laura Knight","Pulkit Mehta","Nishesh Gupta","Chongyang Shi","Saaber Fatehi","Jovana Mitrovic","Alex Grills","Joseph Pagadora","Tsendsuren Munkhdalai","Dessie Petrova","Danielle Eisenbud","Zhishuai Zhang","Damion Yates","Bhavishya Mittal","Nilesh Tripuraneni","Yannis Assael","Thomas Brovelli","Prateek Jain","Mihajlo Velimirovic","Canfer Akbulut","Jiaqi Mu","Wolfgang Macherey","Ravin Kumar","Jun Xu","Haroon Qureshi","Gheorghe Comanici","Jeremy Wiesner","Zhitao Gong","Anton Ruddock","Matthias Bauer","Nick Felt","Anirudh GP","Anurag Arnab","Dustin Zelle","Jonas Rothfuss","Bill Rosgen","Ashish Shenoy","Bryan Seybold","Xinjian Li","Jayaram Mudigonda","Goker Erdogan","Jiawei Xia","Jiri Simsa","Andrea Michi","Yi Yao","Christopher Yew","Steven Kan","Isaac Caswell","Carey Radebaugh","Andre Elisseeff","Pedro Valenzuela","Kay McKinney","Kim Paterson","Albert Cui","Eri Latorre-Chimoto","Solomon Kim","William Zeng","Ken Durden","Priya Ponnapalli","Tiberiu Sosea","Christopher A. Choquette-Choo","James Manyika","Brona Robenek","Harsha Vashisht","Sebastien Pereira","Hoi Lam","Marko Velic","Denese Owusu-Afriyie","Katherine Lee","Tolga Bolukbasi","Alicia Parrish","Shawn Lu","Jane Park","Balaji Venkatraman","Alice Talbert","Lambert Rosique","Yuchung Cheng","Andrei Sozanschi","Adam Paszke","Praveen Kumar","Jessica Austin","Lu Li","Khalid Salama","Bartek Perz","Wooyeol Kim","Nandita Dukkipati","Anthony Baryshnikov","Christos Kaplanis","XiangHai Sheng","Yuri Chervonyi","Caglar Unlu","Diego de Las Casas","Harry Askham","Kathryn Tunyasuvunakool","Felix Gimeno","Siim Poder","Chester Kwak","Matt Miecnikowski","Vahab Mirrokni","Alek Dimitriev","Aaron Parisi","Dangyi Liu","Tomy Tsai","Toby Shevlane","Christina Kouridi","Drew Garmon","Adrian Goedeckemeyer","Adam R. Brown","Anitha Vijayakumar","Ali Elqursh","Sadegh Jazayeri","Jin Huang","Sara Mc Carthy","Jay Hoover","Lucy Kim","Sandeep Kumar","Wei Chen","Courtney Biles","Garrett Bingham","Evan Rosen","Lisa Wang","Qijun Tan","David Engel","Francesco Pongetti","Dario de Cesare","Dongseong Hwang","Lily Yu","Jennifer Pullman","Srini Narayanan","Kyle Levin","Siddharth Gopal","Megan Li","Asaf Aharoni","Trieu Trinh","Jessica Lo","Norman Casagrande","Roopali Vij","Loic Matthey","Bramandia Ramadhana","Austin Matthews","CJ Carey","Matthew Johnson","Kremena Goranova","Rohin Shah","Shereen Ashraf","Kingshuk Dasgupta","Rasmus Larsen","Yicheng Wang","Manish Reddy Vuyyuru","Chong Jiang","Joana Ijazi","Kazuki Osawa","Celine Smith","Ramya Sree Boppana","Taylan Bilal","Yuma Koizumi","Ying Xu","Yasemin Altun","Nir Shabat","Ben Bariach","Alex Korchemniy","Kiam Choo","Olaf Ronneberger","Chimezie Iwuanyanwu","Shubin Zhao","David Soergel","Cho-Jui Hsieh","Irene Cai","Shariq Iqbal","Martin Sundermeyer","Zhe Chen","Elie Bursztein","Chaitanya Malaviya","Fadi Biadsy","Prakash Shroff","Inderjit Dhillon","Tejasi Latkar","Chris Dyer","Hannah Forbes","Massimo Nicosia","Vitaly Nikolaev","Somer Greene","Marin Georgiev","Pidong Wang","Nina Martin","Hanie Sedghi","John Zhang","Praseem Banzal","Doug Fritz","Vikram Rao","Xuezhi Wang","Jiageng Zhang","Viorica Patraucean","Dayou Du","Igor Mordatch","Ivan Jurin","Lewis Liu","Ayush Dubey","Abhi Mohan","Janek Nowakowski","Vlad-Doru Ion","Nan Wei","Reiko Tojo","Maria Abi Raad","Drew A. Hudson","Vaishakh Keshava","Shubham Agrawal","Kevin Ramirez","Zhichun Wu","Hoang Nguyen","Ji Liu","Madhavi Sewak","Bryce Petrini","DongHyun Choi","Ivan Philips","Ziyue Wang","Ioana Bica","Ankush Garg","Jarek Wilkiewicz","Priyanka Agrawal","Xiaowei Li","Danhao Guo","Emily Xue","Naseer Shaik","Andrew Leach","Sadh MNM Khan","Julia Wiesinger","Sammy Jerome","Abhishek Chakladar","Alek Wenjiao Wang","Tina Ornduff","Folake Abu","Alireza Ghaffarkhah","Marcus Wainwright","Mario Cortes","Frederick Liu","Joshua Maynez","Andreas Terzis","Pouya Samangouei","Riham Mansour","Tomasz Kępa","François-Xavier Aubet","Anton Algymr","Dan Banica","Agoston Weisz","Andras Orban","Alexandre Senges","Ewa Andrejczuk","Mark Geller","Niccolo Dal Santo","Valentin Anklin","Majd Al Merey","Martin Baeuml","Trevor Strohman","Junwen Bai","Slav Petrov","Yonghui Wu","Demis Hassabis","Koray Kavukcuoglu","Jeff Dean","Oriol Vinyals"],"pdf_url":"https://arxiv.org/pdf/2403.05530v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12009v1","updated":"2024-12-16T17:36:02Z","published":"2024-12-16T17:36:02Z","title":"SpeechPrune: Context-aware Token Pruning for Speech Information\n  Retrieval","summary":"  We introduce Speech Information Retrieval (SIR), a new long-context task for\nSpeech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-sample\nbenchmark testing models' ability to extract critical details from\napproximately 90-second spoken inputs. While current Speech LLMs excel at\nshort-form tasks, they struggle with the computational and representational\ndemands of longer audio sequences. To address this limitation, we propose\nSpeechPrune, a training-free token pruning strategy that uses speech-text\nsimilarity and approximated attention scores to efficiently discard irrelevant\ntokens. In SPIRAL, SpeechPrune achieves accuracy improvements of 29% and up to\n47% over the original model and the random pruning model at a pruning rate of\n20%, respectively. SpeechPrune can maintain network performance even at a\npruning level of 80%. This approach highlights the potential of token-level\npruning for efficient and scalable long-form speech understanding.\n","authors":["Yueqian Lin","Yuzhe Fu","Jingyang Zhang","Yudong Liu","Jianyi Zhang","Jingwei Sun","Hai \"Helen\" Li","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12009v1.pdf","comment":"Project page and dataset is available at\n  https://speechprune.github.io/"},{"id":"http://arxiv.org/abs/2404.14397v2","updated":"2024-12-16T17:34:22Z","published":"2024-04-22T17:56:26Z","title":"RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?","summary":"  Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment.\n","authors":["Adrian de Wynter","Ishaan Watts","Tua Wongsangaroonsri","Minghui Zhang","Noura Farra","Nektar Ege Altıntoprak","Lena Baur","Samantha Claudet","Pavel Gajdusek","Can Gören","Qilong Gu","Anna Kaminska","Tomasz Kaminski","Ruby Kuo","Akiko Kyuba","Jongho Lee","Kartik Mathur","Petter Merok","Ivana Milovanović","Nani Paananen","Vesa-Matti Paananen","Anna Pavlenko","Bruno Pereira Vidal","Luciano Strika","Yueh Tsao","Davide Turcato","Oleksandr Vakhno","Judit Velcsov","Anna Vickers","Stéphanie Visser","Herdyan Widarmanto","Andrey Zaikin","Si-Qing Chen"],"pdf_url":"https://arxiv.org/pdf/2404.14397v2.pdf","comment":"AAAI 2025--camera ready + extended abstract"},{"id":"http://arxiv.org/abs/2412.12004v1","updated":"2024-12-16T17:32:11Z","published":"2024-12-16T17:32:11Z","title":"The Open Source Advantage in Large Language Models (LLMs)","summary":"  Large language models (LLMs) mark a key shift in natural language processing\n(NLP), having advanced text generation, translation, and domain-specific\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\nextensive computational resources, lead with state-of-the-art performance\ntoday. However, they face criticism for their \"black box\" nature and for\nlimiting accessibility in a manner that hinders reproducibility and equitable\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\nprioritize democratization through community-driven development and\ncomputational efficiency. These models have significantly reduced performance\ngaps, particularly in linguistic diversity and domain-specific applications,\nwhile providing accessible tools for global researchers and developers.\nNotably, both paradigms rely on foundational architectural innovations, such as\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\nby scaling effectively, while open-source models adapt to real-world\napplications in underrepresented languages and domains. Techniques like\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\nmodels to achieve competitive results despite limited resources. To be sure,\nthe tension between closed-source and open-source approaches underscores a\nbroader debate on transparency versus proprietary control in AI. Ethical\nconsiderations further highlight this divide. Closed-source systems restrict\nexternal scrutiny, while open-source models promote reproducibility and\ncollaboration but lack standardized auditing documentation frameworks to\nmitigate biases. Hybrid approaches that leverage the strengths of both\nparadigms are likely to shape the future of LLM innovation, ensuring\naccessibility, competitive technical performance, and ethical deployment.\n","authors":["Jiya Manchanda","Laura Boettcher","Matheus Westphalen","Jasser Jasser"],"pdf_url":"https://arxiv.org/pdf/2412.12004v1.pdf","comment":"7 pages, 0 figures"},{"id":"http://arxiv.org/abs/2412.12001v1","updated":"2024-12-16T17:29:51Z","published":"2024-12-16T17:29:51Z","title":"LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts","summary":"  Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem.\n","authors":["Zhuhao Wang","Yihua Sun","Zihan Li","Xuan Yang","Fang Chen","Hongen Liao"],"pdf_url":"https://arxiv.org/pdf/2412.12001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11990v1","updated":"2024-12-16T17:14:35Z","published":"2024-12-16T17:14:35Z","title":"ExecRepoBench: Multi-level Executable Code Completion Evaluation","summary":"  Code completion has become an essential tool for daily software development.\nExisting evaluation benchmarks often employ static methods that do not fully\ncapture the dynamic nature of real-world coding environments and face\nsignificant challenges, including limited context length, reliance on\nsuperficial evaluation metrics, and potential overfitting to training datasets.\nIn this work, we introduce a novel framework for enhancing code completion in\nsoftware development through the creation of a repository-level benchmark\nExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the\nfunctionality of open-source large language models (LLMs) in real-world coding\nscenarios that involve complex interdependencies across multiple files.\nExecRepoBench includes 1.2K samples from active Python repositories. Plus, we\npresent a multi-level grammar-based completion methodology conditioned on the\nabstract syntax tree to mask code fragments at various logical units (e.g.\nstatements, expressions, and functions). Then, we fine-tune the open-source LLM\nwith 7B parameters on Repo-Instruct to produce a strong code completion\nbaseline model Qwen2.5-Coder-Instruct-C based on the open-source model.\nQwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks,\nincluding MultiPL-E and ExecRepoBench, which consistently outperforms prior\nbaselines across all programming languages. The deployment of \\ourmethod{} can\nbe used as a high-performance, local service for programming\ndevelopment\\footnote{\\url{https://execrepobench.github.io/}}.\n","authors":["Jian Yang","Jiajun Zhang","Jiaxi Yang","Ke Jin","Lei Zhang","Qiyao Peng","Ken Deng","Yibo Miao","Tianyu Liu","Zeyu Cui","Binyuan Hui","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2412.11990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11988v1","updated":"2024-12-16T17:11:48Z","published":"2024-12-16T17:11:48Z","title":"SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with\n  a GAN-Inspired Approach to Synthetic Dataset Generation","summary":"  Consider the problem: ``If one man and one woman can produce one child in one\nyear, how many children will be produced by one woman and three men in 0.5\nyears?\" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview,\nand Gemini Flash frequently answer \"0.5,\" which does not make sense. While\nthese models sometimes acknowledge the unrealistic nature of the question, in\nmany cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5\nchild.\" Additionally, temporal variation has been observed: if an LLM answers\ncorrectly once (by recognizing the faulty nature of the question), subsequent\nresponses are more likely to also reflect this understanding. However, this is\ninconsistent.\n  These types of questions have motivated us to develop a dataset of science\nquestions, SciFaultyQA, where the questions themselves are intentionally\nfaulty. We observed that LLMs often proceed to answer these flawed questions\nwithout recognizing their inherent issues, producing results that are logically\nor scientifically invalid. By analyzing such patterns, we developed a novel\nmethod for generating synthetic datasets to evaluate and benchmark the\nperformance of various LLMs in identifying these flawed questions. We have also\ndeveloped novel approaches to reduce the errors.\n","authors":["Debarshi Kundu"],"pdf_url":"https://arxiv.org/pdf/2412.11988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02920v2","updated":"2024-12-16T17:09:58Z","published":"2024-09-04T17:59:52Z","title":"RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early\n  version)","summary":"  In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples improve the success rate of over 70% for single-arm tasks and over 40%\nfor dual-arm tasks compared to models trained solely on real-world data. This\nsignificant improvement demonstrates RoboTwin's potential to enhance the\ndevelopment and evaluation of dual-arm robotic manipulation systems. Project\nPage: https://robotwin-benchmark.github.io/early-version/.\n","authors":["Yao Mu","Tianxing Chen","Shijia Peng","Zanxin Chen","Zeyu Gao","Yude Zou","Lunkai Lin","Zhiqiang Xie","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2409.02920v2.pdf","comment":"Project page: https://robotwin-benchmark.github.io/early-version/"},{"id":"http://arxiv.org/abs/2412.11986v1","updated":"2024-12-16T17:07:26Z","published":"2024-12-16T17:07:26Z","title":"Speak & Improve Corpus 2025: an L2 English Speech Corpus for Language\n  Assessment and Feedback","summary":"  We introduce the Speak \\& Improve Corpus 2025, a dataset of L2 learner\nEnglish data with holistic scores and language error annotation, collected from\nopen (spontaneous) speaking tests on the Speak \\& Improve learning platform\nhttps://speakandimprove.com . The aim of the corpus release is to address a\nmajor challenge to developing L2 spoken language processing systems, the lack\nof publicly available data with high-quality annotations. It is being made\navailable for non-commercial use on the ELiT website. In designing this corpus\nwe have sought to make it cover a wide-range of speaker attributes, from their\nL1 to their speaking ability, as well as providing manual annotations. This\nenables a range of language-learning tasks to be examined, such as assessing\nspeaking proficiency or providing feedback on grammatical errors in a learner's\nspeech. Additionally, the data supports research into the underlying technology\nrequired for these tasks including automatic speech recognition (ASR) of low\nresource L2 learner English, disfluency detection or spoken grammatical error\ncorrection (GEC). The corpus consists of around 340 hours of L2 English\nlearners audio with holistic scores, and a subset of audio annotated with\ntranscriptions and error labels.\n","authors":["Kate Knill","Diane Nicholls","Mark J. F. Gales","Mengjie Qian","Pawel Stroinski"],"pdf_url":"https://arxiv.org/pdf/2412.11986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11985v1","updated":"2024-12-16T17:05:18Z","published":"2024-12-16T17:05:18Z","title":"Speak & Improve Challenge 2025: Tasks and Baseline Systems","summary":"  This paper presents the \"Speak & Improve Challenge 2025: Spoken Language\nAssessment and Feedback\" -- a challenge associated with the ISCA SLaTE 2025\nWorkshop. The goal of the challenge is to advance research on spoken language\nassessment and feedback, with tasks associated with both the underlying\ntechnology and language learning feedback. Linked with the challenge, the Speak\n& Improve (S&I) Corpus 2025 is being pre-released, a dataset of L2 learner\nEnglish data with holistic scores and language error annotation, collected from\nopen (spontaneous) speaking tests on the Speak & Improve learning platform. The\ncorpus consists of 340 hours of audio data from second language English\nlearners with holistic scores, and a 60-hour subset with manual transcriptions\nand error labels. The Challenge has four shared tasks: Automatic Speech\nRecognition (ASR), Spoken Language Assessment (SLA), Spoken Grammatical Error\nCorrection (SGEC), and Spoken Grammatical Error Correction Feedback (SGECF).\nEach of these tasks has a closed track where a predetermined set of models and\ndata sources are allowed to be used, and an open track where any public\nresource may be used. Challenge participants may do one or more of the tasks.\nThis paper describes the challenge, the S&I Corpus 2025, and the baseline\nsystems released for the Challenge.\n","authors":["Mengjie Qian","Kate Knill","Stefano Banno","Siyuan Tang","Penny Karanasou","Mark J. F. Gales","Diane Nicholls"],"pdf_url":"https://arxiv.org/pdf/2412.11985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11978v1","updated":"2024-12-16T16:59:22Z","published":"2024-12-16T16:59:22Z","title":"Speech Foundation Models and Crowdsourcing for Efficient, High-Quality\n  Data Collection","summary":"  While crowdsourcing is an established solution for facilitating and scaling\nthe collection of speech data, the involvement of non-experts necessitates\nprotocols to ensure final data quality. To reduce the costs of these essential\ncontrols, this paper investigates the use of Speech Foundation Models (SFMs) to\nautomate the validation process, examining for the first time the cost/quality\ntrade-off in data acquisition. Experiments conducted on French, German, and\nKorean data demonstrate that SFM-based validation has the potential to reduce\nreliance on human validation, resulting in an estimated cost saving of over\n40.0% without degrading final data quality. These findings open new\nopportunities for more efficient, cost-effective, and scalable speech data\nacquisition.\n","authors":["Beomseok Lee","Marco Gaido","Ioan Calapodescu","Laurent Besacier","Matteo Negri"],"pdf_url":"https://arxiv.org/pdf/2412.11978v1.pdf","comment":"Accepted at COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2412.11974v1","updated":"2024-12-16T16:58:28Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v1.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.11970v1","updated":"2024-12-16T16:51:27Z","published":"2024-12-16T16:51:27Z","title":"DARWIN 1.5: Large Language Models as Materials Science Adapted Learners","summary":"  Materials discovery and design aim to find components and structures with\ndesirable properties over highly complex and diverse search spaces. Traditional\nsolutions, such as high-throughput simulations and machine learning (ML), often\nrely on complex descriptors, which hinder generalizability and transferability\nacross tasks. Moreover, these descriptors may deviate from experimental data\ndue to inevitable defects and purity issues in the real world, which may reduce\ntheir effectiveness in practical applications. To address these challenges, we\npropose Darwin 1.5, an open-source large language model (LLM) tailored for\nmaterials science. By leveraging natural language as input, Darwin eliminates\nthe need for task-specific descriptors and enables a flexible, unified approach\nto material property prediction and discovery. We employ a two-stage training\nstrategy combining question-answering (QA) fine-tuning with multi-task learning\n(MTL) to inject domain-specific knowledge in various modalities and facilitate\ncross-task knowledge transfer. Through our strategic approach, we achieved a\nsignificant enhancement in the prediction accuracy of LLMs, with a maximum\nimprovement of 60\\% compared to LLaMA-7B base models. It further outperforms\ntraditional machine learning models on various tasks in material science,\nshowcasing the potential of LLMs to provide a more versatile and scalable\nfoundation model for materials discovery and design.\n","authors":["Tong Xie","Yuwei Wan","Yixuan Liu","Yuchen Zeng","Wenjie Zhang","Chunyu Kit","Dongzhan Zhou","Bram Hoex"],"pdf_url":"https://arxiv.org/pdf/2412.11970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11965v1","updated":"2024-12-16T16:45:33Z","published":"2024-12-16T16:45:33Z","title":"Inferring Functionality of Attention Heads from their Parameters","summary":"  Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations.\n","authors":["Amit Elhelo","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2412.11965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11940v1","updated":"2024-12-16T16:24:58Z","published":"2024-12-16T16:24:58Z","title":"The Impact of Token Granularity on the Predictive Power of Language\n  Model Surprisal","summary":"  Word-by-word language model surprisal is often used to model the incremental\nprocessing of human readers, which raises questions about how various choices\nin language modeling influence its predictive power. One factor that has been\noverlooked in cognitive modeling is the granularity of subword tokens, which\nexplicitly encodes information about word length and frequency, and ultimately\ninfluences the quality of vector representations that are learned. This paper\npresents experiments that manipulate the token granularity and evaluate its\nimpact on the ability of surprisal to account for processing difficulty of\nnaturalistic text and garden-path constructions. Experiments with naturalistic\nreading times reveal a substantial influence of token granularity on surprisal,\nwith tokens defined by a vocabulary size of 8,000 resulting in surprisal that\nis most predictive. In contrast, on garden-path constructions, language models\ntrained on coarser-grained tokens generally assigned higher surprisal to\ncritical regions, suggesting their increased sensitivity to syntax. Taken\ntogether, these results suggest a large role of token granularity on the\nquality of language model surprisal for cognitive modeling.\n","authors":["Byung-Doh Oh","William Schuler"],"pdf_url":"https://arxiv.org/pdf/2412.11940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11939v1","updated":"2024-12-16T16:24:36Z","published":"2024-12-16T16:24:36Z","title":"SEAGraph: Unveiling the Whole Story of Paper Review Comments","summary":"  Peer review, as a cornerstone of scientific research, ensures the integrity\nand quality of scholarly work by providing authors with objective feedback for\nrefinement. However, in the traditional peer review process, authors often\nreceive vague or insufficiently detailed feedback, which provides limited\nassistance and leads to a more time-consuming review cycle. If authors can\nidentify some specific weaknesses in their paper, they can not only address the\nreviewer's concerns but also improve their work. This raises the critical\nquestion of how to enhance authors' comprehension of review comments. In this\npaper, we present SEAGraph, a novel framework developed to clarify review\ncomments by uncovering the underlying intentions behind them. We construct two\ntypes of graphs for each paper: the semantic mind graph, which captures the\nauthor's thought process, and the hierarchical background graph, which\ndelineates the research domains related to the paper. A retrieval method is\nthen designed to extract relevant content from both graphs, facilitating\ncoherent explanations for the review comments. Extensive experiments show that\nSEAGraph excels in review comment understanding tasks, offering significant\nbenefits to authors.\n","authors":["Jianxiang Yu","Jiaqi Tan","Zichen Ding","Jiapeng Zhu","Jiahao Li","Yao Cheng","Qier Cui","Yunshi Lan","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2412.11939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11937v1","updated":"2024-12-16T16:22:27Z","published":"2024-12-16T16:22:27Z","title":"Precise Length Control in Large Language Models","summary":"  Large Language Models (LLMs) are increasingly used in production systems,\npowering applications such as chatbots, summarization, and question answering.\nDespite their success, controlling the length of their response remains a\nsignificant challenge, particularly for tasks requiring structured outputs or\nspecific levels of detail. In this work, we propose a method to adapt\npre-trained decoder-only LLMs for precise control of response length. Our\napproach incorporates a secondary length-difference positional encoding (LDPE)\ninto the input embeddings, which counts down to a user-set response termination\nlength. Fine-tuning with LDPE allows the model to learn to terminate responses\ncoherently at the desired length, achieving mean token errors of less than 3\ntokens. We also introduce Max New Tokens++, an extension that enables flexible\nupper-bound length control, rather than an exact target. Experimental results\non tasks such as question answering and document summarization demonstrate that\nour method enables precise length control without compromising response\nquality.\n","authors":["Bradley Butcher","Michael O'Keefe","James Titchener"],"pdf_url":"https://arxiv.org/pdf/2412.11937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11936v1","updated":"2024-12-16T16:21:41Z","published":"2024-12-16T16:21:41Z","title":"A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges","summary":"  Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks.\n","authors":["Yibo Yan","Jiamin Su","Jianxiang He","Fangteng Fu","Xu Zheng","Yuanhuiyi Lyu","Kun Wang","Shen Wang","Qingsong Wen","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2412.11936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16658v2","updated":"2024-12-16T16:21:00Z","published":"2024-10-22T03:19:16Z","title":"Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent","summary":"  Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions.\n","authors":["Janghoon Ock","Tirtha Vinchurkar","Yayati Jadhav","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2410.16658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20246v2","updated":"2024-12-16T16:15:11Z","published":"2024-09-30T12:36:25Z","title":"Analysing Zero-Shot Readability-Controlled Sentence Simplification","summary":"  Readability-controlled text simplification (RCTS) rewrites texts to lower\nreadability levels while preserving their meaning. RCTS models often depend on\nparallel corpora with readability annotations on both source and target sides.\nSuch datasets are scarce and difficult to curate, especially at the sentence\nlevel. To reduce reliance on parallel data, we explore using instruction-tuned\nlarge language models for zero-shot RCTS. Through automatic and manual\nevaluations, we examine: (1) how different types of contextual information\naffect a model's ability to generate sentences with the desired readability,\nand (2) the trade-off between achieving target readability and preserving\nmeaning. Results show that all tested models struggle to simplify sentences\n(especially to the lowest levels) due to models' limitations and\ncharacteristics of the source sentences that impede adequate rewriting. Our\nexperiments also highlight the need for better automatic evaluation metrics\ntailored to RCTS, as standard ones often misinterpret common simplification\noperations, and inaccurately assess readability and meaning preservation.\n","authors":["Abdullah Barayan","Jose Camacho-Collados","Fernando Alva-Manchego"],"pdf_url":"https://arxiv.org/pdf/2409.20246v2.pdf","comment":"Accepted on COLING 2025"},{"id":"http://arxiv.org/abs/2412.11927v1","updated":"2024-12-16T16:13:55Z","published":"2024-12-16T16:13:55Z","title":"Explainable Procedural Mistake Detection","summary":"  Automated task guidance has recently attracted attention from the AI research\ncommunity. Procedural mistake detection (PMD) is a challenging sub-problem of\nclassifying whether a human user (observed through egocentric video) has\nsuccessfully executed the task at hand (specified by a procedural text).\nDespite significant efforts in building resources and models for PMD, machine\nperformance remains nonviable, and the reasoning processes underlying this\nperformance are opaque. As such, we recast PMD to an explanatory self-dialog of\nquestions and answers, which serve as evidence for a decision. As this\nreformulation enables an unprecedented transparency, we leverage a fine-tuned\nnatural language inference (NLI) model to formulate two automated coherence\nmetrics for generated explanations. Our results show that while open-source\nVLMs struggle with this task off-the-shelf, their accuracy, coherence, and\ndialog efficiency can be vastly improved by incorporating these coherence\nmetrics into common inference and fine-tuning methods. Furthermore, our\nmulti-faceted metrics can visualize common outcomes at a glance, highlighting\nareas for improvement.\n","authors":["Shane Storks","Itamar Bar-Yossef","Yayuan Li","Zheyuan Zhang","Jason J. Corso","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2412.11927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02224v4","updated":"2024-12-16T16:13:14Z","published":"2024-06-04T11:36:09Z","title":"FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models","summary":"  Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs.\n","authors":["Tao Fan","Guoqiang Ma","Yan Kang","Hanlin Gu","Yuanfeng Song","Lixin Fan","Kai Chen","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.02224v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12121v2","updated":"2024-12-16T16:09:47Z","published":"2024-02-19T13:16:10Z","title":"IRR: Image Review Ranking Framework for Evaluating Vision-Language\n  Models","summary":"  Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.\n","authors":["Kazuki Hayashi","Kazuma Onishi","Toma Suzuki","Yusuke Ide","Seiji Gobara","Shigeki Saito","Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2402.12121v2.pdf","comment":"18pages, Accepted at COLING25"},{"id":"http://arxiv.org/abs/2412.11923v1","updated":"2024-12-16T16:09:35Z","published":"2024-12-16T16:09:35Z","title":"PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection","summary":"  In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations.\n","authors":["Sepideh Mamooler","Syrielle Montariol","Alexander Mathis","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2412.11923v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.07095v4","updated":"2024-12-16T16:05:09Z","published":"2024-10-09T17:34:27Z","title":"MLE-bench: Evaluating Machine Learning Agents on Machine Learning\n  Engineering","summary":"  We introduce MLE-bench, a benchmark for measuring how well AI agents perform\nat machine learning engineering. To this end, we curate 75 ML\nengineering-related competitions from Kaggle, creating a diverse set of\nchallenging tasks that test real-world ML engineering skills such as training\nmodels, preparing datasets, and running experiments. We establish human\nbaselines for each competition using Kaggle's publicly available leaderboards.\nWe use open-source agent scaffolds to evaluate several frontier language models\non our benchmark, finding that the best-performing setup--OpenAI's o1-preview\nwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in\n16.9% of competitions. In addition to our main results, we investigate various\nforms of resource scaling for AI agents and the impact of contamination from\npre-training. We open-source our benchmark code (github.com/openai/mle-bench/)\nto facilitate future research in understanding the ML engineering capabilities\nof AI agents.\n","authors":["Jun Shern Chan","Neil Chowdhury","Oliver Jaffe","James Aung","Dane Sherburn","Evan Mays","Giulio Starace","Kevin Liu","Leon Maksin","Tejal Patwardhan","Lilian Weng","Aleksander Mądry"],"pdf_url":"https://arxiv.org/pdf/2410.07095v4.pdf","comment":"10 pages, 17 pages appendix. Equal contribution by first seven\n  authors, authors randomized. Corrected footnote 4. Added citation"},{"id":"http://arxiv.org/abs/2412.11919v1","updated":"2024-12-16T16:03:25Z","published":"2024-12-16T16:03:25Z","title":"RetroLLM: Empowering Large Language Models to Retrieve Fine-grained\n  Evidence within Generation","summary":"  Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}.\n","authors":["Xiaoxi Li","Jiajie Jin","Yujia Zhou","Yongkang Wu","Zhonghua Li","Qi Ye","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2412.11919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11912v1","updated":"2024-12-16T15:55:34Z","published":"2024-12-16T15:55:34Z","title":"CharacterBench: Benchmarking Character Customization of Large Language\n  Models","summary":"  Character-based dialogue (aka role-playing) enables users to freely customize\ncharacters for interaction, which often relies on LLMs, raising the need to\nevaluate LLMs' character customization capability. However, existing benchmarks\nfail to ensure a robust evaluation as they often only involve a single\ncharacter category or evaluate limited dimensions. Moreover, the sparsity of\ncharacter features in responses makes feature-focused generative evaluation\nboth ineffective and inefficient. To address these issues, we propose\nCharacterBench, the largest bilingual generative benchmark, with 22,859\nhuman-annotated samples covering 3,956 characters from 25 detailed character\ncategories. We define 11 dimensions of 6 aspects, classified as sparse and\ndense dimensions based on whether character features evaluated by specific\ndimensions manifest in each response. We enable effective and efficient\nevaluation by crafting tailored queries for each dimension to induce\ncharacters' responses related to specific dimensions. Further, we develop\nCharacterJudge model for cost-effective and stable evaluations. Experiments\nshow its superiority over SOTA automatic judges (e.g., GPT-4) and our\nbenchmark's potential to optimize LLMs' character customization. Our repository\nis at https://github.com/thu-coai/CharacterBench.\n","authors":["Jinfeng Zhou","Yongkang Huang","Bosi Wen","Guanqun Bi","Yuxuan Chen","Pei Ke","Zhuang Chen","Xiyao Xiao","Libiao Peng","Kuntian Tang","Rongsheng Zhang","Le Zhang","Tangjie Lv","Zhipeng Hu","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11912v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11908v1","updated":"2024-12-16T15:54:06Z","published":"2024-12-16T15:54:06Z","title":"Can Language Models Rival Mathematics Students? Evaluating Mathematical\n  Reasoning through Textual Manipulation and Human Experiments","summary":"  In this paper we look at the ability of recent large language models (LLMs)\nat solving mathematical problems in combinatorics. We compare models LLaMA-2,\nLLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and\nundergraduates with prior experience in mathematical olympiads. To facilitate\nthese comparisons we introduce the Combi-Puzzles dataset, which contains 125\nproblem variants based on 25 combinatorial reasoning problems. Each problem is\npresented in one of five distinct forms, created by systematically manipulating\nthe problem statements through adversarial additions, numeric parameter\nchanges, and linguistic obfuscation. Our variations preserve the mathematical\ncore and are designed to measure the generalisability of LLM problem-solving\nabilities, while also increasing confidence that problems are submitted to LLMs\nin forms that have not been seen as training instances. We found that a model\nbased on GPT-4 outperformed all other models in producing correct responses,\nand performed significantly better in the mathematical variation of the\nproblems than humans. We also found that modifications to problem statements\nsignificantly impact the LLM's performance, while human performance remains\nunaffected.\n","authors":["Andrii Nikolaiev","Yiannos Stathopoulos","Simone Teufel"],"pdf_url":"https://arxiv.org/pdf/2412.11908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11896v1","updated":"2024-12-16T15:45:10Z","published":"2024-12-16T15:45:10Z","title":"Classification of Spontaneous and Scripted Speech for Multilingual Audio","summary":"  Distinguishing scripted from spontaneous speech is an essential tool for\nbetter understanding how speech styles influence speech processing research. It\ncan also improve recommendation systems and discovery experiences for media\nusers through better segmentation of large recorded speech catalogues. This\npaper addresses the challenge of building a classifier that generalises well\nacross different formats and languages. We systematically evaluate models\nranging from traditional, handcrafted acoustic and prosodic features to\nadvanced audio transformers, utilising a large, multilingual proprietary\npodcast dataset for training and validation. We break down the performance of\neach model across 11 language groups to evaluate cross-lingual biases. Our\nexperimental analysis extends to publicly available datasets to assess the\nmodels' generalisability to non-podcast domains. Our results indicate that\ntransformer-based models consistently outperform traditional feature-based\ntechniques, achieving state-of-the-art performance in distinguishing between\nscripted and spontaneous speech across various languages.\n","authors":["Shahar Elisha","Andrew McDowell","Mariano Beguerisse-Díaz","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2412.11896v1.pdf","comment":"Accepted to IEEE Spoken Language Technology Workshop 2024"},{"id":"http://arxiv.org/abs/2412.11878v1","updated":"2024-12-16T15:27:37Z","published":"2024-12-16T15:27:37Z","title":"Using Instruction-Tuned Large Language Models to Identify Indicators of\n  Vulnerability in Police Incident Narratives","summary":"  Objectives: Compare qualitative coding of instruction tuned large language\nmodels (IT-LLMs) against human coders in classifying the presence or absence of\nvulnerability in routinely collected unstructured text that describes\npolice-public interactions. Evaluate potential bias in IT-LLM codings. Methods:\nAnalyzing publicly available text narratives of police-public interactions\nrecorded by Boston Police Department, we provide humans and IT-LLMs with\nqualitative labelling codebooks and compare labels generated by both, seeking\nto identify situations associated with (i) mental ill health; (ii) substance\nmisuse; (iii) alcohol dependence; and (iv) homelessness. We explore multiple\nprompting strategies and model sizes, and the variability of labels generated\nby repeated prompts. Additionally, to explore model bias, we utilize\ncounterfactual methods to assess the impact of two protected characteristics -\nrace and gender - on IT-LLM classification. Results: Results demonstrate that\nIT-LLMs can effectively support human qualitative coding of police incident\nnarratives. While there is some disagreement between LLM and human generated\nlabels, IT-LLMs are highly effective at screening narratives where no\nvulnerabilities are present, potentially vastly reducing the requirement for\nhuman coding. Counterfactual analyses demonstrate that manipulations to both\ngender and race of individuals described in narratives have very limited\neffects on IT-LLM classifications beyond those expected by chance. Conclusions:\nIT-LLMs offer effective means to augment human qualitative coding in a way that\nrequires much lower levels of resource to analyze large unstructured datasets.\nMoreover, they encourage specificity in qualitative coding, promote\ntransparency, and provide the opportunity for more standardized, replicable\napproaches to analyzing large free-text police data sources.\n","authors":["Sam Relins","Daniel Birks","Charlie Lloyd"],"pdf_url":"https://arxiv.org/pdf/2412.11878v1.pdf","comment":"33 pages, 6 figures Submitted to Journal of Quantitative Criminology"},{"id":"http://arxiv.org/abs/2412.11863v1","updated":"2024-12-16T15:20:03Z","published":"2024-12-16T15:20:03Z","title":"GeoX: Geometric Problem Solving Through Unified Formalized\n  Vision-Language Pre-training","summary":"  Despite their proficiency in general tasks, Multi-modal Large Language Models\n(MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands\nunderstanding diagrams, interpreting symbols, and performing complex reasoning.\nThis limitation arises from their pre-training on natural images and texts,\nalong with the lack of automated verification in the problem-solving process.\nBesides, current geometric specialists are limited by their task-specific\ndesigns, making them less effective for broader geometric problems. To this\nend, we present GeoX, a multi-modal large model focusing on geometric\nunderstanding and reasoning tasks. Given the significant differences between\ngeometric diagram-symbol and natural image-text, we introduce unimodal\npre-training to develop a diagram encoder and symbol decoder, enhancing the\nunderstanding of geometric images and corpora. Furthermore, we introduce\ngeometry-language alignment, an effective pre-training paradigm that bridges\nthe modality gap between unimodal geometric experts. We propose a\nGenerator-And-Sampler Transformer (GS-Former) to generate discriminative\nqueries and eliminate uninformative representations from unevenly distributed\ngeometric signals. Finally, GeoX benefits from visual instruction tuning,\nempowering it to take geometric images and questions as input and generate\nverifiable solutions. Experiments show that GeoX outperforms both generalists\nand geometric specialists on publicly recognized benchmarks, such as GeoQA,\nUniGeo, Geometry3K, and PGPS9k.\n","authors":["Renqiu Xia","Mingsheng Li","Hancheng Ye","Wenjie Wu","Hongbin Zhou","Jiakang Yuan","Tianshuo Peng","Xinyu Cai","Xiangchao Yan","Bin Wang","Conghui He","Botian Shi","Tao Chen","Junchi Yan","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11863v1.pdf","comment":"Our code is available at https://github.com/UniModal4Reasoning/GeoX"},{"id":"http://arxiv.org/abs/2410.02729v2","updated":"2024-12-16T15:11:11Z","published":"2024-10-03T17:49:09Z","title":"Unified Multimodal Interleaved Document Representation for Retrieval","summary":"  Information Retrieval (IR) methods aim to identify documents relevant to a\nquery, which have been widely applied in various natural language tasks.\nHowever, existing approaches typically consider only the textual content within\ndocuments, overlooking the fact that documents can contain multiple modalities,\nincluding images and tables. Also, they often segment each long document into\nmultiple discrete passages for embedding, which prevents them from capturing\nthe overall document context and interactions between paragraphs. To address\nthese two challenges, we propose a method that holistically embeds documents\ninterleaved with multiple modalities by leveraging the capability of recent\nvision-language models that enable the processing and integration of text,\nimages, and tables into a unified format and representation. Moreover, to\nmitigate the information loss from segmenting documents into passages, instead\nof representing and retrieving passages individually, we further merge the\nrepresentations of segmented passages into one single document representation,\nwhile we additionally introduce a reranking strategy to decouple and identify\nthe relevant passage within the document if necessary. Then, through extensive\nexperiments on diverse IR scenarios considering both the textual and multimodal\nqueries, we show that our approach substantially outperforms relevant\nbaselines, thanks to the consideration of the multimodal information within\ndocuments.\n","authors":["Jaewoo Lee","Joonho Ko","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.02729v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.11851v1","updated":"2024-12-16T15:11:03Z","published":"2024-12-16T15:11:03Z","title":"A Benchmark and Robustness Study of In-Context-Learning with Large\n  Language Models in Music Entity Detection","summary":"  Detecting music entities such as song titles or artist names is a useful\napplication to help use cases like processing music search queries or analyzing\nmusic consumption on the web. Recent approaches incorporate smaller language\nmodels (SLMs) like BERT and achieve high results. However, further research\nindicates a high influence of entity exposure during pre-training on the\nperformance of the models. With the advent of large language models (LLMs),\nthese outperform SLMs in a variety of downstream tasks. However, researchers\nare still divided if this is applicable to tasks like entity detection in texts\ndue to issues like hallucination. In this paper, we provide a novel dataset of\nuser-generated metadata and conduct a benchmark and a robustness study using\nrecent LLMs with in-context-learning (ICL). Our results indicate that LLMs in\nthe ICL setting yield higher performance than SLMs. We further uncover the\nlarge impact of entity exposure on the best performing LLM in our study.\n","authors":["Simon Hachmeier","Robert Jäschke"],"pdf_url":"https://arxiv.org/pdf/2412.11851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10497v2","updated":"2024-12-16T15:03:54Z","published":"2024-08-20T02:44:45Z","title":"QUITO-X: A New Perspective on Context Compression from the Information\n  Bottleneck Theory","summary":"  Generative LLM have achieved remarkable success in various industrial\napplications, owing to their promising In-Context Learning capabilities.\nHowever, the issue of long context in complex tasks poses a significant barrier\nto their wider adoption, manifested in two main aspects: (i) The excessively\nlong context leads to high costs and inference delays. (ii) A substantial\namount of task-irrelevant information introduced by long contexts exacerbates\nthe \"lost in the middle\" problem. Existing methods compress context by removing\nredundant tokens using metrics such as self-information or PPL, which is\ninconsistent with the objective of retaining the most important tokens when\nconditioning on a given query. In this study, we introduce information\nbottleneck theory (IB) to model the problem, offering a novel perspective that\nthoroughly addresses the essential properties required for context compression.\nAdditionally, we propose a cross-attention-based approach to approximate mutual\ninformation in IB, which can be flexibly replaced with suitable alternatives in\ndifferent scenarios. Extensive experiments on four datasets demonstrate that\nour method achieves a 25% increase in compression rate compared to the\nstate-of-the-art, while maintaining question answering performance. In\nparticular, the context compressed by our method even outperform the full\ncontext in some cases.\n","authors":["Yihang Wang","Xu Huang","Bowen Tian","Yueyang Su","Lei Yu","Huaming Liao","Yixing Fan","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.10497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05668v2","updated":"2024-12-16T15:02:14Z","published":"2024-02-08T13:42:50Z","title":"Comprehensive Assessment of Jailbreak Attacks Against LLMs","summary":"  Jailbreak attacks aim to bypass the safeguards of LLMs. While researchers\nhave studied different jailbreak attacks in depth, they have done so in\nisolation -- either with unaligned experiment settings or comparing a limited\nrange of methods. To fill this gap, we present the first large-scale\nmeasurement of various jailbreak attack methods. We collect 17 cutting-edge\njailbreak methods, summarize their features, and establish a novel jailbreak\nattack taxonomy. Based on eight popular censored LLMs and 160 questions from 16\nviolation categories, we conduct a unified and impartial assessment of attack\neffectiveness as well as a comprehensive ablation study. Our extensive\nexperimental results demonstrate that all the jailbreak attacks have a powerful\neffect on the LLMs. This indicates that all LLMs fail to cover all the\nviolation categories, and they are susceptible to significant jailbreak risks,\nwith even the well-aligned Llama3 facing a maximum attack success rate of 0.88.\nAdditionally, we test jailbreak attacks under eight advanced external defenses\nand find none of the defenses could mitigate the jailbreak attacks entirely.\nOur study offers valuable insights for future research on jailbreak attacks and\ndefenses and serves as a benchmark tool for researchers and practitioners to\nevaluate them effectively.\n","authors":["Junjie Chu","Yugeng Liu","Ziqing Yang","Xinyue Shen","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.05668v2.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.15594v2","updated":"2024-12-16T15:00:53Z","published":"2024-11-23T16:03:35Z","title":"A Survey on LLM-as-a-Judge","summary":"  Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.\n","authors":["Jiawei Gu","Xuhui Jiang","Zhichao Shi","Hexiang Tan","Xuehao Zhai","Chengjin Xu","Wei Li","Yinghan Shen","Shengjie Ma","Honghao Liu","Yuanzhuo Wang","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2411.15594v2.pdf","comment":"33 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2310.05470 by other authors"},{"id":"http://arxiv.org/abs/2412.11835v1","updated":"2024-12-16T14:56:31Z","published":"2024-12-16T14:56:31Z","title":"Improved Models for Media Bias Detection and Subcategorization","summary":"  We present improved models for the granular detection and sub-classification\nnews media bias in English news articles. We compare the performance of\nzero-shot versus fine-tuned large pre-trained neural transformer language\nmodels, explore how the level of detail of the classes affects performance on a\nnovel taxonomy of 27 news bias-types, and demonstrate how using synthetically\ngenerated example data can be used to improve quality\n","authors":["Tim Menzner","Jochen L. Leidner"],"pdf_url":"https://arxiv.org/pdf/2412.11835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11834v1","updated":"2024-12-16T14:56:28Z","published":"2024-12-16T14:56:28Z","title":"Wonderful Matrices: Combining for a More Efficient and Effective\n  Foundation Model Architecture","summary":"  In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.\n","authors":["Jingze Shi","Bingheng Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11834v1.pdf","comment":"The code is open-sourced at https://github.com/LoserCheems/Doge"},{"id":"http://arxiv.org/abs/2412.11831v1","updated":"2024-12-16T14:55:09Z","published":"2024-12-16T14:55:09Z","title":"Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of\n  Model Uncertainty for Question Difficulty Estimation","summary":"  In an educational setting, an estimate of the difficulty of multiple-choice\nquestions (MCQs), a commonly used strategy to assess learning progress,\nconstitutes very useful information for both teachers and students. Since human\nassessment is costly from multiple points of view, automatic approaches to MCQ\nitem difficulty estimation are investigated, yielding however mixed success\nuntil now. Our approach to this problem takes a different angle from previous\nwork: asking various Large Language Models to tackle the questions included in\ntwo different MCQ datasets, we leverage model uncertainty to estimate item\ndifficulty. By using both model uncertainty features as well as textual\nfeatures in a Random Forest regressor, we show that uncertainty features\ncontribute substantially to difficulty prediction, where difficulty is\ninversely proportional to the number of students who can correctly answer a\nquestion. In addition to showing the value of our approach, we also observe\nthat our model achieves state-of-the-art results on the BEA publicly available\ndataset.\n","authors":["Leonidas Zotos","Hedderik van Rijn","Malvina Nissim"],"pdf_url":"https://arxiv.org/pdf/2412.11831v1.pdf","comment":"14 pages,7 figures"},{"id":"http://arxiv.org/abs/2412.10257v2","updated":"2024-12-16T14:54:00Z","published":"2024-12-13T16:26:34Z","title":"Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models","summary":"  The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).\n","authors":["Harry J. Davies","Giorgos Iacovides","Danilo P. Mandic"],"pdf_url":"https://arxiv.org/pdf/2412.10257v2.pdf","comment":"14 pages, 5 figures, 1 table. Fixing typo with the final weight\n  editing equation"},{"id":"http://arxiv.org/abs/2403.13804v2","updated":"2024-12-16T14:53:21Z","published":"2024-03-20T17:59:43Z","title":"Learning from Synthetic Data for Visual Grounding","summary":"  This paper extensively investigates the effectiveness of synthetic training\ndata to improve the capabilities of vision-and-language models for grounding\ntextual descriptions to image regions. We explore various strategies to best\ngenerate image-text pairs and image-text-box triplets using a series of\npretrained models under different settings and varying degrees of reliance on\nreal data. Through comparative analyses with synthetic, real, and web-crawled\ndata, we identify factors that contribute to performance differences, and\npropose SynGround, an effective pipeline for generating useful synthetic data\nfor visual grounding. Our findings show that SynGround can improve the\nlocalization capabilities of off-the-shelf vision-and-language models and\noffers the potential for arbitrarily large scale data generation. Particularly,\ndata generated with SynGround improves the pointing game accuracy of a\npretrained ALBEF and BLIP models by 4.81% and 17.11% absolute percentage\npoints, respectively, across the RefCOCO+ and the Flickr30k benchmarks.\n","authors":["Ruozhen He","Ziyan Yang","Paola Cascante-Bonilla","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v2.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2412.11823v1","updated":"2024-12-16T14:42:26Z","published":"2024-12-16T14:42:26Z","title":"Advancements and Challenges in Bangla Question Answering Models: A\n  Comprehensive Review","summary":"  The domain of Natural Language Processing (NLP) has experienced notable\nprogress in the evolution of Bangla Question Answering (QA) systems. This paper\npresents a comprehensive review of seven research articles that contribute to\nthe progress in this domain. These research studies explore different aspects\nof creating question-answering systems for the Bangla language. They cover\nareas like collecting data, preparing it for analysis, designing models,\nconducting experiments, and interpreting results. The papers introduce\ninnovative methods like using LSTM-based models with attention mechanisms,\ncontext-based QA systems, and deep learning techniques based on prior\nknowledge. However, despite the progress made, several challenges remain,\nincluding the lack of well-annotated data, the absence of high-quality reading\ncomprehension datasets, and difficulties in understanding the meaning of words\nin context. Bangla QA models' precision and applicability are constrained by\nthese challenges. This review emphasizes the significance of these research\ncontributions by highlighting the developments achieved in creating Bangla QA\nsystems as well as the ongoing effort required to get past roadblocks and\nimprove the performance of these systems for actual language comprehension\ntasks.\n","authors":["Md Iftekhar Islam Tashik","Abdullah Khondoker","Enam Ahmed Taufik","Antara Firoz Parsa","S M Ishtiak Mahmud"],"pdf_url":"https://arxiv.org/pdf/2412.11823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04905v2","updated":"2024-12-16T14:36:19Z","published":"2024-12-06T10:01:38Z","title":"DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling","summary":"  Large language models (LLMs) have made dialogue one of the central modes in\nhuman-machine interaction, leading to the vast amounts of conversation logs and\nincreasing demand for dialogue generation. The dialogue's life-cycle spans from\nthe $\\textit{Prelude}$ through the $\\textit{Interlocution}$ to the\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite the large\nvolumes of dialogue-related studies, there is a lack of benchmark that\nencompasses comprehensive dialogue elements, which hinders precise modeling,\ngeneration and systematic evaluation. To bridge this gap, in this paper, we\nintroduce a new research task $\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks.\n","authors":["Minzheng Wang","Xinghua Zhang","Kun Chen","Nan Xu","Haiyang Yu","Fei Huang","Wenji Mao","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2412.04905v2.pdf","comment":"We release the code and data at https://github.com/MozerWang/DEMO"},{"id":"http://arxiv.org/abs/2412.11814v1","updated":"2024-12-16T14:29:49Z","published":"2024-12-16T14:29:49Z","title":"EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents","summary":"  In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information.\n","authors":["Mengna Zhu","Kaisheng Zeng","Mao Wang","Kaiming Xiao","Lei Hou","Hongbin Huang","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2412.11814v1.pdf","comment":"Extended version for paper accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2407.12363v4","updated":"2024-12-16T14:18:16Z","published":"2024-07-17T07:39:16Z","title":"Conversational Query Reformulation with the Guidance of Retrieved\n  Documents","summary":"  Conversational search seeks to retrieve relevant passages for the given\nquestions in conversational question answering. Conversational Query\nReformulation (CQR) improves conversational search by refining the original\nqueries into de-contextualized forms to resolve the issues in the original\nqueries, such as omissions and coreferences. Previous CQR methods focus on\nimitating human written queries which may not always yield meaningful search\nresults for the retriever. In this paper, we introduce GuideCQR, a framework\nthat refines queries for CQR by leveraging key information from the initially\nretrieved documents. Specifically, GuideCQR extracts keywords and generates\nexpected answers from the retrieved documents, then unifies them with the\nqueries after filtering to add useful information that enhances the search\nprocess. Experimental results demonstrate that our proposed method achieves\nstate-of-the-art performance across multiple datasets, outperforming previous\nCQR methods. Additionally, we show that GuideCQR can get additional performance\ngains in conversational search using various types of queries, even for queries\nwritten by humans.\n","authors":["Jeonghyun Park","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2407.12363v4.pdf","comment":"18 pages, 3 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.11803v1","updated":"2024-12-16T14:14:27Z","published":"2024-12-16T14:14:27Z","title":"UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on\n  Large Language Models","summary":"  Despite demonstrating impressive capabilities, Large Language Models (LLMs)\nstill often struggle to accurately express the factual knowledge they possess,\nespecially in cases where the LLMs' knowledge boundaries are ambiguous. To\nimprove LLMs' factual expressions, we propose the UAlign framework, which\nleverages Uncertainty estimations to represent knowledge boundaries, and then\nexplicitly incorporates these representations as input features into prompts\nfor LLMs to Align with factual knowledge. First, we prepare the dataset on\nknowledge question-answering (QA) samples by calculating two uncertainty\nestimations, including confidence score and semantic entropy, to represent the\nknowledge boundaries for LLMs. Subsequently, using the prepared dataset, we\ntrain a reward model that incorporates uncertainty estimations and then employ\nthe Proximal Policy Optimization (PPO) algorithm for factuality alignment on\nLLMs. Experimental results indicate that, by integrating uncertainty\nrepresentations in LLM alignment, the proposed UAlign can significantly enhance\nthe LLMs' capacities to confidently answer known questions and refuse unknown\nquestions on both in-domain and out-of-domain tasks, showing reliability\nimprovements and good generalizability over various prompt- and training-based\nbaselines.\n","authors":["Boyang Xue","Fei Mi","Qi Zhu","Hongru Wang","Rui Wang","Sheng Wang","Erxin Yu","Xuming Hu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2412.11803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11795v1","updated":"2024-12-16T14:07:39Z","published":"2024-12-16T14:07:39Z","title":"ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible\n  Speech Synthesis","summary":"  Prosody contains rich information beyond the literal meaning of words, which\nis crucial for the intelligibility of speech. Current models still fall short\nin phrasing and intonation; they not only miss or misplace breaks when\nsynthesizing long sentences with complex structures but also produce unnatural\nintonation. We propose ProsodyFM, a prosody-aware text-to-speech synthesis\n(TTS) model with a flow-matching (FM) backbone that aims to enhance the\nphrasing and intonation aspects of prosody. ProsodyFM introduces two key\ncomponents: a Phrase Break Encoder to capture initial phrase break locations,\nfollowed by a Duration Predictor for the flexible adjustment of break\ndurations; and a Terminal Intonation Encoder which integrates a set of\nintonation shape tokens combined with a novel Pitch Processor for more robust\nmodeling of human-perceived intonation change. ProsodyFM is trained with no\nexplicit prosodic labels and yet can uncover a broad spectrum of break\ndurations and intonation patterns. Experimental results demonstrate that\nProsodyFM can effectively improve the phrasing and intonation aspects of\nprosody, thereby enhancing the overall intelligibility compared to four\nstate-of-the-art (SOTA) models. Out-of-distribution experiments show that this\nprosody improvement can further bring ProsodyFM superior generalizability for\nunseen complex sentences and speakers. Our case study intuitively illustrates\nthe powerful and fine-grained controllability of ProsodyFM over phrasing and\nintonation.\n","authors":["Xiangheng He","Junjie Chen","Zixing Zhang","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2412.11795v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2411.04920v3","updated":"2024-12-16T14:05:03Z","published":"2024-11-07T17:57:03Z","title":"GPTKB: Comprehensively Materializing Factual LLM Knowledge","summary":"  LLMs have majorly advanced NLP and AI, and next to their ability to perform a\nwide range of procedural tasks, a major success factor is their internalized\nfactual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has\ngained attention. However, most approaches investigate one question at a time\nvia modest-sized pre-defined samples, introducing an availability bias (Tversky\nand Kahnemann, 1973) that prevents the discovery of knowledge (or beliefs) of\nLLMs beyond the experimenter's predisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterializing an LLM's factual knowledge through recursive querying and result\nconsolidation.\n  As a prototype, we employ GPT-4o-mini to construct GPTKB, a large-scale\nknowledge base (KB) comprising 105 million triples for over 2.9 million\nentities - achieved at 1% of the cost of previous KB projects. This work marks\na milestone in two areas: For LLM research, for the first time, it provides\nconstructive insights into the scope and structure of LLMs' knowledge (or\nbeliefs). For KB construction, it pioneers new pathways for the long-standing\nchallenge of general-domain KB construction. GPTKB is accessible at\nhttps://gptkb.org.\n","authors":["Yujia Hu","Tuan-Phong Nguyen","Shrestha Ghosh","Simon Razniewski"],"pdf_url":"https://arxiv.org/pdf/2411.04920v3.pdf","comment":"13 pages, 4 tables, 10 figures"},{"id":"http://arxiv.org/abs/2412.11787v1","updated":"2024-12-16T13:59:10Z","published":"2024-12-16T13:59:10Z","title":"A Method for Detecting Legal Article Competition for Korean Criminal Law\n  Using a Case-augmented Mention Graph","summary":"  As social systems become increasingly complex, legal articles are also\ngrowing more intricate, making it progressively harder for humans to identify\nany potential competitions among them, particularly when drafting new laws or\napplying existing laws. Despite this challenge, no method for detecting such\ncompetitions has been proposed so far. In this paper, we propose a new legal AI\ntask called Legal Article Competition Detection (LACD), which aims to identify\ncompeting articles within a given law. Our novel retrieval method, CAM-Re2,\noutperforms existing relevant methods, reducing false positives by 20.8% and\nfalse negatives by 8.3%, while achieving a 98.2% improvement in precision@5,\nfor the LACD task. We release our codes at\nhttps://github.com/asmath472/LACD-public.\n","authors":["Seonho An","Young Yik Rhim","Min-Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11787v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2406.12692v2","updated":"2024-12-16T13:52:51Z","published":"2024-06-18T15:06:06Z","title":"MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL","summary":"  Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhance the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. We make all agent interactions publicly available to\nthe research community, to foster further research in this area, offering a\nsynthetic dataset for future explorations into automatic self-correction\nguideline generation.\n","authors":["Arian Askari","Christian Poelitz","Xinye Tang"],"pdf_url":"https://arxiv.org/pdf/2406.12692v2.pdf","comment":"Accepted at Proceedings of the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.04903v2","updated":"2024-12-16T13:47:29Z","published":"2024-12-06T09:59:47Z","title":"EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation","summary":"  Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.\n","authors":["Yongxin Wang","Meng Cao","Haokun Lin","Mingfei Han","Liang Ma","Jin Jiang","Yuhao Cheng","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.04903v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2410.21054v2","updated":"2024-12-16T13:43:50Z","published":"2024-10-28T14:09:52Z","title":"Semantic Component Analysis: Discovering Patterns in Short Texts Beyond\n  Topics","summary":"  Topic modeling is a key method in text analysis, but existing approaches are\nlimited by assuming one topic per document or fail to scale efficiently for\nlarge, noisy datasets of short texts. We introduce Semantic Component Analysis\n(SCA), a novel topic modeling technique that overcomes these limitations by\ndiscovering multiple, nuanced semantic components beyond a single topic in\nshort texts which we accomplish by introducing a decomposition step to the\nclustering-based topic modeling framework. We evaluate SCA on Twitter datasets\nin English, Hausa and Chinese. It achieves competetive coherence and diversity\ncompared to BERTopic, while uncovering at least double the semantic components\nand maintaining a noise rate close to zero. Furthermore, SCA is scalable and\neffective across languages, including an underrepresented one.\n","authors":["Florian Eichin","Carolin M. Schuster","Georg Groh","Michael A. Hedderich"],"pdf_url":"https://arxiv.org/pdf/2410.21054v2.pdf","comment":"5 pages, 3 figures, code:\n  https://github.com/mainlp/semantic_components"},{"id":"http://arxiv.org/abs/2412.11763v1","updated":"2024-12-16T13:28:29Z","published":"2024-12-16T13:28:29Z","title":"QUENCH: Measuring the gap between Indic and Non-Indic Contextual General\n  Reasoning in LLMs","summary":"  The rise of large language models (LLMs) has created a need for advanced\nbenchmarking systems beyond traditional setups. To this end, we introduce\nQUENCH, a novel text-based English Quizzing Benchmark manually curated and\ntranscribed from YouTube quiz videos. QUENCH possesses masked entities and\nrationales for the LLMs to predict via generation. At the intersection of\ngeographical context and common sense reasoning, QUENCH helps assess world\nknowledge and deduction capabilities of LLMs via a zero-shot, open-domain\nquizzing setup. We perform an extensive evaluation on 7 LLMs and 4 metrics,\ninvestigating the influence of model size, prompting style, geographical\ncontext, and gold-labeled rationale generation. The benchmarking concludes with\nan error analysis to which the LLMs are prone.\n","authors":["Mohammad Aflah Khan","Neemesh Yadav","Sarah Masud","Md. Shad Akhtar"],"pdf_url":"https://arxiv.org/pdf/2412.11763v1.pdf","comment":"17 Pages, 6 Figures, 8 Tables, COLING 2025"},{"id":"http://arxiv.org/abs/2412.11757v1","updated":"2024-12-16T13:21:57Z","published":"2024-12-16T13:21:57Z","title":"SCITAT: A Question Answering Benchmark for Scientific Tables and Text\n  Covering Diverse Reasoning Types","summary":"  Scientific question answering (SQA) is an important task aimed at answering\nquestions based on papers. However, current SQA datasets have limited reasoning\ntypes and neglect the relevance between tables and text, creating a significant\ngap with real scenarios. To address these challenges, we propose a QA benchmark\nfor scientific tables and text with diverse reasoning types (SciTaT). To cover\nmore reasoning types, we summarize various reasoning types from real-world\nquestions. To involve both tables and text, we require the questions to\nincorporate tables and text as much as possible. Based on SciTaT, we propose a\nstrong baseline (CaR), which combines various reasoning methods to address\ndifferent reasoning types and process tables and text at the same time. CaR\nbrings average improvements of 12.9% over other baselines on SciTaT, validating\nits effectiveness. Error analysis reveals the challenges of SciTaT, such as\ncomplex numerical calculations and domain knowledge.\n","authors":["Xuanliang Zhang","Dingzirui Wang","Baoxin Wang","Longxu Dou","Xinyuan Lu","Keyan Xu","Dayong Wu","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2412.11757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11750v1","updated":"2024-12-16T13:10:09Z","published":"2024-12-16T13:10:09Z","title":"Common Ground, Diverse Roots: The Difficulty of Classifying Common\n  Examples in Spanish Varieties","summary":"  Variations in languages across geographic regions or cultures are crucial to\naddress to avoid biases in NLP systems designed for culturally sensitive tasks,\nsuch as hate speech detection or dialog with conversational agents. In\nlanguages such as Spanish, where varieties can significantly overlap, many\nexamples can be valid across them, which we refer to as common examples.\nIgnoring these examples may cause misclassifications, reducing model accuracy\nand fairness. Therefore, accounting for these common examples is essential to\nimprove the robustness and representativeness of NLP systems trained on such\ndata. In this work, we address this problem in the context of Spanish\nvarieties. We use training dynamics to automatically detect common examples or\nerrors in existing Spanish datasets. We demonstrate the efficacy of using\npredicted label confidence for our Datamaps\n\\cite{swayamdipta-etal-2020-dataset} implementation for the identification of\nhard-to-classify examples, especially common examples, enhancing model\nperformance in variety identification tasks. Additionally, we introduce a Cuban\nSpanish Variety Identification dataset with common examples annotations\ndeveloped to facilitate more accurate detection of Cuban and Caribbean Spanish\nvarieties. To our knowledge, this is the first dataset focused on identifying\nthe Cuban, or any other Caribbean, Spanish variety.\n","authors":["Javier A. Lopetegui","Arij Riabi","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2412.11750v1.pdf","comment":"Accepted to VARDIAL 2025"},{"id":"http://arxiv.org/abs/2406.11632v3","updated":"2024-12-16T13:04:37Z","published":"2024-06-17T15:13:52Z","title":"Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding\n  for Neural Machine Translation","summary":"  Maximum a posteriori decoding, a commonly used method for neural machine\ntranslation (NMT), aims to maximize the estimated posterior probability.\nHowever, high estimated probability does not always lead to high translation\nquality. Minimum Bayes Risk (MBR) decoding (\\citealp{kumar2004minimum}) offers\nan alternative by seeking hypotheses with the highest expected utility.\n  Inspired by Quality Estimation (QE) reranking which uses the QE model as a\nranker (\\citealp{fernandes-etal-2022-quality}), we propose source-based MBR\n(sMBR) decoding, a novel approach that utilizes quasi-sources (generated via\nparaphrasing or back-translation) as ``support hypotheses'' and a\nreference-free quality estimation metric as the utility function, marking the\nfirst work to solely use sources in MBR decoding. Experiments show that sMBR\noutperforms QE reranking and the standard MBR decoding. Our findings suggest\nthat sMBR is a promising approach for NMT decoding.\n","authors":["Boxuan Lyu","Hidetaka Kamigaito","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2406.11632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11745v1","updated":"2024-12-16T13:03:43Z","published":"2024-12-16T13:03:43Z","title":"Beyond Dataset Creation: Critical View of Annotation Variation and Bias\n  Probing of a Dataset for Online Radical Content Detection","summary":"  The proliferation of radical content on online platforms poses significant\nrisks, including inciting violence and spreading extremist ideologies. Despite\nongoing research, existing datasets and models often fail to address the\ncomplexities of multilingual and diverse data. To bridge this gap, we introduce\na publicly available multilingual dataset annotated with radicalization levels,\ncalls for action, and named entities in English, French, and Arabic. This\ndataset is pseudonymized to protect individual privacy while preserving\ncontextual information. Beyond presenting our\n\\href{https://gitlab.inria.fr/ariabi/counter-dataset-public}{freely available\ndataset}, we analyze the annotation process, highlighting biases and\ndisagreements among annotators and their implications for model performance.\nAdditionally, we use synthetic data to investigate the influence of\nsocio-demographic traits on annotation patterns and model predictions. Our work\noffers a comprehensive examination of the challenges and opportunities in\nbuilding robust datasets for radical content detection, emphasizing the\nimportance of fairness and transparency in model development.\n","authors":["Arij Riabi","Virginie Mouilleron","Menel Mahamdi","Wissam Antoun","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2412.11745v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.11741v1","updated":"2024-12-16T13:01:53Z","published":"2024-12-16T13:01:53Z","title":"CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation","summary":"  The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.\n","authors":["Hongxuan Zhang","Yao Zhao","Jiaqi Zheng","Chenyi Zhuang","Jinjie Gu","Guihai Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03769v2","updated":"2024-12-16T12:57:23Z","published":"2024-10-02T16:34:48Z","title":"SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large\n  Language Models in Scientific Tasks","summary":"  Large language models (LLMs) have a transformative impact on a variety of\nscientific tasks across disciplines including biology, chemistry, medicine, and\nphysics. However, ensuring the safety alignment of these models in scientific\nresearch remains an underexplored area, with existing benchmarks primarily\nfocusing on textual content and overlooking key scientific representations such\nas molecular, protein, and genomic languages. Moreover, the safety mechanisms\nof LLMs in scientific tasks are insufficiently studied. To address these\nlimitations, we introduce SciSafeEval, a comprehensive benchmark designed to\nevaluate the safety alignment of LLMs across a range of scientific tasks.\nSciSafeEval spans multiple scientific languages-including textual, molecular,\nprotein, and genomic-and covers a wide range of scientific domains. We evaluate\nLLMs in zero-shot, few-shot and chain-of-thought settings, and introduce a\n\"jailbreak\" enhancement feature that challenges LLMs equipped with safety\nguardrails, rigorously testing their defenses against malicious intention. Our\nbenchmark surpasses existing safety datasets in both scale and scope, providing\na robust platform for assessing the safety and performance of LLMs in\nscientific contexts. This work aims to facilitate the responsible development\nand deployment of LLMs, promoting alignment with safety and ethical standards\nin scientific research.\n","authors":["Tianhao Li","Jingyu Lu","Chuangxin Chu","Tianyu Zeng","Yujia Zheng","Mei Li","Haotian Huang","Bin Wu","Zuoxian Liu","Kai Ma","Xuejing Yuan","Xingkai Wang","Keyan Ding","Huajun Chen","Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11736v1","updated":"2024-12-16T12:57:19Z","published":"2024-12-16T12:57:19Z","title":"Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users","summary":"  Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLM, but overlooked the diversity of questioners.\nIn this work, we propose a new form of questioner-aware LLM personalization,\ngenerating different responses even for the same query from different\nquestioners. We design a dual-tower model architecture with a cross-questioner\ngeneral encoder and a questioner-specific encoder. We further apply contrastive\nlearning with multi-view augmentation, pulling close the dialogue\nrepresentations of the same questioner, while pulling apart those of different\nquestioners. To mitigate the impact of question diversity on\nquestioner-contrastive learning, we cluster the dialogues based on question\nsimilarity and restrict the scope of contrastive learning within each cluster.\nWe also build a multi-questioner dataset from English and Chinese scripts and\nWeChat records, called MQDialog, containing 173 questioners and 12 responders.\nExtensive evaluation with different metrics shows a significant improvement in\nthe quality of personalized response generation.\n","authors":["Hang Zeng","Chaoyue Niu","Fan Wu","Chengfei Lv","Guihai Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11736v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2407.17940v3","updated":"2024-12-16T12:57:12Z","published":"2024-07-25T10:58:42Z","title":"Positive Text Reframing under Multi-strategy Optimization","summary":"  Differing from sentiment transfer, positive reframing seeks to substitute\nnegative perspectives with positive expressions while preserving the original\nmeaning. With the emergence of pre-trained language models (PLMs), it is\npossible to achieve acceptable results by fine-tuning PLMs. Nevertheless,\ngenerating fluent, diverse and task-constrained reframing text remains a\nsignificant challenge. To tackle this issue, a \\textbf{m}ulti-\\textbf{s}trategy\n\\textbf{o}ptimization \\textbf{f}ramework (MSOF) is proposed in this paper.\nStarting from the objective of positive reframing, we first design positive\nsentiment reward and content preservation reward to encourage the model to\ntransform the negative expressions of the original text while ensuring the\nintegrity and consistency of the semantics. Then, different decoding\noptimization approaches are introduced to improve the quality of text\ngeneration. Finally, based on the modeling formula of positive reframing, we\npropose a multi-dimensional re-ranking method that further selects candidate\nsentences from three dimensions: strategy consistency, text similarity and\nfluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate\nour framework achieves significant improvements on unconstrained and controlled\npositive reframing tasks.\n","authors":["Shutong Jia","Biwei Cao","Qingqing Gao","Jiuxin Cao","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17940v3.pdf","comment":"To appear at COLING 2025"},{"id":"http://arxiv.org/abs/2412.11732v1","updated":"2024-12-16T12:54:52Z","published":"2024-12-16T12:54:52Z","title":"Findings of the WMT 2024 Shared Task on Discourse-Level Literary\n  Translation","summary":"  Following last year, we have continued to host the WMT translation shared\ntask this year, the second edition of the Discourse-Level Literary Translation.\nWe focus on three language directions: Chinese-English, Chinese-German, and\nChinese-Russian, with the latter two ones newly added. This year, we totally\nreceived 10 submissions from 5 academia and industry teams. We employ both\nautomatic and human evaluations to measure the performance of the submitted\nsystems. The official ranking of the systems is based on the overall human\njudgments. We release data, system outputs, and leaderboard at\nhttps://www2.statmt.org/wmt24/literary-translation-task.html.\n","authors":["Longyue Wang","Siyou Liu","Chenyang Lyu","Wenxiang Jiao","Xing Wang","Jiahao Xu","Zhaopeng Tu","Yan Gu","Weiyu Chen","Minghao Wu","Liting Zhou","Philipp Koehn","Andy Way","Yulin Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.11732v1.pdf","comment":"WMT2024"},{"id":"http://arxiv.org/abs/2406.13925v3","updated":"2024-12-16T12:51:46Z","published":"2024-06-20T01:45:44Z","title":"GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models","summary":"  Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs.\n","authors":["Tao Zhang","Ziqian Zeng","Yuxiang Xiao","Huiping Zhuang","Cen Chen","James Foulds","Shimei Pan"],"pdf_url":"https://arxiv.org/pdf/2406.13925v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05813v2","updated":"2024-12-16T12:44:07Z","published":"2024-02-08T16:50:01Z","title":"Selective Forgetting: Advancing Machine Unlearning Techniques and\n  Evaluation in Language Models","summary":"  This paper explores Machine Unlearning (MU), an emerging field that is\ngaining increased attention due to concerns about neural models unintentionally\nremembering personal or sensitive information. We present SeUL, a novel method\nthat enables selective and fine-grained unlearning for language models. Unlike\nprevious work that employs a fully reversed training objective in unlearning,\nSeUL minimizes the negative impact on the capability of language models,\nparticularly in terms of generation. Furthermore, we introduce two innovative\nevaluation metrics, sensitive extraction likelihood (S-EL) and sensitive\nmemorization accuracy (S-MA), specifically designed to assess the effectiveness\nof forgetting sensitive information. In support of the unlearning framework, we\npropose efficient automatic online and offline sensitive span annotation\nmethods. The online selection method, based on language probability scores,\nensures computational efficiency, while the offline annotation involves a\ntwo-stage LLM-based process for robust verification. In summary, this paper\ncontributes a novel selective unlearning method (SeUL), introduces specialized\nevaluation metrics (S-EL and S-MA) for assessing sensitive information\nforgetting, and proposes automatic online and offline sensitive span annotation\nmethods to support the overall unlearning framework and evaluation process.\n","authors":["Lingzhi Wang","Xingshan Zeng","Jinsong Guo","Kam-Fai Wong","Georg Gottlob"],"pdf_url":"https://arxiv.org/pdf/2402.05813v2.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2409.10907v2","updated":"2024-12-16T12:42:07Z","published":"2024-09-17T05:54:25Z","title":"Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised\n  Keyphrase Extraction","summary":"  This paper proposes Attention-Seeker, an unsupervised keyphrase extraction\nmethod that leverages self-attention maps from a Large Language Model to\nestimate the importance of candidate phrases. Our approach identifies specific\ncomponents - such as layers, heads, and attention vectors - where the model\npays significant attention to the key topics of the text. The attention weights\nprovided by these components are then used to score the candidate phrases.\nUnlike previous models that require manual tuning of parameters (e.g.,\nselection of heads, prompts, hyperparameters), Attention-Seeker dynamically\nadapts to the input text without any manual adjustments, enhancing its\npractical applicability. We evaluate Attention-Seeker on four publicly\navailable datasets: Inspec, SemEval2010, SemEval2017, and Krapivin. Our results\ndemonstrate that, even without parameter tuning, Attention-Seeker outperforms\nmost baseline models, achieving state-of-the-art performance on three out of\nfour datasets, particularly excelling in extracting keyphrases from long\ndocuments.\n","authors":["Erwin D. López Z.","Cheng Tang","Atsushi Shimada"],"pdf_url":"https://arxiv.org/pdf/2409.10907v2.pdf","comment":"This version has been accepted for presentation at COLING 2025, and\n  all peer-reviewed changes have been incorporated"},{"id":"http://arxiv.org/abs/2412.11716v1","updated":"2024-12-16T12:36:47Z","published":"2024-12-16T12:36:47Z","title":"LLMs Can Simulate Standardized Patients via Agent Coevolution","summary":"  Training medical personnel using standardized patients (SPs) remains a\ncomplex challenge, requiring extensive domain expertise and role-specific\npractice. Most research on Large Language Model (LLM)-based simulated patients\nfocuses on improving data retrieval accuracy or adjusting prompts through human\nfeedback. However, this focus has overlooked the critical need for patient\nagents to learn a standardized presentation pattern that transforms data into\nhuman-like patient responses through unsupervised simulations. To address this\ngap, we propose EvoPatient, a novel simulated patient framework in which a\npatient agent and doctor agents simulate the diagnostic process through\nmulti-turn dialogues, simultaneously gathering experience to improve the\nquality of both questions and answers, ultimately enabling human doctor\ntraining. Extensive experiments on various cases demonstrate that, by providing\nonly overall SP requirements, our framework improves over existing reasoning\nmethods by more than 10% in requirement alignment and better human preference,\nwhile achieving an optimal balance of resource consumption after evolving over\n200 cases for 10 hours, with excellent generalizability. The code will be\navailable at https://github.com/ZJUMAI/EvoPatient.\n","authors":["Zhuoyun Du","Lujie Zheng","Renjun Hu","Yuyang Xu","Xiawei Li","Ying Sun","Wei Chen","Jian Wu","Haolei Cai","Haohao Ying"],"pdf_url":"https://arxiv.org/pdf/2412.11716v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2412.11713v1","updated":"2024-12-16T12:35:29Z","published":"2024-12-16T12:35:29Z","title":"Seeker: Towards Exception Safety Code Generation with Intermediate\n  Language Agents Framework","summary":"  In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open-source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nBlock, and Distorted Handling Solution. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a\nmulti-agent framework inspired by expert developer strategies for exception\nhandling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler\nto assist LLMs in detecting, capturing, and resolving exceptions more\neffectively. Our work is the first systematic study on leveraging LLMs to\nenhance exception handling practices in real development scenarios, providing\nvaluable insights for future improvements in code reliability.\n","authors":["Xuanming Zhang","Yuxuan Chen","Yiming Zheng","Zhexin Zhang","Yuan Yuan","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11713v1.pdf","comment":"30 pages, 9 figures, submitted to ARR Dec"},{"id":"http://arxiv.org/abs/2412.11711v1","updated":"2024-12-16T12:33:12Z","published":"2024-12-16T12:33:12Z","title":"MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for\n  Table Reasoning","summary":"  Extensive research has been conducted to explore the capability of Large\nLanguage Models (LLMs) for table reasoning and has significantly improved the\nperformance on existing benchmarks. However, tables and user questions in\nreal-world applications are more complex and diverse, presenting an unignorable\ngap compared to the existing benchmarks. To fill the gap, we propose a\n\\textbf{M}ult\\textbf{i}-scale spreadsheet benchmark with \\textbf{M}eta\n\\textbf{o}perations for \\textbf{Table} reasoning, named as MiMoTable.\nSpecifically, MiMoTable incorporates two key features. First, the tables in\nMiMoTable are all spreadsheets used in real-world scenarios, which cover seven\ndomains and contain different types. Second, we define a new criterion with six\ncategories of meta operations for measuring the difficulty of each question in\nMiMoTable, simultaneously as a new perspective for measuring the difficulty of\nthe existing benchmarks. Experimental results show that Claude-3.5-Sonnet\nachieves the best performance with 77.4\\% accuracy, indicating that there is\nstill significant room to improve for LLMs on MiMoTable. Furthermore, we grade\nthe difficulty of existing benchmarks according to our new criteria.\nExperiments have shown that the performance of LLMs decreases as the difficulty\nof benchmarks increases, thereby proving the effectiveness of our proposed new\ncriterion.\n","authors":["Zheng Li","Yang Du","Mao Zheng","Mingyang Song"],"pdf_url":"https://arxiv.org/pdf/2412.11711v1.pdf","comment":"Accepted by COLING 2025"},{"id":"http://arxiv.org/abs/2411.11171v2","updated":"2024-12-16T12:29:41Z","published":"2024-11-17T20:44:34Z","title":"LLäMmlein: Compact and Competitive German-Only Language Models from\n  Scratch","summary":"  We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.\n","authors":["Jan Pfister","Julia Wunderle","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2411.11171v2.pdf","comment":"second draft;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/"},{"id":"http://arxiv.org/abs/2412.11707v1","updated":"2024-12-16T12:29:24Z","published":"2024-12-16T12:29:24Z","title":"Context Filtering with Reward Modeling in Question Answering","summary":"  Question Answering (QA) in NLP is the task of finding answers to a query\nwithin a relevant context retrieved by a retrieval system. Yet, the mix of\nrelevant and irrelevant information in these contexts can hinder performance\nenhancements in QA tasks. To address this, we introduce a context filtering\napproach that removes non-essential details, summarizing crucial content\nthrough Reward Modeling. This method emphasizes keeping vital data while\nomitting the extraneous during summarization model training. We offer a\nframework for developing efficient QA models by discerning useful information\nfrom dataset pairs, bypassing the need for costly human evaluation.\nFurthermore, we show that our approach can significantly outperform the\nbaseline, as evidenced by a 6.8-fold increase in the EM Per Token (EPT) metric,\nwhich we propose as a measure of token efficiency, indicating a notable\ntoken-efficiency boost for low-resource settings.\n","authors":["Sangryul Kim","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2412.11707v1.pdf","comment":"Accepted Main Conference at COLING 2025"},{"id":"http://arxiv.org/abs/2412.11704v1","updated":"2024-12-16T12:26:28Z","published":"2024-12-16T12:26:28Z","title":"Vocabulary Expansion of Chat Models with Unlabeled Target Language Data","summary":"  Chat models (i.e. language models trained to follow instructions through\nconversation with humans) outperform base models (i.e. trained solely on\nunlabeled data) in both conversation and general task-solving abilities. These\nmodels are generally English-centric and require further adaptation for\nlanguages that are underrepresented in or absent from their training data. A\ncommon technique for adapting base models is to extend the model's vocabulary\nwith target language tokens, i.e. vocabulary expansion (VE), and then\ncontinually pre-train it on language-specific data. Using chat data is ideal\nfor chat model adaptation, but often, either this does not exist or is costly\nto construct. Alternatively, adapting chat models with unlabeled data is a\npossible solution, but it could result in catastrophic forgetting. In this\npaper, we investigate the impact of using unlabeled target language data for VE\non chat models for the first time. We first show that off-the-shelf VE\ngenerally performs well across target language tasks and models in 71% of\ncases, though it underperforms in scenarios where source chat models are\nalready strong. To further improve adapted models, we propose post-hoc\ntechniques that inject information from the source model without requiring any\nfurther training. Experiments reveal the effectiveness of our methods, helping\nthe adapted models to achieve performance improvements in 87% of cases.\n","authors":["Atsuki Yamaguchi","Terufumi Morishita","Aline Villavicencio","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2412.11704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11699v1","updated":"2024-12-16T12:21:11Z","published":"2024-12-16T12:21:11Z","title":"CoinMath: Harnessing the Power of Coding Instruction for Math LLMs","summary":"  Large Language Models (LLMs) have shown strong performance in solving\nmathematical problems, with code-based solutions proving particularly\neffective. However, the best practice to leverage coding instruction data to\nenhance mathematical reasoning remains underexplored. This study investigates\nthree key questions: (1) How do different coding styles of mathematical\ncode-based rationales impact LLMs' learning performance? (2) Can general-domain\ncoding instructions improve performance? (3) How does integrating textual\nrationales with code-based ones during training enhance mathematical reasoning\nabilities? Our findings reveal that code-based rationales with concise\ncomments, descriptive naming, and hardcoded solutions are beneficial, while\nimprovements from general-domain coding instructions and textual rationales are\nrelatively minor. Based on these insights, we propose CoinMath, a learning\nstrategy designed to enhance mathematical reasoning by diversifying the coding\nstyles of code-based rationales. CoinMath generates a variety of code-based\nrationales incorporating concise comments, descriptive naming conventions, and\nhardcoded solutions. Experimental results demonstrate that CoinMath\nsignificantly outperforms its baseline model, MAmmoTH, one of the SOTA math\nLLMs.\n","authors":["Chengwei Wei","Bin Wang","Jung-jae Kim","Guimei Liu","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11694v1","updated":"2024-12-16T12:12:45Z","published":"2024-12-16T12:12:45Z","title":"From Specific-MLLM to Omni-MLLM: A Survey about the MLLMs alligned with\n  Multi-Modality","summary":"  From the Specific-MLLM, which excels in single-modal tasks, to the Omni-MLLM,\nwhich extends the range of general modalities, this evolution aims to achieve\nunderstanding and generation of multimodal information. Omni-MLLM treats the\nfeatures of different modalities as different \"foreign languages,\" enabling\ncross-modal interaction and understanding within a unified space. To promote\nthe advancement of related research, we have compiled 47 relevant papers to\nprovide the community with a comprehensive introduction to Omni-MLLM. We first\nexplain the four core components of Omni-MLLM for unified modeling and\ninteraction of multiple modalities. Next, we introduce the effective\nintegration achieved through \"alignment pretraining\" and \"instruction\nfine-tuning,\" and discuss open-source datasets and testing of interaction\ncapabilities. Finally, we summarize the main challenges facing current\nOmni-MLLM and outline future directions.\n","authors":["Shixin Jiang","Jiafeng Liang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2412.11694v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.05644v3","updated":"2024-12-16T12:12:19Z","published":"2024-12-07T13:15:22Z","title":"Mixture of Hidden-Dimensions Transformer","summary":"  Transformer models encounter challenges in scaling hidden dimensions\nefficiently, as uniformly increasing them inflates computational and memory\ncosts while failing to emphasize the most relevant features for each token. For\nfurther understanding, we study hidden dimension sparsity and observe that\ntrained Transformers utilize only a small fraction of token dimensions,\nrevealing an \"activation flow\" pattern. Notably, there are shared\nsub-dimensions with sustained activation across multiple consecutive tokens and\nspecialized sub-dimensions uniquely activated for each token. To better model\ntoken-relevant sub-dimensions, we propose MoHD (Mixture of Hidden Dimensions),\na sparse conditional activation architecture. Particularly, MoHD employs shared\nsub-dimensions for common token features and a routing mechanism to dynamically\nactivate specialized sub-dimensions. To mitigate potential information loss\nfrom sparsity, we design activation scaling and group fusion mechanisms to\npreserve activation flow. In this way, MoHD expands hidden dimensions with\nnegligible increases in computation or parameters, efficient training and\ninference while maintaining performance. Evaluations across 10 NLP tasks show\nthat MoHD surpasses Vanilla Transformers in parameter efficiency and task\nperformance. It achieves 1.7% higher performance with 50% fewer activation\nparameters and 3.7% higher performance with a 3x parameter expansion at\nconstant activation cost. MOHD offers a new perspective for scaling the model,\nshowcasing the potential of hidden dimension sparsity to boost efficiency\n","authors":["Yilong Chen","Junyuan Shang","Zhengyu Zhang","Jiawei Sheng","Tingwen Liu","Shuohuan Wang","Yu Sun","Hua Wu","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.05644v3.pdf","comment":"16 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2412.11691v1","updated":"2024-12-16T12:08:59Z","published":"2024-12-16T12:08:59Z","title":"Multilingual and Explainable Text Detoxification with Parallel Corpora","summary":"  Even with various regulations in place across countries and social media\nplatforms (Government of India, 2021; European Parliament and Council of the\nEuropean Union, 2022, digital abusive speech remains a significant issue. One\npotential approach to address this challenge is automatic text detoxification,\na text style transfer (TST) approach that transforms toxic language into a more\nneutral or non-toxic form. To date, the availability of parallel corpora for\nthe text detoxification task (Logachevavet al., 2022; Atwell et al., 2022;\nDementievavet al., 2024a) has proven to be crucial for state-of-the-art\napproaches. With this work, we extend parallel text detoxification corpus to\nnew languages -- German, Chinese, Arabic, Hindi, and Amharic -- testing in the\nextensive multilingual setup TST baselines. Next, we conduct the first of its\nkind an automated, explainable analysis of the descriptive features of both\ntoxic and non-toxic sentences, diving deeply into the nuances, similarities,\nand differences of toxicity and detoxification across 9 languages. Finally,\nbased on the obtained insights, we experiment with a novel text detoxification\nmethod inspired by the Chain-of-Thoughts reasoning approach, enhancing the\nprompting process through clustering on relevant descriptive attributes.\n","authors":["Daryna Dementieva","Nikolay Babakov","Amit Ronen","Abinew Ali Ayele","Naquee Rizwan","Florian Schneider","Xintong Wang","Seid Muhie Yimam","Daniil Moskovskiy","Elisei Stakovskii","Eran Kaufman","Ashraf Elnagar","Animesh Mukherjee","Alexander Panchenko"],"pdf_url":"https://arxiv.org/pdf/2412.11691v1.pdf","comment":"COLING 2025, main conference, long"},{"id":"http://arxiv.org/abs/2402.04902v5","updated":"2024-12-16T12:06:53Z","published":"2024-02-07T14:35:05Z","title":"L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models","summary":"  Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically apply post-training quantization\n(PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss.\nMeanwhile, this approach has limitations in recovering the accuracy loss. In\nthis paper, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA. By employing a memory-optimized layer design, L4Q\nsignificantly reduces QAT's memory overhead, making its training cost\ncomparable to LoRA, while preserving the advantage of QAT in producing fully\nquantized LLMs with high accuracy. Our experiments demonstrate that this\ncombined approach to quantization and fine-tuning achieves superior accuracy\ncompared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit\nquantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and\nMistral models with instructional datasets, we showcase L4Q's capabilities in\nlanguage tasks and few-shot learning.\n","authors":["Hyesung Jeon","Yulhwa Kim","Jae-joon Kim"],"pdf_url":"https://arxiv.org/pdf/2402.04902v5.pdf","comment":"8 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.07682v2","updated":"2024-12-16T12:06:25Z","published":"2024-12-10T17:13:35Z","title":"TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation","summary":"  The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.\n","authors":["Alfredo Garrachón Ruiz","Tomás de la Rosa","Daniel Borrajo"],"pdf_url":"https://arxiv.org/pdf/2412.07682v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2411.10272v2","updated":"2024-12-16T12:00:34Z","published":"2024-11-15T15:28:42Z","title":"P$^2$ Law: Scaling Law for Post-Training After Model Pruning","summary":"  Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs.\n","authors":["Xiaodong Chen","Yuxuan Hu","Xiaokang Zhang","Yanling Wang","Cuiping Li","Hong Chen","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.10272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11749v2","updated":"2024-12-16T11:53:09Z","published":"2024-08-21T16:16:34Z","title":"Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks","summary":"  Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks.\n","authors":["Yiyi Chen","Russa Biswas","Heather Lent","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2408.11749v2.pdf","comment":"11 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2412.11679v1","updated":"2024-12-16T11:38:23Z","published":"2024-12-16T11:38:23Z","title":"Bias Vector: Mitigating Biases in Language Models with Task Arithmetic\n  Approach","summary":"  The use of language models (LMs) has increased considerably in recent years,\nand the biases and stereotypes in training data that are reflected in the LM\noutputs are causing social problems. In this paper, inspired by the task\narithmetic, we propose the ``Bias Vector'' method for the mitigation of these\nLM biases. The Bias Vector method does not require manually created debiasing\ndata. The three main steps of our approach involve: (1) continual training the\npre-trained LMs on biased data using masked language modeling; (2) constructing\nthe Bias Vector as the difference between the weights of the biased LMs and\nthose of pre-trained LMs; and (3) subtracting the Bias Vector from the weights\nof the pre-trained LMs for debiasing. We evaluated the Bias Vector method on\nthe SEAT across three LMs and confirmed an average improvement of 0.177 points.\nWe demonstrated that the Bias Vector method does not degrade the LM performance\non downstream tasks in the GLUE benchmark. In addition, we examined the impact\nof scaling factors, which control the magnitudes of Bias Vectors, with effect\nsizes on the SEAT and conducted a comprehensive evaluation of our debiased LMs\nacross both the SEAT and GLUE benchmarks.\n","authors":["Daiki Shirafuji","Makoto Takenaka","Shinya Taguchi"],"pdf_url":"https://arxiv.org/pdf/2412.11679v1.pdf","comment":"Accepted to COLING2025"},{"id":"http://arxiv.org/abs/2408.11847v2","updated":"2024-12-16T11:26:21Z","published":"2024-08-12T15:19:59Z","title":"Prompto: An open source library for asynchronous querying of LLM\n  endpoints","summary":"  Recent surge in Large Language Model (LLM) availability has opened exciting\navenues for research. However, efficiently interacting with these models\npresents a significant hurdle since LLMs often reside on proprietary or\nself-hosted API endpoints, each requiring custom code for interaction.\nConducting comparative studies between different models can therefore be\ntime-consuming and necessitate significant engineering effort, hindering\nresearch efficiency and reproducibility. To address these challenges, we\npresent prompto, an open source Python library which facilitates asynchronous\nquerying of LLM endpoints enabling researchers to interact with multiple LLMs\nconcurrently, while maximising efficiency and utilising individual rate limits.\nOur library empowers researchers and developers to interact with LLMs more\neffectively and allowing faster experimentation, data generation and\nevaluation. prompto is released with an introductory video\n(https://youtu.be/lWN9hXBOLyQ) under MIT License and is available via GitHub\n(https://github.com/alan-turing-institute/prompto).\n","authors":["Ryan Sze-Yin Chan","Federico Nanni","Angus R. Williams","Edwin Brown","Liam Burke-Moore","Ed Chapman","Kate Onslow","Tvesha Sippy","Jonathan Bright","Evelina Gabasova"],"pdf_url":"https://arxiv.org/pdf/2408.11847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11671v1","updated":"2024-12-16T11:24:54Z","published":"2024-12-16T11:24:54Z","title":"BioBridge: Unified Bio-Embedding with Bridging Modality in Code-Switched\n  EMR","summary":"  Pediatric Emergency Department (PED) overcrowding presents a significant\nglobal challenge, prompting the need for efficient solutions. This paper\nintroduces the BioBridge framework, a novel approach that applies Natural\nLanguage Processing (NLP) to Electronic Medical Records (EMRs) in written\nfree-text form to enhance decision-making in PED. In non-English speaking\ncountries, such as South Korea, EMR data is often written in a Code-Switching\n(CS) format that mixes the native language with English, with most\ncode-switched English words having clinical significance. The BioBridge\nframework consists of two core modules: \"bridging modality in context\" and\n\"unified bio-embedding.\" The \"bridging modality in context\" module improves the\ncontextual understanding of bilingual and code-switched EMRs. In the \"unified\nbio-embedding\" module, the knowledge of the model trained in the medical domain\nis injected into the encoder-based model to bridge the gap between the medical\nand general domains. Experimental results demonstrate that the proposed\nBioBridge significantly performance traditional machine learning and\npre-trained encoder-based models on several metrics, including F1 score, area\nunder the receiver operating characteristic curve (AUROC), area under the\nprecision-recall curve (AUPRC), and Brier score. Specifically, BioBridge-XLM\nachieved enhancements of 0.85% in F1 score, 0.75% in AUROC, and 0.76% in AUPRC,\nalong with a notable 3.04% decrease in the Brier score, demonstrating marked\nimprovements in accuracy, reliability, and prediction calibration over the\nbaseline XLM model. The source code will be made publicly available.\n","authors":["Jangyeong Jeon","Sangyeon Cho","Dongjoon Lee","Changhee Lee","Junyeong Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11671v1.pdf","comment":"Accepted at IEEE Access 2024"},{"id":"http://arxiv.org/abs/2407.13578v2","updated":"2024-12-16T11:23:14Z","published":"2024-07-18T15:20:18Z","title":"How Reliable are LLMs as Knowledge Bases? Re-thinking Facutality and\n  Consistency","summary":"  Large Language Models (LLMs) are increasingly explored as knowledge bases\n(KBs), yet current evaluation methods focus too narrowly on knowledge\nretention, overlooking other crucial criteria for reliable performance. In this\nwork, we rethink the requirements for evaluating reliable LLM-as-KB usage and\nhighlight two essential factors: factuality, ensuring accurate responses to\nseen and unseen knowledge, and consistency, maintaining stable answers to\nquestions about the same knowledge. We introduce UnseenQA, a dataset designed\nto assess LLM performance on unseen knowledge, and propose new criteria and\nmetrics to quantify factuality and consistency, leading to a final reliability\nscore. Our experiments on 26 LLMs reveal several challenges regarding their use\nas KBs, underscoring the need for more principled and comprehensive evaluation.\n","authors":["Danna Zheng","Mirella Lapata","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2407.13578v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08707v7","updated":"2024-12-16T11:21:30Z","published":"2024-04-11T17:44:56Z","title":"CEM: A Data-Efficient Method for Large Language Models to Continue\n  Evolving From Mistakes","summary":"  As world knowledge advances and new task schemas emerge, Continual Learning\n(CL) becomes essential for keeping Large Language Models (LLMs) current and\naddressing their shortcomings. This process typically involves continual\ninstruction tuning (CIT) and continual pre-training (CPT) to enable these\nmodels to adapt to novel tasks and acquire critical knowledge. However,\ncollecting sufficient CPT data and efficiently bridging knowledge gaps remain\nsignificant challenges. Inspired by the 'summarizing mistakes' strategy, we\npropose the Continue Evolving from Mistakes (CEM) method, a data-efficient\napproach aiming to collect CPT data and continually improve LLMs' performance\nthrough iterative evaluation and supplementation with mistake-relevant\nknowledge. To further optimize data usage and mitigate forgetting, we introduce\na novel training paradigm that combines CIT and CPT. Experiments show that CEM\nsubstantially enhances multiple models' performance on both in-domain and\nout-of-domain QA tasks, achieving gains of up to 29.63%. Code and datasets are\navailable on https://anonymous.4open.science/r/cem-BB25.\n","authors":["Haokun Zhao","Haixia Han","Jie Shi","Chengyu Du","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.08707v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11664v1","updated":"2024-12-16T11:12:45Z","published":"2024-12-16T11:12:45Z","title":"C3oT: Generating Shorter Chain-of-Thought without Compromising\n  Effectiveness","summary":"  Generating Chain-of-Thought (CoT) before deriving the answer can effectively\nimprove the reasoning capabilities of large language models (LLMs) and\nsignificantly improve the accuracy of the generated answer. However, in most\ncases, the length of the generated CoT is much longer than the desired final\nanswer, which results in additional decoding costs. Furthermore, existing\nresearch has discovered that shortening the reasoning steps in CoT, even while\npreserving the key information, diminishes LLMs' abilities. These phenomena\nmake it difficult to use LLMs and CoT in many real-world applications that only\nrequire the final answer and are sensitive to latency, such as search and\nrecommendation. To reduce the costs of model decoding and shorten the length of\nthe generated CoT, this paper presents $\\textbf{C}$onditioned\n$\\textbf{C}$ompressed $\\textbf{C}$hain-of-$\\textbf{T}$hought (C3oT), a CoT\ncompression framework that involves a compressor to compress an original longer\nCoT into a shorter CoT while maintaining key information and interpretability,\na conditioned training method to train LLMs with both longer CoT and shorter\nCoT simultaneously to learn the corresponding relationships between them, and a\nconditioned inference method to gain the reasoning ability learned from longer\nCoT by generating shorter CoT. We conduct experiments over four datasets from\narithmetic and commonsense scenarios, showing that the proposed method is\ncapable of compressing the length of generated CoT by up to more than 50%\nwithout compromising its effectiveness.\n","authors":["Yu Kang","Xianghui Sun","Liangyu Chen","Wei Zou"],"pdf_url":"https://arxiv.org/pdf/2412.11664v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2402.01704v3","updated":"2024-12-16T11:03:31Z","published":"2024-01-24T22:22:00Z","title":"Steering Language Models with Game-Theoretic Solvers","summary":"  Mathematical models of interactions among rational agents have long been\nstudied in game theory. However these interactions are often over a small set\nof discrete game actions which is very different from how humans communicate in\nnatural language. To bridge this gap, we introduce a framework that allows\nequilibrium solvers to work over the space of natural language dialogue\ngenerated by large language models (LLMs). Specifically, by modelling the\nplayers, strategies and payoffs in a \"game\" of dialogue, we create a binding\nfrom natural language interactions to the conventional symbolic logic of game\ntheory. Given this binding, we can ask existing game-theoretic algorithms to\nprovide us with strategic solutions (e.g., what string an LLM should generate\nto maximize payoff in the face of strategic partners or opponents), giving us\npredictors of stable, rational conversational strategies. We focus on three\ndomains that require different negotiation strategies: scheduling meetings,\ntrading fruit and debate, and evaluate an LLM's generated language when guided\nby solvers. We see that LLMs that follow game-theory solvers result in dialogue\ngenerations that are less exploitable than the control (no guidance from\nsolvers), and the language generated results in higher rewards, in all\nnegotiation domains. We discuss future implications of this work, and how\ngame-theoretic solvers that can leverage the expressivity of natural language\ncan open up a new avenue of guiding language research.\n","authors":["Ian Gemp","Roma Patel","Yoram Bachrach","Marc Lanctot","Vibhavari Dasagi","Luke Marris","Georgios Piliouras","Siqi Liu","Karl Tuyls"],"pdf_url":"https://arxiv.org/pdf/2402.01704v3.pdf","comment":"Code available @\n  https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py"},{"id":"http://arxiv.org/abs/2412.11653v1","updated":"2024-12-16T10:54:57Z","published":"2024-12-16T10:54:57Z","title":"Self-Adaptive Paraphrasing and Preference Learning for Improved Claim\n  Verifiability","summary":"  In fact-checking, structure and phrasing of claims critically influence a\nmodel's ability to predict verdicts accurately. Social media content in\nparticular rarely serves as optimal input for verification systems, which\nnecessitates pre-processing to extract the claim from noisy context before fact\nchecking. Prior work suggests extracting a claim representation that humans\nfind to be checkworthy and verifiable. This has two limitations: (1) the format\nmay not be optimal for a fact-checking model, and (2), it requires annotated\ndata to learn the extraction task from. We address both issues and propose a\nmethod to extract claims that is not reliant on labeled training data. Instead,\nour self-adaptive approach only requires a black-box fact checking model and a\ngenerative language model (LM). Given a tweet, we iteratively optimize the LM\nto generate a claim paraphrase that increases the performance of a fact\nchecking model. By learning from preference pairs, we align the LM to the fact\nchecker using direct preference optimization. We show that this novel setup\nextracts a claim paraphrase that is more verifiable than their original social\nmedia formulations, and is on par with competitive baselines. For refuted\nclaims, our method consistently outperforms all baselines.\n","authors":["Amelie Wührl","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2412.11653v1.pdf","comment":"Under review at ACL ARR"},{"id":"http://arxiv.org/abs/2412.11652v1","updated":"2024-12-16T10:53:24Z","published":"2024-12-16T10:53:24Z","title":"SE-GCL: An Event-Based Simple and Effective Graph Contrastive Learning\n  for Text Representation","summary":"  Text representation learning is significant as the cornerstone of natural\nlanguage processing. In recent years, graph contrastive learning (GCL) has been\nwidely used in text representation learning due to its ability to represent and\ncapture complex text information in a self-supervised setting. However, current\nmainstream graph contrastive learning methods often require the incorporation\nof domain knowledge or cumbersome computations to guide the data augmentation\nprocess, which significantly limits the application efficiency and scope of\nGCL. Additionally, many methods learn text representations only by constructing\nword-document relationships, which overlooks the rich contextual semantic\ninformation in the text. To address these issues and exploit representative\ntextual semantics, we present an event-based, simple, and effective graph\ncontrastive learning (SE-GCL) for text representation. Precisely, we extract\nevent blocks from text and construct internal relation graphs to represent\ninter-semantic interconnections, which can ensure that the most critical\nsemantic information is preserved. Then, we devise a streamlined, unsupervised\ngraph contrastive learning framework to leverage the complementary nature of\nthe event semantic and structural information for intricate feature data\ncapture. In particular, we introduce the concept of an event skeleton for core\nrepresentation semantics and simplify the typically complex data augmentation\ntechniques found in existing graph contrastive learning to boost algorithmic\nefficiency. We employ multiple loss functions to prompt diverse embeddings to\nconverge or diverge within a confined distance in the vector space, ultimately\nachieving a harmonious equilibrium. We conducted experiments on the proposed\nSE-GCL on four standard data sets (AG News, 20NG, SougouNews, and THUCNews) to\nverify its effectiveness in text representation learning.\n","authors":["Tao Meng","Wei Ai","Jianbin Li","Ze Wang","Yuntao Shou","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2412.11652v1.pdf","comment":"19 pages, 6 tables"},{"id":"http://arxiv.org/abs/2411.10666v3","updated":"2024-12-16T10:48:28Z","published":"2024-11-16T02:02:49Z","title":"SAM Decoding: Speculative Decoding via Suffix Automaton","summary":"  Speculative decoding (SD) has been demonstrated as an effective technique for\nlossless LLM inference acceleration. Retrieval-based SD methods, one kind of\nmodel-free method, have yielded promising speedup, but they often rely on\nincomplete retrieval resources, inefficient retrieval methods, and are\nconstrained to certain domains. This paper presents a novel retrieval-based\nspeculative decoding method that adapts suffix automaton (SAM) for efficient\nand accurate draft generation by utilizing common text corpus and dynamic text\nsequence. Unlike existing $n$-gram matching methods, SAM-Decoding finds the\nexact longest suffix match, achieving an average time complexity of O(1) per\ngeneration step of SAM update and suffix retrieval. It can also integrate with\nexisting methods, adaptively selecting a draft generation strategy based on\nmatch length to generalize to broader domains. Extensive experiments on\nSpec-Bench show that our method is $18\\%+$ faster than other retrieval-based SD\nmethods. Additionally, when combined with advanced EAGLE-2, it provides an\nadditional speedup of $3.28\\%$ -- $11.13\\%$ across various-sized LLM backbones.\nOur code is available at our\n\\href{https://github.com/hyx1999/SAM-Decoding}{repository}.\n","authors":["Yuxuan Hu","Ke Wang","Xiaokang Zhang","Fanjin Zhang","Cuiping Li","Hong Chen","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.10666v3.pdf","comment":"16 pages, 9 figures, 9 tables"},{"id":"http://arxiv.org/abs/2412.08049v2","updated":"2024-12-16T10:31:03Z","published":"2024-12-11T02:55:00Z","title":"M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified\n  Sentiment and Emotion Analysis","summary":"  Sentiment analysis and emotion recognition are crucial for applications such\nas human-computer interaction and depression detection. Traditional unimodal\nmethods often fail to capture the complexity of emotional expressions due to\nconflicting signals from different modalities. Current Multimodal Large\nLanguage Models (MLLMs) also face challenges in detecting subtle facial\nexpressions and addressing a wide range of emotion-related tasks. To tackle\nthese issues, we propose M2SE, a Multistage Multitask Sentiment and Emotion\nInstruction Tuning Strategy for general-purpose MLLMs. It employs a combined\napproach to train models on tasks such as multimodal sentiment analysis,\nemotion recognition, facial expression recognition, emotion reason inference,\nand emotion cause-pair extraction. We also introduce the Emotion Multitask\ndataset (EMT), a custom dataset that supports these five tasks. Our model,\nEmotion Universe (EmoVerse), is built on a basic MLLM framework without\nmodifications, yet it achieves substantial improvements across these tasks when\ntrained with the M2SE strategy. Extensive experiments demonstrate that EmoVerse\noutperforms existing methods, achieving state-of-the-art results in sentiment\nand emotion tasks. These results highlight the effectiveness of M2SE in\nenhancing multimodal emotion perception. The dataset and code are available at\nhttps://github.com/xiaoyaoxinyi/M2SE.\n","authors":["Ao Li","Longwei Xu","Chen Ling","Jinghui Zhang","Pengwei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11637v1","updated":"2024-12-16T10:26:11Z","published":"2024-12-16T10:26:11Z","title":"On Crowdsourcing Task Design for Discourse Relation Annotation","summary":"  Interpreting implicit discourse relations involves complex reasoning,\nrequiring the integration of semantic cues with background knowledge, as overt\nconnectives like because or then are absent. These relations often allow\nmultiple interpretations, best represented as distributions. In this study, we\ncompare two established methods that crowdsource English implicit discourse\nrelation annotation by connective insertion: a free-choice approach, which\nallows annotators to select any suitable connective, and a forced-choice\napproach, which asks them to select among a set of predefined options.\nSpecifically, we re-annotate the whole DiscoGeM 1.0 corpus -- initially\nannotated with the free-choice method -- using the forced-choice approach. The\nfree-choice approach allows for flexible and intuitive insertion of various\nconnectives, which are context-dependent. Comparison among over 130,000\nannotations, however, shows that the free-choice strategy produces less diverse\nannotations, often converging on common labels. Analysis of the results reveals\nthe interplay between task design and the annotators' abilities to interpret\nand produce discourse relations.\n","authors":["Frances Yung","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2412.11637v1.pdf","comment":"To appear in the workshop of Context and Meaning - Navigating\n  Disagreements in NLP Annotations"},{"id":"http://arxiv.org/abs/2412.11625v1","updated":"2024-12-16T10:10:27Z","published":"2024-12-16T10:10:27Z","title":"Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods","summary":"  While Large Language Models (LLMs) have become central tools in various\nfields, they often provide inaccurate or false information. This study examines\nuser preferences regarding falsehood responses from LLMs. Specifically, we\nevaluate preferences for LLM responses where false statements are explicitly\nmarked versus unmarked responses and preferences for confident falsehoods\ncompared to LLM disclaimers acknowledging a lack of knowledge. Additionally, we\ninvestigate how requiring users to assess the truthfulness of statements\ninfluences these preferences.\n  Surprisingly, 61\\% of users prefer unmarked falsehood responses over marked\nones, and 69\\% prefer confident falsehoods over LLMs admitting lack of\nknowledge. In all our experiments, a total of 300 users participated,\ncontributing valuable data to our analysis and conclusions. When users are\nrequired to evaluate the truthfulness of statements, preferences for unmarked\nand falsehood responses decrease slightly but remain high. These findings\nsuggest that user preferences, which influence LLM training via feedback\nmechanisms, may inadvertently encourage the generation of falsehoods. Future\nresearch should address the ethical and practical implications of aligning LLM\nbehavior with such preferences.\n","authors":["Diana Bar-Or Nirman","Ariel Weizman","Amos Azaria"],"pdf_url":"https://arxiv.org/pdf/2412.11625v1.pdf","comment":"11 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2409.12060v2","updated":"2024-12-16T10:09:23Z","published":"2024-09-18T15:33:48Z","title":"PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase\n  Detection Models","summary":"  The task of determining whether two texts are paraphrases has long been a\nchallenge in NLP. However, the prevailing notion of paraphrase is often quite\nsimplistic, offering only a limited view of the vast spectrum of paraphrase\nphenomena. Indeed, we find that evaluating models in a paraphrase dataset can\nleave uncertainty about their true semantic understanding. To alleviate this,\nwe create PARAPHRASUS, a benchmark designed for multi-dimensional assessment,\nbenchmarking and selection of paraphrase detection models. We find that\nparaphrase detection models under our fine-grained evaluation lens exhibit\ntrade-offs that cannot be captured through a single classification dataset.\nFurthermore, PARAPHRASUS allows prompt calibration for different use cases,\ntailoring LLM models to specific strictness levels. PARAPHRASUS includes 3\nchallenges spanning over 10 datasets, including 8 repurposed and 2 newly\nannotated; we release it along with a benchmarking library at\nhttps://github.com/impresso/paraphrasus\n","authors":["Andrianos Michail","Simon Clematide","Juri Opitz"],"pdf_url":"https://arxiv.org/pdf/2409.12060v2.pdf","comment":"to appear at COLING2025"},{"id":"http://arxiv.org/abs/2412.06926v2","updated":"2024-12-16T10:08:19Z","published":"2024-12-09T19:11:54Z","title":"When Every Token Counts: Optimal Segmentation for Low-Resource Language\n  Models","summary":"  Traditional greedy tokenization methods have been a critical step in Natural\nLanguage Processing (NLP), influencing how text is converted into tokens and\ndirectly impacting model performance. While subword tokenizers like Byte-Pair\nEncoding (BPE) are widely used, questions remain about their optimality across\nmodel scales and languages. In this work, we demonstrate through extensive\nexperiments that an optimal BPE configuration significantly reduces token count\ncompared to greedy segmentation, yielding improvements in token-saving\npercentages and performance benefits, particularly for smaller models. We\nevaluate tokenization performance across various intrinsic and extrinsic tasks,\nincluding generation and classification. Our findings suggest that\ncompression-optimized tokenization strategies could provide substantial\nadvantages for multilingual and low-resource language applications,\nhighlighting a promising direction for further research and inclusive NLP.\n","authors":["Bharath Raj S","Garvit Suri","Vikrant Dewangan","Raghav Sonavane"],"pdf_url":"https://arxiv.org/pdf/2412.06926v2.pdf","comment":"LoResLM @ COLING 2025"},{"id":"http://arxiv.org/abs/2412.11615v1","updated":"2024-12-16T09:57:28Z","published":"2024-12-16T09:57:28Z","title":"MT-LENS: An all-in-one Toolkit for Better Machine Translation Evaluation","summary":"  We introduce MT-LENS, a framework designed to evaluate Machine Translation\n(MT) systems across a variety of tasks, including translation quality, gender\nbias detection, added toxicity, and robustness to misspellings. While several\ntoolkits have become very popular for benchmarking the capabilities of Large\nLanguage Models (LLMs), existing evaluation tools often lack the ability to\nthoroughly assess the diverse aspects of MT performance. MT-LENS addresses\nthese limitations by extending the capabilities of LM-eval-harness for MT,\nsupporting state-of-the-art datasets and a wide range of evaluation metrics. It\nalso offers a user-friendly platform to compare systems and analyze\ntranslations with interactive visualizations. MT-LENS aims to broaden access to\nevaluation strategies that go beyond traditional translation quality\nevaluation, enabling researchers and engineers to better understand the\nperformance of a NMT model and also easily measure system's biases.\n","authors":["Javier García Gilabert","Carlos Escolano","Audrey Mash","Xixian Liao","Maite Melero"],"pdf_url":"https://arxiv.org/pdf/2412.11615v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.11605v1","updated":"2024-12-16T09:47:43Z","published":"2024-12-16T09:47:43Z","title":"SPaR: Self-Play with Tree-Search Refinement to Improve\n  Instruction-Following in Large Language Models","summary":"  Instruction-following is a fundamental capability of language models,\nrequiring the model to recognize even the most subtle requirements in the\ninstructions and accurately reflect them in its output. Such an ability is\nwell-suited for and often optimized by preference learning. However, existing\nmethods often directly sample multiple independent responses from the model\nwhen creating preference pairs. Such practice can introduce content variations\nirrelevant to whether the instruction is precisely followed (e.g., different\nexpressions about the same semantic), interfering with the goal of teaching\nmodels to recognize the key differences that lead to improved instruction\nfollowing. In light of this, we introduce SPaR, a self-play framework\nintegrating tree-search self-refinement to yield valid and comparable\npreference pairs free from distractions. By playing against itself, an LLM\nemploys a tree-search strategy to refine its previous responses with respect to\nthe instruction while minimizing unnecessary variations. Our experiments show\nthat a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses\nGPT-4-Turbo on the IFEval benchmark without losing general capabilities.\nFurthermore, SPaR demonstrates promising scalability and transferability,\ngreatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how\ninference scaling in tree search would impact model performance. Our code and\ndata are publicly available at https://github.com/thu-coai/SPaR.\n","authors":["Jiale Cheng","Xiao Liu","Cunxiang Wang","Xiaotao Gu","Yida Lu","Dan Zhang","Yuxiao Dong","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09014v3","updated":"2024-12-16T09:02:51Z","published":"2024-12-12T07:24:16Z","title":"Improvement in Sign Language Translation Using Text CTC Alignment","summary":"  Current sign language translation (SLT) approaches often rely on gloss-based\nsupervision with Connectionist Temporal Classification (CTC), limiting their\nability to handle non-monotonic alignments between sign language video and\nspoken text. In this work, we propose a novel method combining joint\nCTC/Attention and transfer learning. The joint CTC/Attention introduces\nhierarchical encoding and integrates CTC with the attention mechanism during\ndecoding, effectively managing both monotonic and non-monotonic alignments.\nMeanwhile, transfer learning helps bridge the modality gap between vision and\nlanguage in SLT. Experimental results on two widely adopted benchmarks,\nRWTH-PHOENIX-Weather 2014 T and CSL-Daily, show that our method achieves\nresults comparable to state-of-the-art and outperforms the pure-attention\nbaseline. Additionally, this work opens a new door for future research into\ngloss-free SLT using text-based CTC alignment.\n","authors":["Sihan Tan","Taro Miyazaki","Nabeela Khan","Kazuhiro Nakadai"],"pdf_url":"https://arxiv.org/pdf/2412.09014v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15395v2","updated":"2024-12-16T08:57:29Z","published":"2023-12-24T03:37:11Z","title":"Prompt Valuation Based on Shapley Values","summary":"  Large language models (LLMs) excel on new tasks without additional training,\nsimply by providing natural language prompts that demonstrate how the task\nshould be performed. Prompt ensemble methods comprehensively harness the\nknowledge of LLMs while mitigating individual biases and errors and further\nenhancing performance. However, more prompts do not necessarily lead to better\nresults, and not all prompts are beneficial. A small number of high-quality\nprompts often outperform many low-quality prompts. Currently, there is a lack\nof a suitable method for evaluating the impact of prompts on the results. In\nthis paper, we utilize the Shapley value to fairly quantify the contributions\nof prompts, helping to identify beneficial or detrimental prompts, and\npotentially guiding prompt valuation in data markets. Through extensive\nexperiments employing various ensemble methods and utility functions on diverse\ntasks, we validate the effectiveness of using the Shapley value method for\nprompts as it effectively distinguishes and quantifies the contributions of\neach prompt.\n","authors":["Hanxi Liu","Xiaokai Mao","Haocheng Xia","Jian Lou","Jinfei Liu","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2312.15395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11567v1","updated":"2024-12-16T08:54:21Z","published":"2024-12-16T08:54:21Z","title":"AUEB-Archimedes at RIRAG-2025: Is obligation concatenation really all\n  you need?","summary":"  This paper presents the systems we developed for RIRAG-2025, a shared task\nthat requires answering regulatory questions by retrieving relevant passages.\nThe generated answers are evaluated using RePASs, a reference-free and\nmodel-based metric. Our systems use a combination of three retrieval models and\na reranker. We show that by exploiting a neural component of RePASs that\nextracts important sentences ('obligations') from the retrieved passages, we\nachieve a dubiously high score (0.947), even though the answers are directly\nextracted from the retrieved passages and are not actually generated answers.\nWe then show that by selecting the answer with the best RePASs among a few\ngenerated alternatives and then iteratively refining this answer by reducing\ncontradictions and covering more obligations, we can generate readable,\ncoherent answers that achieve a more plausible and relatively high score\n(0.639).\n","authors":["Ioannis Chasandras","Odysseas S. Chlapanis","Ion Androutsopoulos"],"pdf_url":"https://arxiv.org/pdf/2412.11567v1.pdf","comment":"RIRAG 2025 Shared-Task at RegNLP workshop collocated with COLING 2025"},{"id":"http://arxiv.org/abs/2412.11560v1","updated":"2024-12-16T08:46:53Z","published":"2024-12-16T08:46:53Z","title":"The Role of Natural Language Processing Tasks in Automatic Literary\n  Character Network Construction","summary":"  The automatic extraction of character networks from literary texts is\ngenerally carried out using natural language processing (NLP) cascading\npipelines. While this approach is widespread, no study exists on the impact of\nlow-level NLP tasks on their performance. In this article, we conduct such a\nstudy on a literary dataset, focusing on the role of named entity recognition\n(NER) and coreference resolution when extracting co-occurrence networks. To\nhighlight the impact of these tasks' performance, we start with gold-standard\nannotations, progressively add uniformly distributed errors, and observe their\nimpact in terms of character network quality. We demonstrate that NER\nperformance depends on the tested novel and strongly affects character\ndetection. We also show that NER-detected mentions alone miss a lot of\ncharacter co-occurrences, and that coreference resolution is needed to prevent\nthis. Finally, we present comparison points with 2 methods based on large\nlanguage models (LLMs), including a fully end-to-end one, and show that these\nmodels are outperformed by traditional NLP pipelines in terms of recall.\n","authors":["Arthur Amalvy","Vincent Labatut","Richard Dufour"],"pdf_url":"https://arxiv.org/pdf/2412.11560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06561v4","updated":"2024-12-16T08:43:24Z","published":"2024-01-12T13:15:05Z","title":"Intention Analysis Makes LLMs A Good Jailbreak Defender","summary":"  Aligning large language models (LLMs) with human values, particularly when\nfacing complex and stealthy jailbreak attacks, presents a formidable challenge.\nUnfortunately, existing methods often overlook this intrinsic nature of\njailbreaks, which limits their effectiveness in such complex scenarios. In this\nstudy, we present a simple yet highly effective defense strategy, i.e.,\nIntention Analysis ($\\mathbb{IA}$). $\\mathbb{IA}$ works by triggering LLMs'\ninherent self-correct and improve ability through a two-stage process: 1)\nanalyzing the essential intention of the user input, and 2) providing final\npolicy-aligned responses based on the first round conversation. Notably,\n$\\mathbb{IA}$ is an inference-only method, thus could enhance LLM safety\nwithout compromising their helpfulness. Extensive experiments on varying\njailbreak benchmarks across a wide range of LLMs show that $\\mathbb{IA}$ could\nconsistently and significantly reduce the harmfulness in responses (averagely\n-48.2% attack success rate). Encouragingly, with our $\\mathbb{IA}$, Vicuna-7B\neven outperforms GPT-3.5 regarding attack success rate. We empirically\ndemonstrate that, to some extent, $\\mathbb{IA}$ is robust to errors in\ngenerated intentions. Further analyses reveal the underlying principle of\n$\\mathbb{IA}$: suppressing LLM's tendency to follow jailbreak prompts, thereby\nenhancing safety.\n","authors":["Yuqi Zhang","Liang Ding","Lefei Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2401.06561v4.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2412.11556v1","updated":"2024-12-16T08:42:00Z","published":"2024-12-16T08:42:00Z","title":"Token Prepending: A Training-Free Approach for Eliciting Better Sentence\n  Embeddings from LLMs","summary":"  Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost.\n","authors":["Yuchen Fu","Zifeng Cheng","Zhiwei Jiang","Zhonghui Wang","Yafeng Yin","Zhengliang Li","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2412.11556v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.11543v1","updated":"2024-12-16T08:23:50Z","published":"2024-12-16T08:23:50Z","title":"Error Diversity Matters: An Error-Resistant Ensemble Method for\n  Unsupervised Dependency Parsing","summary":"  We address unsupervised dependency parsing by building an ensemble of diverse\nexisting models through post hoc aggregation of their output dependency parse\nstructures. We observe that these ensembles often suffer from low robustness\nagainst weak ensemble components due to error accumulation. To tackle this\nproblem, we propose an efficient ensemble-selection approach that avoids error\naccumulation. Results demonstrate that our approach outperforms each individual\nmodel as well as previous ensemble techniques. Additionally, our experiments\nshow that the proposed ensemble-selection method significantly enhances the\nperformance and robustness of our ensemble, surpassing previously proposed\nstrategies, which have not accounted for error diversity.\n","authors":["Behzad Shayegh","Hobie H. -B. Lee","Xiaodan Zhu","Jackie Chi Kit Cheung","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2412.11543v1.pdf","comment":"Accepted by the AAAI Conference on Artificial Intelligence (AAAI)\n  2025"},{"id":"http://arxiv.org/abs/2406.15734v2","updated":"2024-12-16T08:19:26Z","published":"2024-06-22T04:52:58Z","title":"RankAdaptor: Hierarchical Rank Allocation for Efficient Fine-Tuning\n  Pruned LLMs via Performance Model","summary":"  The efficient compression of large language models (LLMs) has become\nincreasingly popular. However, recovering the performance of compressed LLMs\nremains a major challenge. The current practice in LLM compression entails the\nimplementation of structural pruning, complemented by a recovery phase that\nleverages the Low-Rank Adaptation (LoRA) algorithm. Structural pruning's uneven\nmodification of model architecture, coupled with standard LoRA's fixed\nconfiguration allocation across layers in an online pipeline, leads to\nsuboptimal performance in various downstream tasks for pruned models. To\naddress this challenge, we introduce RankAdaptor, a hierarchical rank\nallocation method that enables efficient fine-tuning of pruned LLMs according\nto layerwise specific recovery requirements. We employ a performance model that\nconducts offline meta-learning and online incremental learning to explore\noptimal rank values for each layer. Comprehensive experiments on popular\nbenchmarks show that RankAdaptor consistently outperforms state-of-the-art\nmethods across a variety of pruning settings and LLM architectures, with\nimprovements ranging from 0.7\\% to 5.5\\%.\n","authors":["Changhai Zhou","Shijie Han","Lining Yang","Yuhua Zhou","Xu Cheng","Yibin Wang","Hongguang Li"],"pdf_url":"https://arxiv.org/pdf/2406.15734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05299v2","updated":"2024-12-16T08:17:09Z","published":"2024-11-25T07:48:31Z","title":"Specifications: The missing link to making the development of LLM\n  systems an engineering discipline","summary":"  Despite the significant strides made by generative AI in just a few short\nyears, its future progress is constrained by the challenge of building modular\nand robust systems. This capability has been a cornerstone of past\ntechnological revolutions, which relied on combining components to create\nincreasingly sophisticated and reliable systems. Cars, airplanes, computers,\nand software consist of components-such as engines, wheels, CPUs, and\nlibraries-that can be assembled, debugged, and replaced. A key tool for\nbuilding such reliable and modular systems is specification: the precise\ndescription of the expected behavior, inputs, and outputs of each component.\nHowever, the generality of LLMs and the inherent ambiguity of natural language\nmake defining specifications for LLM-based components (e.g., agents) both a\nchallenging and urgent problem. In this paper, we discuss the progress the\nfield has made so far-through advances like structured outputs, process\nsupervision, and test-time compute-and outline several future directions for\nresearch to enable the development of modular and reliable LLM-based systems\nthrough improved specifications.\n","authors":["Ion Stoica","Matei Zaharia","Joseph Gonzalez","Ken Goldberg","Koushik Sen","Hao Zhang","Anastasios Angelopoulos","Shishir G. Patil","Lingjiao Chen","Wei-Lin Chiang","Jared Q. Davis"],"pdf_url":"https://arxiv.org/pdf/2412.05299v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11538v1","updated":"2024-12-16T08:15:19Z","published":"2024-12-16T08:15:19Z","title":"Towards a Speech Foundation Model for Singapore and Beyond","summary":"  This technical report describes the MERaLiON Speech Encoder, a foundation\nmodel designed to support a wide range of downstream speech applications.\nDeveloped as part of Singapore's National Multimodal Large Language Model\nProgramme, the MERaLiON Speech Encoder is tailored to address the speech\nprocessing needs in Singapore and the surrounding Southeast Asian region. The\nmodel currently supports mainly English, including the variety spoken in\nSingapore. We are actively expanding our datasets to gradually cover other\nlanguages in subsequent releases. The MERaLiON Speech Encoder was pre-trained\nfrom scratch on 200K hours of unlabelled speech data using a self-supervised\nlearning approach based on masked language modelling. We describe our training\nprocedure and hyperparameter tuning experiments in detail below. Our evaluation\ndemonstrates improvements to spontaneous and Singapore speech benchmarks for\nspeech recognition, while remaining competitive to other state-of-the-art\nspeech encoders across ten other speech tasks. We commit to releasing our\nmodel, supporting broader research endeavours, both in Singapore and beyond.\n","authors":["Muhammad Huzaifah","Tianchi Liu","Hardik B. Sailor","Kye Min Tan","Tarun K. Vangani","Qiongqiong Wang","Jeremy H. M. Wong","Nancy F. Chen","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2412.11538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11536v1","updated":"2024-12-16T08:13:14Z","published":"2024-12-16T08:13:14Z","title":"Let your LLM generate a few tokens and you will reduce the need for\n  retrieval","summary":"  In this paper, we investigate how efficiently large language models (LLM) can\nbe trained to check whether an answer is already stored in their parametric\nmemory. We distill an LLM-as-a-judge to compute the IK (I Know) score. We found\nthat this method is particularly beneficial in the context of\nretrieval-assisted augmented generation (RAG), with a respectable accuracy of\n80%. It enables a significant reduction (more than 50%) in the number of search\nand reranking steps required for certain data sets. We have also introduced the\nIK score, which serves as a useful tool for characterising datasets by\nfacilitating the classification task. Interestingly, through the inclusion of\nresponse tokens as input, our results suggest that only about 20,000 training\nsamples are required to achieve good performance. The central element of this\nwork is the use of a teacher model - the LLM as a judge - to generate training\ndata. We also assess the robustness of the IK classifier by evaluating it with\nvarious types of teachers, including both string-based methods and LLMs, with\nthe latter providing better results.\n","authors":["Hervé Déjean"],"pdf_url":"https://arxiv.org/pdf/2412.11536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18203v3","updated":"2024-12-16T08:12:17Z","published":"2024-11-27T10:28:57Z","title":"Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning","summary":"  Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n","authors":["Di Zhang","Junxian Li","Jingdi Lei","Xunzhi Wang","Yujie Liu","Zonglin Yang","Jiatong Li","Weida Wang","Suorong Yang","Jianbo Wu","Peng Ye","Wanli Ouyang","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.18203v3.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.14335v2","updated":"2024-12-16T08:08:51Z","published":"2024-09-22T06:43:40Z","title":"MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic\n  Post-Editing in LLM Translation Evaluators","summary":"  Large Language Models (LLMs) have shown significant potential as judges for\nMachine Translation (MT) quality assessment, providing both scores and\nfine-grained feedback. Although approaches such as GEMBA-MQM have shown\nstate-of-the-art performance on reference-free evaluation, the predicted errors\ndo not align well with those annotated by human, limiting their\ninterpretability as feedback signals. To enhance the quality of error\nannotations predicted by LLM evaluators, we introduce a universal and\ntraining-free framework, $\\textbf{MQM-APE}$, based on the idea of filtering out\nnon-impactful errors by Automatically Post-Editing (APE) the original\ntranslation based on each error, leaving only those errors that contribute to\nquality improvement. Specifically, we prompt the LLM to act as 1)\n$\\textit{evaluator}$ to provide error annotations, 2) $\\textit{post-editor}$ to\ndetermine whether errors impact quality improvement and 3) $\\textit{pairwise\nquality verifier}$ as the error filter. Experiments show that our approach\nconsistently improves both the reliability and quality of error spans against\nGEMBA-MQM, across eight LLMs in both high- and low-resource languages.\nOrthogonal to trained approaches, MQM-APE complements translation-specific\nevaluators such as Tower, highlighting its broad applicability. Further\nanalysis confirms the effectiveness of each module and offers valuable insights\ninto evaluator design and LLMs selection.\n","authors":["Qingyu Lu","Liang Ding","Kanjian Zhang","Jinxia Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2409.14335v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2409.03258v3","updated":"2024-12-16T08:06:27Z","published":"2024-09-05T05:34:16Z","title":"GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding","summary":"  Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes.\n","authors":["Yukun Cao","Shuo Han","Zengyi Gao","Zezhong Ding","Xike Xie","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.03258v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05583v2","updated":"2024-12-16T08:06:08Z","published":"2024-05-09T07:15:19Z","title":"OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems\n  and Evaluating the Factuality of Claims and LLMs","summary":"  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. Difficulties lie in assessing the factuality of free-form\nresponses in open domains. Also, different papers use disparate evaluation\nbenchmarks and measurements, which renders them hard to compare and hampers\nfuture progress. To mitigate these issues, we propose OpenFactCheck, a unified\nframework for building customized automatic fact-checking systems, benchmarking\ntheir accuracy, evaluating factuality of LLMs, and verifying claims in a\ndocument. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users\nto easily customize an automatic fact-checker and verify the factual\ncorrectness of documents and claims, (ii) LLMEVAL, a unified evaluation\nframework assesses LLM's factuality ability from various perspectives fairly,\nand (iii) CHECKEREVAL is an extensible solution for gauging the reliability of\nautomatic fact-checkers' verification results using human-annotated datasets.\nData and code are publicly available at\nhttps://github.com/yuxiaw/openfactcheck.\n","authors":["Yuxia Wang","Minghan Wang","Hasan Iqbal","Georgi Georgiev","Jiahui Geng","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2405.05583v2.pdf","comment":"22 pages, 8 tables, 11 figures"},{"id":"http://arxiv.org/abs/2409.11404v2","updated":"2024-12-16T07:57:08Z","published":"2024-09-17T17:59:25Z","title":"AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs","summary":"  Arabic, with its rich diversity of dialects, remains significantly\nunderrepresented in Large Language Models, particularly in dialectal\nvariations. We address this gap by introducing seven synthetic datasets in\ndialects alongside Modern Standard Arabic (MSA), created using Machine\nTranslation (MT) combined with human post-editing. We present AraDiCE, a\nbenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on\ndialect comprehension and generation, focusing specifically on low-resource\nArabic dialects. Additionally, we introduce the first-ever fine-grained\nbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, and\nLevant regions, providing a novel dimension to LLM evaluation. Our findings\ndemonstrate that while Arabic-specific models like Jais and AceGPT outperform\nmultilingual models on dialectal tasks, significant challenges persist in\ndialect identification, generation, and translation. This work contributes\n$\\approx$45K post-edited samples, a cultural benchmark, and highlights the\nimportance of tailored training to improve LLM performance in capturing the\nnuances of diverse Arabic dialects and cultural contexts. We have released the\ndialectal translation models and benchmarks developed in this study\n(https://huggingface.co/datasets/QCRI/AraDiCE).\n","authors":["Basel Mousi","Nadir Durrani","Fatema Ahmad","Md. Arid Hasan","Maram Hasanain","Tameem Kabbani","Fahim Dalvi","Shammur Absar Chowdhury","Firoj Alam"],"pdf_url":"https://arxiv.org/pdf/2409.11404v2.pdf","comment":"Benchmarking, Culturally Informed, Large Language Models, Arabic NLP,\n  LLMs, Arabic Dialect, Dialectal Benchmarking"},{"id":"http://arxiv.org/abs/2411.07037v2","updated":"2024-12-16T07:53:06Z","published":"2024-11-11T14:43:51Z","title":"LIFBench: Evaluating the Instruction Following Performance and Stability\n  of Large Language Models in Long-Context Scenarios","summary":"  As Large Language Models (LLMs) evolve in natural language processing (NLP),\ntheir ability to stably follow instructions in long-context inputs has become\ncritical for real-world applications. However, existing benchmarks seldom focus\non instruction-following in long-context scenarios or stability on different\ninputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed\nto evaluate LLMs' instruction-following capabilities and stability across long\ncontexts. LIFBench comprises three long-context scenarios and eleven diverse\ntasks, featuring 2,766 instructions generated through an automated expansion\nmethod across three dimensions: length, expression, and variables. For\nevaluation, we propose LIFEval, a rubric-based assessment method that enables\nprecise, automated scoring of complex LLM responses without reliance on\nLLM-assisted assessments or human judgment. This method allows for a\ncomprehensive analysis of model performance and stability from multiple\nperspectives. We conduct detailed experiments on 20 prominent LLMs across six\nlength intervals. Our work contributes LIFBench and LIFEval as robust tools for\nassessing LLM performance in complex and long-context settings, offering\nvaluable insights to guide future advancements in LLM development.\n","authors":["Xiaodong Wu","Minhao Wang","Yichen Liu","Xiaoming Shi","He Yan","Xiangju Lu","Junmin Zhu","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.07037v2.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.11517v1","updated":"2024-12-16T07:51:09Z","published":"2024-12-16T07:51:09Z","title":"DART: An AIGT Detector using AMR of Rephrased Text","summary":"  As large language models (LLMs) generate more human-like texts, concerns\nabout the side effects of AI-generated texts (AIGT) have grown. So, researchers\nhave developed methods for detecting AIGT. However, two challenges remain.\nFirst, the performance on detecting black-box LLMs is low, because existing\nmodels have focused on syntactic features. Second, most AIGT detectors have\nbeen tested on a single-candidate setting, which assumes that we know the\norigin of an AIGT and may deviate from the real-world scenario. To resolve\nthese challenges, we propose DART, which consists of four steps: rephrasing,\nsemantic parsing, scoring, and multiclass classification. We conducted several\nexperiments to test the performance of DART by following previous work. The\nexperimental result shows that DART can discriminate multiple black-box LLMs\nwithout using syntactic features and knowing the origin of AIGT.\n","authors":["Hyeonchu Park","Byungjun Kim","Bugeun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11517v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2412.11506v1","updated":"2024-12-16T07:28:36Z","published":"2024-12-16T07:28:36Z","title":"Glimpse: Enabling White-Box Methods to Use Proprietary Models for\n  Zero-Shot LLM-Generated Text Detection","summary":"  Advanced large language models (LLMs) can generate text almost\nindistinguishable from human-written text, highlighting the importance of\nLLM-generated text detection. However, current zero-shot techniques face\nchallenges as white-box methods are restricted to use weaker open-source LLMs,\nand black-box methods are limited by partial observation from stronger\nproprietary LLMs. It seems impossible to enable white-box methods to use\nproprietary models because API-level access to the models neither provides full\npredictive distributions nor inner embeddings. To traverse the divide, we\npropose Glimpse, a probability distribution estimation approach, predicting the\nfull distributions from partial observations. Despite the simplicity of\nGlimpse, we successfully extend white-box methods like Entropy, Rank, Log-Rank,\nand Fast-DetectGPT to latest proprietary models. Experiments show that Glimpse\nwith Fast-DetectGPT and GPT-3.5 achieves an average AUROC of about 0.95 in five\nlatest source models, improving the score by 51% relative to the remaining\nspace of the open source baseline (Table 1). It demonstrates that the latest\nLLMs can effectively detect their own outputs, suggesting that advanced LLMs\nmay be the best shield against themselves.\n","authors":["Guangsheng Bao","Yanbin Zhao","Juncai He","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11506v1.pdf","comment":"10 pages, 9 figures, 10 tables"},{"id":"http://arxiv.org/abs/2410.17657v2","updated":"2024-12-16T07:28:33Z","published":"2024-10-23T08:19:18Z","title":"ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents","summary":"  Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks.\n","authors":["Yusheng Liao","Shuyang Jiang","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17657v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2412.11500v1","updated":"2024-12-16T07:18:40Z","published":"2024-12-16T07:18:40Z","title":"Intention Knowledge Graph Construction for User Intention Relation\n  Modeling","summary":"  Understanding user intentions is challenging for online platforms. Recent\nwork on intention knowledge graphs addresses this but often lacks focus on\nconnecting intentions, which is crucial for modeling user behavior and\npredicting future actions. This paper introduces a framework to automatically\ngenerate an intention knowledge graph, capturing connections between user\nintentions. Using the Amazon m2 dataset, we construct an intention graph with\n351 million edges, demonstrating high plausibility and acceptance. Our model\neffectively predicts new session intentions and enhances product\nrecommendations, outperforming previous state-of-the-art methods and showcasing\nthe approach's practical utility.\n","authors":["Jiaxin Bai","Zhaobo Wang","Junfei Cheng","Dan Yu","Zerui Huang","Weiqi Wang","Xin Liu","Chen Luo","Qi He","Yanming Zhu","Bo Li","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2412.11500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11441v2","updated":"2024-12-16T07:18:06Z","published":"2024-02-18T03:36:26Z","title":"InfuserKI: Enhancing Large Language Models with Knowledge Graphs via\n  Infuser-Guided Knowledge Integration","summary":"  Large Language Models (LLMs) have achieved exceptional capabilities in open\ngeneration across various domains, yet they encounter difficulties with tasks\nthat require intensive knowledge. To address these challenges, methods for\nintegrating knowledge have been developed, which augment LLMs with\ndomain-specific knowledge graphs through external modules. These approaches,\nhowever, face data inefficiency issues as they necessitate the processing of\nboth known and unknown knowledge for fine-tuning. Thus, our research focuses on\na novel problem: efficiently integrating unknown knowledge into LLMs without\nunnecessary overlap of known knowledge. A risk of introducing new knowledge is\nthe potential forgetting of existing knowledge. To mitigate this risk, we\npropose the innovative {\\method} framework. This framework employs transformer\ninternal states to determine when to enrich LLM outputs with additional\ninformation, effectively preventing knowledge forgetting. Performance\nevaluations using the UMLS-2.5k and MetaQA domain knowledge graphs reveal that\n{\\method} not only successfully integrates new knowledge but also outperforms\nstate-of-the-art baselines, reducing knowledge forgetting by 9\\% and 6\\%,\nrespectively.\n","authors":["Fali Wang","Runxue Bao","Suhang Wang","Wenchao Yu","Yanchi Liu","Wei Cheng","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2402.11441v2.pdf","comment":"14 pages, 7 figures, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2402.16313v3","updated":"2024-12-16T07:11:59Z","published":"2024-02-26T05:31:34Z","title":"Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based\n  Question Answering","summary":"  Open-ended question answering requires models to find appropriate evidence to\nform wellreasoned, comprehensive and helpful answers. In practical\napplications, models also need to engage in extended discussions on potential\nscenarios closely relevant to the question. With augmentation of retrieval\nmodule, open-source Large Language Models (LLMs) can produce coherent answers\noften with different focuses, but are still sub-optimal in terms of reliable\nevidence selection and in-depth question analysis. In this paper, we propose a\nnovel Chain-ofDiscussion framework to leverage the synergy among multiple\nopen-source LLMs aiming to provide more correct and more comprehensive answers\nfor open-ended QA, although they are not strong enough individually. Our\nexperiments show that discussions among multiple LLMs play a vital role in\nenhancing the quality of answers.\n","authors":["Mingxu Tao","Dongyan Zhao","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2402.16313v3.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2412.11494v1","updated":"2024-12-16T07:09:46Z","published":"2024-12-16T07:09:46Z","title":"FTP: A Fine-grained Token-wise Pruner for Large Language Models via\n  Token Routing","summary":"  Recently, large language models (LLMs) have demonstrated superior performance\nacross various tasks by adhering to scaling laws, which significantly increase\nmodel size. However, the huge computation overhead during inference hinders the\ndeployment in industrial applications. Many works leverage traditional\ncompression approaches to boost model inference, but these always introduce\nadditional training costs to restore the performance and the pruning results\ntypically show noticeable performance drops compared to the original model when\naiming for a specific level of acceleration. To address these issues, we\npropose a fine-grained token-wise pruning approach for the LLMs, which presents\na learnable router to adaptively identify the less important tokens and skip\nthem across model blocks to reduce computational cost during inference. To\nconstruct the router efficiently, we present a search-based sparsity scheduler\nfor pruning sparsity allocation, a trainable router combined with our proposed\nfour low-dimensional factors as input and three proposed losses. We conduct\nextensive experiments across different benchmarks on different LLMs to\ndemonstrate the superiority of our method. Our approach achieves\nstate-of-the-art (SOTA) pruning results, surpassing other existing pruning\nmethods. For instance, our method outperforms BlockPruner and ShortGPT by\napproximately 10 points on both LLaMA2-7B and Qwen1.5-7B in accuracy retention\nat comparable token sparsity levels.\n","authors":["Zekai Li","Jintu Zheng","Ji Liu","Han Liu","Haowei Zhu","Zeping Li","Fuwei Yang","Haiduo Huang","Jinzhang Peng","Dong Li","Lu Tian","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2412.11494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13474v2","updated":"2024-12-16T07:06:42Z","published":"2024-09-20T13:05:07Z","title":"Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models","summary":"  Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/AlternatePreference-Optimization\n","authors":["Anmol Mekala","Vineeth Dorna","Shreya Dubey","Abhishek Lalwani","David Koleczek","Mukund Rungta","Sadid Hasan","Elita Lobo"],"pdf_url":"https://arxiv.org/pdf/2409.13474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09386v2","updated":"2024-12-16T06:58:49Z","published":"2024-08-18T07:06:57Z","title":"Game Development as Human-LLM Interaction","summary":"  Game development is a highly specialized task that relies on a complex game\nengine powered by complex programming languages, preventing many gaming\nenthusiasts from handling it. This paper introduces the Chat Game Engine\n(ChatGE) powered by LLM, which allows everyone to develop a custom game using\nnatural language through Human-LLM interaction. To enable an LLM to function as\na ChatGE, we instruct it to perform the following processes in each turn: (1)\n$P_{script}$: configure the game script segment based on the user's input; (2)\n$P_{code}$: generate the corresponding code snippet based on the game script\nsegment; (3) $P_{utter}$: interact with the user, including guidance and\nfeedback. We propose a data synthesis pipeline based on LLM to generate game\nscript-code pairs and interactions from a few manually crafted seed data. We\npropose a three-stage progressive training strategy to transfer the\ndialogue-based LLM to our ChatGE smoothly. We construct a ChatGE for poker\ngames as a case study and comprehensively evaluate it from two perspectives:\ninteraction quality and code correctness.\n","authors":["Jiale Hong","Hongqiu Wu","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.09386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11102v2","updated":"2024-12-16T06:50:20Z","published":"2024-06-16T23:42:11Z","title":"Grading Massive Open Online Courses Using Large Language Models","summary":"  Massive open online courses (MOOCs) offer free education globally. Despite\nthis democratization of learning, the massive enrollment in these courses makes\nit impractical for an instructor to assess every student's writing assignment.\nAs a result, peer grading, often guided by a straightforward rubric, is the\nmethod of choice. While convenient, peer grading often falls short in terms of\nreliability and validity. In this study, we explore the feasibility of using\nlarge language models (LLMs) to replace peer grading in MOOCs. To this end, we\nadapt the zero-shot chain-of-thought (ZCoT) prompting technique to automate the\nfeedback process once the LLM assigns a score to an assignment. Specifically,\nto instruct LLMs for grading, we use three distinct prompts based on ZCoT: (1)\nZCoT with instructor-provided correct answers, (2) ZCoT with both\ninstructor-provided correct answers and rubrics, and (3) ZCoT with\ninstructor-provided correct answers and LLM-generated rubrics. We tested these\nprompts in 18 different scenarios using two LLMs, GPT-4 and GPT-3.5, across\nthree MOOCs: Introductory Astronomy, Astrobiology, and the History and\nPhilosophy of Astronomy. Our results show that ZCoT, when augmented with\ninstructor-provided correct answers and rubrics, produces grades that are more\naligned with those assigned by instructors compared to peer grading. Finally,\nour findings indicate a promising potential for automated grading systems in\nMOOCs, especially in subjects with well-defined rubrics, to improve the\nlearning experience for millions of online learners worldwide.\n","authors":["Shahriar Golchin","Nikhil Garuda","Christopher Impey","Matthew Wenger"],"pdf_url":"https://arxiv.org/pdf/2406.11102v2.pdf","comment":"Final version; accepted at COLING 2025"},{"id":"http://arxiv.org/abs/2412.11477v1","updated":"2024-12-16T06:44:39Z","published":"2024-12-16T06:44:39Z","title":"NoteContrast: Contrastive Language-Diagnostic Pretraining for Medical\n  Text","summary":"  Accurate diagnostic coding of medical notes is crucial for enhancing patient\ncare, medical research, and error-free billing in healthcare organizations.\nManual coding is a time-consuming task for providers, and diagnostic codes\noften exhibit low sensitivity and specificity, whereas the free text in medical\nnotes can be a more precise description of a patients status. Thus, accurate\nautomated diagnostic coding of medical notes has become critical for a learning\nhealthcare system. Recent developments in long-document transformer\narchitectures have enabled attention-based deep-learning models to adjudicate\nmedical notes. In addition, contrastive loss functions have been used to\njointly pre-train large language and image models with noisy labels. To further\nimprove the automated adjudication of medical notes, we developed an approach\nbased on i) models for ICD-10 diagnostic code sequences using a large\nreal-world data set, ii) large language models for medical notes, and iii)\ncontrastive pre-training to build an integrated model of both ICD-10 diagnostic\ncodes and corresponding medical text. We demonstrate that a contrastive\napproach for pre-training improves performance over prior state-of-the-art\nmodels for the MIMIC-III-50, MIMIC-III-rare50, and MIMIC-III-full diagnostic\ncoding tasks.\n","authors":["Prajwal Kailas","Max Homilius","Rahul C. Deo","Calum A. MacRae"],"pdf_url":"https://arxiv.org/pdf/2412.11477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17679v2","updated":"2024-12-16T06:40:32Z","published":"2024-11-26T18:44:39Z","title":"Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning","summary":"  Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness.\n","authors":["Zhu Xu","Zhiqiang Zhao","Zihan Zhang","Yuchi Liu","Quanwei Shen","Fei Liu","Yu Kuang","Jian He","Conglin Liu"],"pdf_url":"https://arxiv.org/pdf/2411.17679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09003v5","updated":"2024-12-16T06:13:03Z","published":"2024-01-17T06:48:16Z","title":"Augmenting Math Word Problems via Iterative Question Composing","summary":"  Despite the advancements in large language models (LLMs) for mathematical\nreasoning, solving competition-level math problems remains a significant\nchallenge, especially for open-source LLMs without external tools. We introduce\nthe MMIQC dataset, comprising a mixture of processed web data and synthetic\nquestion-response pairs, aimed at enhancing the mathematical reasoning\ncapabilities of base language models. Models fine-tuned on MMIQC consistently\nsurpass their counterparts in performance on the MATH benchmark across various\nmodel sizes. Notably, Qwen-72B-MMIQC achieves a 45.0% accuracy, exceeding the\nprevious open-source state-of-the-art by 8.2% and outperforming the initial\nversion GPT-4 released in 2023. Extensive evaluation results on Hungarian high\nschool finals suggest that such improvement can generalize to unseen data. Our\nablation study on MMIQC reveals that a large part of the improvement can be\nattributed to our novel augmentation method, Iterative Question Composing\n(IQC), which involves iteratively composing new questions from seed problems\nusing an LLM and applying rejection sampling through another LLM.\n","authors":["Haoxiong Liu","Yifan Zhang","Yifan Luo","Andrew Chi-Chih Yao"],"pdf_url":"https://arxiv.org/pdf/2401.09003v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16252v3","updated":"2024-12-16T05:53:28Z","published":"2024-07-23T07:40:41Z","title":"LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese\n  Legal Consultation","summary":"  Legal Large Language Models (LLMs) have shown promise in providing legal\nconsultations to non-experts. However, most existing Chinese legal consultation\nmodels are based on single-agent systems, which differ from real-world legal\nconsultations, where multiple professionals collaborate to offer more tailored\nresponses. To better simulate real consultations, we propose LawLuo, a\nmulti-agent framework for multi-turn Chinese legal consultations. LawLuo\nincludes four agents: the receptionist agent, which assesses user intent and\nselects a lawyer agent; the lawyer agent, which interacts with the user; the\nsecretary agent, which organizes conversation records and generates\nconsultation reports; and the boss agent, which evaluates the performance of\nthe lawyer and secretary agents to ensure optimal results. These agents'\ninteractions mimic the operations of real law firms. To train them to follow\ndifferent legal instructions, we developed distinct fine-tuning datasets. We\nalso introduce a case graph-based RAG to help the lawyer agent address vague\nuser inputs. Experimental results show that LawLuo outperforms baselines in\ngenerating more personalized and professional responses, handling ambiguous\nqueries, and following legal instructions in multi-turn conversations. Our full\ncode and constructed datasets will be open-sourced upon paper acceptance.\n","authors":["Jingyun Sun","Chengxiao Dai","Zhongze Luo","Yangbo Chang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16252v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.11459v1","updated":"2024-12-16T05:33:05Z","published":"2024-12-16T05:33:05Z","title":"Understanding Knowledge Hijack Mechanism in In-context Learning through\n  Associative Memory","summary":"  In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks without fine-tuning by leveraging contextual information provided\nwithin a prompt. However, ICL relies not only on contextual clues but also on\nthe global knowledge acquired during pretraining for the next token prediction.\nAnalyzing this process has been challenging due to the complex computational\ncircuitry of LLMs. This paper investigates the balance between in-context\ninformation and pretrained bigram knowledge in token prediction, focusing on\nthe induction head mechanism, a key component in ICL. Leveraging the fact that\na two-layer transformer can implement the induction head mechanism with\nassociative memories, we theoretically analyze the logits when a two-layer\ntransformer is given prompts generated by a bigram model. In the experiments,\nwe design specific prompts to evaluate whether the outputs of a two-layer\ntransformer align with the theoretical results.\n","authors":["Shuo Wang","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2412.11459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04498v2","updated":"2024-12-16T05:27:41Z","published":"2024-12-01T15:23:34Z","title":"Large Language Models in Politics and Democracy: A Comprehensive Survey","summary":"  The advancement of generative AI, particularly large language models (LLMs),\nhas a significant impact on politics and democracy, offering potential across\nvarious domains, including policymaking, political communication, analysis, and\ngovernance. This paper surveys the recent and potential applications of LLMs in\npolitics, examining both their promises and the associated challenges. This\npaper examines the ways in which LLMs are being employed in legislative\nprocesses, political communication, and political analysis. Moreover, we\ninvestigate the potential of LLMs in diplomatic and national security contexts,\neconomic and social modeling, and legal applications. While LLMs offer\nopportunities to enhance efficiency, inclusivity, and decision-making in\npolitical processes, they also present challenges related to bias,\ntransparency, and accountability. The paper underscores the necessity for\nresponsible development, ethical considerations, and governance frameworks to\nensure that the integration of LLMs into politics aligns with democratic values\nand promotes a more just and equitable society.\n","authors":["Goshi Aoki"],"pdf_url":"https://arxiv.org/pdf/2412.04498v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.11455v1","updated":"2024-12-16T05:20:18Z","published":"2024-12-16T05:20:18Z","title":"Towards Better Multi-task Learning: A Framework for Optimizing Dataset\n  Combinations in Large Language Models","summary":"  To efficiently select optimal dataset combinations for enhancing multi-task\nlearning (MTL) performance in large language models, we proposed a novel\nframework that leverages a neural network to predict the best dataset\ncombinations. The framework iteratively refines the selection, greatly\nimproving efficiency, while being model-, dataset-, and domain-independent.\nThrough experiments on 12 biomedical datasets across four tasks - named entity\nrecognition, relation extraction, event extraction, and text classification-we\ndemonstrate that our approach effectively identifies better combinations, even\nfor tasks that may seem unpromising from a human perspective. This verifies\nthat our framework provides a promising solution for maximizing MTL potential.\n","authors":["Zaifu Zhan","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11455v1.pdf","comment":"14 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.08085v2","updated":"2024-12-16T05:18:12Z","published":"2024-10-10T16:29:21Z","title":"Can Knowledge Graphs Make Large Language Models More Trustworthy? An\n  Empirical Study over Open-ended Question Answering","summary":"  Recent works integrating Knowledge Graphs (KGs) have led to promising\nimprovements in enhancing reasoning accuracy of Large Language Models (LLMs).\nHowever, current benchmarks focus mainly on closed-ended tasks, leaving a gap\nin the assessment of more complex real-world scenarios. This gap has also\nobscured the evaluation of KGs' potential to mitigate the problem of\nhallucination in LLMs. To fill the gap, we introduce OKGQA, a new benchmark\nspecifically designed to assess LLMs enhanced with KGs under open-ended,\nreal-world question answering scenarios. OKGQA is designed to closely reflect\nthe complexities of practical applications using questions from different\ntypes, and incorporates specific metrics to measure both the reduction in\nhallucinations and the enhancement in reasoning capabilities. To consider the\nscenario in which KGs may have varying levels of mistakes, we propose another\nbenchmark variant OKGQA-P to assess model performance when the semantics and\nstructure of KGs are deliberately perturbed and contaminated. OKGQA aims to (1)\nexplore whether KGs can make LLMs more trustworthy in an open-ended setting,\nand (2) conduct a comparative analysis to shed light on methods and future\ndirections for leveraging KGs to reduce LLMs' hallucination. We believe that\nthis study can facilitate a more complete performance comparison and encourage\ncontinuous improvement in integrating KGs with LLMs.\n","authors":["Yuan Sui","Yufei He","Zifeng Ding","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2410.08085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06736v2","updated":"2024-12-16T05:18:05Z","published":"2023-11-12T05:12:49Z","title":"Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof\n  Generation with Contrastive Stepwise Decoding","summary":"  Logical reasoning remains a pivotal component within the realm of artificial\nintelligence. The recent evolution of large language models (LLMs) has marked\nsignificant progress in this domain. The adoption of strategies like\nchain-of-thought (CoT) has enhanced the performance of LLMs across diverse\nreasoning tasks. Nonetheless, logical reasoning that involves proof planning,\nspecifically those that necessitate the validation of explanation accuracy,\ncontinues to present stumbling blocks. In this study, we first evaluate the\nefficacy of LLMs with advanced CoT strategies concerning such tasks. Our\nanalysis reveals that LLMs still struggle to navigate complex reasoning chains,\nwhich demand the meticulous linkage of premises to derive a cogent conclusion.\nTo address this issue, we finetune a smaller-scale language model, equipping it\nto decompose proof objectives into more manageable subgoals. We also introduce\ncontrastive decoding to stepwise proof generation, making use of negative\nreasoning paths to strengthen the model's capacity for logical deduction.\nExperiments on EntailmentBank underscore the success of our method in\naugmenting the proof planning abilities of language models.\n","authors":["Ying Su","Xiaojin Fu","Mingwen Liu","Zhijiang Guo"],"pdf_url":"https://arxiv.org/pdf/2311.06736v2.pdf","comment":"The paper is currently undergoing extensive revisions and\n  improvements"},{"id":"http://arxiv.org/abs/2412.11453v1","updated":"2024-12-16T05:15:43Z","published":"2024-12-16T05:15:43Z","title":"ACE-$M^3$: Automatic Capability Evaluator for Multimodal Medical Models","summary":"  As multimodal large language models (MLLMs) gain prominence in the medical\nfield, the need for precise evaluation methods to assess their effectiveness\nhas become critical. While benchmarks provide a reliable means to evaluate the\ncapabilities of MLLMs, traditional metrics like ROUGE and BLEU employed for\nopen domain evaluation only focus on token overlap and may not align with human\njudgment. Although human evaluation is more reliable, it is labor-intensive,\ncostly, and not scalable. LLM-based evaluation methods have proven promising,\nbut to date, there is still an urgent need for open-source multimodal LLM-based\nevaluators in the medical field. To address this issue, we introduce ACE-$M^3$,\nan open-sourced \\textbf{A}utomatic \\textbf{C}apability \\textbf{E}valuator for\n\\textbf{M}ultimodal \\textbf{M}edical \\textbf{M}odels specifically designed to\nassess the question answering abilities of medical MLLMs. It first utilizes a\nbranch-merge architecture to provide both detailed analysis and a concise final\nscore based on standard medical evaluation criteria. Subsequently, a reward\ntoken-based direct preference optimization (RTDPO) strategy is incorporated to\nsave training time without compromising performance of our model. Extensive\nexperiments have demonstrated the effectiveness of our ACE-$M^3$\nmodel\\footnote{\\url{https://huggingface.co/collections/AIUSRTMP/ace-m3-67593297ff391b93e3e5d068}}\nin evaluating the capabilities of medical MLLMs.\n","authors":["Xiechi Zhang","Shunfan Zheng","Linlin Wang","Gerard de Melo","Zhu Cao","Xiaoling Wang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2412.11453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09334v3","updated":"2024-12-16T05:06:46Z","published":"2024-06-13T17:15:33Z","title":"ProxyLM: Predicting Language Model Performance on Multilingual Tasks via\n  Proxy Models","summary":"  Performance prediction is a method to estimate the performance of Language\nModels (LMs) on various Natural Language Processing (NLP) tasks, mitigating\ncomputational costs associated with model capacity and data for fine-tuning.\nOur paper presents ProxyLM, a scalable task- and language-agnostic framework\ndesigned to predict the performance of LMs using proxy models. These proxy\nmodels act as surrogates, approximating the performance of the LM of interest.\nBy leveraging these proxy models, ProxyLM significantly reduces computational\noverhead in task evaluations, achieving up to a 37.08x speedup over traditional\nmethods, even with our smallest proxy models. Our results across multiple\nmultilingual NLP tasks and various robustness tests demonstrate that ProxyLM\nnot only adapts well to previously unseen languages in pre-trained LMs, but\nalso generalizes effectively across different datasets, outperforming the\nstate-of-the-art by at least 1.78x in terms of root-mean-square error (RMSE).\n","authors":["David Anugraha","Genta Indra Winata","Chenyue Li","Patrick Amadeus Irawan","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2406.09334v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.16464v5","updated":"2024-12-16T04:13:38Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  Sarcasm in social media, often expressed through text-image combinations,\nposes challenges for sentiment analysis and intention mining. Current\nmulti-modal sarcasm detection methods have been demonstrated to overly rely on\nspurious cues within the textual modality, revealing a limited ability to\ngenuinely identify sarcasm through nuanced text-image interactions. To solve\nthis problem, we propose InterCLIP-MEP, which introduces Interactive CLIP\n(InterCLIP) with an efficient training strategy to extract enriched text-image\nrepresentations by embedding cross-modal information directly into each\nencoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a\ndynamic dual-channel memory that stores valuable test sample knowledge during\ninference, acting as a non-parametric classifier for robust sarcasm\nrecognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP\nachieves state-of-the-art performance, with significant accuracy and F1 score\nimprovements on MMSD and MMSD2.0. Our code is available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.\n","authors":["Junjie Chen","Hang Yu","Subin Huang","Sanmin Liu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16464v5.pdf","comment":"15 pages, 7 figures, 11 tables; Code and data are available at\n  https://github.com/CoderChen01/InterCLIP-MEP"},{"id":"http://arxiv.org/abs/2412.08615v2","updated":"2024-12-16T04:05:45Z","published":"2024-12-11T18:37:56Z","title":"Exploiting the Index Gradients for Optimization-Based Jailbreaking on\n  Large Language Models","summary":"  Despite the advancements in training Large Language Models (LLMs) with\nalignment techniques to enhance the safety of generated content, these models\nremain susceptible to jailbreak, an adversarial attack method that exposes\nsecurity vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG)\nmethod has demonstrated the ability to automatically generate adversarial\nsuffixes that jailbreak state-of-the-art LLMs. However, the optimization\nprocess involved in GCG is highly time-consuming, rendering the jailbreaking\npipeline inefficient. In this paper, we investigate the process of GCG and\nidentify an issue of Indirect Effect, the key bottleneck of the GCG\noptimization. To this end, we propose the Model Attack Gradient Index GCG\n(MAGIC), that addresses the Indirect Effect by exploiting the gradient\ninformation of the suffix tokens, thereby accelerating the procedure by having\nless computation and fewer iterations. Our experiments on AdvBench show that\nMAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates\n(ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of\n74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on\nGPT-3.5. Code is available at https://github.com/jiah-li/magic.\n","authors":["Jiahui Li","Yongchang Hao","Haoyu Xu","Xing Wang","Yu Hong"],"pdf_url":"https://arxiv.org/pdf/2412.08615v2.pdf","comment":"13 pages,2 figures, accepted by COLING 2025"},{"id":"http://arxiv.org/abs/2412.09645v2","updated":"2024-12-16T04:05:05Z","published":"2024-12-10T18:52:39Z","title":"Evaluation Agent: Efficient and Promptable Evaluation Framework for\n  Visual Generative Models","summary":"  Recent advancements in visual generative models have enabled high-quality\nimage and video generation, opening diverse applications. However, evaluating\nthese models often demands sampling hundreds or thousands of images or videos,\nmaking the process computationally expensive, especially for diffusion-based\nmodels with inherently slow sampling. Moreover, existing evaluation methods\nrely on rigid pipelines that overlook specific user needs and provide numerical\nresults without clear explanations. In contrast, humans can quickly form\nimpressions of a model's capabilities by observing only a few samples. To mimic\nthis, we propose the Evaluation Agent framework, which employs human-like\nstrategies for efficient, dynamic, multi-round evaluations using only a few\nsamples per round, while offering detailed, user-tailored analyses. It offers\nfour key advantages: 1) efficiency, 2) promptable evaluation tailored to\ndiverse user needs, 3) explainability beyond single numerical scores, and 4)\nscalability across various models and tools. Experiments show that Evaluation\nAgent reduces evaluation time to 10% of traditional methods while delivering\ncomparable results. The Evaluation Agent framework is fully open-sourced to\nadvance research in visual generative models and their efficient evaluation.\n","authors":["Fan Zhang","Shulin Tian","Ziqi Huang","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09645v2.pdf","comment":"Equal contributions from first three authors. Project page:\n  https://vchitect.github.io/Evaluation-Agent-project Code:\n  https://github.com/Vchitect/Evaluation-Agent"},{"id":"http://arxiv.org/abs/2412.11431v1","updated":"2024-12-16T04:03:58Z","published":"2024-12-16T04:03:58Z","title":"Optimized Quran Passage Retrieval Using an Expanded QA Dataset and\n  Fine-Tuned Language Models","summary":"  Understanding the deep meanings of the Qur'an and bridging the language gap\nbetween modern standard Arabic and classical Arabic is essential to improve the\nquestion-and-answer system for the Holy Qur'an. The Qur'an QA 2023 shared task\ndataset had a limited number of questions with weak model retrieval. To address\nthis challenge, this work updated the original dataset and improved the model\naccuracy. The original dataset, which contains 251 questions, was reviewed and\nexpanded to 629 questions with question diversification and reformulation,\nleading to a comprehensive set of 1895 categorized into single-answer,\nmulti-answer, and zero-answer types. Extensive experiments fine-tuned\ntransformer models, including AraBERT, RoBERTa, CAMeLBERT, AraELECTRA, and\nBERT. The best model, AraBERT-base, achieved a MAP@10 of 0.36 and MRR of 0.59,\nrepresenting improvements of 63% and 59%, respectively, compared to the\nbaseline scores (MAP@10: 0.22, MRR: 0.37). Additionally, the dataset expansion\nled to improvements in handling \"no answer\" cases, with the proposed approach\nachieving a 75% success rate for such instances, compared to the baseline's\n25%. These results demonstrate the effect of dataset improvement and model\narchitecture optimization in increasing the performance of QA systems for the\nHoly Qur'an, with higher accuracy, recall, and precision.\n","authors":["Mohamed Basem","Islam Oshallah","Baraa Hikal","Ali Hamdi","Ammar Mohamed"],"pdf_url":"https://arxiv.org/pdf/2412.11431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20177v3","updated":"2024-12-16T03:39:20Z","published":"2024-07-29T17:06:30Z","title":"AutoScale: Automatic Prediction of Compute-optimal Data Composition for\n  Training LLMs","summary":"  Domain reweighting is an emerging research area aimed at adjusting the\nrelative weights of different data sources to improve the effectiveness and\nefficiency of language model pre-training. This paper demonstrates that the\noptimal composition of training data from different domains is scale-dependent,\nchallenging the existing practice of determining optimal mixtures through\nsmall-scale experiments and directly applying them at larger scales. We derive\nan analytical model for the dependence of optimal weights on data scale and\nintroduce *AutoScale*, a novel, practical approach for optimizing data\ncompositions at potentially large training data scales. *AutoScale* first uses\na principled optimization framework to find optimal compositions at smaller,\nfeasible scales, then predicts optimal compositions at larger scales using our\nderived model. Our evaluation on GPT-2 Large and BERT pre-training demonstrates\n*AutoScale*'s effectiveness in improving training convergence and downstream\nperformance. Particularly, for GPT-2 Large on RedPajama, *AutoScale* decreases\nvalidation perplexity 28% faster than baselines, with up to 38% speed-up over\nunweighted training, achieving the best performance across downstream tasks.\nThis work provides insights into the varying benefits of data sources across\ntraining scales for language models, contributing to the burgeoning research on\nscale-dependent data curation. Code is open-sourced.\n","authors":["Feiyang Kang","Yifan Sun","Bingbing Wen","Si Chen","Dawn Song","Rafid Mahmood","Ruoxi Jia"],"pdf_url":"https://arxiv.org/pdf/2407.20177v3.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2412.12417v1","updated":"2024-12-16T23:50:21Z","published":"2024-12-16T23:50:21Z","title":"Bridging the Gap: Enhancing LLM Performance for Low-Resource African\n  Languages with New Benchmarks, Fine-Tuning, and Cultural Adjustments","summary":"  Large Language Models (LLMs) have shown remarkable performance across various\ntasks, yet significant disparities remain for non-English languages, and\nespecially native African languages. This paper addresses these disparities by\ncreating approximately 1 million human-translated words of new benchmark data\nin 8 low-resource African languages, covering a population of over 160 million\nspeakers of: Amharic, Bambara, Igbo, Sepedi (Northern Sotho), Shona, Sesotho\n(Southern Sotho), Setswana, and Tsonga. Our benchmarks are translations of\nWinogrande and three sections of MMLU: college medicine, clinical knowledge,\nand virology. Using the translated benchmarks, we report previously unknown\nperformance gaps between state-of-the-art (SOTA) LLMs in English and African\nlanguages. Finally, using results from over 400 fine-tuned models, we explore\nseveral methods to reduce the LLM performance gap, including high-quality\ndataset fine-tuning (using an LLM-as-an-Annotator), cross-lingual transfer, and\ncultural appropriateness adjustments. Key findings include average mono-lingual\nimprovements of 5.6% with fine-tuning (with 5.4% average mono-lingual\nimprovements when using high-quality data over low-quality data), 2.9% average\ngains from cross-lingual transfer, and a 3.0% out-of-the-box performance boost\non culturally appropriate questions. The publicly available benchmarks,\ntranslations, and code from this study support further research and development\naimed at creating more inclusive and effective language technologies.\n","authors":["Tuka Alhanai","Adam Kasumovic","Mohammad Ghassemi","Aven Zitzelberger","Jessica Lundin","Guillaume Chabot-Couture"],"pdf_url":"https://arxiv.org/pdf/2412.12417v1.pdf","comment":"Accepted to AAAI 2025. Main content is 9 pages, 3 figures. Includes\n  supplementary materials"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.12096v1","updated":"2024-12-16T18:59:45Z","published":"2024-12-16T18:59:45Z","title":"PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting","summary":"  With the advent of portable 360{\\deg} cameras, panorama has gained\nsignificant attention in applications like virtual reality (VR), virtual tours,\nrobotics, and autonomous driving. As a result, wide-baseline panorama view\nsynthesis has emerged as a vital task, where high resolution, fast inference,\nand memory efficiency are essential. Nevertheless, existing methods are\ntypically constrained to lower resolutions (512 $\\times$ 1024) due to demanding\nmemory and computational requirements. In this paper, we present PanSplat, a\ngeneralizable, feed-forward approach that efficiently supports resolution up to\n4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian\npyramid with a Fibonacci lattice arrangement, enhancing image quality while\nreducing information redundancy. To accommodate the demands of high resolution,\nwe propose a pipeline that integrates a hierarchical spherical cost volume and\nGaussian heads with local operations, enabling two-step deferred\nbackpropagation for memory-efficient training on a single A100 GPU. Experiments\ndemonstrate that PanSplat achieves state-of-the-art results with superior\nefficiency and image quality across both synthetic and real-world datasets.\nCode will be available at \\url{https://github.com/chengzhag/PanSplat}.\n","authors":["Cheng Zhang","Haofei Xu","Qianyi Wu","Camilo Cruz Gambardella","Dinh Phung","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2412.12096v1.pdf","comment":"Project Page: https://chengzhag.github.io/publication/pansplat/ Code:\n  https://github.com/chengzhag/PanSplat"},{"id":"http://arxiv.org/abs/2412.12095v1","updated":"2024-12-16T18:59:29Z","published":"2024-12-16T18:59:29Z","title":"Causal Diffusion Transformers for Generative Modeling","summary":"  We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.\n","authors":["Chaorui Deng","Deyao Zh","Kunchang Li","Shi Guan","Haoqi Fan"],"pdf_url":"https://arxiv.org/pdf/2412.12095v1.pdf","comment":"21 pages, 22 figures"},{"id":"http://arxiv.org/abs/2412.12093v1","updated":"2024-12-16T18:58:51Z","published":"2024-12-16T18:58:51Z","title":"CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View\n  Diffusion Models","summary":"  Reconstructing photorealistic and dynamic portrait avatars from images is\nessential to many applications including advertising, visual effects, and\nvirtual reality. Depending on the application, avatar reconstruction involves\ndifferent capture setups and constraints $-$ for example, visual effects\nstudios use camera arrays to capture hundreds of reference images, while\ncontent creators may seek to animate a single portrait image downloaded from\nthe internet. As such, there is a large and heterogeneous ecosystem of methods\nfor avatar reconstruction. Techniques based on multi-view stereo or neural\nrendering achieve the highest quality results, but require hundreds of\nreference images. Recent generative models produce convincing avatars from a\nsingle reference image, but visual fidelity yet lags behind multi-view\ntechniques. Here, we present CAP4D: an approach that uses a morphable\nmulti-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait\navatars from any number of reference images (i.e., one to 100) and animate and\nrender them in real time. Our approach demonstrates state-of-the-art\nperformance for single-, few-, and multi-image 4D portrait avatar\nreconstruction, and takes steps to bridge the gap in visual fidelity between\nsingle-image and multi-view reconstruction techniques.\n","authors":["Felix Taubner","Ruihang Zhang","Mathieu Tuli","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2412.12093v1.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2412.12091v1","updated":"2024-12-16T18:58:17Z","published":"2024-12-16T18:58:17Z","title":"Wonderland: Navigating 3D Scenes from a Single Image","summary":"  This paper addresses a challenging question: How can we efficiently create\nhigh-quality, wide-scope 3D scenes from a single arbitrary image? Existing\nmethods face several constraints, such as requiring multi-view data,\ntime-consuming per-scene optimization, low visual quality in backgrounds, and\ndistorted reconstructions in unseen areas. We propose a novel pipeline to\novercome these limitations. Specifically, we introduce a large-scale\nreconstruction model that uses latents from a video diffusion model to predict\n3D Gaussian Splattings for the scenes in a feed-forward manner. The video\ndiffusion model is designed to create videos precisely following specified\ncamera trajectories, allowing it to generate compressed video latents that\ncontain multi-view information while maintaining 3D consistency. We train the\n3D reconstruction model to operate on the video latent space with a progressive\ntraining strategy, enabling the efficient generation of high-quality,\nwide-scope, and generic 3D scenes. Extensive evaluations across various\ndatasets demonstrate that our model significantly outperforms existing methods\nfor single-view 3D scene generation, particularly with out-of-domain images.\nFor the first time, we demonstrate that a 3D reconstruction model can be\neffectively built upon the latent space of a diffusion model to realize\nefficient 3D scene generation.\n","authors":["Hanwen Liang","Junli Cao","Vidit Goel","Guocheng Qian","Sergei Korolev","Demetri Terzopoulos","Konstantinos N. Plataniotis","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2412.12091v1.pdf","comment":"Project page: https://snap-research.github.io/wonderland/"},{"id":"http://arxiv.org/abs/2412.12089v1","updated":"2024-12-16T18:56:24Z","published":"2024-12-16T18:56:24Z","title":"Stabilizing Reinforcement Learning in Differentiable Multiphysics\n  Simulation","summary":"  Recent advances in GPU-based parallel simulation have enabled practitioners\nto collect large amounts of data and train complex control policies using deep\nreinforcement learning (RL), on commodity GPUs. However, such successes for RL\nin robotics have been limited to tasks sufficiently simulated by fast\nrigid-body dynamics. Simulation techniques for soft bodies are comparatively\nseveral orders of magnitude slower, thereby limiting the use of RL due to\nsample complexity requirements. To address this challenge, this paper presents\nboth a novel RL algorithm and a simulation platform to enable scaling RL on\ntasks involving rigid bodies and deformables. We introduce Soft Analytic Policy\nOptimization (SAPO), a maximum entropy first-order model-based actor-critic RL\nalgorithm, which uses first-order analytic gradients from differentiable\nsimulation to train a stochastic actor to maximize expected return and entropy.\nAlongside our approach, we develop Rewarped, a parallel differentiable\nmultiphysics simulation platform that supports simulating various materials\nbeyond rigid bodies. We re-implement challenging manipulation and locomotion\ntasks in Rewarped, and show that SAPO outperforms baselines over a range of\ntasks that involve interaction between rigid bodies, articulations, and\ndeformables.\n","authors":["Eliot Xing","Vernon Luk","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2412.12089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12087v1","updated":"2024-12-16T18:56:17Z","published":"2024-12-16T18:56:17Z","title":"Instruction-based Image Manipulation by Watching How Things Move","summary":"  This paper introduces a novel dataset construction pipeline that samples\npairs of frames from videos and uses multimodal large language models (MLLMs)\nto generate editing instructions for training instruction-based image\nmanipulation models. Video frames inherently preserve the identity of subjects\nand scenes, ensuring consistent content preservation during editing.\nAdditionally, video data captures diverse, natural dynamics-such as non-rigid\nsubject motion and complex camera movements-that are difficult to model\notherwise, making it an ideal source for scalable dataset construction. Using\nthis approach, we create a new dataset to train InstructMove, a model capable\nof instruction-based complex manipulations that are difficult to achieve with\nsynthetically generated datasets. Our model demonstrates state-of-the-art\nperformance in tasks such as adjusting subject poses, rearranging elements, and\naltering camera perspectives.\n","authors":["Mingdeng Cao","Xuaner Zhang","Yinqiang Zheng","Zhihao Xia"],"pdf_url":"https://arxiv.org/pdf/2412.12087v1.pdf","comment":"Project page: https://ljzycmd.github.io/projects/InstructMove/"},{"id":"http://arxiv.org/abs/2411.17474v2","updated":"2024-12-16T18:55:09Z","published":"2024-11-25T18:59:50Z","title":"Probing the Mid-level Vision Capabilities of Self-Supervised Learning","summary":"  Mid-level vision capabilities - such as generic object localization and 3D\ngeometric understanding - are not only fundamental to human vision but are also\ncrucial for many real-world applications of computer vision. These abilities\nemerge with minimal supervision during the early stages of human visual\ndevelopment. Despite their significance, current self-supervised learning (SSL)\napproaches are primarily designed and evaluated for high-level recognition\ntasks, leaving their mid-level vision capabilities largely unexamined.\n  In this study, we introduce a suite of benchmark protocols to systematically\nassess mid-level vision capabilities and present a comprehensive, controlled\nevaluation of 22 prominent SSL models across 8 mid-level vision tasks. Our\nexperiments reveal a weak correlation between mid-level and high-level task\nperformance. We also identify several SSL methods with highly imbalanced\nperformance across mid-level and high-level capabilities, as well as some that\nexcel in both. Additionally, we investigate key factors contributing to\nmid-level vision performance, such as pretraining objectives and network\narchitectures. Our study provides a holistic and timely view of what SSL models\nhave learned, complementing existing research that primarily focuses on\nhigh-level vision tasks. We hope our findings guide future SSL research to\nbenchmark models not only on high-level vision tasks but on mid-level as well.\n","authors":["Xuweiyi Chen","Markus Marks","Zezhou Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.17474v2.pdf","comment":"Project Page: https://midvision-probe.cs.virginia.edu/"},{"id":"http://arxiv.org/abs/2412.12083v1","updated":"2024-12-16T18:52:56Z","published":"2024-12-16T18:52:56Z","title":"IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and\n  Illuminations","summary":"  Capturing geometric and material information from images remains a\nfundamental challenge in computer vision and graphics. Traditional\noptimization-based methods often require hours of computational time to\nreconstruct geometry, material properties, and environmental lighting from\ndense multi-view inputs, while still struggling with inherent ambiguities\nbetween lighting and material. On the other hand, learning-based approaches\nleverage rich material priors from existing 3D object datasets but face\nchallenges with maintaining multi-view consistency. In this paper, we introduce\nIDArb, a diffusion-based model designed to perform intrinsic decomposition on\nan arbitrary number of images under varying illuminations. Our method achieves\naccurate and multi-view consistent estimation on surface normals and material\nproperties. This is made possible through a novel cross-view, cross-domain\nattention module and an illumination-augmented, view-adaptive training\nstrategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides\nlarge-scale multi-view intrinsic data and renderings under diverse lighting\nconditions, supporting robust training. Extensive experiments demonstrate that\nIDArb outperforms state-of-the-art methods both qualitatively and\nquantitatively. Moreover, our approach facilitates a range of downstream tasks,\nincluding single-image relighting, photometric stereo, and 3D reconstruction,\nhighlighting its broad applications in realistic 3D content creation.\n","authors":["Zhibing Li","Tong Wu","Jing Tan","Mengchen Zhang","Jiaqi Wang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2412.12083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12079v1","updated":"2024-12-16T18:48:58Z","published":"2024-12-16T18:48:58Z","title":"UniLoc: Towards Universal Place Recognition Using Any Single Modality","summary":"  To date, most place recognition methods focus on single-modality retrieval.\nWhile they perform well in specific environments, cross-modal methods offer\ngreater flexibility by allowing seamless switching between map and query\nsources. It also promises to reduce computation requirements by having a\nunified model, and achieving greater sample efficiency by sharing parameters.\nIn this work, we develop a universal solution to place recognition, UniLoc,\nthat works with any single query modality (natural language, image, or point\ncloud). UniLoc leverages recent advances in large-scale contrastive learning,\nand learns by matching hierarchically at two levels: instance-level matching\nand scene-level matching. Specifically, we propose a novel Self-Attention based\nPooling (SAP) module to evaluate the importance of instance descriptors when\naggregated into a place-level descriptor. Experiments on the KITTI-360 dataset\ndemonstrate the benefits of cross-modality for place recognition, achieving\nsuperior performance in cross-modal settings and competitive results also for\nuni-modal scenarios. Our project page is publicly available at\nhttps://yan-xia.github.io/projects/UniLoc/.\n","authors":["Yan Xia","Zhendong Li","Yun-Jin Li","Letian Shi","Hu Cao","João F. Henriques","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2412.12079v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.12077v1","updated":"2024-12-16T18:46:58Z","published":"2024-12-16T18:46:58Z","title":"CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole\n  Slide Image Analysis in Computational Pathology","summary":"  The emergence of large multimodal models (LMMs) has brought significant\nadvancements to pathology. Previous research has primarily focused on\nseparately training patch-level and whole-slide image (WSI)-level models,\nlimiting the integration of learned knowledge across patches and WSIs, and\nresulting in redundant models. In this work, we introduce CPath-Omni, the first\n15-billion-parameter LMM designed to unify both patch and WSI level image\nanalysis, consolidating a variety of tasks at both levels, including\nclassification, visual question answering, captioning, and visual referring\nprompting. Extensive experiments demonstrate that CPath-Omni achieves\nstate-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42\ndatasets, outperforming or matching task-specific models trained for individual\ntasks. Additionally, we develop a specialized pathology CLIP-based visual\nprocessor for CPath-Omni, CPath-CLIP, which, for the first time, integrates\ndifferent vision models and incorporates a large language model as a text\nencoder to build a more powerful CLIP model, which achieves SOTA performance on\nnine zero-shot and four few-shot datasets. Our findings highlight CPath-Omni's\nability to unify diverse pathology tasks, demonstrating its potential to\nstreamline and advance the field of foundation model in pathology.\n","authors":["Yuxuan Sun","Yixuan Si","Chenglu Zhu","Xuan Gong","Kai Zhang","Pingyi Chen","Ye Zhang","Zhongyi Shui","Tao Lin","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12077v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.12075v1","updated":"2024-12-16T18:46:45Z","published":"2024-12-16T18:46:45Z","title":"CG-Bench: Clue-grounded Question Answering Benchmark for Long Video\n  Understanding","summary":"  Most existing video understanding benchmarks for multimodal large language\nmodels (MLLMs) focus only on short videos. The limited number of benchmarks for\nlong video understanding often rely solely on multiple-choice questions (MCQs).\nHowever, because of the inherent limitation of MCQ-based evaluation and the\nincreasing reasoning ability of MLLMs, models can give the current answer\npurely by combining short video understanding with elimination, without\ngenuinely understanding the video content. To address this gap, we introduce\nCG-Bench, a novel benchmark designed for clue-grounded question answering in\nlong videos. CG-Bench emphasizes the model's ability to retrieve relevant clues\nfor questions, enhancing evaluation credibility. It features 1,219 manually\ncurated videos categorized by a granular system with 14 primary categories, 171\nsecondary categories, and 638 tertiary categories, making it the largest\nbenchmark for long video analysis. The benchmark includes 12,129 QA pairs in\nthree major question types: perception, reasoning, and hallucination.\nCompensating the drawbacks of pure MCQ-based evaluation, we design two novel\nclue-based evaluation methods: clue-grounded white box and black box\nevaluations, to assess whether the model generates answers based on the correct\nunderstanding of the video. We evaluate multiple closed-source and open-source\nMLLMs on CG-Bench. Results indicate that current models significantly\nunderperform in understanding long videos compared to short ones, and a\nsignificant gap exists between open-source and commercial models. We hope\nCG-Bench can advance the development of more trustworthy and capable MLLMs for\nlong video understanding. All annotations and video data are released at\nhttps://cg-bench.github.io/leaderboard/.\n","authors":["Guo Chen","Yicheng Liu","Yifei Huang","Yuping He","Baoqi Pei","Jilan Xu","Yali Wang","Tong Lu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12075v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.12068v1","updated":"2024-12-16T18:42:05Z","published":"2024-12-16T18:42:05Z","title":"SPADE: Spectroscopic Photoacoustic Denoising using an Analytical and\n  Data-free Enhancement Framework","summary":"  Spectroscopic photoacoustic (sPA) imaging uses multiple wavelengths to\ndifferentiate chromophores based on their unique optical absorption spectra.\nThis technique has been widely applied in areas such as vascular mapping, tumor\ndetection, and therapeutic monitoring. However, sPA imaging is highly\nsusceptible to noise, leading to poor signal-to-noise ratio (SNR) and\ncompromised image quality. Traditional denoising techniques like frame\naveraging, though effective in improving SNR, can be impractical for dynamic\nimaging scenarios due to reduced frame rates. Advanced methods, including\nlearning-based approaches and analytical algorithms, have demonstrated promise\nbut often require extensive training data and parameter tuning, limiting their\nadaptability for real-time clinical use. In this work, we propose a sPA\ndenoising using a tuning-free analytical and data-free enhancement (SPADE)\nframework for denoising sPA images. This framework integrates a data-free\nlearning-based method with an efficient BM3D-based analytical approach while\npreserves spectral linearity, providing noise reduction and ensuring that\nfunctional information is maintained. The SPADE framework was validated through\nsimulation, phantom, ex vivo, and in vivo experiments. Results demonstrated\nthat SPADE improved SNR and preserved spectral information, outperforming\nconventional methods, especially in challenging imaging conditions. SPADE\npresents a promising solution for enhancing sPA imaging quality in clinical\napplications where noise reduction and spectral preservation are critical.\n","authors":["Fangzhou Lin","Shang Gao","Yichuan Tang","Xihan Ma","Ryo Murakami","Ziming Zhang","John D. Obayemic","Winston W. Soboyejo","Haichong K. Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12068v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.12050v1","updated":"2024-12-16T18:20:06Z","published":"2024-12-16T18:20:06Z","title":"Exploring Semantic Consistency and Style Diversity for Domain\n  Generalized Semantic Segmentation","summary":"  Domain Generalized Semantic Segmentation (DGSS) seeks to utilize source\ndomain data exclusively to enhance the generalization of semantic segmentation\nacross unknown target domains. Prevailing studies predominantly concentrate on\nfeature normalization and domain randomization, these approaches exhibit\nsignificant limitations. Feature normalization-based methods tend to confuse\nsemantic features in the process of constraining the feature space\ndistribution, resulting in classification misjudgment. Domain\nrandomization-based methods frequently incorporate domain-irrelevant noise due\nto the uncontrollability of style transformations, resulting in segmentation\nambiguity. To address these challenges, we introduce a novel framework, named\nSCSD for Semantic Consistency prediction and Style Diversity generalization. It\ncomprises three pivotal components: Firstly, a Semantic Query Booster is\ndesigned to enhance the semantic awareness and discrimination capabilities of\nobject queries in the mask decoder, enabling cross-domain semantic consistency\nprediction. Secondly, we develop a Text-Driven Style Transform module that\nutilizes domain difference text embeddings to controllably guide the style\ntransformation of image features, thereby increasing inter-domain style\ndiversity. Lastly, to prevent the collapse of similar domain feature spaces, we\nintroduce a Style Synergy Optimization mechanism that fortifies the separation\nof inter-domain features and the aggregation of intra-domain features by\nsynergistically weighting style contrastive loss and style aggregation loss.\nExtensive experiments demonstrate that the proposed SCSD significantly\noutperforms existing state-of-theart methods. Notably, SCSD trained on GTAV\nachieved an average of 49.11 mIoU on the four unseen domain datasets,\nsurpassing the previous state-of-the-art method by +4.08 mIoU. Code is\navailable at https://github.com/nhw649/SCSD.\n","authors":["Hongwei Niu","Linhuang Xie","Jianghang Lin","Shengchuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12050v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12048v1","updated":"2024-12-16T18:18:17Z","published":"2024-12-16T18:18:17Z","title":"A LoRA is Worth a Thousand Pictures","summary":"  Recent advances in diffusion models and parameter-efficient fine-tuning\n(PEFT) have made text-to-image generation and customization widely accessible,\nwith Low Rank Adaptation (LoRA) able to replicate an artist's style or subject\nusing minimal data and computation. In this paper, we examine the relationship\nbetween LoRA weights and artistic styles, demonstrating that LoRA weights alone\ncan serve as an effective descriptor of style, without the need for additional\nimage generation or knowledge of the original training set. Our findings show\nthat LoRA weights yield better performance in clustering of artistic styles\ncompared to traditional pre-trained features, such as CLIP and DINO, with\nstrong structural similarities between LoRA-based and conventional image-based\nembeddings observed both qualitatively and quantitatively. We identify various\nretrieval scenarios for the growing collection of customized models and show\nthat our approach enables more accurate retrieval in real-world settings where\nknowledge of the training images is unavailable and additional generation is\nrequired. We conclude with a discussion on potential future applications, such\nas zero-shot LoRA fine-tuning and model attribution.\n","authors":["Chenxi Liu","Towaki Takikawa","Alec Jacobson"],"pdf_url":"https://arxiv.org/pdf/2412.12048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08628v2","updated":"2024-12-16T18:16:14Z","published":"2024-12-11T18:48:20Z","title":"EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation","summary":"  Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatialaware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.5 PQ, 32.1 mIoU, and\n11.6 FPS on the ADE20K dataset and the inference time of EOV-Seg is 4-19 times\nfaster than state-of-theart methods. Especially, equipped with ResNet50\nbackbone, EOV-Seg runs 23.8 FPS with only 71M parameters on a single RTX 3090\nGPU. Code is available at https://github.com/nhw649/EOV-Seg.\n","authors":["Hongwei Niu","Jie Hu","Jianghang Lin","Guannan Jiang","Shengchuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08628v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12032v1","updated":"2024-12-16T17:58:45Z","published":"2024-12-16T17:58:45Z","title":"FSFM: A Generalizable Face Security Foundation Model via Self-Supervised\n  Facial Representation Learning","summary":"  This work asks: with abundant, unlabeled real faces, how to learn a robust\nand transferable facial representation that boosts various face security tasks\nwith respect to generalization performance? We make the first attempt and\npropose a self-supervised pretraining framework to learn fundamental\nrepresentations of real face images, FSFM, that leverages the synergy between\nmasked image modeling (MIM) and instance discrimination (ID). We explore\nvarious facial masking strategies for MIM and present a simple yet powerful\nCRFR-P masking, which explicitly forces the model to capture meaningful\nintra-region consistency and challenging inter-region coherency. Furthermore,\nwe devise the ID network that naturally couples with MIM to establish\nunderlying local-to-global correspondence via tailored self-distillation. These\nthree learning objectives, namely 3C, empower encoding both local features and\nglobal semantics of real faces. After pretraining, a vanilla ViT serves as a\nuniversal vision foundation model for downstream face security tasks:\ncross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen\ndiffusion facial forgery detection. Extensive experiments on 10 public datasets\ndemonstrate that our model transfers better than supervised pretraining, visual\nand facial self-supervised learning arts, and even outperforms task-specialized\nSOTA methods.\n","authors":["Gaojian Wang","Feng Lin","Tong Wu","Zhenguang Liu","Zhongjie Ba","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2412.12032v1.pdf","comment":"21 pages, 11 figures, project page: https://fsfm-3c.github.io"},{"id":"http://arxiv.org/abs/2412.12031v1","updated":"2024-12-16T17:57:33Z","published":"2024-12-16T17:57:33Z","title":"RepFace: Refining Closed-Set Noise with Progressive Label Correction for\n  Face Recognition","summary":"  Face recognition has made remarkable strides, driven by the expanding scale\nof datasets, advancements in various backbone and discriminative losses.\nHowever, face recognition performance is heavily affected by the label noise,\nespecially closed-set noise. While numerous studies have focused on handling\nlabel noise, addressing closed-set noise still poses challenges. This paper\nidentifies this challenge as training isn't robust to noise at the early-stage\ntraining, and necessitating an appropriate learning strategy for samples with\nlow confidence, which are often misclassified as closed-set noise in later\ntraining phases. To address these issues, we propose a new framework to\nstabilize the training at early stages and split the samples into clean,\nambiguous and noisy groups which are devised with separate training strategies.\nInitially, we employ generated auxiliary closed-set noisy samples to enable the\nmodel to identify noisy data at the early stages of training. Subsequently, we\nintroduce how samples are split into clean, ambiguous and noisy groups by their\nsimilarity to the positive and nearest negative centers. Then we perform label\nfusion for ambiguous samples by incorporating accumulated model predictions.\nFinally, we apply label smoothing within the closed set, adjusting the label to\na point between the nearest negative class and the initially assigned label.\nExtensive experiments validate the effectiveness of our method on mainstream\nface datasets, achieving state-of-the-art results. The code will be released\nupon acceptance.\n","authors":["Jie Zhang","Xun Gong","Zhonglin Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12031v1.pdf","comment":"11 pages, 5 figures, AAAI2025"},{"id":"http://arxiv.org/abs/2412.10316v2","updated":"2024-12-16T17:54:44Z","published":"2024-12-13T17:58:06Z","title":"BrushEdit: All-In-One Image Inpainting and Editing","summary":"  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n","authors":["Yaowei Li","Yuxuan Bian","Xuan Ju","Zhaoyang Zhang","Ying Shan","Yuexian Zou","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10316v2.pdf","comment":"WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/"},{"id":"http://arxiv.org/abs/2409.00397v2","updated":"2024-12-16T17:43:04Z","published":"2024-08-31T09:14:54Z","title":"COSMo: CLIP Talks on Open-Set Multi-Target Domain Adaptation","summary":"  Multi-Target Domain Adaptation (MTDA) entails learning domain-invariant\ninformation from a single source domain and applying it to multiple unlabeled\ntarget domains. Yet, existing MTDA methods predominantly focus on addressing\ndomain shifts within visual features, often overlooking semantic features and\nstruggling to handle unknown classes, resulting in what is known as Open-Set\n(OS) MTDA. While large-scale vision-language foundation models like CLIP show\npromise, their potential for MTDA remains largely unexplored. This paper\nintroduces COSMo, a novel method that learns domain-agnostic prompts through\nsource domain-guided prompt learning to tackle the MTDA problem in the prompt\nspace. By leveraging a domain-specific bias network and separate prompts for\nknown and unknown classes, COSMo effectively adapts across domain and class\nshifts. To the best of our knowledge, COSMo is the first method to address\nOpen-Set Multi-Target DA (OSMTDA), offering a more realistic representation of\nreal-world scenarios and addressing the challenges of both open-set and\nmulti-target DA. COSMo demonstrates an average improvement of $5.1\\%$ across\nthree challenging datasets: Mini-DomainNet, Office-31, and Office-Home,\ncompared to other related DA methods adapted to operate within the OSMTDA\nsetting. Code is available at: https://github.com/munish30monga/COSMo\n","authors":["Munish Monga","Sachin Kumar Giroh","Ankit Jha","Mainak Singha","Biplab Banerjee","Jocelyn Chanussot"],"pdf_url":"https://arxiv.org/pdf/2409.00397v2.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2412.12001v1","updated":"2024-12-16T17:29:51Z","published":"2024-12-16T17:29:51Z","title":"LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts","summary":"  Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem.\n","authors":["Zhuhao Wang","Yihua Sun","Zihan Li","Xuan Yang","Fang Chen","Hongen Liao"],"pdf_url":"https://arxiv.org/pdf/2412.12001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11998v1","updated":"2024-12-16T17:26:06Z","published":"2024-12-16T17:26:06Z","title":"SAMIC: Segment Anything with In-Context Spatial Prompt Engineering","summary":"  Few-shot segmentation is the problem of learning to identify specific types\nof objects (e.g., airplanes) in images from a small set of labeled reference\nimages. The current state of the art is driven by resource-intensive\nconstruction of models for every new domain-specific application. Such models\nmust be trained on enormous labeled datasets of unrelated objects (e.g., cars,\ntrains, animals) so that their ``knowledge'' can be transferred to new types of\nobjects. In this paper, we show how to leverage existing vision foundation\nmodels (VFMs) to reduce the incremental cost of creating few-shot segmentation\nmodels for new domains. Specifically, we introduce SAMIC, a small network that\nlearns how to prompt VFMs in order to segment new types of objects in\ndomain-specific applications. SAMIC enables any task to be approached as a\nfew-shot learning problem. At 2.6 million parameters, it is 94% smaller than\nthe leading models (e.g., having ResNet 101 backbone with 45+ million\nparameters). Even using 1/5th of the training data provided by one-shot\nbenchmarks, SAMIC is competitive with, or sets the state of the art, on a\nvariety of few-shot and semantic segmentation datasets including COCO-$20^i$,\nPascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10.\n","authors":["Savinay Nagendra","Kashif Rashid","Chaopeng Shen","Daniel Kifer"],"pdf_url":"https://arxiv.org/pdf/2412.11998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16726v2","updated":"2024-12-16T17:11:49Z","published":"2024-11-23T04:38:51Z","title":"EmotiveTalk: Expressive Talking Head Generation through Audio\n  Information Decoupling and Emotional Video Diffusion","summary":"  Diffusion models have revolutionized the field of talking head generation,\nyet still face challenges in expressiveness, controllability, and stability in\nlong-time generation. In this research, we propose an EmotiveTalk framework to\naddress these issues. Firstly, to realize better control over the generation of\nlip movement and facial expression, a Vision-guided Audio Information\nDecoupling (V-AID) approach is designed to generate audio-based decoupled\nrepresentations aligned with lip movements and expression. Specifically, to\nachieve alignment between audio and facial expression representation spaces, we\npresent a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within\nV-AID to generate expression-related representations under multi-source emotion\ncondition constraints. Then we propose a well-designed Emotional Talking Head\nDiffusion (ETHD) backbone to efficiently generate highly expressive talking\nhead videos, which contains an Expression Decoupling Injection (EDI) module to\nautomatically decouple the expressions from reference portraits while\nintegrating the target expression information, achieving more expressive\ngeneration performance. Experimental results show that EmotiveTalk can generate\nexpressive talking head videos, ensuring the promised controllability of\nemotions and stability during long-time generation, yielding state-of-the-art\nperformance compared to existing methods.\n","authors":["Haotian Wang","Yuzhe Weng","Yueyan Li","Zilu Guo","Jun Du","Shutong Niu","Jiefeng Ma","Shan He","Xiaoyan Wu","Qiming Hu","Bing Yin","Cong Liu","Qingfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16726v2.pdf","comment":"https://emotivetalk.github.io/"},{"id":"http://arxiv.org/abs/2412.11974v1","updated":"2024-12-16T16:58:28Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v1.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.11972v1","updated":"2024-12-16T16:55:22Z","published":"2024-12-16T16:55:22Z","title":"Controllable Shadow Generation with Single-Step Diffusion Models from\n  Synthetic Data","summary":"  Realistic shadow generation is a critical component for high-quality image\ncompositing and visual effects, yet existing methods suffer from certain\nlimitations: Physics-based approaches require a 3D scene geometry, which is\noften unavailable, while learning-based techniques struggle with control and\nvisual artifacts. We introduce a novel method for fast, controllable, and\nbackground-free shadow generation for 2D object images. We create a large\nsynthetic dataset using a 3D rendering engine to train a diffusion model for\ncontrollable shadow generation, generating shadow maps for diverse light source\nparameters. Through extensive ablation studies, we find that rectified flow\nobjective achieves high-quality results with just a single sampling step\nenabling real-time applications. Furthermore, our experiments demonstrate that\nthe model generalizes well to real-world images. To facilitate further research\nin evaluating quality and controllability in shadow generation, we release a\nnew public benchmark containing a diverse set of object images and shadow maps\nin various settings. The project page is available at\nhttps://gojasper.github.io/controllable-shadow-generation-project/\n","authors":["Onur Tasar","Clément Chadebec","Benjamin Aubin"],"pdf_url":"https://arxiv.org/pdf/2412.11972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16258v2","updated":"2024-12-16T16:46:03Z","published":"2024-08-29T04:40:31Z","title":"GSDiff: Synthesizing Vector Floorplans via Geometry-enhanced Structural\n  Graph Generation","summary":"  Automating architectural floorplan design is vital for housing and interior\ndesign, offering a faster, cost-effective alternative to manual sketches by\narchitects. However, existing methods, including rule-based and learning-based\napproaches, face challenges in design complexity and constrained generation\nwith extensive post-processing, and tend to obvious geometric inconsistencies\nsuch as misalignment, overlap, and gaps. In this work, we propose a novel\ngenerative framework for vector floorplan design via structural graph\ngeneration, called GSDiff, focusing on wall junction generation and wall\nsegment prediction to capture both geometric and semantic aspects of structural\ngraphs. To improve the geometric rationality of generated structural graphs, we\npropose two innovative geometry enhancement methods. In wall junction\ngeneration, we propose a novel alignment loss function to improve geometric\nconsistency. In wall segment prediction, we propose a random self-supervision\nmethod to enhance the model's perception of the overall geometric structure,\nthereby promoting the generation of reasonable geometric structures. Employing\nthe diffusion model and the Transformer model, as well as the geometry\nenhancement strategies, our framework can generate wall junctions, wall\nsegments and room polygons with structural and semantic information, resulting\nin structural graphs that accurately represent floorplans. Extensive\nexperiments show that the proposed method surpasses existing techniques,\nenabling free generation and constrained generation, marking a shift towards\nstructure generation in architectural design. Code and data are available at\nhttps://github.com/SizheHu/GSDiff.\n","authors":["Sizhe Hu","Wenming Wu","Yuntao Wang","Benzhu Xu","Liping Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.16258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v1","updated":"2024-12-16T16:41:51Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modality and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11953v1","updated":"2024-12-16T16:37:03Z","published":"2024-12-16T16:37:03Z","title":"Reliable Breast Cancer Molecular Subtype Prediction based on\n  uncertainty-aware Bayesian Deep Learning by Mammography","summary":"  Breast cancer is a heterogeneous disease with different molecular subtypes,\nclinical behavior, treatment responses as well as survival outcomes. The\ndevelopment of a reliable, accurate, available and inexpensive method to\npredict the molecular subtypes using medical images plays an important role in\nthe diagnosis and prognosis of breast cancer. Recently, deep learning methods\nhave shown good performance in the breast cancer classification tasks using\nvarious medical images. Despite all that success, classical deep learning\ncannot deliver the predictive uncertainty. The uncertainty represents the\nvalidity of the predictions.Therefore, the high predicted uncertainty might\ncause a negative effect in the accurate diagnosis of breast cancer molecular\nsubtypes. To overcome this, uncertainty quantification methods are used to\ndetermine the predictive uncertainty. Accordingly, in this study, we proposed\nan uncertainty-aware Bayesian deep learning model using the full mammogram\nimages. In addition, to increase the performance of the multi-class molecular\nsubtype classification task, we proposed a novel hierarchical classification\nstrategy, named the two-stage classification strategy. The separate AUC of the\nproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,\nluminal and triple-negative classes, respectively. The proposed model not only\nhas a comparable performance to other studies in the field of breast cancer\nmolecular subtypes prediction, even using full mammography images, but it is\nalso more reliable, due to quantify the predictive uncertainty.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2412.11953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11952v1","updated":"2024-12-16T16:35:35Z","published":"2024-12-16T16:35:35Z","title":"Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided\n  Self-Supervised Learning","summary":"  Image Aesthetic Assessment (IAA) is a vital and intricate task that entails\nanalyzing and assessing an image's aesthetic values, and identifying its\nhighlights and areas for improvement. Traditional methods of IAA often\nconcentrate on a single aesthetic task and suffer from inadequate labeled\ndatasets, thus impairing in-depth aesthetic comprehension. Despite efforts to\novercome this challenge through the application of Multi-modal Large Language\nModels (MLLMs), such models remain underdeveloped for IAA purposes. To address\nthis, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic\ninsight. Central to our approach is an innovative multi-scale text-guided\nself-supervised learning technique. This technique features a multi-scale\nfeature alignment module and capitalizes on a wealth of unlabeled data in a\nself-supervised manner to structurally and functionally enhance aesthetic\nability. The empirical evidence indicates that accompanied with extensive\ninstruct-tuning, our model sets new state-of-the-art benchmarks across multiple\ntasks, including aesthetic scoring, aesthetic commenting, and personalized\nimage aesthetic assessment. Remarkably, it also demonstrates zero-shot learning\ncapabilities in the emerging task of aesthetic suggesting. Furthermore, for\npersonalized image aesthetic assessment, we harness the potential of in-context\nlearning and showcase its inherent advantages.\n","authors":["Yuti Liu","Shice Liu","Junyuan Gao","Pengtao Jiang","Hao Zhang","Jinwei Chen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2412.11952v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11949v1","updated":"2024-12-16T16:33:28Z","published":"2024-12-16T16:33:28Z","title":"Coconut Palm Tree Counting on Drone Images with Deep Object Detection\n  and Synthetic Training Data","summary":"  Drones have revolutionized various domains, including agriculture. Recent\nadvances in deep learning have propelled among other things object detection in\ncomputer vision. This study utilized YOLO, a real-time object detector, to\nidentify and count coconut palm trees in Ghanaian farm drone footage. The farm\npresented has lost track of its trees due to different planting phases. While\nmanual counting would be very tedious and error-prone, accurately determining\nthe number of trees is crucial for efficient planning and management of\nagricultural processes, especially for optimizing yields and predicting\nproduction. We assessed YOLO for palm detection within a semi-automated\nframework, evaluated accuracy augmentations, and pondered its potential for\nfarmers. Data was captured in September 2022 via drones. To optimize YOLO with\nscarce data, synthetic images were created for model training and validation.\nThe YOLOv7 model, pretrained on the COCO dataset (excluding coconut palms), was\nadapted using tailored data. Trees from footage were repositioned on synthetic\nimages, with testing on distinct authentic images. In our experiments, we\nadjusted hyperparameters, improving YOLO's mean average precision (mAP). We\nalso tested various altitudes to determine the best drone height. From an\ninitial mAP@.5 of $0.65$, we achieved 0.88, highlighting the value of synthetic\nimages in agricultural scenarios.\n","authors":["Tobias Rohe","Barbara Böhm","Michael Kölle","Jonas Stein","Robert Müller","Claudia Linnhoff-Popien"],"pdf_url":"https://arxiv.org/pdf/2412.11949v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2412.11938v1","updated":"2024-12-16T16:23:05Z","published":"2024-12-16T16:23:05Z","title":"Are the Latent Representations of Foundation Models for Pathology\n  Invariant to Rotation?","summary":"  Self-supervised foundation models for digital pathology encode small patches\nfrom H\\&E whole slide images into latent representations used for downstream\ntasks. However, the invariance of these representations to patch rotation\nremains unexplored. This study investigates the rotational invariance of latent\nrepresentations across twelve foundation models by quantifying the alignment\nbetween non-rotated and rotated patches using mutual $k$-nearest neighbours and\ncosine distance. Models that incorporated rotation augmentation during\nself-supervised training exhibited significantly greater invariance to\nrotations. We hypothesise that the absence of rotational inductive bias in the\ntransformer architecture necessitates rotation augmentation during training to\nachieve learned invariance. Code:\nhttps://github.com/MatousE/rot-invariance-analysis.\n","authors":["Matouš Elphick","Samra Turajlic","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11938v1.pdf","comment":"Samra Turajlic and Guang Yang are joint last authors"},{"id":"http://arxiv.org/abs/2402.12121v2","updated":"2024-12-16T16:09:47Z","published":"2024-02-19T13:16:10Z","title":"IRR: Image Review Ranking Framework for Evaluating Vision-Language\n  Models","summary":"  Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.\n","authors":["Kazuki Hayashi","Kazuma Onishi","Toma Suzuki","Yusuke Ide","Seiji Gobara","Shigeki Saito","Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2402.12121v2.pdf","comment":"18pages, Accepted at COLING25"},{"id":"http://arxiv.org/abs/2412.11917v1","updated":"2024-12-16T16:01:18Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images via text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect. Considering this, we ask how to distinguish the actual discriminative\npower of descriptions from performance boosts that potentially rely on an\nensembling effect. To study this, we propose an alternative evaluation scenario\nthat shows a characteristic behavior if the used descriptions have\ndiscriminative power. Furthermore, we propose a training-free method to select\ndiscriminative descriptions that work independently of classname ensembling\neffects. The training-free method works in the following way: A test image has\na local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then,\nw.r.t. to a small selection set, we extract descriptions that distinguish each\nclass well in the local neighborhood. Using the selected descriptions, we\ndemonstrate improved classification accuracy across seven datasets and provide\nin-depth analysis and insights into the explainability of description-based\nimage classification by VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11906v1","updated":"2024-12-16T15:52:59Z","published":"2024-12-16T15:52:59Z","title":"PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension","summary":"  Multimodal punchlines, which involve humor or sarcasm conveyed in\nimage-caption pairs, are a popular way of communication on online multimedia\nplatforms. With the rapid development of multimodal large language models\n(MLLMs), it is essential to assess their ability to effectively comprehend\nthese punchlines. However, existing benchmarks on punchline comprehension\nsuffer from three major limitations: 1) language shortcuts that allow models to\nsolely rely on text, 2) lack of question diversity, and 3) narrow focus on a\nspecific domain of multimodal content (e.g., cartoon). To address these\nlimitations, we introduce a multimodal \\textbf{Punch}line comprehension\n\\textbf{Bench}mark, named \\textbf{PunchBench}, which is tailored for accurate\nand comprehensive evaluation of punchline comprehension. To enhance the\nevaluation accuracy, we generate synonymous and antonymous captions by\nmodifying original captions, which mitigates the impact of shortcuts in the\ncaptions. To provide a comprehensive evaluation, PunchBench incorporates\ndiverse question formats and image-captions from various domains. On this\nbasis, we conduct extensive evaluations and reveal a significant gap between\nstate-of-the-art MLLMs and humans in punchline comprehension. To improve\npunchline comprehension, we propose Simple-to-Complex Chain-of-Question\n(SC-CoQ) strategy, enabling the models to incrementally address complicated\nquestions by first mastering simple ones. SC-CoQ effectively enhances the\nperformance of various MLLMs on PunchBench, surpassing in-context learning and\nchain-of-thought.\n","authors":["Kun Ouyang","Yuanxin Liu","Shicheng Li","Yi Liu","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2412.11906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02329v3","updated":"2024-12-16T15:42:53Z","published":"2024-01-04T16:06:31Z","title":"Exploring Vacant Classes in Label-Skewed Federated Learning","summary":"  Label skews, characterized by disparities in local label distribution across\nclients, pose a significant challenge in federated learning. As minority\nclasses suffer from worse accuracy due to overfitting on local imbalanced data,\nprior methods often incorporate class-balanced learning techniques during local\ntraining. Although these methods improve the mean accuracy across all classes,\nwe observe that vacant classes-referring to categories absent from a client's\ndata distribution-remain poorly recognized. Besides, there is still a gap in\nthe accuracy of local models on minority classes compared to the global model.\nThis paper introduces FedVLS, a novel approach to label-skewed federated\nlearning that integrates both vacant-class distillation and logit suppression\nsimultaneously. Specifically, vacant-class distillation leverages knowledge\ndistillation during local training on each client to retain essential\ninformation related to vacant classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedVLS, demonstrating superior performance compared to previous\nstate-of-the-art (SOTA) methods across diverse datasets with varying degrees of\nlabel skews. Our code is available at https://github.com/krumpguo/FedVLS.\n","authors":["Kuangpu Guo","Yuhe Ding","Jian Liang","Ran He","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2401.02329v3.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.11892v1","updated":"2024-12-16T15:41:14Z","published":"2024-12-16T15:41:14Z","title":"From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach","summary":"  In this paper, we present CAD2Program, a new method for reconstructing 3D\nparametric models from 2D CAD drawings. Our proposed method is inspired by\nrecent successes in vision-language models (VLMs), and departs from traditional\nmethods which rely on task-specific data representations and/or algorithms.\nSpecifically, on the input side, we simply treat the 2D CAD drawing as a raster\nimage, regardless of its original format, and encode the image with a standard\nViT model. We show that such an encoding scheme achieves competitive\nperformance against existing methods that operate on vector-graphics inputs,\nwhile imposing substantially fewer restrictions on the 2D drawings. On the\noutput side, our method auto-regressively predicts a general-purpose language\ndescribing 3D parametric models in text form. Compared to other sequence\nmodeling methods for CAD which use domain-specific sequence representations\nwith fixed-size slots, our text-based representation is more flexible, and can\nbe easily extended to arbitrary geometric entities and semantic or functional\nproperties. Experimental results on a large-scale dataset of cabinet models\ndemonstrate the effectiveness of our method.\n","authors":["Xilin Wang","Jia Zheng","Yuanchao Hu","Hao Zhu","Qian Yu","Zihan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.11892v1.pdf","comment":"To Appear in AAAI 2025. The project page is at\n  https://manycore-research.github.io/CAD2Program"},{"id":"http://arxiv.org/abs/2412.11890v1","updated":"2024-12-16T15:38:25Z","published":"2024-12-16T15:38:25Z","title":"SegMAN: Omni-scale Context Modeling with State Space Models and Local\n  Attention for Semantic Segmentation","summary":"  High-quality semantic segmentation relies on three key capabilities: global\ncontext modeling, local detail encoding, and multi-scale feature extraction.\nHowever, recent methods struggle to possess all these capabilities\nsimultaneously. Hence, we aim to empower segmentation networks to\nsimultaneously carry out efficient global context modeling, high-quality local\ndetail encoding, and rich multi-scale feature representation for varying input\nresolutions. In this paper, we introduce SegMAN, a novel linear-time model\ncomprising a hybrid feature encoder dubbed SegMAN Encoder, and a decoder based\non state space models. Specifically, the SegMAN Encoder synergistically\nintegrates sliding local attention with dynamic state space models, enabling\nhighly efficient global context modeling while preserving fine-grained local\ndetails. Meanwhile, the MMSCopE module in our decoder enhances multi-scale\ncontext feature extraction and adaptively scales with the input resolution. We\ncomprehensively evaluate SegMAN on three challenging datasets: ADE20K,\nCityscapes, and COCO-Stuff. For instance, SegMAN-B achieves 52.6% mIoU on\nADE20K, outperforming SegNeXt-L by 1.6% mIoU while reducing computational\ncomplexity by over 15% GFLOPs. On Cityscapes, SegMAN-B attains 83.8% mIoU,\nsurpassing SegFormer-B3 by 2.1% mIoU with approximately half the GFLOPs.\nSimilarly, SegMAN-B improves upon VWFormer-B3 by 1.6% mIoU with lower GFLOPs on\nthe COCO-Stuff dataset. Our code is available at\nhttps://github.com/yunxiangfu2001/SegMAN.\n","authors":["Yunxiang Fu","Meng Lou","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11883v1","updated":"2024-12-16T15:32:05Z","published":"2024-12-16T15:32:05Z","title":"Towards Physically-Based Sky-Modeling","summary":"  Accurate environment maps are a key component in rendering photorealistic\noutdoor scenes with coherent illumination. They enable captivating visual arts,\nimmersive virtual reality and a wide range of engineering and scientific\napplications. Recent works have extended sky-models to be more comprehensive\nand inclusive of cloud formations but existing approaches fall short in\nfaithfully recreating key-characteristics in physically captured HDRI. As we\ndemonstrate, environment maps produced by sky-models do not relight scenes with\nthe same tones, shadows, and illumination coherence as physically captured HDR\nimagery. Though the visual quality of DNN-generated LDR and HDR imagery has\ngreatly progressed in recent years, we demonstrate this progress to be\ntangential to sky-modelling. Due to the Extended Dynamic Range (EDR) of 14EV\nrequired for outdoor environment maps inclusive of the sun, sky-modelling\nextends beyond the conventional paradigm of High Dynamic Range Imagery (HDRI).\nIn this work, we propose an all-weather sky-model, learning weathered-skies\ndirectly from physically captured HDR imagery. Per user-controlled positioning\nof the sun and cloud formations, our model (AllSky) allows for emulation of\nphysically captured environment maps with improved retention of the Extended\nDynamic Range (EDR) of the sky.\n","authors":["Ian J. Maquignaz"],"pdf_url":"https://arxiv.org/pdf/2412.11883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11866v1","updated":"2024-12-16T15:20:54Z","published":"2024-12-16T15:20:54Z","title":"Event-based Motion Deblurring via Multi-Temporal Granularity Fusion","summary":"  Conventional frame-based cameras inevitably produce blurry effects due to\nmotion occurring during the exposure time. Event camera, a bio-inspired sensor\noffering continuous visual information could enhance the deblurring\nperformance. Effectively utilizing the high-temporal-resolution event data is\ncrucial for extracting precise motion information and enhancing deblurring\nperformance. However, existing event-based image deblurring methods usually\nutilize voxel-based event representations, losing the fine-grained temporal\ndetails that are mathematically essential for fast motion deblurring. In this\npaper, we first introduce point cloud-based event representation into the image\ndeblurring task and propose a Multi-Temporal Granularity Network (MTGNet). It\ncombines the spatially dense but temporally coarse-grained voxel-based event\nrepresentation and the temporally fine-grained but spatially sparse point\ncloud-based event. To seamlessly integrate such complementary representations,\nwe design a Fine-grained Point Branch. An Aggregation and Mapping Module (AMM)\nis proposed to align the low-level point-based features with frame-based\nfeatures and an Adaptive Feature Diffusion Module (AFDM) is designed to manage\nthe resolution discrepancies between event data and image data by enriching the\nsparse point feature. Extensive subjective and objective evaluations\ndemonstrate that our method outperforms current state-of-the-art approaches on\nboth synthetic and real-world datasets.\n","authors":["Xiaopeng Lin","Hongwei Ren","Yulong Huang","Zunchang Liu","Yue Zhou","Haotian Fu","Biao Pan","Bojun Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.11866v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11863v1","updated":"2024-12-16T15:20:03Z","published":"2024-12-16T15:20:03Z","title":"GeoX: Geometric Problem Solving Through Unified Formalized\n  Vision-Language Pre-training","summary":"  Despite their proficiency in general tasks, Multi-modal Large Language Models\n(MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands\nunderstanding diagrams, interpreting symbols, and performing complex reasoning.\nThis limitation arises from their pre-training on natural images and texts,\nalong with the lack of automated verification in the problem-solving process.\nBesides, current geometric specialists are limited by their task-specific\ndesigns, making them less effective for broader geometric problems. To this\nend, we present GeoX, a multi-modal large model focusing on geometric\nunderstanding and reasoning tasks. Given the significant differences between\ngeometric diagram-symbol and natural image-text, we introduce unimodal\npre-training to develop a diagram encoder and symbol decoder, enhancing the\nunderstanding of geometric images and corpora. Furthermore, we introduce\ngeometry-language alignment, an effective pre-training paradigm that bridges\nthe modality gap between unimodal geometric experts. We propose a\nGenerator-And-Sampler Transformer (GS-Former) to generate discriminative\nqueries and eliminate uninformative representations from unevenly distributed\ngeometric signals. Finally, GeoX benefits from visual instruction tuning,\nempowering it to take geometric images and questions as input and generate\nverifiable solutions. Experiments show that GeoX outperforms both generalists\nand geometric specialists on publicly recognized benchmarks, such as GeoQA,\nUniGeo, Geometry3K, and PGPS9k.\n","authors":["Renqiu Xia","Mingsheng Li","Hancheng Ye","Wenjie Wu","Hongbin Zhou","Jiakang Yuan","Tianshuo Peng","Xinyu Cai","Xiangchao Yan","Bin Wang","Conghui He","Botian Shi","Tao Chen","Junchi Yan","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11863v1.pdf","comment":"Our code is available at https://github.com/UniModal4Reasoning/GeoX"},{"id":"http://arxiv.org/abs/2409.12784v5","updated":"2024-12-16T15:13:24Z","published":"2024-09-19T13:51:21Z","title":"Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering","summary":"  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing text-to-image models can\ncorrectly respond to these questions. The I-HallA v1.0 dataset comprises 1.2K\ndiverse image-text pairs across nine categories with 1,000 rigorously curated\nquestions covering various compositional challenges. We evaluate five\ntext-to-image models using I-HallA and reveal that these state-of-the-art\nmodels often fail to accurately convey factual information. Moreover, we\nvalidate the reliability of our metric by demonstrating a strong Spearman\ncorrelation (rho=0.95) with human judgments. We believe our benchmark dataset\nand metric can serve as a foundation for developing factually accurate\ntext-to-image generation models.\n","authors":["Youngsun Lim","Hojun Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2409.12784v5.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2412.11849v1","updated":"2024-12-16T15:10:53Z","published":"2024-12-16T15:10:53Z","title":"Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tumor Analysis\n  in Multimodal MRI","summary":"  Motivated by the need for advanced solutions in the segmentation and\ninpainting of glioma-affected brain regions in multi-modal magnetic resonance\nimaging (MRI), this study presents an integrated approach leveraging the\nstrengths of ensemble learning with hybrid transformer models and convolutional\nneural networks (CNNs), alongside the innovative application of 3D Pix2Pix\nGenerative Adversarial Network (GAN). Our methodology combines robust tumor\nsegmentation capabilities, utilizing axial attention and transformer encoders\nfor enhanced spatial relationship modeling, with the ability to synthesize\nbiologically plausible brain tissue through 3D Pix2Pix GAN. This integrated\napproach addresses the BraTS 2023 cluster challenges by offering precise\nsegmentation and realistic inpainting, tailored for diverse tumor types and\nsub-regions. The results demonstrate outstanding performance, evidenced by\nquantitative evaluations such as the Dice Similarity Coefficient (DSC),\nHausdorff Distance (HD95) for segmentation, and Structural Similarity Index\nMeasure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean-Square Error (MSE)\nfor inpainting. Qualitative assessments further validate the high-quality,\nclinically relevant outputs. In conclusion, this study underscores the\npotential of combining advanced machine learning techniques for comprehensive\nbrain tumor analysis, promising significant advancements in clinical\ndecision-making and patient care within the realm of medical imaging.\n","authors":["Ramy A. Zeineldin","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2412.11849v1.pdf","comment":"Accepted at the MICCAI BraTS Challenge 2023"},{"id":"http://arxiv.org/abs/2412.11840v1","updated":"2024-12-16T15:03:08Z","published":"2024-12-16T15:03:08Z","title":"Sonar-based Deep Learning in Underwater Robotics: Overview, Robustness\n  and Challenges","summary":"  With the growing interest in underwater exploration and monitoring,\nAutonomous Underwater Vehicles (AUVs) have become essential. The recent\ninterest in onboard Deep Learning (DL) has advanced real-time environmental\ninteraction capabilities relying on efficient and accurate vision-based DL\nmodels. However, the predominant use of sonar in underwater environments,\ncharacterized by limited training data and inherent noise, poses challenges to\nmodel robustness. This autonomy improvement raises safety concerns for\ndeploying such models during underwater operations, potentially leading to\nhazardous situations. This paper aims to provide the first comprehensive\noverview of sonar-based DL under the scope of robustness. It studies\nsonar-based DL perception task models, such as classification, object\ndetection, segmentation, and SLAM. Furthermore, the paper systematizes\nsonar-based state-of-the-art datasets, simulators, and robustness methods such\nas neural network verification, out-of-distribution, and adversarial attacks.\nThis paper highlights the lack of robustness in sonar-based DL research and\nsuggests future research pathways, notably establishing a baseline sonar-based\ndataset and bridging the simulation-to-reality gap.\n","authors":["Martin Aubard","Ana Madureira","Luís Teixeira","José Pinto"],"pdf_url":"https://arxiv.org/pdf/2412.11840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11836v1","updated":"2024-12-16T14:57:40Z","published":"2024-12-16T14:57:40Z","title":"UnMA-CapSumT: Unified and Multi-Head Attention-driven Caption\n  Summarization Transformer","summary":"  Image captioning is the generation of natural language descriptions of images\nwhich have increased immense popularity in the recent past. With this different\ndeep-learning techniques are devised for the development of factual and\nstylized image captioning models. Previous models focused more on the\ngeneration of factual and stylized captions separately providing more than one\ncaption for a single image. The descriptions generated from these suffer from\nout-of-vocabulary and repetition issues. To the best of our knowledge, no such\nwork exists that provided a description that integrates different captioning\nmethods to describe the contents of an image with factual and stylized\n(romantic and humorous) elements. To overcome these limitations, this paper\npresents a novel Unified Attention and Multi-Head Attention-driven Caption\nSummarization Transformer (UnMA-CapSumT) based Captioning Framework. It\nutilizes both factual captions and stylized captions generated by the Modified\nAdaptive Attention-based factual image captioning model (MAA-FIC) and Style\nFactored Bi-LSTM with attention (SF-Bi-ALSTM) driven stylized image captioning\nmodel respectively. SF-Bi-ALSTM-based stylized IC model generates two prominent\nstyles of expression- {romance, and humor}. The proposed summarizer UnMHA-ST\ncombines both factual and stylized descriptions of an input image to generate\nstyled rich coherent summarized captions. The proposed UnMHA-ST transformer\nlearns and summarizes different linguistic styles efficiently by incorporating\nproposed word embedding fastText with Attention Word Embedding (fTA-WE) and\npointer-generator network with coverage mechanism concept to solve the\nout-of-vocabulary issues and repetition problem. Extensive experiments are\nconducted on Flickr8K and a subset of FlickrStyle10K with supporting ablation\nstudies to prove the efficiency and efficacy of the proposed framework.\n","authors":["Dhruv Sharma","Chhavi Dhiman","Dinesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2412.11836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17629v4","updated":"2024-12-16T14:56:52Z","published":"2023-11-29T13:43:17Z","title":"RQFormer: Rotated Query Transformer for End-to-End Oriented Object\n  Detection","summary":"  Oriented object detection presents a challenging task due to the presence of\nobject instances with multiple orientations, varying scales, and dense\ndistributions. Recently, end-to-end detectors have made significant strides by\nemploying attention mechanisms and refining a fixed number of queries through\nconsecutive decoder layers. However, existing end-to-end oriented object\ndetectors still face two primary challenges: 1) misalignment between positional\nqueries and keys, leading to inconsistency between classification and\nlocalization; and 2) the presence of a large number of similar queries, which\ncomplicates one-to-one label assignments and optimization. To address these\nlimitations, we propose an end-to-end oriented detector called the Rotated\nQuery Transformer, which integrates two key technologies: Rotated RoI Attention\n(RRoI Attention) and Selective Distinct Queries (SDQ). First, RRoI Attention\naligns positional queries and keys from oriented regions of interest through\ncross-attention. Second, SDQ collects queries from intermediate decoder layers\nand filters out similar ones to generate distinct queries, thereby facilitating\nthe optimization of one-to-one label assignments. Finally, extensive\nexperiments conducted on four remote sensing datasets and one scene text\ndataset demonstrate the effectiveness of our method. To further validate its\ngeneralization capability, we also extend our approach to horizontal object\ndetection The code is available at\n\\url{https://github.com/wokaikaixinxin/RQFormer}.\n","authors":["Jiaqi Zhao","Zeyu Ding","Yong Zhou","Hancheng Zhu","Wenliang Du","Rui Yao","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2311.17629v4.pdf","comment":"This article is accepted by Expert Systems With Applications (ESWA)\n  2024"},{"id":"http://arxiv.org/abs/2403.13804v2","updated":"2024-12-16T14:53:21Z","published":"2024-03-20T17:59:43Z","title":"Learning from Synthetic Data for Visual Grounding","summary":"  This paper extensively investigates the effectiveness of synthetic training\ndata to improve the capabilities of vision-and-language models for grounding\ntextual descriptions to image regions. We explore various strategies to best\ngenerate image-text pairs and image-text-box triplets using a series of\npretrained models under different settings and varying degrees of reliance on\nreal data. Through comparative analyses with synthetic, real, and web-crawled\ndata, we identify factors that contribute to performance differences, and\npropose SynGround, an effective pipeline for generating useful synthetic data\nfor visual grounding. Our findings show that SynGround can improve the\nlocalization capabilities of off-the-shelf vision-and-language models and\noffers the potential for arbitrarily large scale data generation. Particularly,\ndata generated with SynGround improves the pointing game accuracy of a\npretrained ALBEF and BLIP models by 4.81% and 17.11% absolute percentage\npoints, respectively, across the RefCOCO+ and the Flickr30k benchmarks.\n","authors":["Ruozhen He","Ziyan Yang","Paola Cascante-Bonilla","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v2.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2412.07527v2","updated":"2024-12-16T14:43:29Z","published":"2024-12-10T14:03:41Z","title":"Deep Joint Unrolling for Deblurring and Low-Light Image Enhancement\n  (JUDE)","summary":"  Low-light and blurring issues are prevalent when capturing photos at night,\noften due to the use of long exposure to address dim environments. Addressing\nthese joint problems can be challenging and error-prone if an end-to-end model\nis trained without incorporating an appropriate physical model. In this paper,\nwe introduce JUDE, a Deep Joint Unrolling for Deblurring and Low-Light Image\nEnhancement, inspired by the image physical model. Based on Retinex theory and\nthe blurring model, the low-light blurry input is iteratively deblurred and\ndecomposed, producing sharp low-light reflectance and illuminance through an\nunrolling mechanism. Additionally, we incorporate various modules to estimate\nthe initial blur kernel, enhance brightness, and eliminate noise in the final\nimage. Comprehensive experiments on LOL-Blur and Real-LOL-Blur demonstrate that\nour method outperforms existing techniques both quantitatively and\nqualitatively.\n","authors":["Tu Vo","Chan Y. Park"],"pdf_url":"https://arxiv.org/pdf/2412.07527v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2412.11820v1","updated":"2024-12-16T14:37:16Z","published":"2024-12-16T14:37:16Z","title":"Spatiotemporal Blind-Spot Network with Calibrated Flow Alignment for\n  Self-Supervised Video Denoising","summary":"  Self-supervised video denoising aims to remove noise from videos without\nrelying on ground truth data, leveraging the video itself to recover clean\nframes. Existing methods often rely on simplistic feature stacking or apply\noptical flow without thorough analysis. This results in suboptimal utilization\nof both inter-frame and intra-frame information, and it also neglects the\npotential of optical flow alignment under self-supervised conditions, leading\nto biased and insufficient denoising outcomes. To this end, we first explore\nthe practicality of optical flow in the self-supervised setting and introduce a\nSpatioTemporal Blind-spot Network (STBN) for global frame feature utilization.\nIn the temporal domain, we utilize bidirectional blind-spot feature propagation\nthrough the proposed blind-spot alignment block to ensure accurate temporal\nalignment and effectively capture long-range dependencies. In the spatial\ndomain, we introduce the spatial receptive field expansion module, which\nenhances the receptive field and improves global perception capabilities.\nAdditionally, to reduce the sensitivity of optical flow estimation to noise, we\npropose an unsupervised optical flow distillation mechanism that refines\nfine-grained inter-frame interactions during optical flow alignment. Our method\ndemonstrates superior performance across both synthetic and real-world video\ndenoising datasets. The source code is publicly available at\nhttps://github.com/ZKCCZ/STBN.\n","authors":["Zikang Chen","Tao Jiang","Xiaowan Hu","Wang Zhang","Huaqiu Li","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11819v1","updated":"2024-12-16T14:35:52Z","published":"2024-12-16T14:35:52Z","title":"HiGDA: Hierarchical Graph of Nodes to Learn Local-to-Global Topology for\n  Semi-Supervised Domain Adaptation","summary":"  The enhanced representational power and broad applicability of deep learning\nmodels have attracted significant interest from the research community in\nrecent years. However, these models often struggle to perform effectively under\ndomain shift conditions, where the training data (the source domain) is related\nto but exhibits different distributions from the testing data (the target\ndomain). To address this challenge, previous studies have attempted to reduce\nthe domain gap between source and target data by incorporating a few labeled\ntarget samples during training - a technique known as semi-supervised domain\nadaptation (SSDA). While this strategy has demonstrated notable improvements in\nclassification performance, the network architectures used in these approaches\nprimarily focus on exploiting the features of individual images, leaving room\nfor improvement in capturing rich representations. In this study, we introduce\na Hierarchical Graph of Nodes designed to simultaneously present\nrepresentations at both feature and category levels. At the feature level, we\nintroduce a local graph to identify the most relevant patches within an image,\nfacilitating adaptability to defined main object representations. At the\ncategory level, we employ a global graph to aggregate the features from samples\nwithin the same category, thereby enriching overall representations. Extensive\nexperiments on widely used SSDA benchmark datasets, including Office-Home,\nDomainNet, and VisDA2017, demonstrate that both quantitative and qualitative\nresults substantiate the effectiveness of HiGDA, establishing it as a new\nstate-of-the-art method.\n","authors":["Ba Hung Ngo","Doanh C. Bui","Nhat-Tuong Do-Tran","Tae Jong Choi"],"pdf_url":"https://arxiv.org/pdf/2412.11819v1.pdf","comment":"Accepted for presentation at AAAI2025"},{"id":"http://arxiv.org/abs/2404.13282v2","updated":"2024-12-16T14:33:03Z","published":"2024-04-20T06:01:09Z","title":"Wills Aligner: Multi-Subject Collaborative Brain Visual Decoding","summary":"  Decoding visual information from human brain activity has seen remarkable\nadvancements in recent research. However, the diversity in cortical\nparcellation and fMRI patterns across individuals has prompted the development\nof deep learning models tailored to each subject. The personalization limits\nthe broader applicability of brain visual decoding in real-world scenarios. To\naddress this issue, we introduce Wills Aligner, a novel approach designed to\nachieve multi-subject collaborative brain visual decoding. Wills Aligner begins\nby aligning the fMRI data from different subjects at the anatomical level. It\nthen employs delicate mixture-of-brain-expert adapters and a meta-learning\nstrategy to account for individual fMRI pattern differences. Additionally,\nWills Aligner leverages the semantic relation of visual stimuli to guide the\nlearning of inter-subject commonality, enabling visual decoding for each\nsubject to draw insights from other subjects' data. We rigorously evaluate our\nWills Aligner across various visual decoding tasks, including classification,\ncross-modal retrieval, and image reconstruction. The experimental results\ndemonstrate that Wills Aligner achieves promising performance.\n","authors":["Guangyin Bao","Qi Zhang","Zixuan Gong","Jialei Zhou","Wei Fan","Kun Yi","Usman Naseem","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2404.13282v2.pdf","comment":"AAAI 2025, 16 pages"},{"id":"http://arxiv.org/abs/2412.11815v1","updated":"2024-12-16T14:32:49Z","published":"2024-12-16T14:32:49Z","title":"ColorFlow: Retrieval-Augmented Image Sequence Colorization","summary":"  Automatic black-and-white image sequence colorization while preserving\ncharacter and object identity (ID) is a complex task with significant market\ndemand, such as in cartoon or comic series colorization. Despite advancements\nin visual colorization using large-scale generative models like diffusion\nmodels, challenges with controllability and identity consistency persist,\nmaking current solutions unsuitable for industrial application.To address this,\nwe propose ColorFlow, a three-stage diffusion-based framework tailored for\nimage sequence colorization in industrial applications. Unlike existing methods\nthat require per-ID finetuning or explicit ID embedding extraction, we propose\na novel robust and generalizable Retrieval Augmented Colorization pipeline for\ncolorizing images with relevant color references. Our pipeline also features a\ndual-branch design: one branch for color identity extraction and the other for\ncolorization, leveraging the strengths of diffusion models. We utilize the\nself-attention mechanism in diffusion models for strong in-context learning and\ncolor identity matching. To evaluate our model, we introduce ColorFlow-Bench, a\ncomprehensive benchmark for reference-based colorization. Results show that\nColorFlow outperforms existing models across multiple metrics, setting a new\nstandard in sequential image colorization and potentially benefiting the art\nindustry. We release our codes and models on our project page:\nhttps://zhuang2002.github.io/ColorFlow/.\n","authors":["Junhao Zhuang","Xuan Ju","Zhaoyang Zhang","Yong Liu","Shiyi Zhang","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.11815v1.pdf","comment":"Project Page: https://zhuang2002.github.io/ColorFlow/"},{"id":"http://arxiv.org/abs/2412.11813v1","updated":"2024-12-16T14:29:31Z","published":"2024-12-16T14:29:31Z","title":"Designing Semi-Structured Pruning of Graph Convolutional Networks for\n  Skeleton-based Recognition","summary":"  Deep neural networks (DNNs) are nowadays witnessing a major success in\nsolving many pattern recognition tasks including skeleton-based classification.\nThe deployment of DNNs on edge-devices, endowed with limited time and memory\nresources, requires designing lightweight and efficient variants of these\nnetworks. Pruning is one of the lightweight network design techniques that\noperate by removing unnecessary network parts, in a structured or an\nunstructured manner, including individual weights, neurons or even entire\nchannels. Nonetheless, structured and unstructured pruning methods, when\napplied separately, may either be inefficient or ineffective. In this paper, we\ndevise a novel semi-structured method that discards the downsides of structured\nand unstructured pruning while gathering their upsides to some extent. The\nproposed solution is based on a differentiable cascaded parametrization which\ncombines (i) a band-stop mechanism that prunes weights depending on their\nmagnitudes, (ii) a weight-sharing parametrization that prunes connections\neither individually or group-wise, and (iii) a gating mechanism which\narbitrates between different group-wise and entry-wise pruning. All these\ncascaded parametrizations are built upon a common latent tensor which is\ntrained end-to-end by minimizing a classification loss and a surrogate tensor\nrank regularizer. Extensive experiments, conducted on the challenging tasks of\naction and hand-gesture recognition, show the clear advantage of our proposed\nsemi-structured pruning approach against both structured and unstructured\npruning, when taken separately, as well as the related work.\n","authors":["Hichem Sahbi"],"pdf_url":"https://arxiv.org/pdf/2412.11813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11812v1","updated":"2024-12-16T14:25:52Z","published":"2024-12-16T14:25:52Z","title":"CLDA-YOLO: Visual Contrastive Learning Based Domain Adaptive YOLO\n  Detector","summary":"  Unsupervised domain adaptive (UDA) algorithms can markedly enhance the\nperformance of object detectors under conditions of domain shifts, thereby\nreducing the necessity for extensive labeling and retraining. Current domain\nadaptive object detection algorithms primarily cater to two-stage detectors,\nwhich tend to offer minimal improvements when directly applied to single-stage\ndetectors such as YOLO. Intending to benefit the YOLO detector from UDA, we\nbuild a comprehensive domain adaptive architecture using a teacher-student\ncooperative system for the YOLO detector. In this process, we propose\nuncertainty learning to cope with pseudo-labeling generated by the teacher\nmodel with extreme uncertainty and leverage dynamic data augmentation to\nasymptotically adapt the teacher-student system to the environment. To address\nthe inability of single-stage object detectors to align at multiple stages, we\nutilize a unified visual contrastive learning paradigm that aligns instance at\nbackbone and head respectively, which steadily improves the robustness of the\ndetectors in cross-domain tasks. In summary, we present an unsupervised domain\nadaptive YOLO detector based on visual contrastive learning (CLDA-YOLO), which\nachieves highly competitive results across multiple domain adaptive datasets\nwithout any reduction in inference speed.\n","authors":["Tianheng Qiu","Ka Lung Law","Guanghua Pan","Jufei Wang","Xin Gao","Xuan Huang","Hu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.11812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11807v1","updated":"2024-12-16T14:18:01Z","published":"2024-12-16T14:18:01Z","title":"PhysAug: A Physical-guided and Frequency-based Data Augmentation for\n  Single-Domain Generalized Object Detection","summary":"  Single-Domain Generalized Object Detection~(S-DGOD) aims to train on a single\nsource domain for robust performance across a variety of unseen target domains\nby taking advantage of an object detector. Existing S-DGOD approaches often\nrely on data augmentation strategies, including a composition of visual\ntransformations, to enhance the detector's generalization ability. However, the\nabsence of real-world prior knowledge hinders data augmentation from\ncontributing to the diversity of training data distributions. To address this\nissue, we propose PhysAug, a novel physical model-based non-ideal imaging\ncondition data augmentation method, to enhance the adaptability of the S-DGOD\ntasks. Drawing upon the principles of atmospheric optics, we develop a\nuniversal perturbation model that serves as the foundation for our proposed\nPhysAug. Given that visual perturbations typically arise from the interaction\nof light with atmospheric particles, the image frequency spectrum is harnessed\nto simulate real-world variations during training. This approach fosters the\ndetector to learn domain-invariant representations, thereby enhancing its\nability to generalize across various settings. Without altering the network\narchitecture or loss function, our approach significantly outperforms the\nstate-of-the-art across various S-DGOD datasets. In particular, it achieves a\nsubstantial improvement of $7.3\\%$ and $7.2\\%$ over the baseline on DWD and\nCityscape-C, highlighting its enhanced generalizability in real-world settings.\n","authors":["Xiaoran Xu","Jiangang Yang","Wenhui Shi","Siyuan Ding","Luqing Luo","Jian Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11802v1","updated":"2024-12-16T14:12:06Z","published":"2024-12-16T14:12:06Z","title":"AMI-Net: Adaptive Mask Inpainting Network for Industrial Anomaly\n  Detection and Localization","summary":"  Unsupervised visual anomaly detection is crucial for enhancing industrial\nproduction quality and efficiency. Among unsupervised methods, reconstruction\napproaches are popular due to their simplicity and effectiveness. The key\naspect of reconstruction methods lies in the restoration of anomalous regions,\nwhich current methods have not satisfactorily achieved. To tackle this issue,\nwe introduce a novel \\uline{A}daptive \\uline{M}ask \\uline{I}npainting\n\\uline{Net}work (AMI-Net) from the perspective of adaptive mask-inpainting. In\ncontrast to traditional reconstruction methods that treat non-semantic image\npixels as targets, our method uses a pre-trained network to extract multi-scale\nsemantic features as reconstruction targets. Given the multiscale nature of\nindustrial defects, we incorporate a training strategy involving random\npositional and quantitative masking. Moreover, we propose an innovative\nadaptive mask generator capable of generating adaptive masks that effectively\nmask anomalous regions while preserving normal regions. In this manner, the\nmodel can leverage the visible normal global contextual information to restore\nthe masked anomalous regions, thereby effectively suppressing the\nreconstruction of defects. Extensive experimental results on the MVTec AD and\nBTAD industrial datasets validate the effectiveness of the proposed method.\nAdditionally, AMI-Net exhibits exceptional real-time performance, striking a\nfavorable balance between detection accuracy and speed, rendering it highly\nsuitable for industrial applications. Code is available at:\nhttps://github.com/luow23/AMI-Net\n","authors":["Wei Luo","Haiming Yao","Wenyong Yu","Zhengyong Li"],"pdf_url":"https://arxiv.org/pdf/2412.11802v1.pdf","comment":"Accepted by IEEE Transactions on Automation Science and\n  Engineering.Code is available at: https://github.com/luow23/AMI-Net"},{"id":"http://arxiv.org/abs/2409.09424v2","updated":"2024-12-16T14:05:25Z","published":"2024-09-14T12:25:14Z","title":"NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection","summary":"  Data augmentation has shown significant advancements in computer vision to\nimprove model performance over the years, particularly in scenarios with\nlimited and insufficient data. Currently, most studies focus on adjusting the\nimage or its features to expand the size, quality, and variety of samples\nduring training in various tasks including object detection. However, we argue\nthat it is necessary to investigate bounding box transformations as a data\naugmentation technique rather than image-level transformations, especially in\naerial imagery due to potentially inconsistent bounding box annotations. Hence,\nthis letter presents a thorough investigation of bounding box transformation in\nterms of scaling, rotation, and translation for remote sensing object\ndetection. We call this augmentation strategy NBBOX (Noise Injection into\nBounding Box). We conduct extensive experiments on DOTA and DIOR-R, both\nwell-known datasets that include a variety of rotated generic objects in aerial\nimages. Experimental results show that our approach significantly improves\nremote sensing object detection without whistles and bells and it is more\ntime-efficient than other state-of-the-art augmentation strategies.\n","authors":["Yechan Kim","SooYeon Kim","Moongu Jeon"],"pdf_url":"https://arxiv.org/pdf/2409.09424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11788v1","updated":"2024-12-16T14:00:30Z","published":"2024-12-16T14:00:30Z","title":"Neural Collapse Inspired Knowledge Distillation","summary":"  Existing knowledge distillation (KD) methods have demonstrated their ability\nin achieving student network performance on par with their teachers. However,\nthe knowledge gap between the teacher and student remains significant and may\nhinder the effectiveness of the distillation process. In this work, we\nintroduce the structure of Neural Collapse (NC) into the KD framework. NC\ntypically occurs in the final phase of training, resulting in a graceful\ngeometric structure where the last-layer features form a simplex equiangular\ntight frame. Such phenomenon has improved the generalization of deep network\ntraining. We hypothesize that NC can also alleviate the knowledge gap in\ndistillation, thereby enhancing student performance. This paper begins with an\nempirical analysis to bridge the connection between knowledge distillation and\nneural collapse. Through this analysis, we establish that transferring the\nteacher's NC structure to the student benefits the distillation process.\nTherefore, instead of merely transferring instance-level logits or features, as\ndone by existing distillation methods, we encourage students to learn the\nteacher's NC structure. Thereby, we propose a new distillation paradigm termed\nNeural Collapse-inspired Knowledge Distillation (NCKD). Comprehensive\nexperiments demonstrate that NCKD is simple yet effective, improving the\ngeneralization of all distilled student models and achieving state-of-the-art\naccuracy performance.\n","authors":["Shuoxi Zhang","Zijian Song","Kun He"],"pdf_url":"https://arxiv.org/pdf/2412.11788v1.pdf","comment":"13 pages, 7 figures. Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2404.12630v2","updated":"2024-12-16T13:59:51Z","published":"2024-04-19T05:12:04Z","title":"MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and\n  Semantic Correction","summary":"  Decoding natural visual scenes from brain activity has flourished, with\nextensive research in single-subject tasks and, however, less in cross-subject\ntasks. Reconstructing high-quality images in cross-subject tasks is a\nchallenging problem due to profound individual differences between subjects and\nthe scarcity of data annotation. In this work, we proposed MindTuner for\ncross-subject visual decoding, which achieves high-quality and rich semantic\nreconstructions using only 1 hour of fMRI training data benefiting from the\nphenomena of visual fingerprint in the human visual system and a novel\nfMRI-to-text alignment paradigm. Firstly, we pre-train a multi-subject model\namong 7 subjects and fine-tune it with scarce data on new subjects, where LoRAs\nwith Skip-LoRAs are utilized to learn the visual fingerprint. Then, we take the\nimage modality as the intermediate pivot modality to achieve fMRI-to-text\nalignment, which achieves impressive fMRI-to-text retrieval performance and\ncorrects fMRI-to-image reconstruction with fine-tuned semantics. The results of\nboth qualitative and quantitative analyses demonstrate that MindTuner surpasses\nstate-of-the-art cross-subject visual decoding models on the Natural Scenes\nDataset (NSD), whether using training data of 1 hour or 40 hours.\n","authors":["Zixuan Gong","Qi Zhang","Guangyin Bao","Lei Zhu","Ke Liu","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2404.12630v2.pdf","comment":"AAAI 2025, 14 pages"},{"id":"http://arxiv.org/abs/2412.11785v1","updated":"2024-12-16T13:57:02Z","published":"2024-12-16T13:57:02Z","title":"InterDyn: Controllable Interactive Dynamics with Video Diffusion Models","summary":"  Predicting the dynamics of interacting objects is essential for both humans\nand intelligent systems. However, existing approaches are limited to\nsimplified, toy settings and lack generalizability to complex, real-world\nenvironments. Recent advances in generative models have enabled the prediction\nof state transitions based on interventions, but focus on generating a single\nfuture state which neglects the continuous motion and subsequent dynamics\nresulting from the interaction. To address this gap, we propose InterDyn, a\nnovel framework that generates videos of interactive dynamics given an initial\nframe and a control signal encoding the motion of a driving object or actor.\nOur key insight is that large video foundation models can act as both neural\nrenderers and implicit physics simulators by learning interactive dynamics from\nlarge-scale video data. To effectively harness this capability, we introduce an\ninteractive control mechanism that conditions the video generation process on\nthe motion of the driving entity. Qualitative results demonstrate that InterDyn\ngenerates plausible, temporally consistent videos of complex object\ninteractions while generalizing to unseen objects. Quantitative evaluations\nshow that InterDyn outperforms baselines that focus on static state\ntransitions. This work highlights the potential of leveraging video generative\nmodels as implicit physics engines.\n","authors":["Rick Akkerman","Haiwen Feng","Michael J. Black","Dimitrios Tzionas","Victoria Fernández Abrevaya"],"pdf_url":"https://arxiv.org/pdf/2412.11785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19746v2","updated":"2024-12-16T13:53:50Z","published":"2024-05-30T06:49:59Z","title":"DenseSeg: Joint Learning for Semantic Segmentation and Landmark\n  Detection Using Dense Image-to-Shape Representation","summary":"  Purpose: Semantic segmentation and landmark detection are fundamental tasks\nof medical image processing, facilitating further analysis of anatomical\nobjects. Although deep learning-based pixel-wise classification has set a\nnew-state-of-the-art for segmentation, it falls short in landmark detection, a\nstrength of shape-based approaches. Methods: In this work, we propose a dense\nimage-to-shape representation that enables the joint learning of landmarks and\nsemantic segmentation by employing a fully convolutional architecture. Our\nmethod intuitively allows the extraction of arbitrary landmarks due to its\nrepresentation of anatomical correspondences. We benchmark our method against\nthe state-of-the-art for semantic segmentation (nnUNet), a shape-based approach\nemploying geometric deep learning and a convolutional neural network-based\nmethod for landmark detection. Results: We evaluate our method on two medical\ndataset: one common benchmark featuring the lungs, heart, and clavicle from\nthorax X-rays, and another with 17 different bones in the paediatric wrist.\nWhile our method is on pair with the landmark detection baseline in the thorax\nsetting (error in mm of $2.6\\pm0.9$ vs $2.7\\pm0.9$), it substantially surpassed\nit in the more complex wrist setting ($1.1\\pm0.6$ vs $1.9\\pm0.5$). Conclusion:\nWe demonstrate that dense geometric shape representation is beneficial for\nchallenging landmark detection tasks and outperforms previous state-of-the-art\nusing heatmap regression. While it does not require explicit training on the\nlandmarks themselves, allowing for the addition of new landmarks without\nnecessitating retraining.}\n","authors":["Ron Keuth","Lasse Hansen","Maren Balks","Ronja Jäger","Anne-Nele Schröder","Ludger Tüshaus","Mattias Heinrich"],"pdf_url":"https://arxiv.org/pdf/2405.19746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11779v1","updated":"2024-12-16T13:49:57Z","published":"2024-12-16T13:49:57Z","title":"Impact of Face Alignment on Face Image Quality","summary":"  Face alignment is a crucial step in preparing face images for feature\nextraction in facial analysis tasks. For applications such as face recognition,\nfacial expression recognition, and facial attribute classification, alignment\nis widely utilized during both training and inference to standardize the\npositions of key landmarks in the face. It is well known that the application\nand method of face alignment significantly affect the performance of facial\nanalysis models. However, the impact of alignment on face image quality has not\nbeen thoroughly investigated. Current FIQA studies often assume alignment as a\nprerequisite but do not explicitly evaluate how alignment affects quality\nmetrics, especially with the advent of modern deep learning-based detectors\nthat integrate detection and landmark localization. To address this need, our\nstudy examines the impact of face alignment on face image quality scores. We\nconducted experiments on the LFW, IJB-B, and SCFace datasets, employing MTCNN\nand RetinaFace models for face detection and alignment. To evaluate face image\nquality, we utilized several assessment methods, including SER-FIQ, FaceQAN,\nDifFIQA, and SDD-FIQA. Our analysis included examining quality score\ndistributions for the LFW and IJB-B datasets and analyzing average quality\nscores at varying distances in the SCFace dataset. Our findings reveal that\nface image quality assessment methods are sensitive to alignment. Moreover,\nthis sensitivity increases under challenging real-life conditions, highlighting\nthe importance of evaluating alignment's role in quality assessment.\n","authors":["Eren Onaran","Erdi Sarıtaş","Hazım Kemal Ekenel"],"pdf_url":"https://arxiv.org/pdf/2412.11779v1.pdf","comment":"Accepted at EAI ROSENET 2024 - 8th EAI International Conference on\n  Robotic Sensor Networks"},{"id":"http://arxiv.org/abs/2412.04903v2","updated":"2024-12-16T13:47:29Z","published":"2024-12-06T09:59:47Z","title":"EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation","summary":"  Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.\n","authors":["Yongxin Wang","Meng Cao","Haokun Lin","Mingfei Han","Liang Ma","Jin Jiang","Yuhao Cheng","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.04903v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2412.11771v1","updated":"2024-12-16T13:44:26Z","published":"2024-12-16T13:44:26Z","title":"Point Cloud-Assisted Neural Image Compression","summary":"  High-efficient image compression is a critical requirement. In several\nscenarios where multiple modalities of data are captured by different sensors,\nthe auxiliary information from other modalities are not fully leveraged by\nexisting image-only codecs, leading to suboptimal compression efficiency. In\nthis paper, we increase image compression performance with the assistance of\npoint cloud, which is widely adopted in the area of autonomous driving. We\nfirst unify the data representation for both modalities to facilitate data\nprocessing. Then, we propose the point cloud-assisted neural image codec\n(PCA-NIC) to enhance the preservation of image texture and structure by\nutilizing the high-dimensional point cloud information. We further introduce a\nmulti-modal feature fusion transform module (MMFFT) to capture more\nrepresentative image features, remove redundant information between channels\nand modalities that are not relevant to the image content. Our work is the\nfirst to improve image compression performance using point cloud and achieves\nstate-of-the-art performance.\n","authors":["Ziqun Li","Qi Zhang","Xiaofeng Huang","Zhao Wang","Siwei Ma","Wei Yan"],"pdf_url":"https://arxiv.org/pdf/2412.11771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11767v1","updated":"2024-12-16T13:39:32Z","published":"2024-12-16T13:39:32Z","title":"IDEA-Bench: How Far are Generative Models from Professional Designing?","summary":"  Real-world design tasks - such as picture book creation, film storyboard\ndevelopment using character sets, photo retouching, visual effects, and font\ntransfer - are highly diverse and complex, requiring deep interpretation and\nextraction of various elements from instructions, descriptions, and reference\nimages. The resulting images often implicitly capture key features from\nreferences or user inputs, making it challenging to develop models that can\neffectively address such varied tasks. While existing visual generative models\ncan produce high-quality images based on prompts, they face significant\nlimitations in professional design scenarios that involve varied forms and\nmultiple inputs and outputs, even when enhanced with adapters like ControlNets\nand LoRAs. To address this, we introduce IDEA-Bench, a comprehensive benchmark\nencompassing 100 real-world design tasks, including rendering, visual effects,\nstoryboarding, picture books, fonts, style-based, and identity-preserving\ngeneration, with 275 test cases to thoroughly evaluate a model's\ngeneral-purpose generation capabilities. Notably, even the best-performing\nmodel only achieves 22.48 on IDEA-Bench, while the best general-purpose model\nonly achieves 6.81. We provide a detailed analysis of these results,\nhighlighting the inherent challenges and providing actionable directions for\nimprovement. Additionally, we provide a subset of 18 representative tasks\nequipped with multimodal large language model (MLLM)-based auto-evaluation\ntechniques to facilitate rapid model development and comparison. We releases\nthe benchmark data, evaluation toolkits, and an online leaderboard at\nhttps://github.com/ali-vilab/IDEA-Bench, aiming to drive the advancement of\ngenerative models toward more versatile and applicable intelligent design\nsystems.\n","authors":["Chen Liang","Lianghua Huang","Jingwu Fang","Huanzhang Dou","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Junge Zhang","Xin Zhao","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11762v1","updated":"2024-12-16T13:26:52Z","published":"2024-12-16T13:26:52Z","title":"GS-ProCams: Gaussian Splatting-based Projector-Camera Systems","summary":"  We present GS-ProCams, the first Gaussian Splatting-based framework for\nprojector-camera systems (ProCams). GS-ProCams significantly enhances the\nefficiency of projection mapping (PM) that requires establishing geometric and\nradiometric mappings between the projector and the camera. Previous CNN-based\nProCams are constrained to a specific viewpoint, limiting their applicability\nto novel perspectives. In contrast, NeRF-based ProCams support view-agnostic\nprojection mapping, however, they require an additional colocated light source\nand demand significant computational and memory resources. To address this\nissue, we propose GS-ProCams that employs 2D Gaussian for scene\nrepresentations, and enables efficient view-agnostic ProCams applications. In\nparticular, we explicitly model the complex geometric and photometric mappings\nof ProCams using projector responses, the target surface's geometry and\nmaterials represented by Gaussians, and global illumination component. Then, we\nemploy differentiable physically-based rendering to jointly estimate them from\ncaptured multi-view projections. Compared to state-of-the-art NeRF-based\nmethods, our GS-ProCams eliminates the need for additional devices, achieving\nsuperior ProCams simulation quality. It is also 600 times faster and uses only\n1/10 of the GPU memory.\n","authors":["Qingyue Deng","Jijiang Li","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11755v1","updated":"2024-12-16T13:19:41Z","published":"2024-12-16T13:19:41Z","title":"Generative Inbetweening through Frame-wise Conditions-Driven Video\n  Generation","summary":"  Generative inbetweening aims to generate intermediate frame sequences by\nutilizing two key frames as input. Although remarkable progress has been made\nin video generation models, generative inbetweening still faces challenges in\nmaintaining temporal stability due to the ambiguous interpolation path between\ntwo key frames. This issue becomes particularly severe when there is a large\nmotion gap between input frames. In this paper, we propose a straightforward\nyet highly effective Frame-wise Conditions-driven Video Generation (FCVG)\nmethod that significantly enhances the temporal stability of interpolated video\nframes. Specifically, our FCVG provides an explicit condition for each frame,\nmaking it much easier to identify the interpolation path between two input\nframes and thus ensuring temporally stable production of visually plausible\nvideo frames. To achieve this, we suggest extracting matched lines from two\ninput frames that can then be easily interpolated frame by frame, serving as\nframe-wise conditions seamlessly integrated into existing video generation\nmodels. In extensive evaluations covering diverse scenarios such as natural\nlandscapes, complex human poses, camera movements and animations, existing\nmethods often exhibit incoherent transitions across frames. In contrast, our\nFCVG demonstrates the capability to generate temporally stable videos using\nboth linear and non-linear interpolation curves. Our project page and code are\navailable at \\url{https://fcvg-inbetween.github.io/}.\n","authors":["Tianyi Zhu","Dongwei Ren","Qilong Wang","Xiaohe Wu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2412.11755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11610v3","updated":"2024-12-16T13:16:45Z","published":"2024-10-15T13:46:19Z","title":"Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth\n  Estimation","summary":"  Estimating depth from a single 2D image is a challenging task due to the lack\nof stereo or multi-view data, which are typically required for depth\nperception. This paper introduces a novel deep learning-based approach using an\nenhanced encoder-decoder architecture, where the Inception-ResNet-v2 model\nserves as the encoder. This is the first instance of utilizing\nInception-ResNet-v2 as an encoder for monocular depth estimation, demonstrating\nimproved performance over previous models. Our model effectively captures\ncomplex objects and fine-grained details, which are generally difficult to\npredict. Additionally, it incorporates multi-scale feature extraction to\nenhance depth prediction accuracy across various object sizes and distances. We\npropose a composite loss function comprising depth loss, gradient edge loss,\nand Structural Similarity Index Measure (SSIM) loss, with fine-tuned weights to\noptimize the weighted sum, ensuring a balance across different aspects of depth\nestimation. Experimental results on the NYU Depth V2 dataset show that our\nmodel achieves state-of-the-art performance, with an Absolute Relative Error\n(ARE) of 0.064, Root Mean Square Error (RMSE) of 0.228, and accuracy ($\\delta$\n< 1.25) of 89.3%. These metrics demonstrate that our model can accurately\npredict depth even in challenging scenarios, providing a scalable solution for\nreal-world applications in robotics, 3D reconstruction, and augmented reality.\n","authors":["Dabbrata Das","Argho Deb Das","Farhan Sadaf"],"pdf_url":"https://arxiv.org/pdf/2410.11610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11251v2","updated":"2024-12-16T13:15:13Z","published":"2023-06-20T03:05:28Z","title":"Lipschitz Singularities in Diffusion Models","summary":"  Diffusion models, which employ stochastic differential equations to sample\nimages through integrals, have emerged as a dominant class of generative\nmodels. However, the rationality of the diffusion process itself receives\nlimited attention, leaving the question of whether the problem is well-posed\nand well-conditioned. In this paper, we explore a perplexing tendency of\ndiffusion models: they often display the infinite Lipschitz property of the\nnetwork with respect to time variable near the zero point. We provide\ntheoretical proofs to illustrate the presence of infinite Lipschitz constants\nand empirical results to confirm it. The Lipschitz singularities pose a threat\nto the stability and accuracy during both the training and inference processes\nof diffusion models. Therefore, the mitigation of Lipschitz singularities holds\ngreat potential for enhancing the performance of diffusion models. To address\nthis challenge, we propose a novel approach, dubbed E-TSDM, which alleviates\nthe Lipschitz singularities of the diffusion model near the zero point of\ntimesteps. Remarkably, our technique yields a substantial improvement in\nperformance. Moreover, as a byproduct of our method, we achieve a dramatic\nreduction in the Fr\\'echet Inception Distance of acceleration methods relying\non network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive\nexperiments on diverse datasets validate our theory and method. Our work may\nadvance the understanding of the general diffusion process, and also provide\ninsights for the design of diffusion models.\n","authors":["Zhantao Yang","Ruili Feng","Han Zhang","Yujun Shen","Kai Zhu","Lianghua Huang","Yifei Zhang","Yu Liu","Deli Zhao","Jingren Zhou","Fan Cheng"],"pdf_url":"https://arxiv.org/pdf/2306.11251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11753v1","updated":"2024-12-16T13:12:11Z","published":"2024-12-16T13:12:11Z","title":"DriveGazen: Event-Based Driving Status Recognition using Conventional\n  Camera","summary":"  We introduce a wearable driving status recognition device and our open-source\ndataset, along with a new real-time method robust to changes in lighting\nconditions for identifying driving status from eye observations of drivers. The\ncore of our method is generating event frames from conventional intensity\nframes, and the other is a newly designed Attention Driving State Network\n(ADSN). Compared to event cameras, conventional cameras offer complete\ninformation and lower hardware costs, enabling captured frames to encode rich\nspatial information. However, these textures lack temporal information, posing\nchallenges in effectively identifying driving status. DriveGazen addresses this\nissue from three perspectives. First, we utilize video frames to generate\nrealistic synthetic dynamic vision sensor (DVS) events. Second, we adopt a\nspiking neural network to decode pertinent temporal information. Lastly, ADSN\nextracts crucial spatial cues from corresponding intensity frames and conveys\nspatial attention to convolutional spiking layers during both training and\ninference through a novel guide attention module to guide the feature learning\nand feature enhancement of the event frame. We specifically collected the\nDriving Status (DriveGaze) dataset to demonstrate the effectiveness of our\napproach. Additionally, we validate the superiority of the DriveGazen on the\nSingle-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our\nmethod is the first to utilize guide attention spiking neural networks and\neye-based event frames generated from conventional cameras for driving status\nrecognition. Please refer to our project page for more details:\nhttps://github.com/TooyoungALEX/AAAI25-DriveGazen.\n","authors":["Xiaoyin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11753v1.pdf","comment":"9 pages, 4 figures, (AAAI25)The 39th Annual AAAI Conference on\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2412.11752v1","updated":"2024-12-16T13:11:02Z","published":"2024-12-16T13:11:02Z","title":"Deformable Radial Kernel Splatting","summary":"  Recently, Gaussian splatting has emerged as a robust technique for\nrepresenting 3D scenes, enabling real-time rasterization and high-fidelity\nrendering. However, Gaussians' inherent radial symmetry and smoothness\nconstraints limit their ability to represent complex shapes, often requiring\nthousands of primitives to approximate detailed geometry. We introduce\nDeformable Radial Kernel (DRK), which extends Gaussian splatting into a more\ngeneral and flexible framework. Through learnable radial bases with adjustable\nangles and scales, DRK efficiently models diverse shape primitives while\nenabling precise control over edge sharpness and boundary curvature. iven DRK's\nplanar nature, we further develop accurate ray-primitive intersection\ncomputation for depth sorting and introduce efficient kernel culling strategies\nfor improved rasterization efficiency. Extensive experiments demonstrate that\nDRK outperforms existing methods in both representation efficiency and\nrendering quality, achieving state-of-the-art performance while dramatically\nreducing primitive count.\n","authors":["Yi-Hua Huang","Ming-Xian Lin","Yang-Tian Sun","Ziyi Yang","Xiaoyang Lyu","Yan-Pei Cao","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2412.11752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11735v1","updated":"2024-12-16T12:56:57Z","published":"2024-12-16T12:56:57Z","title":"Transferable Adversarial Face Attack with Text Controlled Attribute","summary":"  Traditional adversarial attacks typically produce adversarial examples under\nnorm-constrained conditions, whereas unrestricted adversarial examples are\nfree-form with semantically meaningful perturbations. Current unrestricted\nadversarial impersonation attacks exhibit limited control over adversarial face\nattributes and often suffer from low transferability. In this paper, we propose\na novel Text Controlled Attribute Attack (TCA$^2$) to generate photorealistic\nadversarial impersonation faces guided by natural language. Specifically, the\ncategory-level personal softmax vector is employed to precisely guide the\nimpersonation attacks. Additionally, we propose both data and model\naugmentation strategies to achieve transferable attacks on unknown target\nmodels. Finally, a generative model, \\textit{i.e}, Style-GAN, is utilized to\nsynthesize impersonated faces with desired attributes. Extensive experiments on\ntwo high-resolution face recognition datasets validate that our TCA$^2$ method\ncan generate natural text-guided adversarial impersonation faces with high\ntransferability. We also evaluate our method on real-world face recognition\nsystems, \\textit{i.e}, Face++ and Aliyun, further demonstrating the practical\npotential of our approach.\n","authors":["Wenyun Li","Zheng Zhang","Xiangyuan Lan","Dongmei Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.11735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01756v2","updated":"2024-12-16T12:53:25Z","published":"2024-11-04T02:43:55Z","title":"ChatTracker: Enhancing Visual Tracking Performance via Chatting with\n  Multimodal Large Language Model","summary":"  Visual object tracking aims to locate a targeted object in a video sequence\nbased on an initial bounding box. Recently, Vision-Language~(VL) trackers have\nproposed to utilize additional natural language descriptions to enhance\nversatility in various applications. However, VL trackers are still inferior to\nState-of-The-Art (SoTA) visual trackers in terms of tracking performance. We\nfound that this inferiority primarily results from their heavy reliance on\nmanual textual annotations, which include the frequent provision of ambiguous\nlanguage descriptions. In this paper, we propose ChatTracker to leverage the\nwealth of world knowledge in the Multimodal Large Language Model (MLLM) to\ngenerate high-quality language descriptions and enhance tracking performance.\nTo this end, we propose a novel reflection-based prompt optimization module to\niteratively refine the ambiguous and inaccurate descriptions of the target with\ntracking feedback. To further utilize semantic information produced by MLLM, a\nsimple yet effective VL tracking framework is proposed and can be easily\nintegrated as a plug-and-play module to boost the performance of both VL and\nvisual trackers. Experimental results show that our proposed ChatTracker\nachieves a performance comparable to existing methods.\n","authors":["Yiming Sun","Fan Yu","Shaoxiang Chen","Yu Zhang","Junwei Huang","Chenhui Li","Yang Li","Changbo Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11715v1","updated":"2024-12-16T12:35:56Z","published":"2024-12-16T12:35:56Z","title":"Discrepancy-Aware Attention Network for Enhanced Audio-Visual Zero-Shot\n  Learning","summary":"  Audio-visual Zero-Shot Learning (ZSL) has attracted significant attention for\nits ability to identify unseen classes and perform well in video classification\ntasks. However, modal imbalance in (G)ZSL leads to over-reliance on the optimal\nmodality, reducing discriminative capabilities for unseen classes. Some studies\nhave attempted to address this issue by modifying parameter gradients, but two\nchallenges still remain: (a) Quality discrepancies, where modalities offer\ndiffering quantities and qualities of information for the same concept. (b)\nContent discrepancies, where sample contributions within a modality vary\nsignificantly. To address these challenges, we propose a Discrepancy-Aware\nAttention Network (DAAN) for Enhanced Audio-Visual ZSL. Our approach introduces\na Quality-Discrepancy Mitigation Attention (QDMA) unit to minimize redundant\ninformation in the high-quality modality and a Contrastive Sample-level\nGradient Modulation (CSGM) block to adjust gradient magnitudes and balance\ncontent discrepancies. We quantify modality contributions by integrating\noptimization and convergence rate for more precise gradient modulation in CSGM.\nExperiments demonstrates DAAN achieves state-of-the-art performance on\nbenchmark datasets, with ablation studies validating the effectiveness of\nindividual modules.\n","authors":["RunLin Yu","Yipu Gong","Wenrui Li","Aiwen Sun","Mengren Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.11715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11710v1","updated":"2024-12-16T12:32:21Z","published":"2024-12-16T12:32:21Z","title":"Re-Attentional Controllable Video Diffusion Editing","summary":"  Editing videos with textual guidance has garnered popularity due to its\nstreamlined process which mandates users to solely edit the text prompt\ncorresponding to the source video. Recent studies have explored and exploited\nlarge-scale text-to-image diffusion models for text-guided video editing,\nresulting in remarkable video editing capabilities. However, they may still\nsuffer from some limitations such as mislocated objects, incorrect number of\nobjects. Therefore, the controllability of video editing remains a formidable\nchallenge. In this paper, we aim to challenge the above limitations by\nproposing a Re-Attentional Controllable Video Diffusion Editing (ReAtCo)\nmethod. Specially, to align the spatial placement of the target objects with\nthe edited text prompt in a training-free manner, we propose a Re-Attentional\nDiffusion (RAD) to refocus the cross-attention activation responses between the\nedited text prompt and the target video during the denoising stage, resulting\nin a spatially location-aligned and semantically high-fidelity manipulated\nvideo. In particular, to faithfully preserve the invariant region content with\nless border artifacts, we propose an Invariant Region-guided Joint Sampling\n(IRJS) strategy to mitigate the intrinsic sampling errors w.r.t the invariant\nregions at each denoising timestep and constrain the generated content to be\nharmonized with the invariant region content. Experimental results verify that\nReAtCo consistently improves the controllability of video diffusion editing and\nachieves superior video editing performance.\n","authors":["Yuanzhi Wang","Yong Li","Mengyi Liu","Xiaoya Zhang","Xin Liu","Zhen Cui","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2412.11710v1.pdf","comment":"Accepted by AAAI 2025. Codes are released at:\n  https://github.com/mdswyz/ReAtCo"},{"id":"http://arxiv.org/abs/2412.11706v1","updated":"2024-12-16T12:28:22Z","published":"2024-12-16T12:28:22Z","title":"AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration","summary":"  Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.\n","authors":["Wenhao Sun","Rong-Cheng Tu","Jingyi Liao","Zhao Jin","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2412.11706v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.11702v1","updated":"2024-12-16T12:25:57Z","published":"2024-12-16T12:25:57Z","title":"Flex-PE: Flexible and SIMD Multi-Precision Processing Element for AI\n  Workloads","summary":"  The rapid adaptation of data driven AI models, such as deep learning\ninference, training, Vision Transformers (ViTs), and other HPC applications,\ndrives a strong need for runtime precision configurable different non linear\nactivation functions (AF) hardware support. Existing solutions support diverse\nprecision or runtime AF reconfigurability but fail to address both\nsimultaneously. This work proposes a flexible and SIMD multiprecision\nprocessing element (FlexPE), which supports diverse runtime configurable AFs,\nincluding sigmoid, tanh, ReLU and softmax, and MAC operation. The proposed\ndesign achieves an improved throughput of up to 16X FxP4, 8X FxP8, 4X FxP16 and\n1X FxP32 in pipeline mode with 100% time multiplexed hardware. This work\nproposes an area efficient multiprecision iterative mode in the SIMD systolic\narrays for edge AI use cases. The design delivers superior performance with up\nto 62X and 371X reductions in DMA reads for input feature maps and weight\nfilters in VGG16, with an energy efficiency of 8.42 GOPS / W within the\naccuracy loss of 2%. The proposed architecture supports emerging 4-bit\ncomputations for DL inference while enhancing throughput in FxP8/16 modes for\ntransformers and other HPC applications. The proposed approach enables future\nenergy-efficient AI accelerators in edge and cloud environments.\n","authors":["Mukul Lokhande","Gopal Raut","Santosh Kumar Vishvakarma"],"pdf_url":"https://arxiv.org/pdf/2412.11702v1.pdf","comment":"10 pages, 5 figures, Preprint, Submitted to TVLSI Regular papers"},{"id":"http://arxiv.org/abs/2311.02778v3","updated":"2024-12-16T12:22:12Z","published":"2023-11-05T21:46:12Z","title":"MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction\n  and Novel View Synthesis","summary":"  Metaverse technologies demand accurate, real-time, and immersive modeling on\nconsumer-grade hardware for both non-human perception (e.g.,\ndrone/robot/autonomous car navigation) and immersive technologies like AR/VR,\nrequiring both structural accuracy and photorealism. However, there exists a\nknowledge gap in how to apply geometric reconstruction and photorealism\nmodeling (novel view synthesis) in a unified framework. To address this gap and\npromote the development of robust and immersive modeling and rendering with\nconsumer-grade devices, we propose a real-world Multi-Sensor Hybrid Room\nDataset (MuSHRoom). Our dataset presents exciting challenges and requires\nstate-of-the-art methods to be cost-effective, robust to noisy data and\ndevices, and can jointly learn 3D reconstruction and novel view synthesis\ninstead of treating them as separate tasks, making them ideal for real-world\napplications. We benchmark several famous pipelines on our dataset for joint 3D\nmesh reconstruction and novel view synthesis. Our dataset and benchmark show\ngreat potential in promoting the improvements for fusing 3D reconstruction and\nhigh-quality rendering in a robust and computationally efficient end-to-end\nfashion. The dataset and code are available at the project website:\nhttps://xuqianren.github.io/publications/MuSHRoom/.\n","authors":["Xuqian Ren","Wenjia Wang","Dingding Cai","Tuuli Tuominen","Juho Kannala","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2311.02778v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08509v2","updated":"2024-12-16T12:19:58Z","published":"2024-07-11T13:46:47Z","title":"Haar Nuclear Norms with Applications to Remote Sensing Imagery\n  Restoration","summary":"  Remote sensing image restoration aims to reconstruct missing or corrupted\nareas within images. To date, low-rank based models have garnered significant\ninterest in this field. This paper proposes a novel low-rank regularization\nterm, named the Haar nuclear norm (HNN), for efficient and effective remote\nsensing image restoration. It leverages the low-rank properties of wavelet\ncoefficients derived from the 2-D frontal slice-wise Haar discrete wavelet\ntransform, effectively modeling the low-rank prior for separated coarse-grained\nstructure and fine-grained textures in the image. Experimental evaluations\nconducted on hyperspectral image inpainting, multi-temporal image cloud\nremoval, and hyperspectral image denoising have revealed the HNN's potential.\nTypically, HNN achieves a performance improvement of 1-4 dB and a speedup of\n10-28x compared to some state-of-the-art methods (e.g., tensor correlated total\nvariation, and fully-connected tensor network) for inpainting tasks.\n","authors":["Shuang Xu","Chang Yu","Jiangjun Peng","Xiangyong Cao","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2407.08509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19271v2","updated":"2024-12-16T12:14:57Z","published":"2024-11-28T17:04:32Z","title":"AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors\n  for Indoor Room Reconstruction Using Smartphones","summary":"  Geometric priors are often used to enhance 3D reconstruction. With many\nsmartphones featuring low-resolution depth sensors and the prevalence of\noff-the-shelf monocular geometry estimators, incorporating geometric priors as\nregularization signals has become common in 3D vision tasks. However, the\naccuracy of depth estimates from mobile devices is typically poor for highly\ndetailed geometry, and monocular estimators often suffer from poor multi-view\nconsistency and precision. In this work, we propose an approach for joint\nsurface depth and normal refinement of Gaussian Splatting methods for accurate\n3D reconstruction of indoor scenes. We develop supervision strategies that\nadaptively filters low-quality depth and normal estimates by comparing the\nconsistency of the priors during optimization. We mitigate regularization in\nregions where prior estimates have high uncertainty or ambiguities. Our\nfiltering strategy and optimization design demonstrate significant improvements\nin both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian\nSplatting-based methods on challenging indoor room datasets. Furthermore, we\nexplore the use of alternative meshing strategies for finer geometry\nextraction. We develop a scale-aware meshing strategy inspired by TSDF and\noctree-based isosurface extraction, which recovers finer details from Gaussian\nmodels compared to other commonly used open-source meshing tools. Our code is\nreleased in https://xuqianren.github.io/ags_mesh_website/.\n","authors":["Xuqian Ren","Matias Turkulainen","Jiepeng Wang","Otto Seiskari","Iaroslav Melekhov","Juho Kannala","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2411.19271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11685v1","updated":"2024-12-16T11:55:26Z","published":"2024-12-16T11:55:26Z","title":"Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning","summary":"  With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.\n","authors":["Xingchi Chen","Zhuoran Zheng","Xuerui Li","Yuying Chen","Shu Wang","Wenqi Ren"],"pdf_url":"https://arxiv.org/pdf/2412.11685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11681v1","updated":"2024-12-16T11:47:07Z","published":"2024-12-16T11:47:07Z","title":"Fast-staged CNN Model for Accurate pulmonary diseases and Lung cancer\n  detection","summary":"  Pulmonary pathologies are a significant global health concern, often leading\nto fatal outcomes if not diagnosed and treated promptly. Chest radiography\nserves as a primary diagnostic tool, but the availability of experienced\nradiologists remains limited. Advances in Artificial Intelligence (AI) and\nmachine learning, particularly in computer vision, offer promising solutions to\naddress this challenge.\n  This research evaluates a deep learning model designed to detect lung cancer,\nspecifically pulmonary nodules, along with eight other lung pathologies, using\nchest radiographs. The study leverages diverse datasets comprising over 135,120\nfrontal chest radiographs to train a Convolutional Neural Network (CNN). A\ntwo-stage classification system, utilizing ensemble methods and transfer\nlearning, is employed to first triage images into Normal or Abnormal categories\nand then identify specific pathologies, including lung nodules.\n  The deep learning model achieves notable results in nodule classification,\nwith a top-performing accuracy of 77%, a sensitivity of 0.713, a specificity of\n0.776 during external validation, and an AUC score of 0.888. Despite these\nsuccesses, some misclassifications were observed, primarily false negatives.\n  In conclusion, the model demonstrates robust potential for generalization\nacross diverse patient populations, attributed to the geographic diversity of\nthe training dataset. Future work could focus on integrating ETL data\ndistribution strategies and expanding the dataset with additional nodule-type\nsamples to further enhance diagnostic accuracy.\n","authors":["Abdelbaki Souid","Mohamed Hamroun","Soufiene Ben Othman","Hedi Sakli","Naceur Abdelkarim"],"pdf_url":"https://arxiv.org/pdf/2412.11681v1.pdf","comment":"IEEE International Workshop on Mechatronic Systems Supervision 2023"},{"id":"http://arxiv.org/abs/2403.05435v5","updated":"2024-12-16T11:43:39Z","published":"2024-03-08T16:38:11Z","title":"OmniCount: Multi-label Object Counting with Semantic-Geometric Priors","summary":"  Object counting is pivotal for understanding the composition of scenes.\nPreviously, this task was dominated by class-specific methods, which have\ngradually evolved into more adaptable class-agnostic strategies. However, these\nstrategies come with their own set of limitations, such as the need for manual\nexemplar input and multiple passes for multiple categories, resulting in\nsignificant inefficiencies. This paper introduces a more practical approach\nenabling simultaneous counting of multiple object categories using an\nopen-vocabulary framework. Our solution, OmniCount, stands out by using\nsemantic and geometric insights (priors) from pre-trained models to count\nmultiple categories of objects as specified by users, all without additional\ntraining. OmniCount distinguishes itself by generating precise object masks and\nleveraging varied interactive prompts via the Segment Anything Model for\nefficient counting. To evaluate OmniCount, we created the OmniCount-191\nbenchmark, a first-of-its-kind dataset with multi-label object counts,\nincluding points, bounding boxes, and VQA annotations. Our comprehensive\nevaluation in OmniCount-191, alongside other leading benchmarks, demonstrates\nOmniCount's exceptional performance, significantly outpacing existing\nsolutions. The project webpage is available at\nhttps://mondalanindya.github.io/OmniCount.\n","authors":["Anindya Mondal","Sauradip Nag","Xiatian Zhu","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2403.05435v5.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11680v1","updated":"2024-12-16T11:42:25Z","published":"2024-12-16T11:42:25Z","title":"EGP3D: Edge-guided Geometric Preserving 3D Point Cloud Super-resolution\n  for RGB-D camera","summary":"  Point clouds or depth images captured by current RGB-D cameras often suffer\nfrom low resolution, rendering them insufficient for applications such as 3D\nreconstruction and robots. Existing point cloud super-resolution (PCSR) methods\nare either constrained by geometric artifacts or lack attention to edge\ndetails. To address these issues, we propose an edge-guided\ngeometric-preserving 3D point cloud super-resolution (EGP3D) method tailored\nfor RGB-D cameras. Our approach innovatively optimizes the point cloud with an\nedge constraint on a projected 2D space, thereby ensuring high-quality edge\npreservation in the 3D PCSR task. To tackle geometric optimization challenges\nin super-resolution point clouds, particularly preserving edge shapes and\nsmoothness, we introduce a multi-faceted loss function that simultaneously\noptimizes the Chamfer distance, Hausdorff distance, and gradient smoothness.\nExisting datasets used for point cloud upsampling are predominantly synthetic\nand inadequately represent real-world scenarios, neglecting noise and stray\nlight effects. To address the scarcity of realistic RGB-D data for PCSR tasks,\nwe built a dataset that captures real-world noise and stray-light effects,\noffering a more accurate representation of authentic environments. Validated\nthrough simulations and real-world experiments, the proposed method exhibited\nsuperior performance in preserving edge clarity and geometric details.\n","authors":["Zheng Fang","Ke Ye","Yaofang Liu","Gongzhe Li","Xianhong Zhao","Jialong Li","Ruxin Wang","Yuchen Zhang","Xiangyang Ji","Qilin Sun"],"pdf_url":"https://arxiv.org/pdf/2412.11680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06727v2","updated":"2024-12-16T11:28:49Z","published":"2024-12-09T18:16:50Z","title":"Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to\n  Evade AIGC Detection","summary":"  The security of AI-generated content (AIGC) detection is crucial for ensuring\nmultimedia content credibility. To enhance detector security, research on\nadversarial attacks has become essential. However, most existing adversarial\nattacks focus only on GAN-generated facial images detection, struggle to be\neffective on multi-class natural images and diffusion-based detectors, and\nexhibit poor invisibility. To fill this gap, we first conduct an in-depth\nanalysis of the vulnerability of AIGC detectors and discover the feature that\ndetectors vary in vulnerability to different post-processing. Then, considering\nthat the detector is agnostic in real-world scenarios and given this discovery,\nwe propose a Realistic-like Robust Black-box Adversarial attack (R$^2$BA) with\npost-processing fusion optimization. Unlike typical perturbations, R$^2$BA uses\nreal-world post-processing, i.e., Gaussian blur, JPEG compression, Gaussian\nnoise and light spot to generate adversarial examples. Specifically, we use a\nstochastic particle swarm algorithm with inertia decay to optimize\npost-processing fusion intensity and explore the detector's decision boundary.\nGuided by the detector's fake probability, R$^2$BA enhances/weakens the\ndetector-vulnerable/detector-robust post-processing intensity to strike a\nbalance between adversariality and invisibility. Extensive experiments on\npopular/commercial AIGC detectors and datasets demonstrate that R$^2$BA\nexhibits impressive anti-detection performance, excellent invisibility, and\nstrong robustness in GAN-based and diffusion-based cases. Compared to\nstate-of-the-art white-box and black-box attacks, R$^2$BA shows significant\nimprovements of 15\\%--72\\% and 21\\%--47\\% in anti-detection performance under\nthe original and robust scenario respectively, offering valuable insights for\nthe security of AIGC detection in real-world applications.\n","authors":["Caiyun Xie","Dengpan Ye","Yunming Zhang","Long Tang","Yunna Lv","Jiacheng Deng","Jiawei Song"],"pdf_url":"https://arxiv.org/pdf/2412.06727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11673v1","updated":"2024-12-16T11:26:46Z","published":"2024-12-16T11:26:46Z","title":"$\\texttt{DINO-Foresight}$: Looking into the Future with DINO","summary":"  Predicting future dynamics is crucial for applications like autonomous\ndriving and robotics, where understanding the environment is key. Existing\npixel-level methods are computationally expensive and often focus on irrelevant\ndetails. To address these challenges, we introduce $\\texttt{DINO-Foresight}$, a\nnovel framework that operates in the semantic feature space of pretrained\nVision Foundation Models (VFMs). Our approach trains a masked feature\ntransformer in a self-supervised manner to predict the evolution of VFM\nfeatures over time. By forecasting these features, we can apply off-the-shelf,\ntask-specific heads for various scene understanding tasks. In this framework,\nVFM features are treated as a latent space, to which different heads attach to\nperform specific tasks for future-frame analysis. Extensive experiments show\nthat our framework outperforms existing methods, demonstrating its robustness\nand scalability. Additionally, we highlight how intermediate transformer\nrepresentations in $\\texttt{DINO-Foresight}$ improve downstream task\nperformance, offering a promising path for the self-supervised enhancement of\nVFM features. We provide the implementation code at\nhttps://github.com/Sta8is/DINO-Foresight .\n","authors":["Efstathios Karypidis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2412.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09441v2","updated":"2024-12-16T11:26:26Z","published":"2024-08-18T11:23:21Z","title":"CLIP-CID: Efficient CLIP Distillation via Cluster-Instance\n  Discrimination","summary":"  Contrastive Language-Image Pre-training (CLIP) has achieved excellent\nperformance over a wide range of tasks. However, the effectiveness of CLIP\nheavily relies on a substantial corpus of pre-training data, resulting in\nnotable consumption of computational resources. Although knowledge distillation\nhas been widely applied in single modality models, how to efficiently expand\nknowledge distillation to vision-language foundation models with extensive data\nremains relatively unexplored. In this paper, we introduce CLIP-CID, a novel\ndistillation mechanism that effectively transfers knowledge from a large\nvision-language foundation model to a smaller model. We initially propose a\nsimple but efficient image semantic balance method to reduce transfer learning\nbias and improve distillation efficiency. This method filters out 43.7% of\nimage-text pairs from the LAION400M while maintaining superior performance.\nAfter that, we leverage cluster-instance discrimination to facilitate knowledge\ntransfer from the teacher model to the student model, thereby empowering the\nstudent model to acquire a holistic semantic comprehension of the pre-training\ndata. Experimental results demonstrate that CLIP-CID achieves state-of-the-art\nperformance on various downstream tasks including linear probe and zero-shot\nclassification.\n","authors":["Kaicheng Yang","Tiancheng Gu","Xiang An","Haiqiang Jiang","Xiangzi Dai","Ziyong Feng","Weidong Cai","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2408.09441v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11668v1","updated":"2024-12-16T11:19:22Z","published":"2024-12-16T11:19:22Z","title":"Online Writer Retrieval with Chinese Handwritten Phrases: A Synergistic\n  Temporal-Frequency Representation Learning Approach","summary":"  Currently, the prevalence of online handwriting has spurred a critical need\nfor effective retrieval systems to accurately search relevant handwriting\ninstances from specific writers, known as online writer retrieval. Despite the\ngrowing demand, this field suffers from a scarcity of well-established\nmethodologies and public large-scale datasets. This paper tackles these\nchallenges with a focus on Chinese handwritten phrases. First, we propose\nDOLPHIN, a novel retrieval model designed to enhance handwriting\nrepresentations through synergistic temporal-frequency analysis. For frequency\nfeature learning, we propose the HFGA block, which performs gated\ncross-attention between the vanilla temporal handwriting sequence and its\nhigh-frequency sub-bands to amplify salient writing details. For temporal\nfeature learning, we propose the CAIR block, tailored to promote channel\ninteraction and reduce channel redundancy. Second, to address data deficit, we\nintroduce OLIWER, a large-scale online writer retrieval dataset encompassing\nover 670,000 Chinese handwritten phrases from 1,731 individuals. Through\nextensive evaluations, we demonstrate the superior performance of DOLPHIN over\nexisting methods. In addition, we explore cross-domain writer retrieval and\nreveal the pivotal role of increasing feature alignment in bridging the\ndistributional gap between different handwriting data. Our findings emphasize\nthe significance of point sampling frequency and pressure features in improving\nhandwriting representation quality and retrieval performance. Code and dataset\nare available at https://github.com/SCUT-DLVCLab/DOLPHIN.\n","authors":["Peirong Zhang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.11668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11663v1","updated":"2024-12-16T11:11:23Z","published":"2024-12-16T11:11:23Z","title":"LMM-Regularized CLIP Embeddings for Image Classification","summary":"  In this paper we deal with image classification tasks using the powerful CLIP\nvision-language model. Our goal is to advance the classification performance\nusing the CLIP's image encoder, by proposing a novel Large Multimodal Model\n(LMM) based regularization method. The proposed method uses an LMM to extract\nsemantic descriptions for the images of the dataset. Then, it uses the CLIP's\ntext encoder, frozen, in order to obtain the corresponding text embeddings and\ncompute the mean semantic class descriptions. Subsequently, we adapt the CLIP's\nimage encoder by adding a classification head, and we train it along with the\nimage encoder output, apart from the main classification objective, with an\nadditional auxiliary objective. The additional objective forces the embeddings\nat the image encoder's output to become similar to their corresponding\nLMM-generated mean semantic class descriptions. In this way, it produces\nembeddings with enhanced discrimination ability, leading to improved\nclassification performance. The effectiveness of the proposed regularization\nmethod is validated through extensive experiments on three image classification\ndatasets.\n","authors":["Maria Tzelepi","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2412.11663v1.pdf","comment":"Accepted for publication, 26th Int. Symp. on Multimedia (IEEE ISM\n  2024), Tokyo, Japan, Dec. 2024. This is the authors' \"accepted version\""},{"id":"http://arxiv.org/abs/2411.14280v2","updated":"2024-12-16T11:06:19Z","published":"2024-11-21T16:33:35Z","title":"EasyHOI: Unleashing the Power of Large Models for Reconstructing\n  Hand-Object Interactions in the Wild","summary":"  Our work aims to reconstruct hand-object interactions from a single-view\nimage, which is a fundamental but ill-posed task. Unlike methods that\nreconstruct from videos, multi-view images, or predefined 3D templates,\nsingle-view reconstruction faces significant challenges due to inherent\nambiguities and occlusions. These challenges are further amplified by the\ndiverse nature of hand poses and the vast variety of object shapes and sizes.\nOur key insight is that current foundational models for segmentation,\ninpainting, and 3D reconstruction robustly generalize to in-the-wild images,\nwhich could provide strong visual and geometric priors for reconstructing\nhand-object interactions. Specifically, given a single image, we first design a\nnovel pipeline to estimate the underlying hand pose and object shape using\noff-the-shelf large models. Furthermore, with the initial reconstruction, we\nemploy a prior-guided optimization scheme, which optimizes hand pose to comply\nwith 3D physical constraints and the 2D input image content. We perform\nexperiments across several datasets and show that our method consistently\noutperforms baselines and faithfully reconstructs a diverse set of hand-object\ninteractions. Here is the link of our project page:\nhttps://lym29.github.io/EasyHOI-page/\n","authors":["Yumeng Liu","Xiaoxiao Long","Zemin Yang","Yuan Liu","Marc Habermann","Christian Theobalt","Yuexin Ma","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14280v2.pdf","comment":"Project page: https://lym29.github.io/EasyHOI-page/"},{"id":"http://arxiv.org/abs/2412.11657v1","updated":"2024-12-16T11:00:02Z","published":"2024-12-16T11:00:02Z","title":"CNNtention: Can CNNs do better with Attention?","summary":"  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n","authors":["Julian Glattki","Nikhil Kapila","Tejas Rathi"],"pdf_url":"https://arxiv.org/pdf/2412.11657v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.15500v2","updated":"2024-12-16T10:53:08Z","published":"2024-07-22T09:31:30Z","title":"TextureCrop: Enhancing Synthetic Image Detection through Texture-based\n  Cropping","summary":"  Generative AI technologies produce increasingly realistic imagery, which,\ndespite its potential for creative applications, can also be misused to produce\nmisleading and harmful content. This renders Synthetic Image Detection (SID)\nmethods essential for identifying AI-generated content online. State-of-the-art\nSID methods typically resize or center-crop input images due to architectural\nor computational constraints, which hampers the detection of artifacts that\nappear in high-resolution images. To address this limitation, we propose\nTextureCrop, an image pre-processing component that can be plugged in any\npre-trained SID model to improve its performance. By focusing on high-frequency\nimage parts where generative artifacts are prevalent, TextureCrop enhances SID\nperformance with manageable memory requirements. Experimental results\ndemonstrate a consistent improvement in AUC across various detectors by 6.1%\ncompared to center cropping and by 15% compared to resizing, across\nhigh-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.\n","authors":["Despina Konstantinidou","Christos Koutlis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.15500v2.pdf","comment":"10 pages, 6 images"},{"id":"http://arxiv.org/abs/2412.11650v1","updated":"2024-12-16T10:50:52Z","published":"2024-12-16T10:50:52Z","title":"Image Gradient-Aided Photometric Stereo Network","summary":"  Photometric stereo (PS) endeavors to ascertain surface normals using shading\nclues from photometric images under various illuminations. Recent deep\nlearning-based PS methods often overlook the complexity of object surfaces.\nThese neural network models, which exclusively rely on photometric images for\ntraining, often produce blurred results in high-frequency regions characterized\nby local discontinuities, such as wrinkles and edges with significant gradient\nchanges. To address this, we propose the Image Gradient-Aided Photometric\nStereo Network (IGA-PSN), a dual-branch framework extracting features from both\nphotometric images and their gradients. Furthermore, we incorporate an\nhourglass regression network along with supervision to regularize normal\nregression. Experiments on DiLiGenT benchmarks show that IGA-PSN outperforms\nprevious methods in surface normal estimation, achieving a mean angular error\nof 6.46 while preserving textures and geometric shapes in complex regions.\n","authors":["Kaixuan Wang","Lin Qi","Shiyu Qin","Kai Luo","Yakun Ju","Xia Li","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2412.11650v1.pdf","comment":"13 pages, 5 figures, published to Springer"},{"id":"http://arxiv.org/abs/2412.09319v2","updated":"2024-12-16T10:45:46Z","published":"2024-12-12T14:44:05Z","title":"FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot\n  Medical Image Segmentation","summary":"  Existing few-shot medical image segmentation (FSMIS) models fail to address a\npractical issue in medical imaging: the domain shift caused by different\nimaging techniques, which limits the applicability to current FSMIS tasks. To\novercome this limitation, we focus on the cross-domain few-shot medical image\nsegmentation (CD-FSMIS) task, aiming to develop a generalized model capable of\nadapting to a broader range of medical image segmentation scenarios with\nlimited labeled data from the novel target domain. Inspired by the\ncharacteristics of frequency domain similarity across different domains, we\npropose a Frequency-aware Matching Network (FAMNet), which includes two key\ncomponents: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion\n(MSF) module. The FAM module tackles two problems during the meta-learning\nphase: 1) intra-domain variance caused by the inherent support-query bias, due\nto the different appearances of organs and lesions, and 2) inter-domain\nvariance caused by different medical imaging techniques. Additionally, we\ndesign an MSF module to integrate the different frequency features decoupled by\nthe FAM module, and further mitigate the impact of inter-domain variance on the\nmodel's segmentation performance. Combining these two modules, our FAMNet\nsurpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation\nmodels on three cross-domain datasets, achieving state-of-the-art performance\nin the CD-FSMIS task.\n","authors":["Yuntian Bo","Yazhou Zhu","Lunbo Li","Haofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09319v2.pdf","comment":"Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2411.13623v2","updated":"2024-12-16T10:34:06Z","published":"2024-11-20T13:12:43Z","title":"Unsupervised Foundation Model-Agnostic Slide-Level Representation\n  Learning","summary":"  Representation learning of pathology whole-slide images(WSIs) has primarily\nrelied on weak supervision with Multiple Instance Learning (MIL). This approach\nleads to slide representations highly tailored to a specific clinical task.\nSelf-supervised learning (SSL) has been successfully applied to train\nhistopathology foundation models (FMs) for patch embedding generation. However,\ngenerating patient or slide level embeddings remains challenging. Existing\napproaches for slide representation learning extend the principles of SSL from\npatch level learning to entire slides by aligning different augmentations of\nthe slide or by utilizing multimodal data. By integrating tile embeddings from\nmultiple FMs, we propose a new single modality SSL method in feature space that\ngenerates useful slide representations. Our contrastive pretraining strategy,\ncalled COBRA, employs multiple FMs and an architecture based on Mamba-2. COBRA\nexceeds performance of state-of-the-art slide encoders on four different public\nClinical Protemic Tumor Analysis Consortium (CPTAC) cohorts on average by at\nleast +4.5% AUC, despite only being pretrained on 3048 WSIs from The Cancer\nGenome Atlas (TCGA). Additionally, COBRA is readily compatible at inference\ntime with previously unseen feature extractors. Code available at\nhttps://github.com/KatherLab/COBRA.\n","authors":["Tim Lenz","Peter Neidlinger","Marta Ligero","Georg Wölflein","Marko van Treeck","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2411.13623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11639v1","updated":"2024-12-16T10:33:10Z","published":"2024-12-16T10:33:10Z","title":"High-speed and High-quality Vision Reconstruction of Spike Camera with\n  Spike Stability Theorem","summary":"  Neuromorphic vision sensors, such as the dynamic vision sensor (DVS) and\nspike camera, have gained increasing attention in recent years. The spike\ncamera can detect fine textures by mimicking the fovea in the human visual\nsystem, and output a high-frequency spike stream. Real-time high-quality vision\nreconstruction from the spike stream can build a bridge to high-level vision\ntask applications of the spike camera. To realize high-speed and high-quality\nvision reconstruction of the spike camera, we propose a new spike stability\ntheorem that reveals the relationship between spike stream characteristics and\nstable light intensity. Based on the spike stability theorem, two\nparameter-free algorithms are designed for the real-time vision reconstruction\nof the spike camera. To demonstrate the performances of our algorithms, two\ndatasets (a public dataset PKU-Spike-High-Speed and a newly constructed dataset\nSpikeCityPCL) are used to compare the reconstruction quality and speed of\nvarious reconstruction methods. Experimental results show that, compared with\nthe current state-of-the-art (SOTA) reconstruction methods, our reconstruction\nmethods obtain the best tradeoff between the reconstruction quality and speed.\nAdditionally, we design the FPGA implementation method of our algorithms to\nrealize the real-time (running at 20,000 FPS) visual reconstruction. Our work\nprovides new theorem and algorithm foundations for the real-time edge-end\nvision processing of the spike camera.\n","authors":["Wei Zhang","Weiquan Yan","Yun Zhao","Wenxiang Cheng","Gang Chen","Huihui Zhou","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2412.11639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v2","updated":"2024-12-16T10:28:02Z","published":"2024-10-11T15:50:31Z","title":"HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable\n  Deep Learning","summary":"  Prototype Learning methods provide an interpretable alternative to black-box\ndeep learning models. Approaches such as ProtoPNet learn, which part of a test\nimage \"look like\" known prototypical parts from training images, combining\npredictive power with the inherent interpretability of case-based reasoning.\nHowever, existing approaches have two main drawbacks: A) They rely solely on\ndeterministic similarity scores without statistical confidence. B) The\nprototypes are learned in a black-box manner without human input. This work\nintroduces HyperPg, a new prototype representation leveraging Gaussian\ndistributions on a hypersphere in latent space, with learnable mean and\nvariance. HyperPg prototypes adapt to the spread of clusters in the latent\nspace and output likelihood scores. The new architecture, HyperPgNet, leverages\nHyperPg to learn prototypes aligned with human concepts from pixel-level\nannotations. Consequently, each prototype represents a specific concept such as\ncolor, image texture, or part of the image subject. A concept extraction\npipeline built on foundation models provides pixel-level annotations,\nsignificantly reducing human labeling effort. Experiments on CUB-200-2011 and\nStanford Cars datasets demonstrate that HyperPgNet outperforms other prototype\nlearning architectures while using fewer parameters and training steps.\nAdditionally, the concept-aligned HyperPg prototypes are learned transparently,\nenhancing model interpretability.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11638v1","updated":"2024-12-16T10:27:48Z","published":"2024-12-16T10:27:48Z","title":"IDProtector: An Adversarial Noise Encoder to Protect Against\n  ID-Preserving Image Generation","summary":"  Recently, zero-shot methods like InstantID have revolutionized\nidentity-preserving generation. Unlike multi-image finetuning approaches such\nas DreamBooth, these zero-shot methods leverage powerful facial encoders to\nextract identity information from a single portrait photo, enabling efficient\nidentity-preserving generation through a single inference pass. However, this\nconvenience introduces new threats to the facial identity protection. This\npaper aims to safeguard portrait photos from unauthorized encoder-based\ncustomization. We introduce IDProtector, an adversarial noise encoder that\napplies imperceptible adversarial noise to portrait photos in a single forward\npass. Our approach offers universal protection for portraits against multiple\nstate-of-the-art encoder-based methods, including InstantID, IP-Adapter, and\nPhotoMaker, while ensuring robustness to common image transformations such as\nJPEG compression, resizing, and affine transformations. Experiments across\ndiverse portrait datasets and generative models reveal that IDProtector\ngeneralizes effectively to unseen data and even closed-source proprietary\nmodels.\n","authors":["Yiren Song","Pei Yang","Hai Ci","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2412.11638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17821v2","updated":"2024-12-16T10:27:35Z","published":"2024-05-28T04:41:02Z","title":"RITUAL: Random Image Transformations as a Universal Anti-hallucination\n  Lever in Large Vision Language Models","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have\nrevolutionized how machines understand and generate textual responses based on\nvisual inputs, yet they often produce \"hallucinatory\" outputs that misinterpret\nvisual information, posing challenges in reliability and trustworthiness. We\npropose RITUAL, a simple decoding method that reduces hallucinations by\nleveraging randomly transformed images as complementary inputs during decoding,\nadjusting the output probability distribution without additional training or\nexternal models. Our key insight is that random transformations expose the\nmodel to diverse visual perspectives, enabling it to correct misinterpretations\nthat lead to hallucinations. Specifically, when a model hallucinates based on\nthe original image, the transformed images -- altered in aspects such as\norientation, scale, or color -- provide alternative viewpoints that help\nrecalibrate the model's predictions. By integrating the probability\ndistributions from both the original and transformed images, RITUAL effectively\nreduces hallucinations. To further improve reliability and address potential\ninstability from arbitrary transformations, we introduce RITUAL+, an extension\nthat selects image transformations based on self-feedback from the LVLM.\nInstead of applying transformations randomly, RITUAL+ uses the LVLM to evaluate\nand choose transformations that are most beneficial for reducing hallucinations\nin a given context. This self-adaptive approach mitigates the potential\nnegative impact of certain transformations on specific tasks, ensuring more\nconsistent performance across different scenarios. Experiments demonstrate that\nRITUAL and RITUAL+ significantly reduce hallucinations across several object\nhallucination benchmarks.\n","authors":["Sangmin Woo","Jaehyuk Jang","Donguk Kim","Yubin Choi","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2405.17821v2.pdf","comment":"Project: https://sangminwoo.github.io/RITUAL/"},{"id":"http://arxiv.org/abs/2412.11634v1","updated":"2024-12-16T10:25:03Z","published":"2024-12-16T10:25:03Z","title":"Predicting the Original Appearance of Damaged Historical Documents","summary":"  Historical documents encompass a wealth of cultural treasures but suffer from\nsevere damages including character missing, paper damage, and ink erosion over\ntime. However, existing document processing methods primarily focus on\nbinarization, enhancement, etc., neglecting the repair of these damages. To\nthis end, we present a new task, termed Historical Document Repair (HDR), which\naims to predict the original appearance of damaged historical documents. To\nfill the gap in this field, we propose a large-scale dataset HDR28K and a\ndiffusion-based network DiffHDR for historical document repair. Specifically,\nHDR28K contains 28,552 damaged-repaired image pairs with character-level\nannotations and multi-style degradations. Moreover, DiffHDR augments the\nvanilla diffusion framework with semantic and spatial information and a\nmeticulously designed character perceptual loss for contextual and visual\ncoherence. Experimental results demonstrate that the proposed DiffHDR trained\nusing HDR28K significantly surpasses existing approaches and exhibits\nremarkable performance in handling real damaged documents. Notably, DiffHDR can\nalso be extended to document editing and text block generation, showcasing its\nhigh flexibility and generalization capacity. We believe this study could\npioneer a new direction of document processing and contribute to the\ninheritance of invaluable cultures and civilizations. The dataset and code is\navailable at https://github.com/yeungchenwa/HDR.\n","authors":["Zhenhua Yang","Dezhi Peng","Yongxin Shi","Yuyi Zhang","Chongyu Liu","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.11634v1.pdf","comment":"Accepted to AAAI 2025; Github Page:\n  https://github.com/yeungchenwa/HDR"},{"id":"http://arxiv.org/abs/2408.05092v2","updated":"2024-12-16T10:10:10Z","published":"2024-08-09T14:33:34Z","title":"PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural\n  Networks","summary":"  The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., facial or\nmedical images. In this work, we propose a method to perform the training phase\nof a deep learning model on both an edge device and a cloud server that\nprevents sensitive content being transmitted to the cloud while retaining the\ndesired information. The proposed privacy-preserving method uses adversarial\nearly exits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial and medical datasets with\ndiverse attributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box, deep and\nGAN-based reconstruction attacks. This approach is designed for\nresource-constrained edge devices, ensuring minimal memory usage and\ncomputational overhead.\n","authors":["Yamin Sepehri","Pedram Pad","Pascal Frossard","L. Andrea Dunbar"],"pdf_url":"https://arxiv.org/pdf/2408.05092v2.pdf","comment":"21 pages, 19 figures, 11 tables"},{"id":"http://arxiv.org/abs/2412.11621v1","updated":"2024-12-16T10:08:38Z","published":"2024-12-16T10:08:38Z","title":"VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video\n  Prompting","summary":"  Large Language Model (LLM)-based agents have shown promise in procedural\ntasks, but the potential of multimodal instructions augmented by texts and\nvideos to assist users remains under-explored. To address this gap, we propose\nthe Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel\nLLM-empowered Multimodal Procedural Planning (MPP) framework. It generates\ncohesive text and video procedural plans given a specified high-level\nobjective. The main challenges are achieving textual and visual\ninformativeness, temporal coherence, and accuracy in procedural plans. VG-TVP\nleverages the zero-shot reasoning capability of LLMs, the video-to-text\ngeneration ability of the video captioning models, and the text-to-video\ngeneration ability of diffusion models. VG-TVP improves the interaction between\nmodalities by proposing a novel Fusion of Captioning (FoC) method and using\nText-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs\nto guide the generation of visually-grounded text plans and textual-grounded\nvideo plans. To address the scarcity of datasets suitable for MPP, we have\ncurated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We\nconduct comprehensive experiments and benchmarks to evaluate human preferences\n(regarding textual and visual informativeness, temporal coherence, and plan\naccuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP\ndataset.\n","authors":["Muhammet Furkan Ilaslan","Ali Koksal","Kevin Qinhong Lin","Burak Satar","Mike Zheng Shou","Qianli Xu"],"pdf_url":"https://arxiv.org/pdf/2412.11621v1.pdf","comment":"Accepted for The 39th Annual AAAI Conference on Artificial\n  Intelligence 2025 in Main Track, 19 pages, 24 figures"},{"id":"http://arxiv.org/abs/2412.11620v1","updated":"2024-12-16T10:07:15Z","published":"2024-12-16T10:07:15Z","title":"Combating Semantic Contamination in Learning with Label Noise","summary":"  Noisy labels can negatively impact the performance of deep neural networks.\nOne common solution is label refurbishment, which involves reconstructing noisy\nlabels through predictions and distributions. However, these methods may\nintroduce problematic semantic associations, a phenomenon that we identify as\nSemantic Contamination. Through an analysis of Robust LR, a representative\nlabel refurbishment method, we found that utilizing the logits of views for\nrefurbishment does not adequately balance the semantic information of\nindividual classes. Conversely, using the logits of models fails to maintain\nconsistent semantic relationships across models, which explains why label\nrefurbishment methods frequently encounter issues related to Semantic\nContamination. To address this issue, we propose a novel method called\nCollaborative Cross Learning, which utilizes semi-supervised learning on\nrefurbished labels to extract appropriate semantic associations from embeddings\nacross views and models. Experimental results show that our method outperforms\nexisting approaches on both synthetic and real-world noisy datasets,\neffectively mitigating the impact of label noise and Semantic Contamination.\n","authors":["Wenxiao Fan","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2412.11620v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2308.14930v2","updated":"2024-12-16T10:04:38Z","published":"2023-08-28T23:08:32Z","title":"Application of Quantum Pre-Processing Filter for Binary Image\n  Classification with Small Samples","summary":"  Over the past few years, there has been significant interest in Quantum\nMachine Learning (QML) among researchers, as it has the potential to transform\nthe field of machine learning. Several models that exploit the properties of\nquantum mechanics have been developed for practical applications. In this\nstudy, we investigated the application of our previously proposed quantum\npre-processing filter (QPF) to binary image classification. We evaluated the\nQPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits\nand alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic\nsign images). Similar to our previous multi-class classification results, the\napplication of QPF improved the binary image classification accuracy using\nneural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8%\nto 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from\n93.5% to 92.0%. We then applied QPF in cases using a smaller number of training\nand testing samples, i.e. 80 and 20 samples per class, respectively. In order\nto derive statistically stable results, we conducted the experiment with 100\ntrials choosing randomly different training and testing samples and averaging\nthe results. The result showed that the application of QPF did not improve the\nimage classification accuracy against MNIST and EMNIST but improved it against\nCIFAR-10 and GTSRB from 65.8% to 67.2% and 90.5% to 91.8%, respectively.\nFurther research will be conducted as part of future work to investigate the\npotential of QPF to assess the scalability of the proposed approach to larger\nand complex datasets.\n","authors":["Farina Riaz","Shahab Abdulla","Hajime Suzuki","Srinjoy Ganguly","Ravinesh C. Deo","Susan Hopkins"],"pdf_url":"https://arxiv.org/pdf/2308.14930v2.pdf","comment":"This paper is accepted by Journal of Data Science and Intelligent\n  Systems (JDSIS)"},{"id":"http://arxiv.org/abs/2412.08221v2","updated":"2024-12-16T09:54:46Z","published":"2024-12-11T09:17:39Z","title":"Generate Any Scene: Evaluating and Improving Text-to-Vision Generation\n  with Scene Graph Programming","summary":"  DALL-E and Sora have gained attention by producing implausible images, such\nas \"astronauts riding a horse in space.\" Despite the proliferation of\ntext-to-vision models that have inundated the internet with synthetic visuals,\nfrom images to 3D assets, current benchmarks predominantly evaluate these\nmodels on real-world scenes paired with captions. We introduce Generate Any\nScene, a framework that systematically enumerates scene graphs representing a\nvast array of visual scenes, spanning realistic to imaginative compositions.\nGenerate Any Scene leverages 'scene graph programming', a method for\ndynamically constructing scene graphs of varying complexity from a structured\ntaxonomy of visual elements. This taxonomy includes numerous objects,\nattributes, and relations, enabling the synthesis of an almost infinite variety\nof scene graphs. Using these structured representations, Generate Any Scene\ntranslates each scene graph into a caption, enabling scalable evaluation of\ntext-to-vision models through standard metrics. We conduct extensive\nevaluations across multiple text-to-image, text-to-video, and text-to-3D\nmodels, presenting key findings on model performance. We find that DiT-backbone\ntext-to-image models align more closely with input captions than UNet-backbone\nmodels. Text-to-video models struggle with balancing dynamics and consistency,\nwhile both text-to-video and text-to-3D models show notable gaps in human\npreference alignment. We demonstrate the effectiveness of Generate Any Scene by\nconducting three practical applications leveraging captions generated by\nGenerate Any Scene: 1) a self-improving framework where models iteratively\nenhance their performance using generated data, 2) a distillation process to\ntransfer specific strengths from proprietary models to open-source\ncounterparts, and 3) improvements in content moderation by identifying and\ngenerating challenging synthetic data.\n","authors":["Ziqi Gao","Weikai Huang","Jieyu Zhang","Aniruddha Kembhavi","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2412.08221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06618v2","updated":"2024-12-16T09:52:32Z","published":"2024-10-09T07:14:49Z","title":"Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N\n  1-to-1 Relationships for Text-Video Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11609v1","updated":"2024-12-16T09:50:09Z","published":"2024-12-16T09:50:09Z","title":"CLIP-SR: Collaborative Linguistic and Image Processing for\n  Super-Resolution","summary":"  Convolutional Neural Networks (CNNs) have advanced Image Super-Resolution\n(SR), but most CNN-based methods rely solely on pixel-based transformations,\noften leading to artifacts and blurring, particularly with severe downsampling\n(e.g., 8x or 16x). Recent text-guided SR methods attempt to leverage textual\ninformation for enhanced detail, but they frequently struggle with effective\nalignment, resulting in inconsistent semantic coherence. To address these\nlimitations, we introduce a multi-modal semantic enhancement approach that\ncombines textual semantics with visual features, effectively tackling semantic\nmismatches and detail loss in highly degraded LR images. Our proposed\nmulti-modal collaborative framework enables the production of realistic and\nhigh-quality SR images at significant up-scaling factors. The framework\nintegrates text and image inputs, employing a prompt predictor, Text-Image\nFusion Block (TIFBlock), and Iterative Refinement Module alongside CLIP\n(Contrastive Language-Image Pretraining) features to guide a progressive\nenhancement process with fine-grained alignment. This alignment produces\nhigh-resolution outputs with crisp details and semantic coherence, even at\nlarge scaling factors. Through extensive comparative experiments and ablation\nstudies, we validate the effectiveness of our approach. Additionally, by\nincorporating textual semantic guidance, our technique enables a degree of\nsuper-resolution editability while maintaining semantic coherence.\n","authors":["Bingwen Hu","Heng Liu","Zhedong Zheng","Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11609v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.11608v1","updated":"2024-12-16T09:49:59Z","published":"2024-12-16T09:49:59Z","title":"Towards Adversarial Robustness of Model-Level Mixture-of-Experts\n  Architectures for Semantic Segmentation","summary":"  Vulnerability to adversarial attacks is a well-known deficiency of deep\nneural networks. Larger networks are generally more robust, and ensembling is\none method to increase adversarial robustness: each model's weaknesses are\ncompensated by the strengths of others. While an ensemble uses a deterministic\nrule to combine model outputs, a mixture of experts (MoE) includes an\nadditional learnable gating component that predicts weights for the outputs of\nthe expert models, thus determining their contributions to the final\nprediction. MoEs have been shown to outperform ensembles on specific tasks, yet\ntheir susceptibility to adversarial attacks has not been studied yet. In this\nwork, we evaluate the adversarial vulnerability of MoEs for semantic\nsegmentation of urban and highway traffic scenes. We show that MoEs are, in\nmost cases, more robust to per-instance and universal white-box adversarial\nattacks and can better withstand transfer attacks. Our code is available at\n\\url{https://github.com/KASTEL-MobilityLab/mixtures-of-experts/}.\n","authors":["Svetlana Pavlitska","Enrico Eisen","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2412.11608v1.pdf","comment":"Accepted for publication at ICMLA 2024"},{"id":"http://arxiv.org/abs/2309.08927v4","updated":"2024-12-16T09:49:16Z","published":"2023-09-16T08:46:59Z","title":"DynaMoN: Motion-Aware Fast and Robust Camera Localization for Dynamic\n  Neural Radiance Fields","summary":"  The accurate reconstruction of dynamic scenes with neural radiance fields is\nsignificantly dependent on the estimation of camera poses. Widely used\nstructure-from-motion pipelines encounter difficulties in accurately tracking\nthe camera trajectory when faced with separate dynamics of the scene content\nand the camera movement. To address this challenge, we propose Dynamic\nMotion-Aware Fast and Robust Camera Localization for Dynamic Neural Radiance\nFields (DynaMoN). DynaMoN utilizes semantic segmentation and generic motion\nmasks to handle dynamic content for initial camera pose estimation and\nstatics-focused ray sampling for fast and accurate novel-view synthesis. Our\nnovel iterative learning scheme switches between training the NeRF and updating\nthe pose parameters for an improved reconstruction and trajectory estimation\nquality. The proposed pipeline shows significant acceleration of the training\nprocess. We extensively evaluate our approach on two real-world dynamic\ndatasets, the TUM RGB-D dataset and the BONN RGB-D Dynamic dataset. DynaMoN\nimproves over the state-of-the-art both in terms of reconstruction quality and\ntrajectory accuracy. We plan to make our code public to enhance research in\nthis area.\n","authors":["Nicolas Schischka","Hannah Schieber","Mert Asim Karaoglu","Melih Görgülü","Florian Grötzner","Alexander Ladikos","Daniel Roth","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2309.08927v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11599v1","updated":"2024-12-16T09:37:52Z","published":"2024-12-16T09:37:52Z","title":"3D$^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic\n  Gaussian Avatar Modeling","summary":"  Advancements in neural implicit representations and differentiable rendering\nhave markedly improved the ability to learn animatable 3D avatars from sparse\nmulti-view RGB videos. However, current methods that map observation space to\ncanonical space often face challenges in capturing pose-dependent details and\ngeneralizing to novel poses. While diffusion models have demonstrated\nremarkable zero-shot capabilities in 2D image generation, their potential for\ncreating animatable 3D avatars from 2D inputs remains underexplored. In this\nwork, we introduce 3D$^2$-Actor, a novel approach featuring a pose-conditioned\n3D-aware human modeling pipeline that integrates iterative 2D denoising and 3D\nrectifying steps. The 2D denoiser, guided by pose cues, generates detailed\nmulti-view images that provide the rich feature set necessary for high-fidelity\n3D reconstruction and pose rendering. Complementing this, our Gaussian-based 3D\nrectifier renders images with enhanced 3D consistency through a two-stage\nprojection strategy and a novel local coordinate representation. Additionally,\nwe propose an innovative sampling strategy to ensure smooth temporal continuity\nacross frames in video synthesis. Our method effectively addresses the\nlimitations of traditional numerical solutions in handling ill-posed mappings,\nproducing realistic and animatable 3D human avatars. Experimental results\ndemonstrate that 3D$^2$-Actor excels in high-fidelity avatar modeling and\nrobustly generalizes to novel poses. Code is available at:\nhttps://github.com/silence-tang/GaussianActor.\n","authors":["Zichen Tang","Hongyu Yang","Hanchen Zhang","Jiaxin Chen","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11599v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2405.01124v5","updated":"2024-12-16T09:37:17Z","published":"2024-05-02T09:38:07Z","title":"Investigating Self-Supervised Image Denoising with Denaturation","summary":"  Self-supervised learning for image denoising problems in the presence of\ndenaturation for noisy data is a crucial approach in machine learning. However,\ntheoretical understanding of the performance of the approach that uses\ndenatured data is lacking. To provide better understanding of the approach, in\nthis paper, we analyze a self-supervised denoising algorithm that uses\ndenatured data in depth through theoretical analysis and numerical experiments.\nThrough the theoretical analysis, we discuss that the algorithm finds desired\nsolutions to the optimization problem with the population risk, while the\nguarantee for the empirical risk depends on the hardness of the denoising task\nin terms of denaturation levels. We also conduct several experiments to\ninvestigate the performance of an extended algorithm in practice. The results\nindicate that the algorithm training with denatured images works, and the\nempirical performance aligns with the theoretical results. These results\nsuggest several insights for further improvement of self-supervised image\ndenoising that uses denatured data in future directions.\n","authors":["Hiroki Waida","Kimihiro Yamazaki","Atsushi Tokuhisa","Mutsuyo Wada","Yuichiro Wada"],"pdf_url":"https://arxiv.org/pdf/2405.01124v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11596v1","updated":"2024-12-16T09:35:08Z","published":"2024-12-16T09:35:08Z","title":"MeshArt: Generating Articulated Meshes with Structure-guided\n  Transformers","summary":"  Articulated 3D object generation is fundamental for creating realistic,\nfunctional, and interactable virtual assets which are not simply static. We\nintroduce MeshArt, a hierarchical transformer-based approach to generate\narticulated 3D meshes with clean, compact geometry, reminiscent of\nhuman-crafted 3D models. We approach articulated mesh generation in a\npart-by-part fashion across two stages. First, we generate a high-level\narticulation-aware object structure; then, based on this structural\ninformation, we synthesize each part's mesh faces. Key to our approach is\nmodeling both articulation structures and part meshes as sequences of quantized\ntriangle embeddings, leading to a unified hierarchical framework with\ntransformers for autoregressive generation. Object part structures are first\ngenerated as their bounding primitives and articulation modes; a second\ntransformer, guided by these articulation structures, then generates each\npart's mesh triangles. To ensure coherency among generated parts, we introduce\nstructure-guided conditioning that also incorporates local part mesh\nconnectivity. MeshArt shows significant improvements over state of the art,\nwith 57.1% improvement in structure coverage and a 209-point improvement in\nmesh generation FID.\n","authors":["Daoyi Gao","Yawar Siddiqui","Lei Li","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2412.11596v1.pdf","comment":"Project Page: https://daoyig.github.io/Mesh_Art/"},{"id":"http://arxiv.org/abs/2412.11594v1","updated":"2024-12-16T09:32:23Z","published":"2024-12-16T09:32:23Z","title":"VersaGen: Unleashing Versatile Visual Control for Text-to-Image\n  Synthesis","summary":"  Despite the rapid advancements in text-to-image (T2I) synthesis, enabling\nprecise visual control remains a significant challenge. Existing works\nattempted to incorporate multi-facet controls (text and sketch), aiming to\nenhance the creative control over generated images. However, our pilot study\nreveals that the expressive power of humans far surpasses the capabilities of\ncurrent methods. Users desire a more versatile approach that can accommodate\ntheir diverse creative intents, ranging from controlling individual subjects to\nmanipulating the entire scene composition. We present VersaGen, a generative AI\nagent that enables versatile visual control in T2I synthesis. VersaGen admits\nfour types of visual controls: i) single visual subject; ii) multiple visual\nsubjects; iii) scene background; iv) any combination of the three above or\nmerely no control at all. We train an adaptor upon a frozen T2I model to\naccommodate the visual information into the text-dominated diffusion process.\nWe introduce three optimization strategies during the inference phase of\nVersaGen to improve generation results and enhance user experience.\nComprehensive experiments on COCO and Sketchy validate the effectiveness and\nflexibility of VersaGen, as evidenced by both qualitative and quantitative\nresults.\n","authors":["Zhipeng Chen","Lan Yang","Yonggang Qi","Honggang Zhang","Kaiyue Pang","Ke Li","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2412.11594v1.pdf","comment":"The paper has been accepted by AAAI 2025. Paper code:\n  https://github.com/FelixChan9527/VersaGen_official"},{"id":"http://arxiv.org/abs/2412.07147v2","updated":"2024-12-16T09:28:53Z","published":"2024-12-10T03:12:35Z","title":"MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation","summary":"  Image Translation (IT) holds immense potential across diverse domains,\nenabling the translation of textual content within images into various\nlanguages. However, existing datasets often suffer from limitations in scale,\ndiversity, and quality, hindering the development and evaluation of IT models.\nTo address this issue, we introduce MIT-10M, a large-scale parallel corpus of\nmultilingual image translation with over 10M image-text pairs derived from\nreal-world data, which has undergone extensive data cleaning and multilingual\ntranslation validation. It contains 840K images in three sizes, 28 categories,\ntasks with three levels of difficulty and 14 languages image-text pairs, which\nis a considerable improvement on existing datasets. We conduct extensive\nexperiments to evaluate and train models on MIT-10M. The experimental results\nclearly indicate that our dataset has higher adaptability when it comes to\nevaluating the performance of the models in tackling challenging and complex\nimage translation tasks in the real world. Moreover, the performance of the\nmodel fine-tuned with MIT-10M has tripled compared to the baseline model,\nfurther confirming its superiority.\n","authors":["Bo Li","Shaolin Zhu","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2412.07147v2.pdf","comment":"Accepted in COLING 2025"},{"id":"http://arxiv.org/abs/2412.06365v2","updated":"2024-12-16T09:25:31Z","published":"2024-12-09T10:35:39Z","title":"Is Self-Supervision Enough? Benchmarking Foundation Models Against\n  End-to-End Training for Mitotic Figure Classification","summary":"  Foundation models (FMs), i.e., models trained on a vast amount of typically\nunlabeled data, have become popular and available recently for the domain of\nhistopathology. The key idea is to extract semantically rich vectors from any\ninput patch, allowing for the use of simple subsequent classification networks\npotentially reducing the required amounts of labeled data, and increasing\ndomain robustness. In this work, we investigate to which degree this also holds\nfor mitotic figure classification. Utilizing two popular public mitotic figure\ndatasets, we compared linear probing of five publicly available FMs against\nmodels trained on ImageNet and a simple ResNet50 end-to-end-trained baseline.\nWe found that the end-to-end-trained baseline outperformed all FM-based\nclassifiers, regardless of the amount of data provided. Additionally, we did\nnot observe the FM-based classifiers to be more robust against domain shifts,\nrendering both of the above assumptions incorrect.\n","authors":["Jonathan Ganz","Jonas Ammeling","Emely Rosbach","Ludwig Lausser","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2412.06365v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.16471v6","updated":"2024-12-16T09:21:03Z","published":"2024-04-25T09:55:35Z","title":"COBRA -- COnfidence score Based on shape Regression Analysis for\n  method-independent quality assessment of object pose estimation from single\n  images","summary":"  We propose a generic procedure for assessing 6D object pose estimates. Our\napproach relies on the evaluation of discrepancies in the geometry of the\nobserved object, in particular its respective estimated back-projection in 3D,\nagainst a putative functional shape representation comprising mixtures of\nGaussian Processes, that act as a template. Each Gaussian Process is trained to\nyield a fragment of the object's surface in a radial fashion with respect to\ndesignated reference points. We further define a pose confidence measure as the\naverage probability of pixel back-projections in the Gaussian mixture. The goal\nof our experiments is two-fold. a) We demonstrate that our functional\nrepresentation is sufficiently accurate as a shape template on which the\nprobability of back-projected object points can be evaluated, and, b) we show\nthat the resulting confidence scores based on these probabilities are indeed a\nconsistent quality measure of pose.\n","authors":["Panagiotis Sapoutzoglou","Georgios Giapitzakis","Georgios Floros","George Terzakis","Maria Pateraki"],"pdf_url":"https://arxiv.org/pdf/2404.16471v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09126v5","updated":"2024-12-16T09:18:51Z","published":"2024-08-17T07:27:14Z","title":"Barbie: Text to Barbie-Style 3D Avatars","summary":"  Recent advances in text-guided 3D avatar generation have made substantial\nprogress by distilling knowledge from diffusion models. Despite the plausible\ngenerated appearance, existing methods cannot achieve fine-grained\ndisentanglement or high-fidelity modeling between inner body and outfit. In\nthis paper, we propose Barbie, a novel framework for generating 3D avatars that\ncan be dressed in diverse and high-quality Barbie-like garments and\naccessories. Instead of relying on a holistic model, Barbie achieves\nfine-grained disentanglement on avatars by semantic-aligned separated models\nfor human body and outfits. These disentangled 3D representations are then\noptimized by different expert models to guarantee the domain-specific fidelity.\nTo balance geometry diversity and reasonableness, we propose a series of losses\nfor template-preserving and human-prior evolving. The final avatar is enhanced\nby unified texture refinement for superior texture consistency. Extensive\nexperiments demonstrate that Barbie outperforms existing methods in both\ndressed human and outfit generation, supporting flexible apparel combination\nand animation. Our project page is:\nhttps://xiaokunsun.github.io/Barbie.github.io.\n","authors":["Xiaokun Sun","Zhenyu Zhang","Ying Tai","Qian Wang","Hao Tang","Zili Yi","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2408.09126v5.pdf","comment":"Project page: https://xiaokunsun.github.io/Barbie.github.io"},{"id":"http://arxiv.org/abs/2405.03959v4","updated":"2024-12-16T09:18:28Z","published":"2024-05-07T02:45:50Z","title":"Joint Identity Verification and Pose Alignment for Partial Fingerprints","summary":"  Currently, portable electronic devices are becoming more and more popular.\nFor lightweight considerations, their fingerprint recognition modules usually\nuse limited-size sensors. However, partial fingerprints have few matchable\nfeatures, especially when there are differences in finger pressing posture or\nimage quality, which makes partial fingerprint verification challenging. Most\nexisting methods regard fingerprint position rectification and identity\nverification as independent tasks, ignoring the coupling relationship between\nthem -- relative pose estimation typically relies on paired features as\nanchors, and authentication accuracy tends to improve with more precise pose\nalignment. In this paper, we propose a novel framework for joint identity\nverification and pose alignment of partial fingerprint pairs, aiming to\nleverage their inherent correlation to improve each other. To achieve this, we\npresent a multi-task CNN (Convolutional Neural Network)-Transformer hybrid\nnetwork, and design a pre-training task to enhance the feature extraction\ncapability. Experiments on multiple public datasets (NIST SD14, FVC2002 DB1A &\nDB3A, FVC2004 DB1A & DB2A, FVC2006 DB1A) and an in-house dataset show that our\nmethod achieves state-of-the-art performance in both partial fingerprint\nverification and relative pose estimation, while being more efficient than\nprevious methods.\n","authors":["Xiongjun Guan","Zhiyu Pan","Jianjiang Feng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.03959v4.pdf","comment":"15 pages, in IEEE Transactions on Information Forensics and Security,\n  2024"},{"id":"http://arxiv.org/abs/2412.11586v1","updated":"2024-12-16T09:17:36Z","published":"2024-12-16T09:17:36Z","title":"StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair\n  Geometric Priors","summary":"  While haircut indicates distinct personality, existing avatar generation\nmethods fail to model practical hair due to the general or entangled\nrepresentation. We propose StrandHead, a novel text to 3D head avatar\ngeneration method capable of generating disentangled 3D hair with strand\nrepresentation. Without using 3D data for supervision, we demonstrate that\nrealistic hair strands can be generated from prompts by distilling 2D\ngenerative diffusion models. To this end, we propose a series of reliable\npriors on shape initialization, geometric primitives, and statistical haircut\nfeatures, leading to a stable optimization and text-aligned performance.\nExtensive experiments show that StrandHead achieves the state-of-the-art\nreality and diversity of generated 3D head and hair. The generated 3D hair can\nalso be easily implemented in the Unreal Engine for physical simulation and\nother applications. The code will be available at\nhttps://xiaokunsun.github.io/StrandHead.github.io.\n","authors":["Xiaokun Sun","Zeyu Cai","Zhenyu Zhang","Ying Tai","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11586v1.pdf","comment":"Project page: https://xiaokunsun.github.io/StrandHead.github.io"},{"id":"http://arxiv.org/abs/2412.05271v2","updated":"2024-12-16T09:14:43Z","published":"2024-12-06T18:57:08Z","title":"Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling","summary":"  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n","authors":["Zhe Chen","Weiyun Wang","Yue Cao","Yangzhou Liu","Zhangwei Gao","Erfei Cui","Jinguo Zhu","Shenglong Ye","Hao Tian","Zhaoyang Liu","Lixin Gu","Xuehui Wang","Qingyun Li","Yimin Ren","Zixuan Chen","Jiapeng Luo","Jiahao Wang","Tan Jiang","Bo Wang","Conghui He","Botian Shi","Xingcheng Zhang","Han Lv","Yi Wang","Wenqi Shao","Pei Chu","Zhongying Tu","Tong He","Zhiyong Wu","Huipeng Deng","Jiaye Ge","Kai Chen","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.05271v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.11582v1","updated":"2024-12-16T09:14:32Z","published":"2024-12-16T09:14:32Z","title":"Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic\n  Unbiased Learning","summary":"  Detecting oriented tiny objects, which are limited in appearance information\nyet prevalent in real-world applications, remains an intricate and\nunder-explored problem. To address this, we systemically introduce a new\ndataset, benchmark, and a dynamic coarse-to-fine learning scheme in this study.\nOur proposed dataset, AI-TOD-R, features the smallest object sizes among all\noriented object detection datasets. Based on AI-TOD-R, we present a benchmark\nspanning a broad range of detection paradigms, including both fully-supervised\nand label-efficient approaches. Through investigation, we identify a learning\nbias presents across various learning pipelines: confident objects become\nincreasingly confident, while vulnerable oriented tiny objects are further\nmarginalized, hindering their detection performance. To mitigate this issue, we\npropose a Dynamic Coarse-to-Fine Learning (DCFL) scheme to achieve unbiased\nlearning. DCFL dynamically updates prior positions to better align with the\nlimited areas of oriented tiny objects, and it assigns samples in a way that\nbalances both quantity and quality across different object shapes, thus\nmitigating biases in prior settings and sample selection. Extensive experiments\nacross eight challenging object detection datasets demonstrate that DCFL\nachieves state-of-the-art accuracy, high efficiency, and remarkable\nversatility. The dataset, benchmark, and code are available at\nhttps://chasel-tsui.github.io/AI-TOD-R/.\n","authors":["Chang Xu","Ruixiang Zhang","Wen Yang","Haoran Zhu","Fang Xu","Jian Ding","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11579v1","updated":"2024-12-16T09:09:42Z","published":"2024-12-16T09:09:42Z","title":"SweepEvGS: Event-Based 3D Gaussian Splatting for Macro and Micro\n  Radiance Field Rendering from a Single Sweep","summary":"  Recent advancements in 3D Gaussian Splatting (3D-GS) have demonstrated the\npotential of using 3D Gaussian primitives for high-speed, high-fidelity, and\ncost-efficient novel view synthesis from continuously calibrated input views.\nHowever, conventional methods require high-frame-rate dense and high-quality\nsharp images, which are time-consuming and inefficient to capture, especially\nin dynamic environments. Event cameras, with their high temporal resolution and\nability to capture asynchronous brightness changes, offer a promising\nalternative for more reliable scene reconstruction without motion blur. In this\npaper, we propose SweepEvGS, a novel hardware-integrated method that leverages\nevent cameras for robust and accurate novel view synthesis across various\nimaging settings from a single sweep. SweepEvGS utilizes the initial static\nframe with dense event streams captured during a single camera sweep to\neffectively reconstruct detailed scene views. We also introduce different\nreal-world hardware imaging systems for real-world data collection and\nevaluation for future research. We validate the robustness and efficiency of\nSweepEvGS through experiments in three different imaging settings: synthetic\nobjects, real-world macro-level, and real-world micro-level view synthesis. Our\nresults demonstrate that SweepEvGS surpasses existing methods in visual\nrendering quality, rendering speed, and computational efficiency, highlighting\nits potential for dynamic practical applications.\n","authors":["Jingqian Wu","Shuo Zhu","Chutian Wang","Boxin Shi","Edmund Y. Lam"],"pdf_url":"https://arxiv.org/pdf/2412.11579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11578v1","updated":"2024-12-16T09:09:10Z","published":"2024-12-16T09:09:10Z","title":"DVP-MVS: Synergize Depth-Edge and Visibility Prior for Multi-View Stereo","summary":"  Patch deformation-based methods have recently exhibited substantial\neffectiveness in multi-view stereo, due to the incorporation of deformable and\nexpandable perception to reconstruct textureless areas. However, such\napproaches typically focus on exploring correlative reliable pixels to\nalleviate match ambiguity during patch deformation, but ignore the deformation\ninstability caused by mistaken edge-skipping and visibility occlusion, leading\nto potential estimation deviation. To remedy the above issues, we propose\nDVP-MVS, which innovatively synergizes depth-edge aligned and cross-view prior\nfor robust and visibility-aware patch deformation. Specifically, to avoid\nunexpected edge-skipping, we first utilize Depth Anything V2 followed by the\nRoberts operator to initialize coarse depth and edge maps respectively, both of\nwhich are further aligned through an erosion-dilation strategy to generate\nfine-grained homogeneous boundaries for guiding patch deformation. In addition,\nwe reform view selection weights as visibility maps and restore visible areas\nby cross-view depth reprojection, then regard them as cross-view prior to\nfacilitate visibility-aware patch deformation. Finally, we improve propagation\nand refinement with multi-view geometry consistency by introducing aggregated\nvisible hemispherical normals based on view selection and local projection\ndepth differences based on epipolar lines, respectively. Extensive evaluations\non ETH3D and Tanks & Temples benchmarks demonstrate that our method can achieve\nstate-of-the-art performance with excellent robustness and generalization.\n","authors":["Zhenlong Yuan","Jinguo Luo","Fei Shen","Zhaoxin Li","Cong Liu","Tianlu Mao","Zhaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11576v1","updated":"2024-12-16T09:04:58Z","published":"2024-12-16T09:04:58Z","title":"Aligning Visual and Semantic Interpretability through Visually Grounded\n  Concept Bottleneck Models","summary":"  The performance of neural networks increases steadily, but our understanding\nof their decision-making lags behind. Concept Bottleneck Models (CBMs) address\nthis issue by incorporating human-understandable concepts into the prediction\nprocess, thereby enhancing transparency and interpretability. Since existing\napproaches often rely on large language models (LLMs) to infer concepts, their\nresults may contain inaccurate or incomplete mappings, especially in complex\nvisual domains. We introduce visually Grounded Concept Bottleneck Models\n(GCBM), which derive concepts on the image level using segmentation and\ndetection foundation models. Our method generates inherently interpretable\nconcepts, which can be grounded in the input image using attribution methods,\nallowing interpretations to be traced back to the image plane. We show that\nGCBM concepts are meaningful interpretability vehicles, which aid our\nunderstanding of model embedding spaces. GCBMs allow users to control the\ngranularity, number, and naming of concepts, providing flexibility and are\neasily adaptable to new datasets without pre-training or additional data\nneeded. Prediction accuracy is within 0.3-6% of the linear probe and GCBMs\nperform especially well for fine-grained classification interpretability on\nCUB, due to their dataset specificity. Our code is available on\nhttps://github.com/KathPra/GCBM.\n","authors":["Patrick Knab","Katharina Prasse","Sascha Marton","Christian Bartelt","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2412.11576v1.pdf","comment":"*Equal contribution"},{"id":"http://arxiv.org/abs/2407.00676v2","updated":"2024-12-16T09:04:24Z","published":"2024-06-30T12:13:34Z","title":"Instruct-IPT: All-in-One Image Processing Transformer via Weight\n  Modulation","summary":"  Due to the unaffordable size and intensive computation costs of low-level\nvision models, All-in-One models that are designed to address a handful of\nlow-level vision tasks simultaneously have been popular. However, existing\nAll-in-One models are limited in terms of the range of tasks and performance.\nTo overcome these limitations, we propose Instruct-IPT -- an All-in-One Image\nProcessing Transformer (IPT) that could effectively address manifold image\nrestoration tasks with large inter-task gaps, such as denoising, deblurring,\nderaining, dehazing, and desnowing. While most research propose feature\nadaptation methods, we reveal their failure in addressing highly distinct\ntasks, and suggest weight modulation that adapts weights to specific tasks.\nFirstly, we search for task-sensitive weights and introduce task-specific\nbiases on top of them. Secondly, we conduct rank analysis for a good\ncompression strategy and perform low-rank decomposition on the biases. Thirdly,\nwe propose synchronous training that updates the task-general backbone model\nand the task-specific biases simultaneously. In this way, the model is\ninstructed to learn both general and task-specific knowledge. Via our simple\nyet effective method that instructs the IPT to be task experts, Instruct-IPT\ncould better cooperate between tasks with distinct characteristics at humble\ncosts. As an additional feature, we enable Instruct-IPT to receive human\nprompts. We have conducted experiments on Instruct-IPT to demonstrate the\neffectiveness of our method on manifold tasks, and we have effectively extended\nour method to diffusion denoisers as well. The code is available at\nhttps://github.com/huawei-noah/Pretrained-IPT.\n","authors":["Yuchuan Tian","Jianhong Han","Hanting Chen","Yuanyuan Xi","Ning Ding","Jie Hu","Chao Xu","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2407.00676v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.11574v1","updated":"2024-12-16T09:01:32Z","published":"2024-12-16T09:01:32Z","title":"PyPotteryLens: An Open-Source Deep Learning Framework for Automated\n  Digitisation of Archaeological Pottery Documentation","summary":"  Archaeological pottery documentation and study represents a crucial but\ntime-consuming aspect of archaeology. While recent years have seen advances in\ndigital documentation methods, vast amounts of legacy data remain locked in\ntraditional publications. This paper introduces PyPotteryLens, an open-source\nframework that leverages deep learning to automate the digitisation and\nprocessing of archaeological pottery drawings from published sources. The\nsystem combines state-of-the-art computer vision models (YOLO for instance\nsegmentation and EfficientNetV2 for classification) with an intuitive user\ninterface, making advanced digital methods accessible to archaeologists\nregardless of technical expertise. The framework achieves over 97\\% precision\nand recall in pottery detection and classification tasks, while reducing\nprocessing time by up to 5x to 20x compared to manual methods. Testing across\ndiverse archaeological contexts demonstrates robust generalisation\ncapabilities. Also, the system's modular architecture facilitates extension to\nother archaeological materials, while its standardised output format ensures\nlong-term preservation and reusability of digitised data as well as solid basis\nfor training machine learning algorithms. The software, documentation, and\nexamples are available on GitHub\n(https://github.com/lrncrd/PyPottery/tree/PyPotteryLens).\n","authors":["Lorenzo Cardarelli"],"pdf_url":"https://arxiv.org/pdf/2412.11574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10261v2","updated":"2024-12-16T08:54:43Z","published":"2024-12-13T16:30:35Z","title":"MVQ:Towards Efficient DNN Compression and Acceleration with Masked\n  Vector Quantization","summary":"  Vector quantization(VQ) is a hardware-friendly DNN compression method that\ncan reduce the storage cost and weight-loading datawidth of hardware\naccelerators. However, conventional VQ techniques lead to significant accuracy\nloss because the important weights are not well preserved. To tackle this\nproblem, a novel approach called MVQ is proposed, which aims at better\napproximating important weights with a limited number of codewords. At the\nalgorithm level, our approach removes the less important weights through N:M\npruning and then minimizes the vector clustering error between the remaining\nweights and codewords by the masked k-means algorithm. Only distances between\nthe unpruned weights and the codewords are computed, which are then used to\nupdate the codewords. At the architecture level, our accelerator implements\nvector quantization on an EWS (Enhanced weight stationary) CNN accelerator and\nproposes a sparse systolic array design to maximize the benefits brought by\nmasked vector quantization.\\\\ Our algorithm is validated on various models for\nimage classification, object detection, and segmentation tasks. Experimental\nresults demonstrate that MVQ not only outperforms conventional vector\nquantization methods at comparable compression ratios but also reduces FLOPs.\nUnder ASIC evaluation, our MVQ accelerator boosts energy efficiency by\n2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared\nwith the base EWS accelerator. Compared to the previous sparse accelerators,\nMVQ achieves 1.73$\\times$ higher energy efficiency.\n","authors":["Shuaiting Li","Chengxuan Wang","Juncan Deng","Zeyu Wang","Zewen Ye","Zongsheng Wang","Haibin Shen","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.10261v2.pdf","comment":"Accepted by ASPLOS '25"},{"id":"http://arxiv.org/abs/2412.11561v1","updated":"2024-12-16T08:47:55Z","published":"2024-12-16T08:47:55Z","title":"RADARSAT Constellation Mission Compact Polarisation SAR Data for Burned\n  Area Mapping with Deep Learning","summary":"  Monitoring wildfires has become increasingly critical due to the sharp rise\nin wildfire incidents in recent years. Optical satellites like Sentinel-2 and\nLandsat are extensively utilized for mapping burned areas. However, the\neffectiveness of optical sensors is compromised by clouds and smoke, which\nobstruct the detection of burned areas. Thus, satellites equipped with\nSynthetic Aperture Radar (SAR), such as dual-polarization Sentinel-1 and\nquad-polarization RADARSAT-1/-2 C-band SAR, which can penetrate clouds and\nsmoke, are investigated for mapping burned areas. However, there is limited\nresearch on using compact polarisation (compact-pol) C-band RADARSAT\nConstellation Mission (RCM) SAR data for this purpose. This study aims to\ninvestigate the capacity of compact polarisation RCM data for burned area\nmapping through deep learning. Compact-pol m-chi decomposition and Compact-pol\nRadar Vegetation Index (CpRVI) are derived from the RCM Multi-look Complex\nproduct. A deep-learning-based processing pipeline incorporating ConvNet-based\nand Transformer-based models is applied for burned area mapping, with three\ndifferent input settings: using only log-ratio dual-polarization intensity\nimages images, using only compact-pol decomposition plus CpRVI, and using all\nthree data sources. The results demonstrate that compact-pol m-chi\ndecomposition and CpRVI images significantly complement log-ratio images for\nburned area mapping. The best-performing Transformer-based model, UNETR,\ntrained with log-ratio, m-chi decomposition, and CpRVI data, achieved an F1\nScore of 0.718 and an IoU Score of 0.565, showing a notable improvement\ncompared to the same model trained using only log-ratio images.\n","authors":["Yu Zhao","Yifang Ban"],"pdf_url":"https://arxiv.org/pdf/2412.11561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11555v1","updated":"2024-12-16T08:40:12Z","published":"2024-12-16T08:40:12Z","title":"TS-SatFire: A Multi-Task Satellite Image Time-Series Dataset for\n  Wildfire Detection and Prediction","summary":"  Wildfire monitoring and prediction are essential for understanding wildfire\nbehaviour. With extensive Earth observation data, these tasks can be integrated\nand enhanced through multi-task deep learning models. We present a\ncomprehensive multi-temporal remote sensing dataset for active fire detection,\ndaily wildfire monitoring, and next-day wildfire prediction. Covering wildfire\nevents in the contiguous U.S. from January 2017 to October 2021, the dataset\nincludes 3552 surface reflectance images and auxiliary data such as weather,\ntopography, land cover, and fuel information, totalling 71 GB. The lifecycle of\neach wildfire is documented, with labels for active fires (AF) and burned areas\n(BA), supported by manual quality assurance of AF and BA test labels. The\ndataset supports three tasks: a) active fire detection, b) daily burned area\nmapping, and c) wildfire progression prediction. Detection tasks use pixel-wise\nclassification of multi-spectral, multi-temporal images, while prediction tasks\nintegrate satellite and auxiliary data to model fire dynamics. This dataset and\nits benchmarks provide a foundation for advancing wildfire research using deep\nlearning.\n","authors":["Yu Zhao","Sebastian Gerard","Yifang Ban"],"pdf_url":"https://arxiv.org/pdf/2412.11555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11553v1","updated":"2024-12-16T08:37:58Z","published":"2024-12-16T08:37:58Z","title":"Training Strategies for Isolated Sign Language Recognition","summary":"  This paper introduces a comprehensive model training pipeline for Isolated\nSign Language Recognition (ISLR) designed to accommodate the distinctive\ncharacteristics and constraints of the Sign Language (SL) domain. The\nconstructed pipeline incorporates carefully selected image and video\naugmentations to tackle the challenges of low data quality and varying sign\nspeeds. Including an additional regression head combined with IoU-balanced\nclassification loss enhances the model's awareness of the gesture and\nsimplifies capturing temporal information. Extensive experiments demonstrate\nthat the developed training pipeline easily adapts to different datasets and\narchitectures. Additionally, the ablation study shows that each proposed\ncomponent expands the potential to consider ISLR task specifics. The presented\nstrategies improve recognition performance on a broad set of ISLR benchmarks.\nMoreover, we achieved a state-of-the-art result on the WLASL and Slovo\nbenchmarks with 1.63% and 14.12% improvements compared to the previous best\nsolution, respectively.\n","authors":["Karina Kvanchiani","Roman Kraynov","Elizaveta Petrova","Petr Surovcev","Aleksandr Nagaev","Alexander Kapitanov"],"pdf_url":"https://arxiv.org/pdf/2412.11553v1.pdf","comment":"sign language recognition, training strategies, computer vision"},{"id":"http://arxiv.org/abs/2310.19180v3","updated":"2024-12-16T08:34:52Z","published":"2023-10-29T22:51:49Z","title":"JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music\n  Generation","summary":"  With rapid advances in generative artificial intelligence, the text-to-music\nsynthesis task has emerged as a promising direction for music generation.\nNevertheless, achieving precise control over multi-track generation remains an\nopen challenge. While existing models excel in directly generating multi-track\nmix, their limitations become evident when it comes to composing individual\ntracks and integrating them in a controllable manner. This departure from the\ntypical workflows of professional composers hinders the ability to refine\ndetails in specific tracks. To address this gap, we propose JEN-1 Composer, a\nunified framework designed to efficiently model marginal, conditional, and\njoint distributions over multi-track music using a single model. Building upon\nan audio latent diffusion model, JEN-1 Composer extends the versatility of\nmulti-track music generation. We introduce a progressive curriculum training\nstrategy, which gradually escalates the difficulty of training tasks while\nensuring the model's generalization ability and facilitating smooth transitions\nbetween different scenarios. During inference, users can iteratively generate\nand select music tracks, thus incrementally composing entire musical pieces in\naccordance with the Human-AI co-composition workflow. Our approach demonstrates\nstate-of-the-art performance in controllable and high-fidelity multi-track\nmusic synthesis, marking a significant advancement in interactive AI-assisted\nmusic creation. Our demo pages are available at www.jenmusic.ai/research.\n","authors":["Yao Yao","Peike Li","Boyu Chen","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19180v3.pdf","comment":"9 pages, 3 figures, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11549v1","updated":"2024-12-16T08:31:55Z","published":"2024-12-16T08:31:55Z","title":"MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion\n  Models","summary":"  Diffusion models have received wide attention in generation tasks. However,\nthe expensive computation cost prevents the application of diffusion models in\nresource-constrained scenarios. Quantization emerges as a practical solution\nthat significantly saves storage and computation by reducing the bit-width of\nparameters. However, the existing quantization methods for diffusion models\nstill cause severe degradation in performance, especially under extremely low\nbit-widths (2-4 bit). The primary decrease in performance comes from the\nsignificant discretization of activation values at low bit quantization. Too\nfew activation candidates are unfriendly for outlier significant weight channel\nquantization, and the discretized features prevent stable learning over\ndifferent time steps of the diffusion model. This paper presents MPQ-DM, a\nMixed-Precision Quantization method for Diffusion Models. The proposed MPQ-DM\nmainly relies on two techniques:(1) To mitigate the quantization error caused\nby outlier severe weight channels, we propose an Outlier-Driven Mixed\nQuantization (OMQ) technique that uses $Kurtosis$ to quantify outlier salient\nchannels and apply optimized intra-layer mixed-precision bit-width allocation\nto recover accuracy performance within target efficiency.(2) To robustly learn\nrepresentations crossing time steps, we construct a Time-Smoothed Relation\nDistillation (TRD) scheme between the quantized diffusion model and its\nfull-precision counterpart, transferring discrete and continuous latent to a\nunified relation space to reduce the representation inconsistency.\nComprehensive experiments demonstrate that MPQ-DM achieves significant accuracy\ngains under extremely low bit-widths compared with SOTA quantization methods.\nMPQ-DM achieves a 58\\% FID decrease under W2A4 setting compared with baseline,\nwhile all other methods even collapse.\n","authors":["Weilun Feng","Haotong Qin","Chuanguang Yang","Zhulin An","Libo Huang","Boyu Diao","Fei Wang","Renshuai Tao","Yongjun Xu","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2412.11549v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11542v1","updated":"2024-12-16T08:22:23Z","published":"2024-12-16T08:22:23Z","title":"Meta Curvature-Aware Minimization for Domain Generalization","summary":"  Domain generalization (DG) aims to enhance the ability of models trained on\nsource domains to generalize effectively to unseen domains. Recently,\nSharpness-Aware Minimization (SAM) has shown promise in this area by reducing\nthe sharpness of the loss landscape to obtain more generalized models. However,\nSAM and its variants sometimes fail to guide the model toward a flat minimum,\nand their training processes exhibit limitations, hindering further\nimprovements in model generalization. In this paper, we first propose an\nimproved model training process aimed at encouraging the model to converge to a\nflat minima. To achieve this, we design a curvature metric that has a minimal\neffect when the model is far from convergence but becomes increasingly\ninfluential in indicating the curvature of the minima as the model approaches a\nlocal minimum. Then we derive a novel algorithm from this metric, called Meta\nCurvature-Aware Minimization (MeCAM), to minimize the curvature around the\nlocal minima. Specifically, the optimization objective of MeCAM simultaneously\nminimizes the regular training loss, the surrogate gap of SAM, and the\nsurrogate gap of meta-learning. We provide theoretical analysis on MeCAM's\ngeneralization error and convergence rate, and demonstrate its superiority over\nexisting DG methods through extensive experiments on five benchmark DG\ndatasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code\nwill be available on GitHub.\n","authors":["Ziyang Chen","Yiwen Ye","Feilong Tang","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11542v1.pdf","comment":"21 pages, 5 figures, 17 tables"},{"id":"http://arxiv.org/abs/2412.11540v1","updated":"2024-12-16T08:21:09Z","published":"2024-12-16T08:21:09Z","title":"SP$^2$T: Sparse Proxy Attention for Dual-stream Point Transformer","summary":"  In 3D understanding, point transformers have yielded significant advances in\nbroadening the receptive field. However, further enhancement of the receptive\nfield is hindered by the constraints of grouping attention. The proxy-based\nmodel, as a hot topic in image and language feature extraction, uses global or\nlocal proxies to expand the model's receptive field. But global proxy-based\nmethods fail to precisely determine proxy positions and are not suited for\ntasks like segmentation and detection in the point cloud, and exist local\nproxy-based methods for image face difficulties in global-local balance, proxy\nsampling in various point clouds, and parallel cross-attention computation for\nsparse association. In this paper, we present SP$^2$T, a local proxy-based dual\nstream point transformer, which promotes global receptive field while\nmaintaining a balance between local and global information. To tackle robust 3D\nproxy sampling, we propose a spatial-wise proxy sampling with vertex-based\npoint proxy associations, ensuring robust point-cloud sampling in many scales\nof point cloud. To resolve economical association computation, we introduce\nsparse proxy attention combined with table-based relative bias, which enables\nlow-cost and precise interactions between proxy and point features.\nComprehensive experiments across multiple datasets reveal that our model\nachieves SOTA performance in downstream tasks. The code has been released in\nhttps://github.com/TerenceWallel/Sparse-Proxy-Point-Transformer .\n","authors":["Jiaxu Wan","Hong Zhang","Ziqi He","Qishu Wang","Ding Yuan","Yifan Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11540v1.pdf","comment":"13 pages, 14 figures, 14 tables"},{"id":"http://arxiv.org/abs/2412.11535v1","updated":"2024-12-16T08:13:05Z","published":"2024-12-16T08:13:05Z","title":"Near Large Far Small: Relative Distance Based Partition Learning for\n  UAV-view Geo-Localization","summary":"  UAV-view Geo-Localization (UVGL) presents substantial challenges, primarily\ndue to appearance differences between drone-view and satellite-view. Existing\nmethods develop partition learning strategies aimed at mining more\ncomprehensive information by constructing diverse part-level feature\nrepresentations, which rely on consistent cross-view scales. However,\nvariations of UAV flight state leads to the scale mismatch of cross-views,\nresulting in serious performance degradation of partition-based methods. To\novercome this issue, we propose a partition learning framework based on\nrelative distance, which alleviates the dependence on scale consistency while\nmining fine-grained features. Specifically, we propose a distance guided\ndynamic partition learning strategy (DGDPL), consisting of a square partition\nstrategy and a dynamic-guided adjustment strategy. The former is utilized to\nextract fine-grained features and global features in a simple manner. The\nlatter calculates the relative distance ratio between drone- and satellite-view\nto adjust the partition size, thereby aligning the semantic information between\npartition pairs. Furthermore, we propose a saliency-guided refinement strategy\nto refine part-level features, so as to further improve the retrieval accuracy.\nExtensive experiments show that our approach achieves superior geo-localization\naccuracy across various scale-inconsistent scenarios, and exhibits remarkable\nrobustness against scale variations. The code will be released.\n","authors":["Quan Chen","Tingyu Wang","Rongfeng Lu","Bolun Zheng","Zhedong Zheng","Chenggang Yan"],"pdf_url":"https://arxiv.org/pdf/2412.11535v1.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2411.18203v3","updated":"2024-12-16T08:12:17Z","published":"2024-11-27T10:28:57Z","title":"Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning","summary":"  Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n","authors":["Di Zhang","Junxian Li","Jingdi Lei","Xunzhi Wang","Yujie Liu","Zonglin Yang","Jiatong Li","Weida Wang","Suorong Yang","Jianbo Wu","Peng Ye","Wanli Ouyang","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.18203v3.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.11530v1","updated":"2024-12-16T08:08:35Z","published":"2024-12-16T08:08:35Z","title":"RoMeO: Robust Metric Visual Odometry","summary":"  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a\nfundamental building block for many applications such as VR/AR and robotics.\nThis work focuses on monocular RGB VO where the input is a monocular RGB video\nwithout IMU or 3D sensors. Existing approaches lack robustness under this\nchallenging scenario and fail to generalize to unseen data (especially\noutdoors); they also cannot recover metric-scale poses. We propose Robust\nMetric Visual Odometry (RoMeO), a novel method that resolves these issues\nleveraging priors from pre-trained depth models. RoMeO incorporates both\nmonocular metric depth and multi-view stereo (MVS) models to recover\nmetric-scale, simplify correspondence search, provide better initialization and\nregularize optimization. Effective strategies are proposed to inject noise\nduring training and adaptively filter noisy depth priors, which ensure the\nrobustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the\nstate-of-the-art (SOTA) by a large margin across 6 diverse datasets covering\nboth indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO\nreduces the relative (align the trajectory scale with GT) and absolute\ntrajectory errors both by >50%. The performance gain also transfers to the full\nSLAM pipeline (with global BA & loop closure). Code will be released upon\nacceptance.\n","authors":["Junda Cheng","Zhipeng Cai","Zhaoxing Zhang","Wei Yin","Matthias Muller","Michael Paulitsch","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11529v1","updated":"2024-12-16T08:07:53Z","published":"2024-12-16T08:07:53Z","title":"Cross-View Geo-Localization with Street-View and VHR Satellite Imagery\n  in Decentrality Settings","summary":"  Cross-View Geo-Localization tackles the problem of image geo-localization in\nGNSS-denied environments by matching street-view query images with geo-tagged\naerial-view reference images. However, existing datasets and methods often\nassume center-aligned settings or only consider limited decentrality (i.e., the\noffset of the query image from the reference image center). This assumption\noverlooks the challenges present in real-world applications, where large\ndecentrality can significantly enhance localization efficiency but\nsimultaneously lead to a substantial degradation in localization accuracy. To\naddress this limitation, we introduce CVSat, a novel dataset designed to\nevaluate cross-view geo-localization with a large geographic scope and diverse\nlandscapes, emphasizing the decentrality issue. Meanwhile, we propose AuxGeo\n(Auxiliary Enhanced Geo-Localization), which leverages a multi-metric\noptimization strategy with two novel modules: the Bird's-eye view Intermediary\nModule (BIM) and the Position Constraint Module (PCM). BIM uses bird's-eye view\nimages derived from street-view panoramas as an intermediary, simplifying the\ncross-view challenge with decentrality to a cross-view problem and a\ndecentrality problem. PCM leverages position priors between cross-view images\nto establish multi-grained alignment constraints. These modules improve the\nperformance of cross-view geo-localization with the decentrality problem.\nExtensive experiments demonstrate that AuxGeo outperforms previous methods on\nour proposed CVSat dataset, mitigating the issue of large decentrality, and\nalso achieves state-of-the-art performance on existing public datasets such as\nCVUSA, CVACT, and VIGOR.\n","authors":["Panwang Xia","Lei Yu","Yi Wan","Qiong Wu","Peiqi Chen","Liheng Zhong","Yongxiang Yao","Dong Wei","Xinyi Liu","Lixiang Ru","Yingying Zhang","Jiangwei Lao","Jingdong Chen","Ming Yang","Yongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11525v1","updated":"2024-12-16T08:00:50Z","published":"2024-12-16T08:00:50Z","title":"Sequence Matters: Harnessing Video Models in Super-Resolution","summary":"  3D super-resolution aims to reconstruct high-fidelity 3D models from\nlow-resolution (LR) multi-view images. Early studies primarily focused on\nsingle-image super-resolution (SISR) models to upsample LR images into\nhigh-resolution images. However, these methods often lack view consistency\nbecause they operate independently on each image. Although various\npost-processing techniques have been extensively explored to mitigate these\ninconsistencies, they have yet to fully resolve the issues. In this paper, we\nperform a comprehensive study of 3D super-resolution by leveraging video\nsuper-resolution (VSR) models. By utilizing VSR models, we ensure a higher\ndegree of spatial consistency and can reference surrounding spatial\ninformation, leading to more accurate and detailed reconstructions. Our\nfindings reveal that VSR models can perform remarkably well even on sequences\nthat lack precise spatial alignment. Given this observation, we propose a\nsimple yet practical approach to align LR images without involving fine-tuning\nor generating 'smooth' trajectory from the trained 3D models over LR images.\nThe experimental results show that the surprisingly simple algorithms can\nachieve the state-of-the-art results of 3D super-resolution tasks on standard\nbenchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.\nProject page: https://ko-lani.github.io/Sequence-Matters\n","authors":["Hyun-kyu Ko","Dongheok Park","Youngin Park","Byeonghyeon Lee","Juhee Han","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2412.11525v1.pdf","comment":"Project page: https://ko-lani.github.io/Sequence-Matters"},{"id":"http://arxiv.org/abs/2412.08929v2","updated":"2024-12-16T07:56:34Z","published":"2024-12-12T04:34:28Z","title":"CAPrompt: Cyclic Prompt Aggregation for Pre-Trained Model Based Class\n  Incremental Learning","summary":"  Recently, prompt tuning methods for pre-trained models have demonstrated\npromising performance in Class Incremental Learning (CIL). These methods\ntypically involve learning task-specific prompts and predicting the task ID to\nselect the appropriate prompts for inference. However, inaccurate task ID\npredictions can cause severe inconsistencies between the prompts used during\ntraining and inference, leading to knowledge forgetting and performance\ndegradation. Additionally, existing prompt tuning methods rely solely on the\npre-trained model to predict task IDs, without fully leveraging the knowledge\nembedded in the learned prompt parameters, resulting in inferior prediction\nperformance. To address these issues, we propose a novel Cyclic Prompt\nAggregation (CAPrompt) method that eliminates the dependency on task ID\nprediction by cyclically aggregating the knowledge from different prompts.\nSpecifically, rather than predicting task IDs, we introduce an innovative\nprompt aggregation strategy during both training and inference to overcome\nprompt inconsistency by utilizing a weighted sum of different prompts. Thorough\ntheoretical analysis demonstrates that under concave conditions, the aggregated\nprompt achieves lower error compared to selecting a single task-specific\nprompt. Consequently, we incorporate a concave constraint and a linear\nconstraint to guide prompt learning, ensuring compliance with the concave\ncondition requirement. Furthermore, to fully exploit the prompts and achieve\nmore accurate prompt weights, we develop a cyclic weight prediction strategy.\nThis strategy begins with equal weights for each task and automatically adjusts\nthem to more appropriate values in a cyclical manner. Experiments on various\ndatasets demonstrate that our proposed CAPrompt outperforms state-of-the-art\nmethods by 2%-3%. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-CAPrompt.\n","authors":["Qiwei Li","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.08929v2.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.11520v1","updated":"2024-12-16T07:56:04Z","published":"2024-12-16T07:56:04Z","title":"EditSplat: Multi-View Fusion and Attention-Guided Optimization for\n  View-Consistent 3D Scene Editing with 3D Gaussian Splatting","summary":"  Recent advancements in 3D editing have highlighted the potential of\ntext-driven methods in real-time, user-friendly AR/VR applications. However,\ncurrent methods rely on 2D diffusion models without adequately considering\nmulti-view information, resulting in multi-view inconsistency. While 3D\nGaussian Splatting (3DGS) significantly improves rendering quality and speed,\nits 3D editing process encounters difficulties with inefficient optimization,\nas pre-trained Gaussians retain excessive source information, hindering\noptimization. To address these limitations, we propose \\textbf{EditSplat}, a\nnovel 3D editing framework that integrates Multi-view Fusion Guidance (MFG) and\nAttention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by\nincorporating essential multi-view information into the diffusion process,\nleveraging classifier-free guidance from the text-to-image diffusion model and\nthe geometric properties of 3DGS. Additionally, our AGT leverages the explicit\nrepresentation of 3DGS to selectively prune and optimize 3D Gaussians,\nenhancing optimization efficiency and enabling precise, semantically rich local\nedits. Through extensive qualitative and quantitative evaluations, EditSplat\nachieves superior multi-view consistency and editing quality over existing\nmethods, significantly enhancing overall efficiency.\n","authors":["Dong In Lee","Hyeongcheol Park","Jiyoung Seo","Eunbyung Park","Hyunje Park","Ha Dam Baek","Shin Sangheon","Sangmin kim","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11519v1","updated":"2024-12-16T07:54:45Z","published":"2024-12-16T07:54:45Z","title":"LineArt: A Knowledge-guided Training-free High-quality Appearance\n  Transfer for Design Drawing with Diffusion Model","summary":"  Image rendering from line drawings is vital in design and image generation\ntechnologies reduce costs, yet professional line drawings demand preserving\ncomplex details. Text prompts struggle with accuracy, and image translation\nstruggles with consistency and fine-grained control. We present LineArt, a\nframework that transfers complex appearance onto detailed design drawings,\nfacilitating design and artistic creation. It generates high-fidelity\nappearance while preserving structural accuracy by simulating hierarchical\nvisual cognition and integrating human artistic experience to guide the\ndiffusion process. LineArt overcomes the limitations of current methods in\nterms of difficulty in fine-grained control and style degradation in design\ndrawings. It requires no precise 3D modeling, physical property specs, or\nnetwork training, making it more convenient for design tasks. LineArt consists\nof two stages: a multi-frequency lines fusion module to supplement the input\ndesign drawing with detailed structural information and a two-part painting\nprocess for Base Layer Shaping and Surface Layer Coloring. We also present a\nnew design drawing dataset ProLines for evaluation. The experiments show that\nLineArt performs better in accuracy, realism, and material precision compared\nto SOTAs.\n","authors":["Xi Wang","Hongzhen Li","Heng Fang","Yichen Peng","Haoran Xie","Xi Yang","Chuntao Li"],"pdf_url":"https://arxiv.org/pdf/2412.11519v1.pdf","comment":"Project Page: https://meaoxixi.github.io/LineArt/"},{"id":"http://arxiv.org/abs/2412.09920v2","updated":"2024-12-16T07:50:46Z","published":"2024-12-13T07:15:52Z","title":"Precision-Enhanced Human-Object Contact Detection via Depth-Aware\n  Perspective Interaction and Object Texture Restoration","summary":"  Human-object contact (HOT) is designed to accurately identify the areas where\nhumans and objects come into contact. Current methods frequently fail to\naccount for scenarios where objects are frequently blocking the view, resulting\nin inaccurate identification of contact areas. To tackle this problem, we\nsuggest using a perspective interaction HOT detector called PIHOT, which\nutilizes a depth map generation model to offer depth information of humans and\nobjects related to the camera, thereby preventing false interaction detection.\nFurthermore, we use mask dilatation and object restoration techniques to\nrestore the texture details in covered areas, improve the boundaries between\nobjects, and enhance the perception of humans interacting with objects.\nMoreover, a spatial awareness perception is intended to concentrate on the\ncharacteristic features close to the points of contact. The experimental\nresults show that the PIHOT algorithm achieves state-of-the-art performance on\nthree benchmark datasets for HOT detection tasks. Compared to the most recent\nDHOT, our method enjoys an average improvement of 13%, 27.5%, 16%, and 18.5% on\nSC-Acc., C-Acc., mIoU, and wIoU metrics, respectively.\n","authors":["Yuxiao Wang","Wenpeng Neng","Zhenao Wei","Yu Lei","Weiying Xue","Nan Zhuang","Yanwu Xu","Xinyu Jiang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09920v2.pdf","comment":"Accepted by AAAl 2025"},{"id":"http://arxiv.org/abs/2412.11513v1","updated":"2024-12-16T07:48:30Z","published":"2024-12-16T07:48:30Z","title":"IGR: Improving Diffusion Model for Garment Restoration from Person Image","summary":"  Garment restoration, the inverse of virtual try-on task, focuses on restoring\nstandard garment from a person image, requiring accurate capture of garment\ndetails. However, existing methods often fail to preserve the identity of the\ngarment or rely on complex processes. To address these limitations, we propose\nan improved diffusion model for restoring authentic garments. Our approach\nemploys two garment extractors to independently capture low-level features and\nhigh-level semantics from the person image. Leveraging a pretrained latent\ndiffusion model, these features are integrated into the denoising process\nthrough garment fusion blocks, which combine self-attention and cross-attention\nlayers to align the restored garment with the person image. Furthermore, a\ncoarse-to-fine training strategy is introduced to enhance the fidelity and\nauthenticity of the generated garments. Experimental results demonstrate that\nour model effectively preserves garment identity and generates high-quality\nrestorations, even in challenging scenarios such as complex garments or those\nwith occlusions.\n","authors":["Le Shen","Rong Huang","Zhijie Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11512v1","updated":"2024-12-16T07:42:49Z","published":"2024-12-16T07:42:49Z","title":"SpatialMe: Stereo Video Conversion Using Depth-Warping and\n  Blend-Inpainting","summary":"  Stereo video conversion aims to transform monocular videos into immersive\nstereo format. Despite the advancements in novel view synthesis, it still\nremains two major challenges: i) difficulty of achieving high-fidelity and\nstable results, and ii) insufficiency of high-quality stereo video data. In\nthis paper, we introduce SpatialMe, a novel stereo video conversion framework\nbased on depth-warping and blend-inpainting. Specifically, we propose a\nmask-based hierarchy feature update (MHFU) refiner, which integrate and refine\nthe outputs from designed multi-branch inpainting module, using feature update\nunit (FUU) and mask mechanism. We also propose a disparity expansion strategy\nto address the problem of foreground bleeding. Furthermore, we conduct a\nhigh-quality real-world stereo video dataset -- StereoV1K, to alleviate the\ndata shortage. It contains 1000 stereo videos captured in real-world at a\nresolution of 1180 x 1180, covering various indoor and outdoor scenes.\nExtensive experiments demonstrate the superiority of our approach in generating\nstereo videos over state-of-the-art methods.\n","authors":["Jiale Zhang","Qianxi Jia","Yang Liu","Wei Zhang","Wei Wei","Xin Tian"],"pdf_url":"https://arxiv.org/pdf/2412.11512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11509v1","updated":"2024-12-16T07:33:23Z","published":"2024-12-16T07:33:23Z","title":"Skip Tuning: Pre-trained Vision-Language Models are Effective and\n  Efficient Adapters Themselves","summary":"  Prompt tuning (PT) has long been recognized as an effective and efficient\nparadigm for transferring large pre-trained vision-language models (VLMs) to\ndownstream tasks by learning a tiny set of context vectors. Nevertheless, in\nthis work, we reveal that freezing the parameters of VLMs during learning the\ncontext vectors neither facilitates the transferability of pre-trained\nknowledge nor improves the memory and time efficiency significantly. Upon\nfurther investigation, we find that reducing both the length and width of the\nfeature-gradient propagation flows of the full fine-tuning (FT) baseline is key\nto achieving effective and efficient knowledge transfer. Motivated by this, we\npropose Skip Tuning, a novel paradigm for adapting VLMs to downstream tasks.\nUnlike existing PT or adapter-based methods, Skip Tuning applies Layer-wise\nSkipping (LSkip) and Class-wise Skipping (CSkip) upon the FT baseline without\nintroducing extra context vectors or adapter modules. Extensive experiments\nacross a wide spectrum of benchmarks demonstrate the superior effectiveness and\nefficiency of our Skip Tuning over both PT and adapter-based methods. Code:\nhttps://github.com/Koorye/SkipTuning.\n","authors":["Shihan Wu","Ji Zhang","Pengpeng Zeng","Lianli Gao","Jingkuan Song","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.11509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06367v3","updated":"2024-12-16T07:32:01Z","published":"2024-06-10T15:26:48Z","title":"MVGamba: Unify 3D Content Generation as State Space Sequence Modeling","summary":"  Recent 3D large reconstruction models (LRMs) can generate high-quality 3D\ncontent in sub-seconds by integrating multi-view diffusion models with scalable\nmulti-view reconstructors. Current works further leverage 3D Gaussian Splatting\nas 3D representation for improved visual quality and rendering efficiency.\nHowever, we observe that existing Gaussian reconstruction models often suffer\nfrom multi-view inconsistency and blurred textures. We attribute this to the\ncompromise of multi-view information propagation in favor of adopting powerful\nyet computationally intensive architectures (e.g., Transformers). To address\nthis issue, we introduce MVGamba, a general and lightweight Gaussian\nreconstruction model featuring a multi-view Gaussian reconstructor based on the\nRNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal\ncontext containing multi-view information for cross-view self-refinement while\ngenerating a long sequence of Gaussians for fine-detail modeling with linear\ncomplexity. With off-the-shelf multi-view diffusion models integrated, MVGamba\nunifies 3D generation tasks from a single image, sparse images, or text\nprompts. Extensive experiments demonstrate that MVGamba outperforms\nstate-of-the-art baselines in all 3D content generation scenarios with\napproximately only $0.1\\times$ of the model size.\n","authors":["Xuanyu Yi","Zike Wu","Qiuhong Shen","Qingshan Xu","Pan Zhou","Joo-Hwee Lim","Shuicheng Yan","Xinchao Wang","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06367v3.pdf","comment":"Accepted by NeurIPS 2024. Code is included in\n  https://github.com/SkyworkAI/MVGamba"},{"id":"http://arxiv.org/abs/2412.11495v1","updated":"2024-12-16T07:15:13Z","published":"2024-12-16T07:15:13Z","title":"Exploring More from Multiple Gait Modalities for Human Identification","summary":"  The gait, as a kind of soft biometric characteristic, can reflect the\ndistinct walking patterns of individuals at a distance, exhibiting a promising\ntechnique for unrestrained human identification. With largely excluding\ngait-unrelated cues hidden in RGB videos, the silhouette and skeleton, though\nvisually compact, have acted as two of the most prevailing gait modalities for\na long time. Recently, several attempts have been made to introduce more\ninformative data forms like human parsing and optical flow images to capture\ngait characteristics, along with multi-branch architectures. However, due to\nthe inconsistency within model designs and experiment settings, we argue that a\ncomprehensive and fair comparative study among these popular gait modalities,\ninvolving the representational capacity and fusion strategy exploration, is\nstill lacking. From the perspectives of fine vs. coarse-grained shape and whole\nvs. pixel-wise motion modeling, this work presents an in-depth investigation of\nthree popular gait representations, i.e., silhouette, human parsing, and\noptical flow, with various fusion evaluations, and experimentally exposes their\nsimilarities and differences. Based on the obtained insights, we further\ndevelop a C$^2$Fusion strategy, consequently building our new framework\nMultiGait++. C$^2$Fusion preserves commonalities while highlighting differences\nto enrich the learning of gait features. To verify our findings and\nconclusions, extensive experiments on Gait3D, GREW, CCPG, and SUSTech1K are\nconducted. The code is available at https://github.com/ShiqiYu/OpenGait.\n","authors":["Dongyang Jin","Chao Fan","Weihua Chen","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11489v1","updated":"2024-12-16T07:06:17Z","published":"2024-12-16T07:06:17Z","title":"HGSFusion: Radar-Camera Fusion with Hybrid Generation and\n  Synchronization for 3D Object Detection","summary":"  Millimeter-wave radar plays a vital role in 3D object detection for\nautonomous driving due to its all-weather and all-lighting-condition\ncapabilities for perception. However, radar point clouds suffer from pronounced\nsparsity and unavoidable angle estimation errors. To address these limitations,\nincorporating a camera may partially help mitigate the shortcomings.\nNevertheless, the direct fusion of radar and camera data can lead to negative\nor even opposite effects due to the lack of depth information in images and\nlow-quality image features under adverse lighting conditions. Hence, in this\npaper, we present the radar-camera fusion network with Hybrid Generation and\nSynchronization (HGSFusion), designed to better fuse radar potentials and image\nfeatures for 3D object detection. Specifically, we propose the Radar Hybrid\nGeneration Module (RHGM), which fully considers the Direction-Of-Arrival (DOA)\nestimation errors in radar signal processing. This module generates denser\nradar points through different Probability Density Functions (PDFs) with the\nassistance of semantic information. Meanwhile, we introduce the Dual Sync\nModule (DSM), comprising spatial sync and modality sync, to enhance image\nfeatures with radar positional information and facilitate the fusion of\ndistinct characteristics in different modalities. Extensive experiments\ndemonstrate the effectiveness of our approach, outperforming the\nstate-of-the-art methods in the VoD and TJ4DRadSet datasets by $6.53\\%$ and\n$2.03\\%$ in RoI AP and BEV AP, respectively. The code is available at\nhttps://github.com/garfield-cpp/HGSFusion.\n","authors":["Zijian Gu","Jianwei Ma","Yan Huang","Honghao Wei","Zhanye Chen","Hui Zhang","Wei Hong"],"pdf_url":"https://arxiv.org/pdf/2412.11489v1.pdf","comment":"12 pages, 8 figures, 7 tables. Accepted by AAAI 2025 , the 39th\n  Annual AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2408.07343v3","updated":"2024-12-16T07:06:15Z","published":"2024-08-14T07:37:07Z","title":"Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation","summary":"  Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels are available at https://github.com/Chen-Ziyang/GraTa.\n","authors":["Ziyang Chen","Yiwen Ye","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2408.07343v3.pdf","comment":"9 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.18844v4","updated":"2024-12-16T06:59:33Z","published":"2024-06-27T02:31:03Z","title":"Revisiting Backdoor Attacks against Large Vision-Language Models from\n  Domain Shift","summary":"  Instruction tuning enhances large vision-language models (LVLMs) but\nincreases their vulnerability to backdoor attacks due to their open design.\nUnlike prior studies in static settings, this paper explores backdoor attacks\nin LVLM instruction tuning across mismatched training and testing domains. We\nintroduce a new evaluation dimension, backdoor domain generalization, to assess\nattack robustness under visual and text domain shifts. Our findings reveal two\ninsights: (1) backdoor generalizability improves when distinctive trigger\npatterns are independent of specific data domains or model architectures, and\n(2) the competitive interaction between trigger patterns and clean semantic\nregions, where guiding the model to predict triggers enhances attack\ngeneralizability. Based on these insights, we propose a multimodal attribution\nbackdoor attack (MABA) that injects domain-agnostic triggers into critical\nareas using attributional interpretation. Experiments with OpenFlamingo,\nBlip-2, and Otter show that MABA significantly boosts the attack success rate\nof generalization by 36.4%, achieving a 97% success rate at a 0.2% poisoning\nrate. This study reveals limitations in current evaluations and highlights how\nenhanced backdoor generalizability poses a security threat to LVLMs, even\nwithout test data access.\n","authors":["Siyuan Liang","Jiawei Liang","Tianyu Pang","Chao Du","Aishan Liu","Mingli Zhu","Xiaochun Cao","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2406.18844v4.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.11484v1","updated":"2024-12-16T06:53:00Z","published":"2024-12-16T06:53:00Z","title":"Efficient Policy Adaptation with Contrastive Prompt Ensemble for\n  Embodied Agents","summary":"  For embodied reinforcement learning (RL) agents interacting with the\nenvironment, it is desirable to have rapid policy adaptation to unseen visual\nobservations, but achieving zero-shot adaptation capability is considered as a\nchallenging problem in the RL context. To address the problem, we present a\nnovel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained\nvision-language model and a set of visual prompts, thus enabling efficient\npolicy learning and adaptation upon a wide range of environmental and physical\nchanges encountered by embodied agents. Specifically, we devise a\nguided-attention-based ensemble approach with multiple visual prompts on the\nvision-language model to construct robust state representations. Each prompt is\ncontrastively learned in terms of an individual domain factor that\nsignificantly affects the agent's egocentric perception and observation. For a\ngiven task, the attention-based ensemble and policy are jointly learned so that\nthe resulting state representations not only generalize to various domains but\nare also optimized for learning the task. Through experiments, we show that\nConPE outperforms other state-of-the-art algorithms for several embodied agent\ntasks including navigation in AI2THOR, manipulation in egocentric-Metaworld,\nand autonomous driving in CARLA, while also improving the sample efficiency of\npolicy learning and adaptation.\n","authors":["Wonje Choi","Woo Kyung Kim","SeungHyun Kim","Honguk Woo"],"pdf_url":"https://arxiv.org/pdf/2412.11484v1.pdf","comment":"Accepted at NeurIPS 2023"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.12092v1","updated":"2024-12-16T18:58:28Z","published":"2024-12-16T18:58:28Z","title":"No More Tuning: Prioritized Multi-Task Learning with Lagrangian\n  Differential Multiplier Methods","summary":"  Given the ubiquity of multi-task in practical systems, Multi-Task Learning\n(MTL) has found widespread application across diverse domains. In real-world\nscenarios, these tasks often have different priorities. For instance, In web\nsearch, relevance is often prioritized over other metrics, such as\nclick-through rates or user engagement. Existing frameworks pay insufficient\nattention to the prioritization among different tasks, which typically adjust\ntask-specific loss function weights to differentiate task priorities. However,\nthis approach encounters challenges as the number of tasks grows, leading to\nexponential increases in hyper-parameter tuning complexity. Furthermore, the\nsimultaneous optimization of multiple objectives can negatively impact the\nperformance of high-priority tasks due to interference from lower-priority\ntasks.\n  In this paper, we introduce a novel multi-task learning framework employing\nLagrangian Differential Multiplier Methods for step-wise multi-task\noptimization. It is designed to boost the performance of high-priority tasks\nwithout interference from other tasks. Its primary advantage lies in its\nability to automatically optimize multiple objectives without requiring\nbalancing hyper-parameters for different tasks, thereby eliminating the need\nfor manual tuning. Additionally, we provide theoretical analysis demonstrating\nthat our method ensures optimization guarantees, enhancing the reliability of\nthe process. We demonstrate its effectiveness through experiments on multiple\npublic datasets and its application in Taobao search, a large-scale industrial\nsearch ranking system, resulting in significant improvements across various\nbusiness metrics.\n","authors":["Zhengxing Cheng","Yuheng Huang","Zhixuan Zhang","Dan Ou","Qingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12092v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.06954v3","updated":"2024-12-16T16:18:28Z","published":"2024-12-09T20:01:59Z","title":"CURE: A dataset for Clinical Understanding & Retrieval Evaluation","summary":"  Given the dominance of dense retrievers that do not generalize well beyond\ntheir training dataset distributions, domain-specific test sets are essential\nin evaluating retrieval. There are few test datasets for retrieval systems\nintended for use by healthcare providers in a point-of-care setting. To fill\nthis gap we have collaborated with medical professionals to create CURE, an\nad-hoc retrieval test dataset for passage ranking with 2000 queries spanning 10\nmedical domains with a monolingual (English) and two cross-lingual\n(French/Spanish -> English) conditions. In this paper, we describe how CURE was\nconstructed and provide baseline results to showcase its effectiveness as an\nevaluation tool. CURE is published with a Creative Commons Attribution Non\nCommercial 4.0 license and can be accessed on Hugging Face.\n","authors":["Nadia Sheikh","Anne-Laure Jousse","Daniel Buades Marcos","Akintunde Oladipo","Olivier Rousseau","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2412.06954v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11919v1","updated":"2024-12-16T16:03:25Z","published":"2024-12-16T16:03:25Z","title":"RetroLLM: Empowering Large Language Models to Retrieve Fine-grained\n  Evidence within Generation","summary":"  Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}.\n","authors":["Xiaoxi Li","Jiajie Jin","Yujia Zhou","Yongkang Wu","Zhonghua Li","Qi Ye","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2412.11919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11905v1","updated":"2024-12-16T15:52:17Z","published":"2024-12-16T15:52:17Z","title":"One for Dozens: Adaptive REcommendation for All Domains with\n  Counterfactual Augmentation","summary":"  Multi-domain recommendation (MDR) aims to enhance recommendation performance\nacross various domains. However, real-world recommender systems in online\nplatforms often need to handle dozens or even hundreds of domains, far\nexceeding the capabilities of traditional MDR algorithms, which typically focus\non fewer than five domains. Key challenges include a substantial increase in\nparameter count, high maintenance costs, and intricate knowledge transfer\npatterns across domains. Furthermore, minor domains often suffer from data\nsparsity, leading to inadequate training in classical methods. To address these\nissues, we propose Adaptive REcommendation for All Domains with counterfactual\naugmentation (AREAD). AREAD employs a hierarchical structure with a limited\nnumber of expert networks at several layers, to effectively capture domain\nknowledge at different granularities. To adaptively capture the knowledge\ntransfer pattern across domains, we generate and iteratively prune a\nhierarchical expert network selection mask for each domain during training.\nAdditionally, counterfactual assumptions are used to augment data in minor\ndomains, supporting their iterative mask pruning. Our experiments on two public\ndatasets, each encompassing over twenty domains, demonstrate AREAD's\neffectiveness, especially in data-sparse domains. Source code is available at\nhttps://github.com/Chrissie-Law/AREAD-Multi-Domain-Recommendation.\n","authors":["Huishi Luo","Yiwen Chen","Yiqing Wu","Fuzhen Zhuang","Deqing Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11905v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11864v1","updated":"2024-12-16T15:20:13Z","published":"2024-12-16T15:20:13Z","title":"Investigating Mixture of Experts in Dense Retrieval","summary":"  While Dense Retrieval Models (DRMs) have advanced Information Retrieval (IR),\none limitation of these neural models is their narrow generalizability and\nrobustness. To cope with this issue, one can leverage the Mixture-of-Experts\n(MoE) architecture. While previous IR studies have incorporated MoE\narchitectures within the Transformer layers of DRMs, our work investigates an\narchitecture that integrates a single MoE block (SB-MoE) after the output of\nthe final Transformer layer. Our empirical evaluation investigates how SB-MoE\ncompares, in terms of retrieval effectiveness, to standard fine-tuning. In\ndetail, we fine-tune three DRMs (TinyBERT, BERT, and Contriever) across four\nbenchmark collections with and without adding the MoE block. Moreover, since\nMoE showcases performance variations with respect to its parameters (i.e., the\nnumber of experts), we conduct additional experiments to investigate this\naspect further. The findings show the effectiveness of SB-MoE especially for\nDRMs with a low number of parameters (i.e., TinyBERT), as it consistently\noutperforms the fine-tuned underlying model on all four benchmarks. For DRMs\nwith a higher number of parameters (i.e., BERT and Contriever), SB-MoE requires\nlarger numbers of training samples to yield better retrieval performance.\n","authors":["Effrosyni Sokli","Pranav Kasela","Georgios Peikos","Gabriella Pasi"],"pdf_url":"https://arxiv.org/pdf/2412.11864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02729v2","updated":"2024-12-16T15:11:11Z","published":"2024-10-03T17:49:09Z","title":"Unified Multimodal Interleaved Document Representation for Retrieval","summary":"  Information Retrieval (IR) methods aim to identify documents relevant to a\nquery, which have been widely applied in various natural language tasks.\nHowever, existing approaches typically consider only the textual content within\ndocuments, overlooking the fact that documents can contain multiple modalities,\nincluding images and tables. Also, they often segment each long document into\nmultiple discrete passages for embedding, which prevents them from capturing\nthe overall document context and interactions between paragraphs. To address\nthese two challenges, we propose a method that holistically embeds documents\ninterleaved with multiple modalities by leveraging the capability of recent\nvision-language models that enable the processing and integration of text,\nimages, and tables into a unified format and representation. Moreover, to\nmitigate the information loss from segmenting documents into passages, instead\nof representing and retrieving passages individually, we further merge the\nrepresentations of segmented passages into one single document representation,\nwhile we additionally introduce a reranking strategy to decouple and identify\nthe relevant passage within the document if necessary. Then, through extensive\nexperiments on diverse IR scenarios considering both the textual and multimodal\nqueries, we show that our approach substantially outperforms relevant\nbaselines, thanks to the consideration of the multimodal information within\ndocuments.\n","authors":["Jaewoo Lee","Joonho Ko","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.02729v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.11846v1","updated":"2024-12-16T15:08:44Z","published":"2024-12-16T15:08:44Z","title":"SPGL: Enhancing Session-based Recommendation with Single Positive Graph\n  Learning","summary":"  Session-based recommendation seeks to forecast the next item a user will be\ninterested in, based on their interaction sequences. Due to limited interaction\ndata, session-based recommendation faces the challenge of limited data\navailability. Traditional methods enhance feature learning by constructing\ncomplex models to generate positive and negative samples. This paper proposes a\nsession-based recommendation model using Single Positive optimization loss and\nGraph Learning (SPGL) to deal with the problem of data sparsity, high model\ncomplexity and weak transferability. SPGL utilizes graph convolutional networks\nto generate global item representations and batch session representations,\neffectively capturing intrinsic relationships between items. The use of single\npositive optimization loss improves uniformity of item representations, thereby\nenhancing recommendation accuracy. In the intent extractor, SPGL considers the\nhop count of the adjacency matrix when constructing the directed global graph\nto fully integrate spatial information. It also takes into account the reverse\npositional information of items when constructing session representations to\nincorporate temporal information. Comparative experiments across three\nbenchmark datasets, Tmall, RetailRocket and Diginetica, demonstrate the model's\neffectiveness. The source code can be accessed on\nhttps://github.com/liang-tian-tian/SPGL .\n","authors":["Tiantian Liang","Zhe Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11846v1.pdf","comment":"ICONIP 2024"},{"id":"http://arxiv.org/abs/2412.11832v1","updated":"2024-12-16T14:55:57Z","published":"2024-12-16T14:55:57Z","title":"A Distributed Collaborative Retrieval Framework Excelling in All Queries\n  and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation","summary":"  Numerous retrieval models, including sparse, dense and llm-based methods,\nhave demonstrated remarkable performance in predicting the relevance between\nqueries and corpora. However, the preliminary effectiveness analysis\nexperiments indicate that these models fail to achieve satisfactory performance\non the majority of queries and corpora, revealing their effectiveness\nrestricted to specific scenarios. Thus, to tackle this problem, we propose a\nnovel Distributed Collaborative Retrieval Framework (DCRF), outperforming each\nsingle model across all queries and corpora. Specifically, the framework\nintegrates various retrieval models into a unified system and dynamically\nselects the optimal results for each user's query. It can easily aggregate any\nretrieval model and expand to any application scenarios, illustrating its\nflexibility and scalability.Moreover, to reduce maintenance and training costs,\nwe design four effective prompting strategies with large language models (LLMs)\nto evaluate the quality of ranks without reliance of labeled data. Extensive\nexperiments demonstrate that proposed framework, combined with 8 efficient\nretrieval models, can achieve performance comparable to effective listwise\nmethods like RankGPT and ListT5, while offering superior efficiency. Besides,\nDCRF surpasses all selected retrieval models on the most datasets, indicating\nthe effectiveness of our prompting strategies on rank-oriented automatic\nevaluation.\n","authors":["Tian-Yi Che","Xian-Ling Mao","Chun Xu","Cheng-Xin Xin","Heng-Da Xu","Jin-Yu Liu","Heyan Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11818v1","updated":"2024-12-16T14:35:32Z","published":"2024-12-16T14:35:32Z","title":"Leveraging User-Generated Metadata of Online Videos for Cover Song\n  Identification","summary":"  YouTube is a rich source of cover songs. Since the platform itself is\norganized in terms of videos rather than songs, the retrieval of covers is not\ntrivial. The field of cover song identification addresses this problem and\nprovides approaches that usually rely on audio content. However, including the\nuser-generated video metadata available on YouTube promises improved\nidentification results. In this paper, we propose a multi-modal approach for\ncover song identification on online video platforms. We combine the entity\nresolution models with audio-based approaches using a ranking model. Our\nfindings implicate that leveraging user-generated metadata can stabilize cover\nsong identification performance on YouTube.\n","authors":["Simon Hachmeier","Robert Jäschke"],"pdf_url":"https://arxiv.org/pdf/2412.11818v1.pdf","comment":"accepted for presentation at NLP for Music and Audio (NLP4MusA) 2024"},{"id":"http://arxiv.org/abs/2412.11787v1","updated":"2024-12-16T13:59:10Z","published":"2024-12-16T13:59:10Z","title":"A Method for Detecting Legal Article Competition for Korean Criminal Law\n  Using a Case-augmented Mention Graph","summary":"  As social systems become increasingly complex, legal articles are also\ngrowing more intricate, making it progressively harder for humans to identify\nany potential competitions among them, particularly when drafting new laws or\napplying existing laws. Despite this challenge, no method for detecting such\ncompetitions has been proposed so far. In this paper, we propose a new legal AI\ntask called Legal Article Competition Detection (LACD), which aims to identify\ncompeting articles within a given law. Our novel retrieval method, CAM-Re2,\noutperforms existing relevant methods, reducing false positives by 20.8% and\nfalse negatives by 8.3%, while achieving a 98.2% improvement in precision@5,\nfor the LACD task. We release our codes at\nhttps://github.com/asmath472/LACD-public.\n","authors":["Seonho An","Young Yik Rhim","Min-Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11787v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2412.11758v1","updated":"2024-12-16T13:22:34Z","published":"2024-12-16T13:22:34Z","title":"Establishing a Foundation for Tetun Text Ad-Hoc Retrieval: Indexing,\n  Stemming, Retrieval, and Ranking","summary":"  Searching for information on the internet and digital platforms to satisfy an\ninformation need requires effective retrieval solutions. However, such\nsolutions are not yet available for Tetun, making it challenging to find\nrelevant documents for text-based search queries in this language. To address\nthese challenges, this study investigates Tetun text retrieval with a focus on\nthe ad-hoc retrieval task. It begins by developing essential language resources\n-- including a list of stopwords, a stemmer, and a test collection -- which\nserve as foundational components for solutions tailored to Tetun text\nretrieval. Various strategies are then explored using both document titles and\ncontent to evaluate retrieval effectiveness. The results show that retrieving\ndocument titles, after removing hyphens and apostrophes without applying\nstemming, significantly improves retrieval performance compared to the\nbaseline. Efficiency increases by 31.37%, while effectiveness achieves an\naverage gain of 9.40% in MAP@10 and 30.35% in nDCG@10 with DFR BM25. Beyond the\ntop-10 cutoff point, Hiemstra LM demonstrates strong performance across various\nretrieval strategies and evaluation metrics. Contributions of this work include\nthe development of Labadain-Stopwords (a list of 160 Tetun stopwords),\nLabadain-Stemmer (a Tetun stemmer with three variants), and\nLabadain-Avaliad\\'or (a Tetun test collection containing 59 topics, 33,550\ndocuments, and 5,900 qrels).\n","authors":["Gabriel de Jesus","Sérgio Nunes"],"pdf_url":"https://arxiv.org/pdf/2412.11758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11747v1","updated":"2024-12-16T13:05:13Z","published":"2024-12-16T13:05:13Z","title":"Beyond Graph Convolution: Multimodal Recommendation with Topology-aware\n  MLPs","summary":"  Given the large volume of side information from different modalities,\nmultimodal recommender systems have become increasingly vital, as they exploit\nricher semantic information beyond user-item interactions. Recent works\nhighlight that leveraging Graph Convolutional Networks (GCNs) to explicitly\nmodel multimodal item-item relations can significantly enhance recommendation\nperformance. However, due to the inherent over-smoothing issue of GCNs,\nexisting models benefit only from shallow GCNs with limited representation\npower. This drawback is especially pronounced when facing complex and\nhigh-dimensional patterns such as multimodal data, as it requires\nlarge-capacity models to accommodate complicated correlations. To this end, in\nthis paper, we investigate bypassing GCNs when modeling multimodal item-item\nrelationship. More specifically, we propose a Topology-aware Multi-Layer\nPerceptron (TMLP), which uses MLPs instead of GCNs to model the relationships\nbetween items. TMLP enhances MLPs with topological pruning to denoise item-item\nrelations and intra (inter)-modality learning to integrate higher-order\nmodality correlations. Extensive experiments on three real-world datasets\nverify TMLP's superiority over nine baselines. We also find that by discarding\nthe internal message passing in GCNs, which is sensitive to node connections,\nTMLP achieves significant improvements in both training efficiency and\nrobustness against existing models.\n","authors":["Junjie Huang","Jiarui Qin","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11747v1.pdf","comment":"AAAI 2025. 11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.11729v1","updated":"2024-12-16T12:53:06Z","published":"2024-12-16T12:53:06Z","title":"STAIR: Manipulating Collaborative and Multimodal Information for\n  E-Commerce Recommendation","summary":"  While the mining of modalities is the focus of most multimodal recommendation\nmethods, we believe that how to fully utilize both collaborative and multimodal\ninformation is pivotal in e-commerce scenarios where, as clarified in this\nwork, the user behaviors are rarely determined entirely by multimodal features.\nIn order to combine the two distinct types of information, some additional\nchallenges are encountered: 1) Modality erasure: Vanilla graph convolution,\nwhich proves rather useful in collaborative filtering, however erases\nmultimodal information; 2) Modality forgetting: Multimodal information tends to\nbe gradually forgotten as the recommendation loss essentially facilitates the\nlearning of collaborative information. To this end, we propose a novel approach\nnamed STAIR, which employs a novel STepwise grAph convolution to enable a\nco-existence of collaborative and multimodal Information in e-commerce\nRecommendation. Besides, it starts with the raw multimodal features as an\ninitialization, and the forgetting problem can be significantly alleviated\nthrough constrained embedding updates. As a result, STAIR achieves\nstate-of-the-art recommendation performance on three public e-commerce datasets\nwith minimal computational and memory costs. Our code is available at\nhttps://github.com/yhhe2004/STAIR.\n","authors":["Cong Xu","Yunhang He","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11729v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2409.10907v2","updated":"2024-12-16T12:42:07Z","published":"2024-09-17T05:54:25Z","title":"Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised\n  Keyphrase Extraction","summary":"  This paper proposes Attention-Seeker, an unsupervised keyphrase extraction\nmethod that leverages self-attention maps from a Large Language Model to\nestimate the importance of candidate phrases. Our approach identifies specific\ncomponents - such as layers, heads, and attention vectors - where the model\npays significant attention to the key topics of the text. The attention weights\nprovided by these components are then used to score the candidate phrases.\nUnlike previous models that require manual tuning of parameters (e.g.,\nselection of heads, prompts, hyperparameters), Attention-Seeker dynamically\nadapts to the input text without any manual adjustments, enhancing its\npractical applicability. We evaluate Attention-Seeker on four publicly\navailable datasets: Inspec, SemEval2010, SemEval2017, and Krapivin. Our results\ndemonstrate that, even without parameter tuning, Attention-Seeker outperforms\nmost baseline models, achieving state-of-the-art performance on three out of\nfour datasets, particularly excelling in extracting keyphrases from long\ndocuments.\n","authors":["Erwin D. López Z.","Cheng Tang","Atsushi Shimada"],"pdf_url":"https://arxiv.org/pdf/2409.10907v2.pdf","comment":"This version has been accepted for presentation at COLING 2025, and\n  all peer-reviewed changes have been incorporated"},{"id":"http://arxiv.org/abs/2410.06618v2","updated":"2024-12-16T09:52:32Z","published":"2024-10-09T07:14:49Z","title":"Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N\n  1-to-1 Relationships for Text-Video Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11589v1","updated":"2024-12-16T09:20:29Z","published":"2024-12-16T09:20:29Z","title":"Future Sight and Tough Fights: Revolutionizing Sequential Recommendation\n  with FENRec","summary":"  Sequential recommendation (SR) systems predict user preferences by analyzing\ntime-ordered interaction sequences. A common challenge for SR is data sparsity,\nas users typically interact with only a limited number of items. While\ncontrastive learning has been employed in previous approaches to address the\nchallenges, these methods often adopt binary labels, missing finer patterns and\noverlooking detailed information in subsequent behaviors of users.\nAdditionally, they rely on random sampling to select negatives in contrastive\nlearning, which may not yield sufficiently hard negatives during later training\nstages. In this paper, we propose Future data utilization with Enduring\nNegatives for contrastive learning in sequential Recommendation (FENRec). Our\napproach aims to leverage future data with time-dependent soft labels and\ngenerate enduring hard negatives from existing data, thereby enhancing the\neffectiveness in tackling data sparsity. Experiment results demonstrate our\nstate-of-the-art performance across four benchmark datasets, with an average\nimprovement of 6.16\\% across all metrics.\n","authors":["Yu-Hsuan Huang","Ling Lo","Hongxia Xie","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.11589v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11557v1","updated":"2024-12-16T08:42:43Z","published":"2024-12-16T08:42:43Z","title":"Enhancing Healthcare Recommendation Systems with a Multimodal LLMs-based\n  MOE Architecture","summary":"  With the increasing availability of multimodal data, many fields urgently\nrequire advanced architectures capable of effectively integrating these diverse\ndata sources to address specific problems. This study proposes a hybrid\nrecommendation model that combines the Mixture of Experts (MOE) framework with\nlarge language models to enhance the performance of recommendation systems in\nthe healthcare domain. We built a small dataset for recommending healthy food\nbased on patient descriptions and evaluated the model's performance on several\nkey metrics, including Precision, Recall, NDCG, and MAP@5. The experimental\nresults show that the hybrid model outperforms the baseline models, which use\nMOE or large language models individually, in terms of both accuracy and\npersonalized recommendation effectiveness. The paper finds image data provided\nrelatively limited improvement in the performance of the personalized\nrecommendation system, particularly in addressing the cold start problem. Then,\nthe issue of reclassification of images also affected the recommendation\nresults, especially when dealing with low-quality images or changes in the\nappearance of items, leading to suboptimal performance. The findings provide\nvaluable insights into the development of powerful, scalable, and\nhigh-performance recommendation systems, advancing the application of\npersonalized recommendation technologies in real-world domains such as\nhealthcare.\n","authors":["Jingyu Xu","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11557v1.pdf","comment":"10 page, accpted by Conf-SMPL conference"},{"id":"http://arxiv.org/abs/2412.00430v4","updated":"2024-12-16T07:46:03Z","published":"2024-11-30T10:56:30Z","title":"Predictive Models in Sequential Recommendations: Bridging Performance\n  Laws with Data Quality Insights","summary":"  Sequential Recommendation (SR) plays a critical role in predicting users'\nsequential preferences. Despite its growing prominence in various industries,\nthe increasing scale of SR models incurs substantial computational costs and\nunpredictability, challenging developers to manage resources efficiently. Under\nthis predicament, Scaling Laws have achieved significant success by examining\nthe loss as models scale up. However, there remains a disparity between loss\nand model performance, which is of greater concern in practical applications.\nMoreover, as data continues to expand, it incorporates repetitive and\ninefficient data. In response, we introduce the Performance Law for SR models,\nwhich aims to theoretically investigate and model the relationship between\nmodel performance and data quality. Specifically, we first fit the HR and NDCG\nmetrics to transformer-based SR models. Subsequently, we propose Approximate\nEntropy (ApEn) to assess data quality, presenting a more nuanced approach\ncompared to traditional data quantity metrics. Our method enables accurate\npredictions across various dataset scales and model sizes, demonstrating a\nstrong correlation in large SR models and offering insights into achieving\noptimal performance for any given model configuration.\n","authors":["Tingjia Shen","Hao Wang","Chuhan Wu","Jin Yao Chin","Wei Guo","Yong Liu","Huifeng Guo","Defu Lian","Ruiming Tang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.00430v4.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.11431v1","updated":"2024-12-16T04:03:58Z","published":"2024-12-16T04:03:58Z","title":"Optimized Quran Passage Retrieval Using an Expanded QA Dataset and\n  Fine-Tuned Language Models","summary":"  Understanding the deep meanings of the Qur'an and bridging the language gap\nbetween modern standard Arabic and classical Arabic is essential to improve the\nquestion-and-answer system for the Holy Qur'an. The Qur'an QA 2023 shared task\ndataset had a limited number of questions with weak model retrieval. To address\nthis challenge, this work updated the original dataset and improved the model\naccuracy. The original dataset, which contains 251 questions, was reviewed and\nexpanded to 629 questions with question diversification and reformulation,\nleading to a comprehensive set of 1895 categorized into single-answer,\nmulti-answer, and zero-answer types. Extensive experiments fine-tuned\ntransformer models, including AraBERT, RoBERTa, CAMeLBERT, AraELECTRA, and\nBERT. The best model, AraBERT-base, achieved a MAP@10 of 0.36 and MRR of 0.59,\nrepresenting improvements of 63% and 59%, respectively, compared to the\nbaseline scores (MAP@10: 0.22, MRR: 0.37). Additionally, the dataset expansion\nled to improvements in handling \"no answer\" cases, with the proposed approach\nachieving a 75% success rate for such instances, compared to the baseline's\n25%. These results demonstrate the effect of dataset improvement and model\narchitecture optimization in increasing the performance of QA systems for the\nHoly Qur'an, with higher accuracy, recall, and precision.\n","authors":["Mohamed Basem","Islam Oshallah","Baraa Hikal","Ali Hamdi","Ammar Mohamed"],"pdf_url":"https://arxiv.org/pdf/2412.11431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12330v1","updated":"2024-12-16T20:00:51Z","published":"2024-12-16T20:00:51Z","title":"Searching Personal Collections","summary":"  This article describes the history of information retrieval on personal\ndocument collections.\n","authors":["Michael Bendersky","Donald Metzler","Marc Najork","Xuanhui Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12322v1","updated":"2024-12-16T19:40:26Z","published":"2024-12-16T19:40:26Z","title":"RAG Playground: A Framework for Systematic Evaluation of Retrieval\n  Strategies and Prompt Engineering in RAG Systems","summary":"  We present RAG Playground, an open-source framework for systematic evaluation\nof Retrieval-Augmented Generation (RAG) systems. The framework implements and\ncompares three retrieval approaches: naive vector search, reranking, and hybrid\nvector-keyword search, combined with ReAct agents using different prompting\nstrategies. We introduce a comprehensive evaluation framework with novel\nmetrics and provide empirical results comparing different language models\n(Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our\nexperiments demonstrate significant performance improvements through hybrid\nsearch methods and structured self-evaluation prompting, achieving up to 72.7%\npass rate on our multi-metric evaluation framework. The results also highlight\nthe importance of prompt engineering in RAG systems, with our custom-prompted\nagents showing consistent improvements in retrieval accuracy and response\nquality.\n","authors":["Ioannis Papadimitriou","Ilias Gialampoukidis","Stefanos Vrochidis"," Ioannis"," Kompatsiaris"],"pdf_url":"https://arxiv.org/pdf/2412.12322v1.pdf","comment":"Work In Progress"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2412.12098v1","updated":"2024-12-16T18:59:53Z","published":"2024-12-16T18:59:53Z","title":"MaxInfoRL: Boosting exploration in reinforcement learning through\n  information gain maximization","summary":"  Reinforcement learning (RL) algorithms aim to balance exploiting the current\nbest strategy with exploring new options that could lead to higher rewards.\nMost common RL algorithms use undirected exploration, i.e., select random\nsequences of actions. Exploration can also be directed using intrinsic rewards,\nsuch as curiosity or model epistemic uncertainty. However, effectively\nbalancing task and intrinsic rewards is challenging and often task-dependent.\nIn this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and\nextrinsic exploration. MaxInfoRL steers exploration towards informative\ntransitions, by maximizing intrinsic rewards such as the information gain about\nthe underlying task. When combined with Boltzmann exploration, this approach\nnaturally trades off maximization of the value function with that of the\nentropy over states, rewards, and actions. We show that our approach achieves\nsublinear regret in the simplified setting of multi-armed bandits. We then\napply this general formulation to a variety of off-policy model-free RL methods\nfor continuous state-action spaces, yielding novel algorithms that achieve\nsuperior performance across hard exploration problems and complex scenarios\nsuch as visual control tasks.\n","authors":["Bhavya Sukhija","Stelian Coros","Andreas Krause","Pieter Abbeel","Carmelo Sferrazza"],"pdf_url":"https://arxiv.org/pdf/2412.12098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12094v1","updated":"2024-12-16T18:58:57Z","published":"2024-12-16T18:58:57Z","title":"SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator","summary":"  Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.\n","authors":["Guoxuan Chen","Han Shi","Jiawei Li","Yihang Gao","Xiaozhe Ren","Yimeng Chen","Xin Jiang","Zhenguo Li","Weiyang Liu","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12092v1","updated":"2024-12-16T18:58:28Z","published":"2024-12-16T18:58:28Z","title":"No More Tuning: Prioritized Multi-Task Learning with Lagrangian\n  Differential Multiplier Methods","summary":"  Given the ubiquity of multi-task in practical systems, Multi-Task Learning\n(MTL) has found widespread application across diverse domains. In real-world\nscenarios, these tasks often have different priorities. For instance, In web\nsearch, relevance is often prioritized over other metrics, such as\nclick-through rates or user engagement. Existing frameworks pay insufficient\nattention to the prioritization among different tasks, which typically adjust\ntask-specific loss function weights to differentiate task priorities. However,\nthis approach encounters challenges as the number of tasks grows, leading to\nexponential increases in hyper-parameter tuning complexity. Furthermore, the\nsimultaneous optimization of multiple objectives can negatively impact the\nperformance of high-priority tasks due to interference from lower-priority\ntasks.\n  In this paper, we introduce a novel multi-task learning framework employing\nLagrangian Differential Multiplier Methods for step-wise multi-task\noptimization. It is designed to boost the performance of high-priority tasks\nwithout interference from other tasks. Its primary advantage lies in its\nability to automatically optimize multiple objectives without requiring\nbalancing hyper-parameters for different tasks, thereby eliminating the need\nfor manual tuning. Additionally, we provide theoretical analysis demonstrating\nthat our method ensures optimization guarantees, enhancing the reliability of\nthe process. We demonstrate its effectiveness through experiments on multiple\npublic datasets and its application in Taobao search, a large-scale industrial\nsearch ranking system, resulting in significant improvements across various\nbusiness metrics.\n","authors":["Zhengxing Cheng","Yuheng Huang","Zhixuan Zhang","Dan Ou","Qingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12092v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12089v1","updated":"2024-12-16T18:56:24Z","published":"2024-12-16T18:56:24Z","title":"Stabilizing Reinforcement Learning in Differentiable Multiphysics\n  Simulation","summary":"  Recent advances in GPU-based parallel simulation have enabled practitioners\nto collect large amounts of data and train complex control policies using deep\nreinforcement learning (RL), on commodity GPUs. However, such successes for RL\nin robotics have been limited to tasks sufficiently simulated by fast\nrigid-body dynamics. Simulation techniques for soft bodies are comparatively\nseveral orders of magnitude slower, thereby limiting the use of RL due to\nsample complexity requirements. To address this challenge, this paper presents\nboth a novel RL algorithm and a simulation platform to enable scaling RL on\ntasks involving rigid bodies and deformables. We introduce Soft Analytic Policy\nOptimization (SAPO), a maximum entropy first-order model-based actor-critic RL\nalgorithm, which uses first-order analytic gradients from differentiable\nsimulation to train a stochastic actor to maximize expected return and entropy.\nAlongside our approach, we develop Rewarped, a parallel differentiable\nmultiphysics simulation platform that supports simulating various materials\nbeyond rigid bodies. We re-implement challenging manipulation and locomotion\ntasks in Rewarped, and show that SAPO outperforms baselines over a range of\ntasks that involve interaction between rigid bodies, articulations, and\ndeformables.\n","authors":["Eliot Xing","Vernon Luk","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2412.12089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12074v1","updated":"2024-12-16T18:46:43Z","published":"2024-12-16T18:46:43Z","title":"Extrapolating Jet Radiation with Autoregressive Transformers","summary":"  Generative networks are an exciting tool for fast LHC event generation.\nUsually, they are used to generate configurations with a fixed number of\nparticles. Autoregressive transformers allow us to generate events with\nvariable numbers of particles, very much in line with the physics of QCD jet\nradiation. We show how they can learn a factorized likelihood for jet radiation\nand extrapolate in terms of the number of generated jets. For this\nextrapolation, bootstrapping training data and training with modifications of\nthe likelihood loss can be used.\n","authors":["Anja Butter","François Charton","Javier Mariño Villadamigo","Ayodele Ore","Tilman Plehn","Jonas Spinner"],"pdf_url":"https://arxiv.org/pdf/2412.12074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10799v4","updated":"2024-12-16T18:31:27Z","published":"2024-03-16T04:12:50Z","title":"Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment","summary":"  Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82\\% in\naccuracy across seven downstream tasks when pruning LLaMA-7B by 50\\%.\n","authors":["Jun Liu","Zhenglun Kong","Pu Zhao","Changdi Yang","Hao Tang","Xuan Shen","Geng Yuan","Wei Niu","Wenbin Zhang","Xue Lin","Dong Huang","Yanzhi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10799v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08039v2","updated":"2024-12-16T18:28:19Z","published":"2024-06-12T09:41:12Z","title":"Differentially Private Prototypes for Imbalanced Transfer Learning","summary":"  Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\n\\textit{pure DP}. We additionally show that privacy-utility trade-offs can be\nfurther improved when leveraging the public data beyond pre-training of the\nencoder: in particular, we can privately sample our DP prototypes from the\npublicly available data points used to train the encoder. Our experimental\nevaluation with four state-of-the-art encoders, four vision datasets, and under\ndifferent data and imbalancedness regimes demonstrate DPPL's high performance\nunder strong privacy guarantees in challenging private learning setups\n","authors":["Dariush Wahdany","Matthew Jagielski","Adam Dziedzic","Franziska Boenisch"],"pdf_url":"https://arxiv.org/pdf/2406.08039v2.pdf","comment":"To be published at the 39th Annual AAAI Conference on Artificial\n  Intelligence, Philadelphia, 2025"},{"id":"http://arxiv.org/abs/2410.12172v2","updated":"2024-12-16T18:25:35Z","published":"2024-10-16T02:31:31Z","title":"The State of Robot Motion Generation","summary":"  This paper reviews the large spectrum of methods for generating robot motion\nproposed over the 50 years of robotics research culminating in recent\ndevelopments. It crosses the boundaries of methodologies, typically not\nsurveyed together, from those that operate over explicit models to those that\nlearn implicit ones. The paper discusses the current state-of-the-art as well\nas properties of varying methodologies, highlighting opportunities for\nintegration.\n","authors":["Kostas E. Bekris","Joe Doerr","Patrick Meng","Sumanth Tangirala"],"pdf_url":"https://arxiv.org/pdf/2410.12172v2.pdf","comment":"Presented at the International Symposium of Robotics Research (ISRR),\n  2024. Website:\n  https://pracsys.cs.rutgers.edu/papers/the-state-of-robot-motion-generation/"},{"id":"http://arxiv.org/abs/2412.12049v1","updated":"2024-12-16T18:18:47Z","published":"2024-12-16T18:18:47Z","title":"Bilevel Learning with Inexact Stochastic Gradients","summary":"  Bilevel learning has gained prominence in machine learning, inverse problems,\nand imaging applications, including hyperparameter optimization, learning\ndata-adaptive regularizers, and optimizing forward operators. The large-scale\nnature of these problems has led to the development of inexact and\ncomputationally efficient methods. Existing adaptive methods predominantly rely\non deterministic formulations, while stochastic approaches often adopt a\ndoubly-stochastic framework with impractical variance assumptions, enforces a\nfixed number of lower-level iterations, and requires extensive tuning. In this\nwork, we focus on bilevel learning with strongly convex lower-level problems\nand a nonconvex sum-of-functions in the upper-level. Stochasticity arises from\ndata sampling in the upper-level which leads to inexact stochastic\nhypergradients. We establish their connection to state-of-the-art stochastic\noptimization theory for nonconvex objectives. Furthermore, we prove the\nconvergence of inexact stochastic bilevel optimization under mild assumptions.\nOur empirical results highlight significant speed-ups and improved\ngeneralization in imaging tasks such as image denoising and deblurring in\ncomparison with adaptive deterministic bilevel methods.\n","authors":["Mohammad Sadegh Salehi","Subhadip Mukherjee","Lindon Roberts","Matthias J. Ehrhardt"],"pdf_url":"https://arxiv.org/pdf/2412.12049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16368v2","updated":"2024-12-16T18:11:07Z","published":"2024-05-25T22:37:43Z","title":"Qsco: A Quantum Scoring Module for Open-set Supervised Anomaly Detection","summary":"  Open set anomaly detection (OSAD) is a crucial task that aims to identify\nabnormal patterns or behaviors in data sets, especially when the anomalies\nobserved during training do not represent all possible classes of anomalies.\nThe recent advances in quantum computing in handling complex data structures\nand improving machine learning models herald a paradigm shift in anomaly\ndetection methodologies. This study proposes a Quantum Scoring Module (Qsco),\nembedding quantum variational circuits into neural networks to enhance the\nmodel's processing capabilities in handling uncertainty and unlabeled data.\nExtensive experiments conducted across eight real-world anomaly detection\ndatasets demonstrate our model's superior performance in detecting anomalies\nacross varied settings and reveal that integrating quantum simulators does not\nresult in prohibitive time complexities. Our study validates the feasibility of\nquantum-enhanced anomaly detection methods in practical applications.\n","authors":["Yifeng Peng","Xinyi Li","Zhiding Liang","Ying Wang"],"pdf_url":"https://arxiv.org/pdf/2405.16368v2.pdf","comment":"The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.12038v1","updated":"2024-12-16T18:03:57Z","published":"2024-12-16T18:03:57Z","title":"LLMs for Cold-Start Cutting Plane Separator Configuration","summary":"  Mixed integer linear programming (MILP) solvers ship with a staggering number\nof parameters that are challenging to select a priori for all but expert\noptimization users, but can have an outsized impact on the performance of the\nMILP solver. Existing machine learning (ML) approaches to configure solvers\nrequire training ML models by solving thousands of related MILP instances,\ngeneralize poorly to new problem sizes, and often require implementing complex\nML pipelines and custom solver interfaces that can be difficult to integrate\ninto existing optimization workflows. In this paper, we introduce a new\nLLM-based framework to configure which cutting plane separators to use for a\ngiven MILP problem with little to no training data based on characteristics of\nthe instance, such as a natural language description of the problem and the\nassociated LaTeX formulation. We augment these LLMs with descriptions of\ncutting plane separators available in a given solver, grounded by summarizing\nthe existing research literature on separators. While individual solver\nconfigurations have a large variance in performance, we present a novel\nensembling strategy that clusters and aggregates configurations to create a\nsmall portfolio of high-performing configurations. Our LLM-based methodology\nrequires no custom solver interface, can find a high-performing configuration\nby solving only a small number of MILPs, and can generate the configuration\nwith simple API calls that run in under a second. Numerical results show our\napproach is competitive with existing configuration approaches on a suite of\nclassic combinatorial optimization problems and real-world datasets with only a\nfraction of the training data and computation time.\n","authors":["Connor Lawless","Yingxi Li","Anders Wikum","Madeleine Udell","Ellen Vitercik"],"pdf_url":"https://arxiv.org/pdf/2412.12038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12036v1","updated":"2024-12-16T18:03:23Z","published":"2024-12-16T18:03:23Z","title":"LeARN: Learnable and Adaptive Representations for Nonlinear Dynamics in\n  System Identification","summary":"  System identification, the process of deriving mathematical models of\ndynamical systems from observed input-output data, has undergone a paradigm\nshift with the advent of learning-based methods. Addressing the intricate\nchallenges of data-driven discovery in nonlinear dynamical systems, these\nmethods have garnered significant attention. Among them, Sparse Identification\nof Nonlinear Dynamics (SINDy) has emerged as a transformative approach,\ndistilling complex dynamical behaviors into interpretable linear combinations\nof basis functions. However, SINDy relies on domain-specific expertise to\nconstruct its foundational \"library\" of basis functions, which limits its\nadaptability and universality. In this work, we introduce a nonlinear system\nidentification framework called LeARN that transcends the need for prior domain\nknowledge by learning the library of basis functions directly from data. To\nenhance adaptability to evolving system dynamics under varying noise\nconditions, we employ a novel meta-learning-based system identification\napproach that uses a lightweight deep neural network (DNN) to dynamically\nrefine these basis functions. This not only captures intricate system behaviors\nbut also adapts seamlessly to new dynamical regimes. We validate our framework\non the Neural Fly dataset, showcasing its robust adaptation and generalization\ncapabilities. Despite its simplicity, our LeARN achieves competitive dynamical\nerror performance compared to SINDy. This work presents a step toward the\nautonomous discovery of dynamical systems, paving the way for a future where\nmachine learning uncovers the governing principles of complex systems without\nrequiring extensive domain-specific interventions.\n","authors":["Arunabh Singh","Joyjit Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2412.12036v1.pdf","comment":"This work has been submitted to the 7th Annual Learning for Dynamics\n  & Control Conference for review"},{"id":"http://arxiv.org/abs/2412.12034v1","updated":"2024-12-16T18:01:40Z","published":"2024-12-16T18:01:40Z","title":"Thermodynamics-informed graph neural networks for real-time simulation\n  of digital human twins","summary":"  The growing importance of real-time simulation in the medical field has\nexposed the limitations and bottlenecks inherent in the digital representation\nof complex biological systems. This paper presents a novel methodology aimed at\nadvancing current lines of research in soft tissue simulation. The proposed\napproach introduces a hybrid model that integrates the geometric bias of graph\nneural networks with the physical bias derived from the imposition of a\nmetriplectic structure as soft and hard constrains in the architecture, being\nable to simulate hepatic tissue with dissipative properties. This approach\nprovides an efficient solution capable of generating predictions at high\nfeedback rate while maintaining a remarkable generalization ability for\npreviously unseen anatomies. This makes these features particularly relevant in\nthe context of precision medicine and haptic rendering.\n  Based on the adopted methodologies, we propose a model that predicts human\nliver responses to traction and compression loads in as little as 7.3\nmilliseconds for optimized configurations and as fast as 1.65 milliseconds in\nthe most efficient cases, all in the forward pass. The model achieves relative\nposition errors below 0.15\\%, with stress tensor and velocity estimations\nmaintaining relative errors under 7\\%. This demonstrates the robustness of the\napproach developed, which is capable of handling diverse load states and\nanatomies effectively. This work highlights the feasibility of integrating\nreal-time simulation with patient-specific geometries through deep learning,\npaving the way for more robust digital human twins in medical applications.\n","authors":["Lucas Tesán","David González","Pedro Martins","Elías Cueto"],"pdf_url":"https://arxiv.org/pdf/2412.12034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00986v3","updated":"2024-12-16T18:00:10Z","published":"2024-03-01T21:16:29Z","title":"Merging Text Transformer Models from Different Initializations","summary":"  Recent work on permutation-based model merging has shown impressive low- or\nzero-barrier mode connectivity between models from completely different\ninitializations. However, this line of work has not yet extended to the\nTransformer architecture, despite its dominant popularity in the language\ndomain. Therefore, in this work, we investigate the extent to which separate\nTransformer minima learn similar features, and propose a model merging\ntechnique to investigate the relationship between these minima in the loss\nlandscape. The specifics of the architecture, like its residual connections,\nmulti-headed attention, and discrete, sequential input, require specific\ninterventions in order to compute model permutations that remain within the\nsame functional equivalence class. In merging these models with our method, we\nconsistently find lower loss barriers between minima compared to model\naveraging, across models trained on a masked-language modeling task or\nfine-tuned on a language understanding benchmark. Our results show that the\nminima of these models are less sharp and isolated than previously understood,\nand provide a basis for future work on merging separately trained Transformer\nmodels.\n","authors":["Neha Verma","Maha Elbayad"],"pdf_url":"https://arxiv.org/pdf/2403.00986v3.pdf","comment":"TMLR, November 2024"},{"id":"http://arxiv.org/abs/2412.12030v1","updated":"2024-12-16T17:55:55Z","published":"2024-12-16T17:55:55Z","title":"Memory-Reduced Meta-Learning with Guaranteed Convergence","summary":"  The optimization-based meta-learning approach is gaining increased traction\nbecause of its unique ability to quickly adapt to a new task using only small\namounts of data. However, existing optimization-based meta-learning approaches,\nsuch as MAML, ANIL and their variants, generally employ backpropagation for\nupper-level gradient estimation, which requires using historical lower-level\nparameters/gradients and thus increases computational and memory overhead in\neach iteration. In this paper, we propose a meta-learning algorithm that can\navoid using historical parameters/gradients and significantly reduce memory\ncosts in each iteration compared to existing optimization-based meta-learning\napproaches. In addition to memory reduction, we prove that our proposed\nalgorithm converges sublinearly with the iteration number of upper-level\noptimization, and the convergence error decays sublinearly with the batch size\nof sampled tasks. In the specific case in terms of deterministic meta-learning,\nwe also prove that our proposed algorithm converges to an exact solution.\nMoreover, we quantify that the computational complexity of the algorithm is on\nthe order of $\\mathcal{O}(\\epsilon^{-1})$, which matches existing convergence\nresults on meta-learning even without using any historical\nparameters/gradients. Experimental results on meta-learning benchmarks confirm\nthe efficacy of our proposed algorithm.\n","authors":["Honglin Yang","Ji Ma","Xiao Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12030v1.pdf","comment":"18 pages, 2 figures; Accepted by the 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI)"},{"id":"http://arxiv.org/abs/2412.12024v1","updated":"2024-12-16T17:51:09Z","published":"2024-12-16T17:51:09Z","title":"Learning to Navigate in Mazes with Novel Layouts using Abstract Top-down\n  Maps","summary":"  Learning navigation capabilities in different environments has long been one\nof the major challenges in decision-making. In this work, we focus on zero-shot\nnavigation ability using given abstract $2$-D top-down maps. Like human\nnavigation by reading a paper map, the agent reads the map as an image when\nnavigating in a novel layout, after learning to navigate on a set of training\nmaps. We propose a model-based reinforcement learning approach for this\nmulti-task learning problem, where it jointly learns a hypermodel that takes\ntop-down maps as input and predicts the weights of the transition network. We\nuse the DeepMind Lab environment and customize layouts using generated maps.\nOur method can adapt better to novel environments in zero-shot and is more\nrobust to noise.\n","authors":["Linfeng Zhao","Lawson L. S. Wong"],"pdf_url":"https://arxiv.org/pdf/2412.12024v1.pdf","comment":"Published at Reinforcement Learning Conference (RLC) 2024. Website:\n  http://lfzhao.com/map-nav/"},{"id":"http://arxiv.org/abs/2412.12016v1","updated":"2024-12-16T17:41:33Z","published":"2024-12-16T17:41:33Z","title":"Deep-learning-based identification of individual motion characteristics\n  from upper-limb trajectories towards disorder stage evaluation","summary":"  The identification of individual movement characteristics sets the foundation\nfor the assessment of personal rehabilitation progress and can provide\ndiagnostic information on levels and stages of movement disorders. This work\npresents a preliminary study for differentiating individual motion patterns\nusing a dataset of 3D upper-limb transport trajectories measured in task-space.\nIdentifying individuals by deep time series learning can be a key step to\nabstracting individual motion properties. In this study, a classification\naccuracy of about 95% is reached for a subset of nine, and about 78% for the\nfull set of 31 individuals. This provides insights into the separability of\npatient attributes by exerting a simple standardized task to be transferred to\nportable systems.\n","authors":["Tim Sziburis","Susanne Blex","Tobias Glasmachers","Ioannis Iossifidis"],"pdf_url":"https://arxiv.org/pdf/2412.12016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12014v1","updated":"2024-12-16T17:40:05Z","published":"2024-12-16T17:40:05Z","title":"Generalization Analysis for Deep Contrastive Representation Learning","summary":"  In this paper, we present generalization bounds for the unsupervised risk in\nthe Deep Contrastive Representation Learning framework, which employs deep\nneural networks as representation functions. We approach this problem from two\nangles. On the one hand, we derive a parameter-counting bound that scales with\nthe overall size of the neural networks. On the other hand, we provide a\nnorm-based bound that scales with the norms of neural networks' weight\nmatrices. Ignoring logarithmic factors, the bounds are independent of $k$, the\nsize of the tuples provided for contrastive learning. To the best of our\nknowledge, this property is only shared by one other work, which employed a\ndifferent proof strategy and suffers from very strong exponential dependence on\nthe depth of the network which is due to a use of the peeling technique. Our\nresults circumvent this by leveraging powerful results on covering numbers with\nrespect to uniform norms over samples. In addition, we utilize loss\naugmentation techniques to further reduce the dependency on matrix norms and\nthe implicit dependence on network depth. In fact, our techniques allow us to\nproduce many bounds for the contrastive learning setting with similar\narchitectural dependencies as in the study of the sample complexity of ordinary\nloss functions, thereby bridging the gap between the learning theories of\ncontrastive learning and DNNs.\n","authors":["Nong Minh Hieu","Antoine Ledent","Yunwen Lei","Cheng Yeaw Ku"],"pdf_url":"https://arxiv.org/pdf/2412.12014v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2404.14397v2","updated":"2024-12-16T17:34:22Z","published":"2024-04-22T17:56:26Z","title":"RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?","summary":"  Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment.\n","authors":["Adrian de Wynter","Ishaan Watts","Tua Wongsangaroonsri","Minghui Zhang","Noura Farra","Nektar Ege Altıntoprak","Lena Baur","Samantha Claudet","Pavel Gajdusek","Can Gören","Qilong Gu","Anna Kaminska","Tomasz Kaminski","Ruby Kuo","Akiko Kyuba","Jongho Lee","Kartik Mathur","Petter Merok","Ivana Milovanović","Nani Paananen","Vesa-Matti Paananen","Anna Pavlenko","Bruno Pereira Vidal","Luciano Strika","Yueh Tsao","Davide Turcato","Oleksandr Vakhno","Judit Velcsov","Anna Vickers","Stéphanie Visser","Herdyan Widarmanto","Andrey Zaikin","Si-Qing Chen"],"pdf_url":"https://arxiv.org/pdf/2404.14397v2.pdf","comment":"AAAI 2025--camera ready + extended abstract"},{"id":"http://arxiv.org/abs/2412.12004v1","updated":"2024-12-16T17:32:11Z","published":"2024-12-16T17:32:11Z","title":"The Open Source Advantage in Large Language Models (LLMs)","summary":"  Large language models (LLMs) mark a key shift in natural language processing\n(NLP), having advanced text generation, translation, and domain-specific\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\nextensive computational resources, lead with state-of-the-art performance\ntoday. However, they face criticism for their \"black box\" nature and for\nlimiting accessibility in a manner that hinders reproducibility and equitable\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\nprioritize democratization through community-driven development and\ncomputational efficiency. These models have significantly reduced performance\ngaps, particularly in linguistic diversity and domain-specific applications,\nwhile providing accessible tools for global researchers and developers.\nNotably, both paradigms rely on foundational architectural innovations, such as\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\nby scaling effectively, while open-source models adapt to real-world\napplications in underrepresented languages and domains. Techniques like\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\nmodels to achieve competitive results despite limited resources. To be sure,\nthe tension between closed-source and open-source approaches underscores a\nbroader debate on transparency versus proprietary control in AI. Ethical\nconsiderations further highlight this divide. Closed-source systems restrict\nexternal scrutiny, while open-source models promote reproducibility and\ncollaboration but lack standardized auditing documentation frameworks to\nmitigate biases. Hybrid approaches that leverage the strengths of both\nparadigms are likely to shape the future of LLM innovation, ensuring\naccessibility, competitive technical performance, and ethical deployment.\n","authors":["Jiya Manchanda","Laura Boettcher","Matheus Westphalen","Jasser Jasser"],"pdf_url":"https://arxiv.org/pdf/2412.12004v1.pdf","comment":"7 pages, 0 figures"},{"id":"http://arxiv.org/abs/2412.11988v1","updated":"2024-12-16T17:11:48Z","published":"2024-12-16T17:11:48Z","title":"SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with\n  a GAN-Inspired Approach to Synthetic Dataset Generation","summary":"  Consider the problem: ``If one man and one woman can produce one child in one\nyear, how many children will be produced by one woman and three men in 0.5\nyears?\" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview,\nand Gemini Flash frequently answer \"0.5,\" which does not make sense. While\nthese models sometimes acknowledge the unrealistic nature of the question, in\nmany cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5\nchild.\" Additionally, temporal variation has been observed: if an LLM answers\ncorrectly once (by recognizing the faulty nature of the question), subsequent\nresponses are more likely to also reflect this understanding. However, this is\ninconsistent.\n  These types of questions have motivated us to develop a dataset of science\nquestions, SciFaultyQA, where the questions themselves are intentionally\nfaulty. We observed that LLMs often proceed to answer these flawed questions\nwithout recognizing their inherent issues, producing results that are logically\nor scientifically invalid. By analyzing such patterns, we developed a novel\nmethod for generating synthetic datasets to evaluate and benchmark the\nperformance of various LLMs in identifying these flawed questions. We have also\ndeveloped novel approaches to reduce the errors.\n","authors":["Debarshi Kundu"],"pdf_url":"https://arxiv.org/pdf/2412.11988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11983v1","updated":"2024-12-16T17:04:40Z","published":"2024-12-16T17:04:40Z","title":"Cost-Effective Label-free Node Classification with LLMs","summary":"  Graph neural networks (GNNs) have emerged as go-to models for node\nclassification in graph data due to their powerful abilities in fusing graph\nstructures and attributes. However, such models strongly rely on adequate\nhigh-quality labeled data for training, which are expensive to acquire in\npractice. With the advent of large language models (LLMs), a promising way is\nto leverage their superb zero-shot capabilities and massive knowledge for node\nlabeling. Despite promising results reported, this methodology either demands\nconsiderable queries to LLMs, or suffers from compromised performance caused by\nnoisy labels produced by LLMs.\n  To remedy these issues, this work presents Cella, an active self-training\nframework that integrates LLMs into GNNs in a cost-effective manner. The design\nrecipe of Cella is to iteratively identify small sets of \"critical\" samples\nusing GNNs and extract informative pseudo-labels for them with both LLMs and\nGNNs as additional supervision signals to enhance model training. Particularly,\nCella includes three major components: (i) an effective active node selection\nstrategy for initial annotations; (ii) a judicious sample selection scheme to\nsift out the \"critical\" nodes based on label disharmonicity and entropy; and\n(iii) a label refinement module combining LLMs and GNNs with rewired topology.\nOur extensive experiments over five benchmark text-attributed graph datasets\ndemonstrate that Cella significantly outperforms the state of the arts under\nthe same query budget to LLMs in terms of label-free node classification. In\nparticular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an\n8.08% conspicuous improvement in accuracy over the state-of-the-art at a cost\nof less than one cent.\n","authors":["Taiyan Zhang","Renchi Yang","Mingyu Yan","Xiaochun Ye","Dongrui Fan","Yurui Lai"],"pdf_url":"https://arxiv.org/pdf/2412.11983v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.11982v1","updated":"2024-12-16T17:04:10Z","published":"2024-12-16T17:04:10Z","title":"Echo State network for coarsening dynamics of charge density waves","summary":"  An echo state network (ESN) is a type of reservoir computer that uses a\nrecurrent neural network with a sparsely connected hidden layer. Compared with\nother recurrent neural networks, one great advantage of ESN is the simplicity\nof its training process. Yet, despite the seemingly restricted learnable\nparameters, ESN has been shown to successfully capture the spatial-temporal\ndynamics of complex patterns. Here we build an ESN to model the coarsening\ndynamics of charge-density waves (CDW) in a semi-classical Holstein model,\nwhich exhibits a checkerboard electron density modulation at half-filling\nstabilized by a commensurate lattice distortion. The inputs to the ESN are\nlocal CDW order-parameters in a finite neighborhood centered around a given\nsite, while the output is the predicted CDW order of the center site at the\nnext time step. Special care is taken in the design of couplings between hidden\nlayer and input nodes to ensure lattice symmetries are properly incorporated\ninto the ESN model. Since the model predictions depend only on CDW\nconfigurations of a finite domain, the ESN is scalable and transferrable in the\nsense that a model trained on dataset from a small system can be directly\napplied to dynamical simulations on larger lattices. Our work opens a new\navenue for efficient dynamical modeling of pattern formations in functional\nelectron materials.\n","authors":["Clement Dinh","Yunhao Fan","Gia-Wei Chern"],"pdf_url":"https://arxiv.org/pdf/2412.11982v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11981v1","updated":"2024-12-16T17:03:04Z","published":"2024-12-16T17:03:04Z","title":"Industrial-scale Prediction of Cement Clinker Phases using Machine\n  Learning","summary":"  Cement production, exceeding 4.1 billion tonnes and contributing 2.4 tonnes\nof CO2 annually, faces critical challenges in quality control and process\noptimization. While traditional process models for cement manufacturing are\nconfined to steady-state conditions with limited predictive capability for\nmineralogical phases, modern plants operate under dynamic conditions that\ndemand real-time quality assessment. Here, exploiting a comprehensive two-year\noperational dataset from an industrial cement plant, we present a machine\nlearning framework that accurately predicts clinker mineralogy from process\ndata. Our model achieves unprecedented prediction accuracy for major clinker\nphases while requiring minimal input parameters, demonstrating robust\nperformance under varying operating conditions. Through post-hoc explainable\nalgorithms, we interpret the hierarchical relationships between clinker oxides\nand phase formation, providing insights into the functioning of an otherwise\nblack-box model. This digital twin framework can potentially enable real-time\noptimization of cement production, thereby providing a route toward reducing\nmaterial waste and ensuring quality while reducing the associated emissions\nunder real plant conditions. Our approach represents a significant advancement\nin industrial process control, offering a scalable solution for sustainable\ncement manufacturing.\n","authors":["Sheikh Junaid Fayaz","Nestor Montiel-Bohorquez","Shashank Bishnoi","Matteo Romano","Manuele Gatti","N. M. Anoop Krishnan"],"pdf_url":"https://arxiv.org/pdf/2412.11981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11979v1","updated":"2024-12-16T16:59:55Z","published":"2024-12-16T16:59:55Z","title":"AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power\n  Laws","summary":"  Neural scaling laws are observed in a range of domains, to date with no clear\nunderstanding of why they occur. Recent theories suggest that loss power laws\narise from Zipf's law, a power law observed in domains like natural language.\nOne theory suggests that language scaling laws emerge when Zipf-distributed\ntask quanta are learned in descending order of frequency. In this paper we\nexamine power-law scaling in AlphaZero, a reinforcement learning algorithm,\nusing a theory of language-model scaling. We find that game states in training\nand inference data scale with Zipf's law, which is known to arise from the tree\nstructure of the environment, and examine the correlation between scaling-law\nand Zipf's-law exponents. In agreement with quanta scaling theory, we find that\nagents optimize state loss in descending order of frequency, even though this\norder scales inversely with modelling complexity. We also find that inverse\nscaling, the failure of models to improve with size, is correlated with unusual\nZipf curves where end-game states are among the most frequent states. We show\nevidence that larger models shift their focus to these less-important states,\nsacrificing their understanding of important early-game states.\n","authors":["Oren Neumann","Claudius Gros"],"pdf_url":"https://arxiv.org/pdf/2412.11979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06694v2","updated":"2024-12-16T16:58:18Z","published":"2024-09-10T17:55:59Z","title":"DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos\n  Enhanced Kaleidoscopic Images","summary":"  Cancer is a complex disease characterized by uncontrolled cell growth. T cell\nreceptors (TCRs), crucial proteins in the immune system, play a key role in\nrecognizing antigens, including those associated with cancer. Recent\nadvancements in sequencing technologies have facilitated comprehensive\nprofiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity\nand enabling TCR-based immunotherapies. However, analyzing these intricate\nbiomolecules necessitates efficient representations that capture their\nstructural and functional information. T-cell protein sequences pose unique\nchallenges due to their relatively smaller lengths compared to other\nbiomolecules. An image-based representation approach becomes a preferred choice\nfor efficient embeddings, allowing for the preservation of essential details\nand enabling comprehensive analysis of T-cell protein sequences. In this paper,\nwe propose to generate images from the protein sequences using the idea of\nChaos Game Representation (CGR) using the Kaleidoscopic images approach. This\nDeep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced\nKaleidoscopic Images (called DANCE) provides a unique way to visualize protein\nsequences by recursively applying chaos game rules around a central seed point.\nwe perform the classification of the T cell receptors (TCRs) protein sequences\nin terms of their respective target cancer cells, as TCRs are known for their\nimmune response against cancer disease. The TCR sequences are converted into\nimages using the DANCE method. We employ deep-learning vision models to perform\nthe classification to obtain insights into the relationship between the visual\npatterns observed in the generated kaleidoscopic images and the underlying\nprotein properties. By combining CGR-based image generation with deep learning\nclassification, this study opens novel possibilities in the protein analysis\ndomain.\n","authors":["Taslim Murad","Prakash Chourasia","Sarwan Ali","Imdad Ullah Khan","Murray Patterson"],"pdf_url":"https://arxiv.org/pdf/2409.06694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11973v1","updated":"2024-12-16T16:55:34Z","published":"2024-12-16T16:55:34Z","title":"Neural general circulation models optimized to predict satellite-based\n  precipitation observations","summary":"  Climate models struggle to accurately simulate precipitation, particularly\nextremes and the diurnal cycle. Here, we present a hybrid model that is trained\ndirectly on satellite-based precipitation observations. Our model runs at\n2.8$^\\circ$ resolution and is built on the differentiable NeuralGCM framework.\nThe model demonstrates significant improvements over existing general\ncirculation models, the ERA5 reanalysis, and a global cloud-resolving model in\nsimulating precipitation. Our approach yields reduced biases, a more realistic\nprecipitation distribution, improved representation of extremes, and a more\naccurate diurnal cycle. Furthermore, it outperforms the mid-range precipitation\nforecast of the ECMWF ensemble. This advance paves the way for more reliable\nsimulations of current climate and demonstrates how training on observations\ncan be used to directly improve GCMs.\n","authors":["Janni Yuval","Ian Langmore","Dmitrii Kochkov","Stephan Hoyer"],"pdf_url":"https://arxiv.org/pdf/2412.11973v1.pdf","comment":"20 pages, 6 figures in Main. 29 pages, 30 figures in SI"},{"id":"http://arxiv.org/abs/2412.11967v1","updated":"2024-12-16T16:47:15Z","published":"2024-12-16T16:47:15Z","title":"A Digital twin for Diesel Engines: Operator-infused PINNs with Transfer\n  Learning for Engine Health Monitoring","summary":"  Improving diesel engine efficiency and emission reduction have been critical\nresearch topics. Recent government regulations have shifted this focus to\nanother important area related to engine health and performance monitoring.\nAlthough the advancements in the use of deep learning methods for system\nmonitoring have shown promising results in this direction, designing efficient\nmethods suitable for field systems remains an open research challenge. The\nobjective of this study is to develop a computationally efficient neural\nnetwork-based approach for identifying unknown parameters of a mean value\ndiesel engine model to facilitate physics-based health monitoring and\nmaintenance forecasting. We propose a hybrid method combining physics informed\nneural networks, PINNs, and a deep neural operator, DeepONet to predict unknown\nparameters and gas flow dynamics in a diesel engine. The operator network\npredicts independent actuator dynamics learnt through offline training, thereby\nreducing the PINNs online computational cost. To address PINNs need for\nretraining with changing input scenarios, we propose two transfer learning (TL)\nstrategies. The first strategy involves multi-stage transfer learning for\nparameter identification. While this method is computationally efficient as\ncompared to online PINN training, improvements are required to meet field\nrequirements. The second TL strategy focuses solely on training the output\nweights and biases of a subset of multi-head networks pretrained on a larger\ndataset, substantially reducing computation time during online prediction. We\nalso evaluate our model for epistemic and aleatoric uncertainty by\nincorporating dropout in pretrained networks and Gaussian noise in the training\ndataset. This strategy offers a tailored, computationally inexpensive, and\nphysics-based approach for parameter identification in diesel engine sub\nsystems.\n","authors":["Kamaljyoti Nath","Varun Kumar","Daniel J. Smith","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2412.11967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11964v1","updated":"2024-12-16T16:45:26Z","published":"2024-12-16T16:45:26Z","title":"BetaExplainer: A Probabilistic Method to Explain Graph Neural Networks","summary":"  Graph neural networks (GNNs) are powerful tools for conducting inference on\ngraph data but are often seen as \"black boxes\" due to difficulty in extracting\nmeaningful subnetworks driving predictive performance. Many interpretable GNN\nmethods exist, but they cannot quantify uncertainty in edge weights and suffer\nin predictive accuracy when applied to challenging graph structures. In this\nwork, we proposed BetaExplainer which addresses these issues by using a\nsparsity-inducing prior to mask unimportant edges during model training. To\nevaluate our approach, we examine various simulated data sets with diverse\nreal-world characteristics. Not only does this implementation provide a notion\nof edge importance uncertainty, it also improves upon evaluation metrics for\nchallenging datasets compared to state-of-the art explainer methods.\n","authors":["Whitney Sloneker","Shalin Patel","Michael Wang","Lorin Crawford","Ritambhara Singh"],"pdf_url":"https://arxiv.org/pdf/2412.11964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v1","updated":"2024-12-16T16:41:51Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modality and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08585v2","updated":"2024-12-16T16:37:07Z","published":"2024-12-11T18:03:05Z","title":"TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs","summary":"  Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.\n","authors":["Hao Kang","Srikant Bharadwaj","James Hensman","Tushar Krishna","Victor Ruhle","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2412.08585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11952v1","updated":"2024-12-16T16:35:35Z","published":"2024-12-16T16:35:35Z","title":"Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided\n  Self-Supervised Learning","summary":"  Image Aesthetic Assessment (IAA) is a vital and intricate task that entails\nanalyzing and assessing an image's aesthetic values, and identifying its\nhighlights and areas for improvement. Traditional methods of IAA often\nconcentrate on a single aesthetic task and suffer from inadequate labeled\ndatasets, thus impairing in-depth aesthetic comprehension. Despite efforts to\novercome this challenge through the application of Multi-modal Large Language\nModels (MLLMs), such models remain underdeveloped for IAA purposes. To address\nthis, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic\ninsight. Central to our approach is an innovative multi-scale text-guided\nself-supervised learning technique. This technique features a multi-scale\nfeature alignment module and capitalizes on a wealth of unlabeled data in a\nself-supervised manner to structurally and functionally enhance aesthetic\nability. The empirical evidence indicates that accompanied with extensive\ninstruct-tuning, our model sets new state-of-the-art benchmarks across multiple\ntasks, including aesthetic scoring, aesthetic commenting, and personalized\nimage aesthetic assessment. Remarkably, it also demonstrates zero-shot learning\ncapabilities in the emerging task of aesthetic suggesting. Furthermore, for\npersonalized image aesthetic assessment, we harness the potential of in-context\nlearning and showcase its inherent advantages.\n","authors":["Yuti Liu","Shice Liu","Junyuan Gao","Pengtao Jiang","Hao Zhang","Jinwei Chen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2412.11952v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11951v1","updated":"2024-12-16T16:35:31Z","published":"2024-12-16T16:35:31Z","title":"The Impact of Generalization Techniques on the Interplay Among Privacy,\n  Utility, and Fairness in Image Classification","summary":"  This study investigates the trade-offs between fairness, privacy, and utility\nin image classification using machine learning (ML). Recent research suggests\nthat generalization techniques can improve the balance between privacy and\nutility. One focus of this work is sharpness-aware training (SAT) and its\nintegration with differential privacy (DP-SAT) to further improve this balance.\nAdditionally, we examine fairness in both private and non-private learning\nmodels trained on datasets with synthetic and real-world biases. We also\nmeasure the privacy risks involved in these scenarios by performing membership\ninference attacks (MIAs) and explore the consequences of eliminating\nhigh-privacy risk samples, termed outliers. Moreover, we introduce a new\nmetric, named \\emph{harmonic score}, which combines accuracy, privacy, and\nfairness into a single measure.\n  Through empirical analysis using generalization techniques, we achieve an\naccuracy of 81.11\\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\\%\nreported by De et al. (2022). Moreover, our experiments show that memorization\nof training samples can begin before the overfitting point, and generalization\ntechniques do not guarantee the prevention of this memorization. Our analysis\nof synthetic biases shows that generalization techniques can amplify model bias\nin both private and non-private models. Additionally, our results indicate that\nincreased bias in training data leads to reduced accuracy, greater\nvulnerability to privacy attacks, and higher model bias. We validate these\nfindings with the CelebA dataset, demonstrating that similar trends persist\nwith real-world attribute imbalances. Finally, our experiments show that\nremoving outlier data decreases accuracy and further amplifies model bias.\n","authors":["Ahmad Hassanpour","Amir Zarei","Khawla Mallat","Anderson Santana de Oliveira","Bian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11951v1.pdf","comment":"Published as a conference paper at the 25th Privacy Enhancing\n  Technologies Symposium (PETS 2025)"},{"id":"http://arxiv.org/abs/2412.11950v1","updated":"2024-12-16T16:34:48Z","published":"2024-12-16T16:34:48Z","title":"Asynchronous Distributed Gaussian Process Regression for Online Learning\n  and Dynamical Systems: Complementary Document","summary":"  This is a complementary document for the paper titled \"Asynchronous\nDistributed Gaussian Process Regression for Online Learning and Dynamical\nSystems\".\n","authors":["Zewen Yang","Xiaobing Dai","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2412.11950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13015v2","updated":"2024-12-16T16:34:34Z","published":"2023-11-21T21:44:28Z","title":"Fast and Interpretable Mortality Risk Scores for Critical Care Patients","summary":"  Prediction of mortality in intensive care unit (ICU) patients typically\nrelies on black box models (that are unacceptable for use in hospitals) or\nhand-tuned interpretable models (that might lead to the loss in performance).\nWe aim to bridge the gap between these two categories by building on modern\ninterpretable ML techniques to design interpretable mortality risk scores that\nare as accurate as black boxes. We developed a new algorithm, GroupFasterRisk,\nwhich has several important benefits: it uses both hard and soft direct\nsparsity regularization, it incorporates group sparsity to allow more cohesive\nmodels, it allows for monotonicity constraint to include domain knowledge, and\nit produces many equally-good models, which allows domain experts to choose\namong them. For evaluation, we leveraged the largest existing public ICU\nmonitoring datasets (MIMIC III and eICU). Models produced by GroupFasterRisk\noutperformed OASIS and SAPS II scores and performed similarly to APACHE IV/IVa\nwhile using at most a third of the parameters. For patients with\nsepsis/septicemia, acute myocardial infarction, heart failure, and acute kidney\nfailure, GroupFasterRisk models outperformed OASIS and SOFA. Finally, different\nmortality prediction ML approaches performed better based on variables selected\nby GroupFasterRisk as compared to OASIS variables. GroupFasterRisk's models\nperformed better than risk scores currently used in hospitals, and on par with\nblack box ML models, while being orders of magnitude sparser. Because\nGroupFasterRisk produces a variety of risk scores, it allows design flexibility\n- the key enabler of practical model creation. GroupFasterRisk is a fast,\naccessible, and flexible procedure that allows learning a diverse set of sparse\nrisk scores for mortality prediction.\n","authors":["Chloe Qinyu Zhu","Muhang Tian","Lesia Semenova","Jiachang Liu","Jack Xu","Joseph Scarpa","Cynthia Rudin"],"pdf_url":"https://arxiv.org/pdf/2311.13015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04607v3","updated":"2024-12-16T16:30:54Z","published":"2024-08-08T17:27:29Z","title":"Risk and cross validation in ridge regression with correlated samples","summary":"  Recent years have seen substantial advances in our understanding of\nhigh-dimensional ridge regression, but existing theories assume that training\nexamples are independent. By leveraging techniques from random matrix theory\nand free probability, we provide sharp asymptotics for the in- and\nout-of-sample risks of ridge regression when the data points have arbitrary\ncorrelations. We demonstrate that in this setting, the generalized cross\nvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.\nHowever, in the case where the noise residuals have the same correlations as\nthe data points, one can modify the GCV to yield an efficiently-computable\nunbiased estimator that concentrates in the high-dimensional limit, which we\ndub CorrGCV. We further extend our asymptotic analysis to the case where the\ntest point has nontrivial correlations with the training set, a setting often\nencountered in time series forecasting. Assuming knowledge of the correlation\nstructure of the time series, this again yields an extension of the GCV\nestimator, and sharply characterizes the degree to which such test points yield\nan overly optimistic prediction of long-time risk. We validate the predictions\nof our theory across a variety of high dimensional data.\n","authors":["Alexander Atanasov","Jacob A. Zavatone-Veth","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2408.04607v3.pdf","comment":"44 pages, 18 figures. v3: minor typos fixed"},{"id":"http://arxiv.org/abs/2412.11943v1","updated":"2024-12-16T16:25:58Z","published":"2024-12-16T16:25:58Z","title":"autrainer: A Modular and Extensible Deep Learning Toolkit for Computer\n  Audition Tasks","summary":"  This work introduces the key operating principles for autrainer, our new deep\nlearning training framework for computer audition tasks. autrainer is a\nPyTorch-based toolkit that allows for rapid, reproducible, and easily\nextensible training on a variety of different computer audition tasks.\nConcretely, autrainer offers low-code training and supports a wide range of\nneural networks as well as preprocessing routines. In this work, we present an\noverview of its inner workings and key capabilities.\n","authors":["Simon Rampp","Andreas Triantafyllopoulos","Manuel Milling","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2412.11943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01371v3","updated":"2024-12-16T16:17:46Z","published":"2024-02-02T12:48:49Z","title":"Two-Timescale Critic-Actor for Average Reward MDPs with Function\n  Approximation","summary":"  Several recent works have focused on carrying out non-asymptotic convergence\nanalyses for AC algorithms. Recently, a two-timescale critic-actor algorithm\nhas been presented for the discounted cost setting in the look-up table case\nwhere the timescales of the actor and the critic are reversed and only\nasymptotic convergence shown. In our work, we present the first two-timescale\ncritic-actor algorithm with function approximation in the long-run average\nreward setting and present the first finite-time non-asymptotic as well as\nasymptotic convergence analysis for such a scheme. We obtain optimal learning\nrates and prove that our algorithm achieves a sample complexity of\n{$\\mathcal{\\tilde{O}}(\\epsilon^{-(2+\\delta)})$ with $\\delta >0$ arbitrarily\nclose to zero,} for the mean squared error of the critic to be upper bounded by\n$\\epsilon$ which is better than the one obtained for two-timescale AC in a\nsimilar setting. A notable feature of our analysis is that we present the\nasymptotic convergence analysis of our scheme in addition to the finite-time\nbounds that we obtain and show the almost sure asymptotic convergence of the\n(slower) critic recursion to the attractor of an associated differential\ninclusion with actor parameters corresponding to local maxima of a perturbed\naverage reward objective. We also show the results of numerical experiments on\nthree benchmark settings and observe that our critic-actor algorithm performs\nthe best amongst all algorithms.\n","authors":["Prashansa Panda","Shalabh Bhatnagar"],"pdf_url":"https://arxiv.org/pdf/2402.01371v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11930v1","updated":"2024-12-16T16:15:36Z","published":"2024-12-16T16:15:36Z","title":"Hierarchical Meta-Reinforcement Learning via Automated Macro-Action\n  Discovery","summary":"  Meta-Reinforcement Learning (Meta-RL) enables fast adaptation to new testing\ntasks. Despite recent advancements, it is still challenging to learn performant\npolicies across multiple complex and high-dimensional tasks. To address this,\nwe propose a novel architecture with three hierarchical levels for 1) learning\ntask representations, 2) discovering task-agnostic macro-actions in an\nautomated manner, and 3) learning primitive actions. The macro-action can guide\nthe low-level primitive policy learning to more efficiently transition to goal\nstates. This can address the issue that the policy may forget previously\nlearned behavior while learning new, conflicting tasks. Moreover, the\ntask-agnostic nature of the macro-actions is enabled by removing task-specific\ncomponents from the state space. Hence, this makes them amenable to\nre-composition across different tasks and leads to promising fast adaptation to\nnew tasks. Also, the prospective instability from the tri-level hierarchies is\neffectively mitigated by our innovative, independently tailored training\nschemes. Experiments in the MetaWorld framework demonstrate the improved sample\nefficiency and success rate of our approach compared to previous\nstate-of-the-art methods.\n","authors":["Minjae Cho","Chuangchuang Sun"],"pdf_url":"https://arxiv.org/pdf/2412.11930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13482v2","updated":"2024-12-16T16:04:31Z","published":"2024-09-20T13:15:26Z","title":"Invertible ResNets for Inverse Imaging Problems: Competitive Performance\n  with Provable Regularization Properties","summary":"  Learning-based methods have demonstrated remarkable performance in solving\ninverse problems, particularly in image reconstruction tasks. Despite their\nsuccess, these approaches often lack theoretical guarantees, which are crucial\nin sensitive applications such as medical imaging. Recent works by Arndt et al\n(2023 Inverse Problems 39 125018, 2024 Inverse Problems 40 045021) addressed\nthis gap by analyzing a data-driven reconstruction method based on invertible\nresidual networks (iResNets). They revealed that, under reasonable assumptions,\nthis approach constitutes a convergent regularization scheme. However, the\nperformance of the reconstruction method was only validated on academic toy\nproblems and small-scale iResNet architectures. In this work, we address this\ngap by evaluating the performance of iResNets on two real-world imaging tasks:\na linear blurring operator and a nonlinear diffusion operator. To do so, we\nextend some of the theoretical results from Arndt et al to encompass nonlinear\ninverse problems and offer insights for the design of large-scale performant\niResNet architectures. Through numerical experiments, we compare the\nperformance of our iResNet models against state-of-the-art neural networks,\nconfirming their efficacy. Additionally, we numerically investigate the\ntheoretical guarantees of this approach and demonstrate how the invertibility\nof the network enables a deeper analysis of the learned forward operator and\nits learned regularization.\n","authors":["Clemens Arndt","Judith Nickel"],"pdf_url":"https://arxiv.org/pdf/2409.13482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18329v2","updated":"2024-12-16T15:43:48Z","published":"2024-02-28T13:49:23Z","title":"Robust Synthetic Data-Driven Detection of Living-Off-the-Land Reverse\n  Shells","summary":"  Living-off-the-land (LOTL) techniques pose a significant challenge to\nsecurity operations, exploiting legitimate tools to execute malicious commands\nthat evade traditional detection methods. To address this, we present a robust\naugmentation framework for cyber defense systems as Security Information and\nEvent Management (SIEM) solutions, enabling the detection of LOTL attacks such\nas reverse shells through machine learning. Leveraging real-world threat\nintelligence and adversarial training, our framework synthesizes diverse\nmalicious datasets while preserving the variability of legitimate activity,\nensuring high accuracy and low false-positive rates. We validate our approach\nthrough extensive experiments on enterprise-scale datasets, achieving a 90\\%\nimprovement in detection rates over non-augmented baselines at an\nindustry-grade False Positive Rate (FPR) of $10^{-5}$. We define black-box\ndata-driven attacks that successfully evade unprotected models, and develop\ndefenses to mitigate them, producing adversarially robust variants of ML\nmodels. Ethical considerations are central to this work; we discuss safeguards\nfor synthetic data generation and the responsible release of pre-trained models\nacross four best performing architectures, including both adversarially and\nregularly trained variants: https://huggingface.co/dtrizna/quasarnix.\nFurthermore, we provide a malicious LOTL dataset containing over 1 million\naugmented attack variants to enable reproducible research and community\ncollaboration: https://huggingface.co/datasets/dtrizna/QuasarNix. This work\noffers a reproducible, scalable, and production-ready defense against evolving\nLOTL threats.\n","authors":["Dmitrijs Trizna","Luca Demetrio","Battista Biggio","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2402.18329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02329v3","updated":"2024-12-16T15:42:53Z","published":"2024-01-04T16:06:31Z","title":"Exploring Vacant Classes in Label-Skewed Federated Learning","summary":"  Label skews, characterized by disparities in local label distribution across\nclients, pose a significant challenge in federated learning. As minority\nclasses suffer from worse accuracy due to overfitting on local imbalanced data,\nprior methods often incorporate class-balanced learning techniques during local\ntraining. Although these methods improve the mean accuracy across all classes,\nwe observe that vacant classes-referring to categories absent from a client's\ndata distribution-remain poorly recognized. Besides, there is still a gap in\nthe accuracy of local models on minority classes compared to the global model.\nThis paper introduces FedVLS, a novel approach to label-skewed federated\nlearning that integrates both vacant-class distillation and logit suppression\nsimultaneously. Specifically, vacant-class distillation leverages knowledge\ndistillation during local training on each client to retain essential\ninformation related to vacant classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedVLS, demonstrating superior performance compared to previous\nstate-of-the-art (SOTA) methods across diverse datasets with varying degrees of\nlabel skews. Our code is available at https://github.com/krumpguo/FedVLS.\n","authors":["Kuangpu Guo","Yuhe Ding","Jian Liang","Ran He","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2401.02329v3.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2405.16173v3","updated":"2024-12-16T15:42:46Z","published":"2024-05-25T10:45:46Z","title":"Diffusion-based Reinforcement Learning via Q-weighted Variational Policy\n  Optimization","summary":"  Diffusion models have garnered widespread attention in Reinforcement Learning\n(RL) for their powerful expressiveness and multimodality. It has been verified\nthat utilizing diffusion policies can significantly improve the performance of\nRL algorithms in continuous control tasks by overcoming the limitations of\nunimodal policies, such as Gaussian policies, and providing the agent with\nenhanced exploration capabilities. However, existing works mainly focus on the\napplication of diffusion policies in offline RL, while their incorporation into\nonline RL is less investigated. The training objective of the diffusion model,\nknown as the variational lower bound, cannot be optimized directly in online RL\ndue to the unavailability of 'good' actions. This leads to difficulties in\nconducting diffusion policy improvement. To overcome this, we propose a novel\nmodel-free diffusion-based online RL algorithm, Q-weighted Variational Policy\nOptimization (QVPO). Specifically, we introduce the Q-weighted variational\nloss, which can be proved to be a tight lower bound of the policy objective in\nonline RL under certain conditions. To fulfill these conditions, the Q-weight\ntransformation functions are introduced for general scenarios. Additionally, to\nfurther enhance the exploration capability of the diffusion policy, we design a\nspecial entropy regularization term. We also develop an efficient behavior\npolicy to enhance sample efficiency by reducing the variance of the diffusion\npolicy during online interactions. Consequently, the QVPO algorithm leverages\nthe exploration capabilities and multimodality of diffusion policies,\npreventing the RL agent from converging to a sub-optimal policy. To verify the\neffectiveness of QVPO, we conduct comprehensive experiments on MuJoCo\nbenchmarks. The final results demonstrate that QVPO achieves state-of-the-art\nperformance on both cumulative reward and sample efficiency.\n","authors":["Shutong Ding","Ke Hu","Zhenhao Zhang","Kan Ren","Weinan Zhang","Jingyi Yu","Jingya Wang","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2405.16173v3.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2412.11875v1","updated":"2024-12-16T15:27:28Z","published":"2024-12-16T15:27:28Z","title":"Bayesian Surrogate Training on Multiple Data Sources: A Hybrid Modeling\n  Strategy","summary":"  Surrogate models are often used as computationally efficient approximations\nto complex simulation models, enabling tasks such as solving inverse problems,\nsensitivity analysis, and probabilistic forward predictions, which would\notherwise be computationally infeasible. During training, surrogate parameters\nare fitted such that the surrogate reproduces the simulation model's outputs as\nclosely as possible. However, the simulation model itself is merely a\nsimplification of the real-world system, often missing relevant processes or\nsuffering from misspecifications e.g., in inputs or boundary conditions. Hints\nabout these might be captured in real-world measurement data, and yet, we\ntypically ignore those hints during surrogate building. In this paper, we\npropose two novel probabilistic approaches to integrate simulation data and\nreal-world measurement data during surrogate training. The first method trains\nseparate surrogate models for each data source and combines their predictive\ndistributions, while the second incorporates both data sources by training a\nsingle surrogate. We show the conceptual differences and benefits of the two\napproaches through both synthetic and real-world case studies. The results\ndemonstrate the potential of these methods to improve predictive accuracy,\npredictive coverage, and to diagnose problems in the underlying simulation\nmodel. These insights can improve system understanding and future model\ndevelopment.\n","authors":["Philipp Reiser","Paul-Christian Bürkner","Anneli Guthke"],"pdf_url":"https://arxiv.org/pdf/2412.11875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11867v1","updated":"2024-12-16T15:21:04Z","published":"2024-12-16T15:21:04Z","title":"Transformers Use Causal World Models in Maze-Solving Tasks","summary":"  Recent studies in interpretability have explored the inner workings of\ntransformer models trained on tasks across various domains, often discovering\nthat these networks naturally develop surprisingly structured representations.\nWhen such representations comprehensively reflect the task domain's structure,\nthey are commonly referred to as ``World Models'' (WMs). In this work, we\ndiscover such WMs in transformers trained on maze tasks. In particular, by\nemploying Sparse Autoencoders (SAEs) and analysing attention patterns, we\nexamine the construction of WMs and demonstrate consistency between the circuit\nanalysis and the SAE feature-based analysis. We intervene upon the isolated\nfeatures to confirm their causal role and, in doing so, find asymmetries\nbetween certain types of interventions. Surprisingly, we find that models are\nable to reason with respect to a greater number of active features than they\nsee during training, even if attempting to specify these in the input token\nsequence would lead the model to fail. Futhermore, we observe that varying\npositional encodings can alter how WMs are encoded in a model's residual\nstream. By analyzing the causal role of these WMs in a toy domain we hope to\nmake progress toward an understanding of emergent structure in the\nrepresentations acquired by Transformers, leading to the development of more\ninterpretable and controllable AI systems.\n","authors":["Alex F. Spies","William Edwards","Michael I. Ivanitskiy","Adrians Skapars","Tilman Räuker","Katsumi Inoue","Alessandra Russo","Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2412.11867v1.pdf","comment":"Main paper: 9 pages, 9 figures. Supplementary material: 10 pages, 17\n  additional figures. Code and data will be available upon publication.\n  Corresponding author: A. F. Spies (afspies@imperial.ac.uk)"},{"id":"http://arxiv.org/abs/2412.11850v1","updated":"2024-12-16T15:11:02Z","published":"2024-12-16T15:11:02Z","title":"Causal Invariance Learning via Efficient Optimization of a Nonconvex\n  Objective","summary":"  Data from multiple environments offer valuable opportunities to uncover\ncausal relationships among variables. Leveraging the assumption that the causal\noutcome model remains invariant across heterogeneous environments,\nstate-of-the-art methods attempt to identify causal outcome models by learning\ninvariant prediction models and rely on exhaustive searches over all\n(exponentially many) covariate subsets. These approaches present two major\nchallenges: 1) determining the conditions under which the invariant prediction\nmodel aligns with the causal outcome model, and 2) devising computationally\nefficient causal discovery algorithms that scale polynomially, instead of\nexponentially, with the number of covariates. To address both challenges, we\nfocus on the additive intervention regime and propose nearly necessary and\nsufficient conditions for ensuring that the invariant prediction model matches\nthe causal outcome model. Exploiting the essentially necessary identifiability\nconditions, we introduce Negative Weight Distributionally Robust Optimization\nNegDRO a nonconvex continuous minimax optimization whose global optimizer\nrecovers the causal outcome model. Unlike standard group DRO problems that\nmaximize over the simplex, NegDRO allows negative weights on environment\nlosses, which break the convexity. Despite its nonconvexity, we demonstrate\nthat a standard gradient method converges to the causal outcome model, and we\nestablish the convergence rate with respect to the sample size and the number\nof iterations. Our algorithm avoids exhaustive search, making it scalable\nespecially when the number of covariates is large. The numerical results\nfurther validate the efficiency of the proposed method.\n","authors":["Zhenyu Wang","Yifan Hu","Peter Bühlmann","Zijian Guo"],"pdf_url":"https://arxiv.org/pdf/2412.11850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11849v1","updated":"2024-12-16T15:10:53Z","published":"2024-12-16T15:10:53Z","title":"Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tumor Analysis\n  in Multimodal MRI","summary":"  Motivated by the need for advanced solutions in the segmentation and\ninpainting of glioma-affected brain regions in multi-modal magnetic resonance\nimaging (MRI), this study presents an integrated approach leveraging the\nstrengths of ensemble learning with hybrid transformer models and convolutional\nneural networks (CNNs), alongside the innovative application of 3D Pix2Pix\nGenerative Adversarial Network (GAN). Our methodology combines robust tumor\nsegmentation capabilities, utilizing axial attention and transformer encoders\nfor enhanced spatial relationship modeling, with the ability to synthesize\nbiologically plausible brain tissue through 3D Pix2Pix GAN. This integrated\napproach addresses the BraTS 2023 cluster challenges by offering precise\nsegmentation and realistic inpainting, tailored for diverse tumor types and\nsub-regions. The results demonstrate outstanding performance, evidenced by\nquantitative evaluations such as the Dice Similarity Coefficient (DSC),\nHausdorff Distance (HD95) for segmentation, and Structural Similarity Index\nMeasure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean-Square Error (MSE)\nfor inpainting. Qualitative assessments further validate the high-quality,\nclinically relevant outputs. In conclusion, this study underscores the\npotential of combining advanced machine learning techniques for comprehensive\nbrain tumor analysis, promising significant advancements in clinical\ndecision-making and patient care within the realm of medical imaging.\n","authors":["Ramy A. Zeineldin","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2412.11849v1.pdf","comment":"Accepted at the MICCAI BraTS Challenge 2023"},{"id":"http://arxiv.org/abs/2412.11846v1","updated":"2024-12-16T15:08:44Z","published":"2024-12-16T15:08:44Z","title":"SPGL: Enhancing Session-based Recommendation with Single Positive Graph\n  Learning","summary":"  Session-based recommendation seeks to forecast the next item a user will be\ninterested in, based on their interaction sequences. Due to limited interaction\ndata, session-based recommendation faces the challenge of limited data\navailability. Traditional methods enhance feature learning by constructing\ncomplex models to generate positive and negative samples. This paper proposes a\nsession-based recommendation model using Single Positive optimization loss and\nGraph Learning (SPGL) to deal with the problem of data sparsity, high model\ncomplexity and weak transferability. SPGL utilizes graph convolutional networks\nto generate global item representations and batch session representations,\neffectively capturing intrinsic relationships between items. The use of single\npositive optimization loss improves uniformity of item representations, thereby\nenhancing recommendation accuracy. In the intent extractor, SPGL considers the\nhop count of the adjacency matrix when constructing the directed global graph\nto fully integrate spatial information. It also takes into account the reverse\npositional information of items when constructing session representations to\nincorporate temporal information. Comparative experiments across three\nbenchmark datasets, Tmall, RetailRocket and Diginetica, demonstrate the model's\neffectiveness. The source code can be accessed on\nhttps://github.com/liang-tian-tian/SPGL .\n","authors":["Tiantian Liang","Zhe Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11846v1.pdf","comment":"ICONIP 2024"},{"id":"http://arxiv.org/abs/2412.08435v2","updated":"2024-12-16T15:02:52Z","published":"2024-12-11T14:57:10Z","title":"Proactive Model Adaptation Against Concept Drift for Online Time Series\n  Forecasting","summary":"  Time series forecasting always faces the challenge of concept drift, where\ndata distributions evolve over time, leading to a decline in forecast model\nperformance. Existing solutions are based on online learning, which continually\norganize recent time series observations as new training samples and update\nmodel parameters according to the forecasting feedback on recent data. However,\nthey overlook a critical issue: obtaining ground-truth future values of each\nsample should be delayed until after the forecast horizon. This delay creates a\ntemporal gap between the training samples and the test sample. Our empirical\nanalysis reveals that the gap can introduce concept drift, causing forecast\nmodels to adapt to outdated concepts. In this paper, we present\n\\textsc{Proceed}, a novel proactive model adaptation framework for online time\nseries forecasting. \\textsc{Proceed} first estimates the concept drift between\nthe recently used training samples and the current test sample. It then employs\nan adaptation generator to efficiently translate the estimated drift into\nparameter adjustments, proactively adapting the model to the test sample. To\nenhance the generalization capability of the framework, \\textsc{Proceed} is\ntrained on synthetic diverse concept drifts. Extensive experiments on five\nreal-world datasets across various forecast models demonstrate that\n\\textsc{Proceed} brings more performance improvements than the state-of-the-art\nonline learning methods, significantly facilitating forecast models' resilience\nagainst concept drifts. Code is available at\n\\url{https://github.com/SJTU-DMTai/OnlineTSF}.\n","authors":["Lifan Zhao","Yanyan Shen"],"pdf_url":"https://arxiv.org/pdf/2412.08435v2.pdf","comment":"Accepted by KDD 2025. Preprint version"},{"id":"http://arxiv.org/abs/2402.05668v2","updated":"2024-12-16T15:02:14Z","published":"2024-02-08T13:42:50Z","title":"Comprehensive Assessment of Jailbreak Attacks Against LLMs","summary":"  Jailbreak attacks aim to bypass the safeguards of LLMs. While researchers\nhave studied different jailbreak attacks in depth, they have done so in\nisolation -- either with unaligned experiment settings or comparing a limited\nrange of methods. To fill this gap, we present the first large-scale\nmeasurement of various jailbreak attack methods. We collect 17 cutting-edge\njailbreak methods, summarize their features, and establish a novel jailbreak\nattack taxonomy. Based on eight popular censored LLMs and 160 questions from 16\nviolation categories, we conduct a unified and impartial assessment of attack\neffectiveness as well as a comprehensive ablation study. Our extensive\nexperimental results demonstrate that all the jailbreak attacks have a powerful\neffect on the LLMs. This indicates that all LLMs fail to cover all the\nviolation categories, and they are susceptible to significant jailbreak risks,\nwith even the well-aligned Llama3 facing a maximum attack success rate of 0.88.\nAdditionally, we test jailbreak attacks under eight advanced external defenses\nand find none of the defenses could mitigate the jailbreak attacks entirely.\nOur study offers valuable insights for future research on jailbreak attacks and\ndefenses and serves as a benchmark tool for researchers and practitioners to\nevaluate them effectively.\n","authors":["Junjie Chu","Yugeng Liu","Ziqing Yang","Xinyue Shen","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.05668v2.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.11839v1","updated":"2024-12-16T15:01:53Z","published":"2024-12-16T15:01:53Z","title":"Evaluating the Efficacy of Vectocardiographic and ECG Parameters for\n  Efficient Tertiary Cardiology Care Allocation Using Decision Tree Analysis","summary":"  Use real word data to evaluate the performance of the electrocardiographic\nmarkers of GEH as features in a machine learning model with Standard ECG\nfeatures and Risk Factors in Predicting Outcome of patients in a population\nreferred to a tertiary cardiology hospital.\n  Patients forwarded to specific evaluation in a cardiology specialized\nhospital performed an ECG and a risk factor anamnesis. A series of follow up\nattendances occurred in periods of 6 months, 12 months and 15 months to check\nfor cardiovascular related events (mortality or new nonfatal cardiovascular\nevents (Stroke, MI, PCI, CS), as identified during 1-year phone follow-ups.\n  The first attendance ECG was measured by a specialist and processed in order\nto obtain the global electric heterogeneity (GEH) using the Kors Matriz. The\nECG measurements, GEH parameters and risk factors were combined for training\nmultiple instances of XGBoost decision trees models. Each instance were\noptmized for the AUCPR and the instance with higher AUC is chosen as\nrepresentative to the model. The importance of each parameter for the winner\ntree model was compared to better understand the improvement from using GEH\nparameters.\n  The GEH parameters turned out to have statistical significance for this\npopulation specially the QRST angle and the SVG. The combined model with the\ntree parameters class had the best performance. The findings suggest that using\nVCG features can facilitate more accurate identification of patients who\nrequire tertiary care, thereby optimizing resource allocation and improving\npatient outcomes. Moreover, the decision tree model's transparency and ability\nto pinpoint critical features make it a valuable tool for clinical\ndecision-making and align well with existing clinical practices.\n","authors":["Lucas José da Costa","Vinicius Ruiz Uemoto","Mariana F. N. de Marchi","Renato de Aguiar Hortegal","Renata Valeri de Freitas"],"pdf_url":"https://arxiv.org/pdf/2412.11839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11837v1","updated":"2024-12-16T15:00:15Z","published":"2024-12-16T15:00:15Z","title":"The Eclipsing Binaries via Artificial Intelligence. II. Need for Speed\n  in PHOEBE Forward Models","summary":"  In modern astronomy, the quantity of data collected has vastly exceeded the\ncapacity for manual analysis, necessitating the use of advanced artificial\nintelligence (AI) techniques to assist scientists with the most labor-intensive\ntasks. AI can optimize simulation codes where computational bottlenecks arise\nfrom the time required to generate forward models. One such example is PHOEBE,\na modeling code for eclipsing binaries (EBs), where simulating individual\nsystems is feasible, but analyzing observables for extensive parameter\ncombinations is highly time-consuming.\n  To address this, we present a fully connected feedforward artificial neural\nnetwork (ANN) trained on a dataset of over one million synthetic light curves\ngenerated with PHOEBE. Optimization of the ANN architecture yielded a model\nwith six hidden layers, each containing 512 nodes, provides an optimized\nbalance between accuracy and computational complexity. Extensive testing\nenabled us to establish ANN's applicability limits and to quantify the\nsystematic and statistical errors associated with using such networks for EB\nanalysis. Our findings demonstrate the critical role of dilution effects in\nparameter estimation for EBs, and we outline methods to incorporate these\neffects in AI-based models.\n  This proposed ANN framework enables a speedup of over four orders of\nmagnitude compared to traditional methods, with systematic errors not exceeding\n1\\%, and often as low as 0.01\\%, across the entire parameter space.\n","authors":["Marcin Wrona","Andrej Prša"],"pdf_url":"https://arxiv.org/pdf/2412.11837v1.pdf","comment":"Submitted to AAS Journals. 26 pages, 21 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.12639v3","updated":"2024-12-16T15:00:05Z","published":"2024-04-19T05:45:43Z","title":"Data-Incremental Continual Offline Reinforcement Learning","summary":"  In this work, we propose a new setting of continual learning:\ndata-incremental continual offline reinforcement learning (DICORL), in which an\nagent is asked to learn a sequence of datasets of a single offline\nreinforcement learning (RL) task continually, instead of learning a sequence of\noffline RL tasks with respective datasets. Then, we propose that this new\nsetting will introduce a unique challenge to continual learning: active\nforgetting, which means that the agent will forget the learnt skill actively.\nThe main reason for active forgetting is conservative learning used by offline\nRL, which is used to solve the overestimation problem. With conservative\nlearning, the offline RL method will suppress the value of all actions, learnt\nor not, without selection, unless it is in the just learning dataset.\nTherefore, inferior data may overlay premium data because of the learning\nsequence. To solve this problem, we propose a new algorithm, called\nexperience-replay-based ensemble implicit Q-learning (EREIQL), which introduces\nmultiple value networks to reduce the initial value and avoid using\nconservative learning, and the experience replay to relieve catastrophic\nforgetting. Our experiments show that EREIQL relieves active forgetting in\nDICORL and performs well.\n","authors":["Sibo Gai","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12639v3.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.00503v3","updated":"2024-12-16T14:59:05Z","published":"2024-11-30T15:03:41Z","title":"Homeostasis and Sparsity in Transformer","summary":"  The transformer architecture has become an integral part of the field of\nmodern neural networks, playing a crucial role in a variety of tasks, such as\ntext generation, machine translation, image and audio processing, among others.\nThere is also an alternative approach to building intelligent systems, proposed\nby Jeff Hawkins and inspired by the processes occurring in the neocortex. In\nour article we want to combine some of these ideas and to propose the use of\nhomeostasis mechanisms, such as RFB-kWTA and \"Smart\" Inhibition, in the\nattention mechanism of the transformer and at the output of the transformer\nblock, as well as conducting an experiment involving the introduction of sparse\ndistributed representations of the transformer at various points. RFB-kWTA\nutilizes statistics of layer activations across time to adjust the entire\nlayer, enhancing the values of rare activations while reducing those of\nfrequent ones. \"Smart\" Inhibition also uses activation statistics to sample\nsparsity masks, with rarer activation times are more likely to be activated.\nOur proposed mechanisms significantly outperform the classical transformer\n0.2768 BLEU and a model that only makes use of dropout in the attention\nmechanism and output of the transformer block 0.3007 BLEU, achieving a score of\n0.3062 on the Multi30K dataset.\n","authors":["Leonid Kotyuzanskiy","Artem Klimov"],"pdf_url":"https://arxiv.org/pdf/2412.00503v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11834v1","updated":"2024-12-16T14:56:28Z","published":"2024-12-16T14:56:28Z","title":"Wonderful Matrices: Combining for a More Efficient and Effective\n  Foundation Model Architecture","summary":"  In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.\n","authors":["Jingze Shi","Bingheng Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11834v1.pdf","comment":"The code is open-sourced at https://github.com/LoserCheems/Doge"},{"id":"http://arxiv.org/abs/2403.13804v2","updated":"2024-12-16T14:53:21Z","published":"2024-03-20T17:59:43Z","title":"Learning from Synthetic Data for Visual Grounding","summary":"  This paper extensively investigates the effectiveness of synthetic training\ndata to improve the capabilities of vision-and-language models for grounding\ntextual descriptions to image regions. We explore various strategies to best\ngenerate image-text pairs and image-text-box triplets using a series of\npretrained models under different settings and varying degrees of reliance on\nreal data. Through comparative analyses with synthetic, real, and web-crawled\ndata, we identify factors that contribute to performance differences, and\npropose SynGround, an effective pipeline for generating useful synthetic data\nfor visual grounding. Our findings show that SynGround can improve the\nlocalization capabilities of off-the-shelf vision-and-language models and\noffers the potential for arbitrarily large scale data generation. Particularly,\ndata generated with SynGround improves the pointing game accuracy of a\npretrained ALBEF and BLIP models by 4.81% and 17.11% absolute percentage\npoints, respectively, across the RefCOCO+ and the Flickr30k benchmarks.\n","authors":["Ruozhen He","Ziyan Yang","Paola Cascante-Bonilla","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v2.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2412.04905v2","updated":"2024-12-16T14:36:19Z","published":"2024-12-06T10:01:38Z","title":"DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling","summary":"  Large language models (LLMs) have made dialogue one of the central modes in\nhuman-machine interaction, leading to the vast amounts of conversation logs and\nincreasing demand for dialogue generation. The dialogue's life-cycle spans from\nthe $\\textit{Prelude}$ through the $\\textit{Interlocution}$ to the\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite the large\nvolumes of dialogue-related studies, there is a lack of benchmark that\nencompasses comprehensive dialogue elements, which hinders precise modeling,\ngeneration and systematic evaluation. To bridge this gap, in this paper, we\nintroduce a new research task $\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks.\n","authors":["Minzheng Wang","Xinghua Zhang","Kun Chen","Nan Xu","Haiyang Yu","Fei Huang","Wenji Mao","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2412.04905v2.pdf","comment":"We release the code and data at https://github.com/MozerWang/DEMO"},{"id":"http://arxiv.org/abs/2412.11810v1","updated":"2024-12-16T14:23:31Z","published":"2024-12-16T14:23:31Z","title":"Optimal Gradient Checkpointing for Sparse and Recurrent Architectures\n  using Off-Chip Memory","summary":"  Recurrent neural networks (RNNs) are valued for their computational\nefficiency and reduced memory requirements on tasks involving long sequence\nlengths but require high memory-processor bandwidth to train. Checkpointing\ntechniques can reduce the memory requirements by only storing a subset of\nintermediate states, the checkpoints, but are still rarely used due to the\ncomputational overhead of the additional recomputation phase. This work\naddresses these challenges by introducing memory-efficient gradient\ncheckpointing strategies tailored for the general class of sparse RNNs and\nSpiking Neural Networks (SNNs). SNNs are energy efficient alternatives to RNNs\nthanks to their local, event-driven operation and potential neuromorphic\nimplementation. We use the Intelligence Processing Unit (IPU) as an exemplary\nplatform for architectures with distributed local memory. We exploit its\nsuitability for sparse and irregular workloads to scale SNN training on long\nsequence lengths. We find that Double Checkpointing emerges as the most\neffective method, optimizing the use of local memory resources while minimizing\nrecomputation overhead. This approach reduces dependency on slower large-scale\nmemory access, enabling training on sequences over 10 times longer or 4 times\nlarger networks than previously feasible, with only marginal time overhead. The\npresented techniques demonstrate significant potential to enhance scalability\nand efficiency in training sparse and recurrent networks across diverse\nhardware platforms, and highlights the benefits of sparse activations for\nscalable recurrent neural network training.\n","authors":["Wadjih Bencheikh","Jan Finkbeiner","Emre Neftci"],"pdf_url":"https://arxiv.org/pdf/2412.11810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07179v2","updated":"2024-12-16T14:22:23Z","published":"2024-07-09T18:47:25Z","title":"TrackFormers: In Search of Transformer-Based Particle Tracking for the\n  High-Luminosity LHC Era","summary":"  High-Energy Physics experiments are facing a multi-fold data increase with\nevery new iteration. This is certainly the case for the upcoming\nHigh-Luminosity LHC upgrade. Such increased data processing requirements forces\nrevisions to almost every step of the data processing pipeline. One such step\nin need of an overhaul is the task of particle track reconstruction, a.k.a.,\ntracking. A Machine Learning-assisted solution is expected to provide\nsignificant improvements, since the most time-consuming step in tracking is the\nassignment of hits to particles or track candidates. This is the topic of this\npaper.\n  We take inspiration from large language models. As such, we consider two\napproaches: the prediction of the next word in a sentence (next hit point in a\ntrack), as well as the one-shot prediction of all hits within an event. In an\nextensive design effort, we have experimented with three models based on the\nTransformer architecture and one model based on the U-Net architecture,\nperforming track association predictions for collision event hit points. In our\nevaluation, we consider a spectrum of simple to complex representations of the\nproblem, eliminating designs with lower metrics early on. We report extensive\nresults, covering both prediction accuracy (score) and computational\nperformance. We have made use of the REDVID simulation framework, as well as\nreductions applied to the TrackML data set, to compose five data sets from\nsimple to complex, for our experiments. The results highlight distinct\nadvantages among different designs in terms of prediction accuracy and\ncomputational performance, demonstrating the efficiency of our methodology.\nMost importantly, the results show the viability of a one-shot\nencoder-classifier based Transformer solution as a practical approach for the\ntask of tracking.\n","authors":["Sascha Caron","Nadezhda Dobreva","Antonio Ferrer Sánchez","José D. Martín-Guerrero","Uraz Odyurt","Roberto Ruiz de Austri Bazan","Zef Wolffs","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.07179v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11800v1","updated":"2024-12-16T14:11:28Z","published":"2024-12-16T14:11:28Z","title":"Scalable Temporal Anomaly Causality Discovery in Large Systems:\n  Achieving Computational Efficiency with Binary Anomaly Flag Data","summary":"  Extracting anomaly causality facilitates diagnostics once monitoring systems\ndetect system faults. Identifying anomaly causes in large systems involves\ninvestigating a more extensive set of monitoring variables across multiple\nsubsystems. However, learning causal graphs comes with a significant\ncomputational burden that restrains the applicability of most existing methods\nin real-time and large-scale deployments. In addition, modern monitoring\napplications for large systems often generate large amounts of binary alarm\nflags, and the distinct characteristics of binary anomaly data -- the meaning\nof state transition and data sparsity -- challenge existing causality learning\nmechanisms. This study proposes an anomaly causal discovery approach\n(AnomalyCD), addressing the accuracy and computational challenges of generating\ncausal graphs from binary flag data sets. The AnomalyCD framework presents\nseveral strategies, such as anomaly flag characteristics incorporating\ncausality testing, sparse data and link compression, and edge pruning\nadjustment approaches. We validate the performance of this framework on two\ndatasets: monitoring sensor data of the readout-box system of the Compact Muon\nSolenoid experiment at CERN, and a public data set for information technology\nmonitoring. The results demonstrate the considerable reduction of the\ncomputation overhead and moderate enhancement of the accuracy of temporal\ncausal discovery on binary anomaly data sets.\n","authors":["Mulugeta Weldezgina Asres","Christian Walter Omlin","The CMS-HCAL Collaboration"],"pdf_url":"https://arxiv.org/pdf/2412.11800v1.pdf","comment":"30 pages, 17 figures, 9 tables"},{"id":"http://arxiv.org/abs/2412.11787v1","updated":"2024-12-16T13:59:10Z","published":"2024-12-16T13:59:10Z","title":"A Method for Detecting Legal Article Competition for Korean Criminal Law\n  Using a Case-augmented Mention Graph","summary":"  As social systems become increasingly complex, legal articles are also\ngrowing more intricate, making it progressively harder for humans to identify\nany potential competitions among them, particularly when drafting new laws or\napplying existing laws. Despite this challenge, no method for detecting such\ncompetitions has been proposed so far. In this paper, we propose a new legal AI\ntask called Legal Article Competition Detection (LACD), which aims to identify\ncompeting articles within a given law. Our novel retrieval method, CAM-Re2,\noutperforms existing relevant methods, reducing false positives by 20.8% and\nfalse negatives by 8.3%, while achieving a 98.2% improvement in precision@5,\nfor the LACD task. We release our codes at\nhttps://github.com/asmath472/LACD-public.\n","authors":["Seonho An","Young Yik Rhim","Min-Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11787v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2412.11777v1","updated":"2024-12-16T13:48:40Z","published":"2024-12-16T13:48:40Z","title":"Fast and Slow Gradient Approximation for Binary Neural Network\n  Optimization","summary":"  Binary Neural Networks (BNNs) have garnered significant attention due to\ntheir immense potential for deployment on edge devices. However, the\nnon-differentiability of the quantization function poses a challenge for the\noptimization of BNNs, as its derivative cannot be backpropagated. To address\nthis issue, hypernetwork based methods, which utilize neural networks to learn\nthe gradients of non-differentiable quantization functions, have emerged as a\npromising approach due to their adaptive learning capabilities to reduce\nestimation errors. However, existing hypernetwork based methods typically rely\nsolely on current gradient information, neglecting the influence of historical\ngradients. This oversight can lead to accumulated gradient errors when\ncalculating gradient momentum during optimization. To incorporate historical\ngradient information, we design a Historical Gradient Storage (HGS) module,\nwhich models the historical gradient sequence to generate the first-order\nmomentum required for optimization. To further enhance gradient generation in\nhypernetworks, we propose a Fast and Slow Gradient Generation (FSG) method.\nAdditionally, to produce more precise gradients, we introduce Layer Recognition\nEmbeddings (LRE) into the hypernetwork, facilitating the generation of\nlayer-specific fine gradients. Extensive comparative experiments on the\nCIFAR-10 and CIFAR-100 datasets demonstrate that our method achieves faster\nconvergence and lower loss values, outperforming existing baselines.Code is\navailable at http://github.com/two-tiger/FSG .\n","authors":["Xinquan Chen","Junqi Gao","Biqing Qi","Dong Li","Yiang Luo","Fangyuan Li","Pengfei Li"],"pdf_url":"https://arxiv.org/pdf/2412.11777v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2405.12462v4","updated":"2024-12-16T13:47:34Z","published":"2024-05-21T02:37:47Z","title":"Enhancing Transformer-based models for Long Sequence Time Series\n  Forecasting via Structured Matrix","summary":"  Recently, Transformer-based models for long sequence time series forecasting\nhave demonstrated promising results. The self-attention mechanism as the core\ncomponent of these Transformer-based models exhibits great potential in\ncapturing various dependencies among data points. Despite these advancements,\nit has been a subject of concern to improve the efficiency of the\nself-attention mechanism. Unfortunately, current specific optimization methods\nare facing the challenges in applicability and scalability for the future\ndesign of long sequence time series forecasting models. Hence, in this article,\nwe propose a novel architectural framework that enhances Transformer-based\nmodels through the integration of Surrogate Attention Blocks (SAB) and\nSurrogate Feed-Forward Neural Network Blocks (SFB). The framework reduces both\ntime and space complexity by the replacement of the self-attention and\nfeed-forward layers with SAB and SFB while maintaining their expressive power\nand architectural advantages. The equivalence of this substitution is fully\ndemonstrated. The extensive experiments on 10 Transformer-based models across\nfive distinct time series tasks demonstrate an average performance improvement\nof 12.4%, alongside 61.3% reduction in parameter counts.\n","authors":["Zhicheng Zhang","Yong Wang","Shaoqi Tan","Bowei Xia","Yujie Luo"],"pdf_url":"https://arxiv.org/pdf/2405.12462v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04903v2","updated":"2024-12-16T13:47:29Z","published":"2024-12-06T09:59:47Z","title":"EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation","summary":"  Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.\n","authors":["Yongxin Wang","Meng Cao","Haokun Lin","Mingfei Han","Liang Ma","Jin Jiang","Yuhao Cheng","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.04903v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2412.11768v1","updated":"2024-12-16T13:41:37Z","published":"2024-12-16T13:41:37Z","title":"No More Adam: Learning Rate Scaling at Initialization is All You Need","summary":"  In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings.\n","authors":["Minghao Xu","Lichuan Xiang","Xu Cai","Hongkai Wen"],"pdf_url":"https://arxiv.org/pdf/2412.11768v1.pdf","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2312.00502v5","updated":"2024-12-16T13:32:52Z","published":"2023-12-01T11:06:00Z","title":"Which Augmentation Should I Use? An Empirical Investigation of\n  Augmentations for Self-Supervised Phonocardiogram Representation Learning","summary":"  Despite recent advancements in deep learning, its application in real-world\nmedical settings, such as phonocardiogram (PCG) classification, remains\nlimited. A significant barrier is the lack of high-quality annotated datasets,\nwhich hampers the development of robust, generalizable models that can perform\nwell on newly collected, out-of-distribution (OOD) data. Self-Supervised\nLearning (SSL) contrastive learning, has shown promise in mitigating the issue\nof data scarcity by using unlabeled data to enhance model robustness. Even\nthough SSL methods have been proposed and researched in other domains, works\nfocusing on the impact of data augmentations on model robustness for PCG\nclassification are limited. In particular, while augmentations are a key\ncomponent in SSL, selecting the most suitable policy during training is highly\nchallenging. Improper augmentations can lead to substantial performance\ndegradation and even hinder a network's ability to learn meaningful\nrepresentations. Addressing this gap, our research aims to explore and evaluate\na wide range of audio-based augmentations and uncover combinations that enhance\nSSL model performance in PCG classification. We conduct a comprehensive\ncomparative analysis across multiple datasets, assessing the impact of various\naugmentations on model performance. Our findings reveal that depending on the\ntraining distribution, augmentation choice significantly influences model\nrobustness, with fully-supervised models experiencing up to a 32\\% drop in\neffectiveness when evaluated on unseen data, while SSL models demonstrate\ngreater resilience, losing only 10\\% or even improving in some cases. This\nstudy also highlights the most promising and appropriate augmentations for PCG\nsignal processing, by calculating their effect size on training. These insights\nequip researchers with valuable guidelines for developing reliable models in\nPCG signal processing.\n","authors":["Aristotelis Ballas","Vasileios Papapanagiotou","Christos Diou"],"pdf_url":"https://arxiv.org/pdf/2312.00502v5.pdf","comment":"Accepted in IEEE ACCESS"},{"id":"http://arxiv.org/abs/2412.11764v1","updated":"2024-12-16T13:31:26Z","published":"2024-12-16T13:31:26Z","title":"What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study","summary":"  Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines, and achieves 70% improvement\nover the traditional MPC. The policy derived by SimpleFlight consistently\nexcels across both smooth polynominal trajectories and challenging infeasible\nzigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline\nmethods struggle with high-speed or infeasible trajectories. To support further\nresearch and reproducibility, we integrate SimpleFlight into a GPU-based\nsimulator Omnidrones and provide open-source access to the code and model\ncheckpoints. We hope SimpleFlight will offer valuable insights for advancing\nRL-based quadrotor control. For more details, visit our project website at\nhttps://sites.google.com/view/simpleflight/.\n","authors":["Jiayu Chen","Chao Yu","Yuqing Xie","Feng Gao","Yinuo Chen","Shu'ang Yu","Wenhao Tang","Shilong Ji","Mo Mu","Yi Wu","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11764v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2407.20158v2","updated":"2024-12-16T13:28:45Z","published":"2024-07-29T16:34:47Z","title":"Machine Learning for Predicting Chaotic Systems","summary":"  Predicting chaotic dynamical systems is critical in many scientific fields,\nsuch as weather forecasting, but challenging due to the characteristic\nsensitive dependence on initial conditions. Traditional modeling approaches\nrequire extensive domain knowledge, often leading to a shift towards\ndata-driven methods using machine learning. However, existing research provides\ninconclusive results on which machine learning methods are best suited for\npredicting chaotic systems. In this paper, we compare different lightweight and\nheavyweight machine learning architectures using extensive existing benchmark\ndatabases, as well as a newly introduced database that allows for uncertainty\nquantification in the benchmark results. In addition to state-of-the-art\nmethods from the literature, we also present new advantageous variants of\nestablished methods. Hyperparameter tuning is adjusted based on computational\ncost, with more tuning allocated to less costly methods. Furthermore, we\nintroduce the cumulative maximum error, a novel metric that combines desirable\nproperties of traditional metrics and is tailored for chaotic systems. Our\nresults show that well-tuned simple methods, as well as untuned baseline\nmethods, often outperform state-of-the-art deep learning models, but their\nperformance can vary significantly with different experimental setups. These\nfindings highlight the importance of aligning prediction methods with data\ncharacteristics and caution against the indiscriminate use of overly complex\nmodels.\n","authors":["Christof Schötz","Alistair White","Maximilian Gelbrecht","Niklas Boers"],"pdf_url":"https://arxiv.org/pdf/2407.20158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11744v1","updated":"2024-12-16T13:03:18Z","published":"2024-12-16T13:03:18Z","title":"Conditional Diffusion Models Based Conditional Independence Testing","summary":"  Conditional independence (CI) testing is a fundamental task in modern\nstatistics and machine learning. The conditional randomization test (CRT) was\nrecently introduced to test whether two random variables, $X$ and $Y$, are\nconditionally independent given a potentially high-dimensional set of random\nvariables, $Z$. The CRT operates exceptionally well under the assumption that\nthe conditional distribution $X|Z$ is known. However, since this distribution\nis typically unknown in practice, accurately approximating it becomes crucial.\nIn this paper, we propose using conditional diffusion models (CDMs) to learn\nthe distribution of $X|Z$. Theoretically and empirically, it is shown that CDMs\nclosely approximate the true conditional distribution. Furthermore, CDMs offer\na more accurate approximation of $X|Z$ compared to GANs, potentially leading to\na CRT that performs better than those based on GANs. To accommodate complex\ndependency structures, we utilize a computationally efficient classifier-based\nconditional mutual information (CMI) estimator as our test statistic. The\nproposed testing procedure performs effectively without requiring assumptions\nabout specific distribution forms or feature dependencies, and is capable of\nhandling mixed-type conditioning sets that include both continuous and discrete\nvariables. Theoretical analysis shows that our proposed test achieves a valid\ncontrol of the type I error. A series of experiments on synthetic data\ndemonstrates that our new test effectively controls both type-I and type-II\nerrors, even in high dimensional scenarios.\n","authors":["Yanfeng Yang","Shuai Li","Yingjie Zhang","Zhuoran Sun","Hai Shu","Ziqi Chen","Renmin Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11744v1.pdf","comment":"17 pages, 7 figures, aaai 2025"},{"id":"http://arxiv.org/abs/2412.11743v1","updated":"2024-12-16T13:02:17Z","published":"2024-12-16T13:02:17Z","title":"Generalized Bayesian deep reinforcement learning","summary":"  Bayesian reinforcement learning (BRL) is a method that merges principles from\nBayesian statistics and reinforcement learning to make optimal decisions in\nuncertain environments. Similar to other model-based RL approaches, it involves\ntwo key components: (1) Inferring the posterior distribution of the data\ngenerating process (DGP) modeling the true environment and (2) policy learning\nusing the learned posterior. We propose to model the dynamics of the unknown\nenvironment through deep generative models assuming Markov dependence. In\nabsence of likelihood functions for these models we train them by learning a\ngeneralized predictive-sequential (or prequential) scoring rule (SR) posterior.\nWe use sequential Monte Carlo (SMC) samplers to draw samples from this\ngeneralized Bayesian posterior distribution. In conjunction, to achieve\nscalability in the high dimensional parameter space of the neural networks, we\nuse the gradient based Markov chain Monte Carlo (MCMC) kernels within SMC. To\njustify the use of the prequential scoring rule posterior we prove a\nBernstein-von Misses type theorem. For policy learning, we propose expected\nThompson sampling (ETS) to learn the optimal policy by maximizing the expected\nvalue function with respect to the posterior distribution. This improves upon\ntraditional Thompson sampling (TS) and its extensions which utilize only one\nsample drawn from the posterior distribution. This improvement is studied both\ntheoretically and using simulation studies assuming discrete action and\nstate-space. Finally we successfully extend our setup for a challenging problem\nwith continuous action space without theoretical guarantees.\n","authors":["Shreya Sinha Roy","Richard G. Everitt","Christian P. Robert","Ritabrata Dutta"],"pdf_url":"https://arxiv.org/pdf/2412.11743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11739v1","updated":"2024-12-16T13:00:49Z","published":"2024-12-16T13:00:49Z","title":"Asymmetric Learning for Spectral Graph Neural Networks","summary":"  Optimizing spectral graph neural networks (GNNs) remains a critical challenge\nin the field, yet the underlying processes are not well understood. In this\npaper, we investigate the inherent differences between graph convolution\nparameters and feature transformation parameters in spectral GNNs and their\nimpact on the optimization landscape. Our analysis reveals that these\ndifferences contribute to a poorly conditioned problem, resulting in suboptimal\nperformance. To address this issue, we introduce the concept of the block\ncondition number of the Hessian matrix, which characterizes the difficulty of\npoorly conditioned problems in spectral GNN optimization. We then propose an\nasymmetric learning approach, dynamically preconditioning gradients during\ntraining to alleviate poorly conditioned problems. Theoretically, we\ndemonstrate that asymmetric learning can reduce block condition numbers,\nfacilitating easier optimization. Extensive experiments on eighteen benchmark\ndatasets show that asymmetric learning consistently improves the performance of\nspectral GNNs for both heterophilic and homophilic graphs. This improvement is\nespecially notable for heterophilic graphs, where the optimization process is\ngenerally more complex than for homophilic graphs. Code is available at\nhttps://github.com/Mia-321/asym-opt.git.\n","authors":["Fangbing Liu","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15815v2","updated":"2024-12-16T13:00:48Z","published":"2024-10-21T09:28:46Z","title":"Solvation Free Energies from Neural Thermodynamic Integration","summary":"  We present a method for computing free-energy differences using thermodynamic\nintegration with a neural network potential that interpolates between two\ntarget Hamiltonians. The interpolation is defined at the sample distribution\nlevel, and the neural network potential is optimized to match the corresponding\nequilibrium potential at every intermediate time-step. Once the interpolating\npotentials and samples are well-aligned, the free-energy difference can be\nestimated using (neural) thermodynamic integration. To target molecular\nsystems, we simultaneously couple Lennard-Jones and electrostatic interactions\nand model the rigid-body rotation of molecules. We report accurate results for\nseveral benchmark systems: a Lennard-Jones particle in a Lennard-Jones fluid,\nas well as the insertion of both water and methane solutes in a water solvent\nat atomistic resolution using a simple three-body neural-network potential.\n","authors":["Bálint Máté","François Fleuret","Tristan Bereau"],"pdf_url":"https://arxiv.org/pdf/2410.15815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11737v1","updated":"2024-12-16T12:58:21Z","published":"2024-12-16T12:58:21Z","title":"Efficiently Achieving Secure Model Training and Secure Aggregation to\n  Ensure Bidirectional Privacy-Preservation in Federated Learning","summary":"  Bidirectional privacy-preservation federated learning is crucial as both\nlocal gradients and the global model may leak privacy. However, only a few\nworks attempt to achieve it, and they often face challenges such as excessive\ncommunication and computational overheads, or significant degradation of model\naccuracy, which hinders their practical applications. In this paper, we design\nan efficient and high-accuracy bidirectional privacy-preserving scheme for\nfederated learning to complete secure model training and secure aggregation. To\nefficiently achieve bidirectional privacy, we design an efficient and\naccuracy-lossless model perturbation method on the server side (called\n$\\mathbf{MP\\_Server}$) that can be combined with local differential privacy\n(LDP) to prevent clients from accessing the model, while ensuring that the\nlocal gradients obtained on the server side satisfy LDP. Furthermore, to ensure\nmodel accuracy, we customize a distributed differential privacy mechanism on\nthe client side (called $\\mathbf{DDP\\_Client}$). When combined with\n$\\mathbf{MP\\_Server}$, it ensures LDP of the local gradients, while ensuring\nthat the aggregated result matches the accuracy of central differential privacy\n(CDP). Extensive experiments demonstrate that our scheme significantly\noutperforms state-of-the-art bidirectional privacy-preservation baselines\n(SOTAs) in terms of computational cost, model accuracy, and defense ability\nagainst privacy attacks. Particularly, given target accuracy, the training time\nof SOTAs is approximately $200$ times, or even over $1000$ times, longer than\nthat of our scheme. When the privacy budget is set relatively small, our scheme\nincurs less than $6\\%$ accuracy loss compared to the privacy-ignoring method,\nwhile SOTAs suffer up to $20\\%$ accuracy loss. Experimental results also show\nthat the defense capability of our scheme outperforms than SOTAs.\n","authors":["Xue Yang","Depan Peng","Yan Feng","Xiaohu Tang","Weijun Fang","Jun Shao"],"pdf_url":"https://arxiv.org/pdf/2412.11737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11171v2","updated":"2024-12-16T12:29:41Z","published":"2024-11-17T20:44:34Z","title":"LLäMmlein: Compact and Competitive German-Only Language Models from\n  Scratch","summary":"  We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.\n","authors":["Jan Pfister","Julia Wunderle","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2411.11171v2.pdf","comment":"second draft;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/"},{"id":"http://arxiv.org/abs/2412.11695v1","updated":"2024-12-16T12:15:16Z","published":"2024-12-16T12:15:16Z","title":"CiTrus: Squeezing Extra Performance out of Low-data Bio-signal Transfer\n  Learning","summary":"  Transfer learning for bio-signals has recently become an important technique\nto improve prediction performance on downstream tasks with small bio-signal\ndatasets. Recent works have shown that pre-training a neural network model on a\nlarge dataset (e.g. EEG) with a self-supervised task, replacing the\nself-supervised head with a linear classification head, and fine-tuning the\nmodel on different downstream bio-signal datasets (e.g., EMG or ECG) can\ndramatically improve the performance on those datasets. In this paper, we\npropose a new convolution-transformer hybrid model architecture with masked\nauto-encoding for low-data bio-signal transfer learning, introduce a\nfrequency-based masked auto-encoding task, employ a more comprehensive\nevaluation framework, and evaluate how much and when (multimodal) pre-training\nimproves fine-tuning performance. We also introduce a dramatically more\nperformant method of aligning a downstream dataset with a different temporal\nlength and sampling rate to the original pre-training dataset. Our findings\nindicate that the convolution-only part of our hybrid model can achieve\nstate-of-the-art performance on some low-data downstream tasks. The performance\nis often improved even further with our full model. In the case of\ntransformer-based models we find that pre-training especially improves\nperformance on downstream datasets, multimodal pre-training often increases\nthose gains further, and our frequency-based pre-training performs the best on\naverage for the lowest and highest data regimes.\n","authors":["Eloy Geenjaar","Lie Lu"],"pdf_url":"https://arxiv.org/pdf/2412.11695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11694v1","updated":"2024-12-16T12:12:45Z","published":"2024-12-16T12:12:45Z","title":"From Specific-MLLM to Omni-MLLM: A Survey about the MLLMs alligned with\n  Multi-Modality","summary":"  From the Specific-MLLM, which excels in single-modal tasks, to the Omni-MLLM,\nwhich extends the range of general modalities, this evolution aims to achieve\nunderstanding and generation of multimodal information. Omni-MLLM treats the\nfeatures of different modalities as different \"foreign languages,\" enabling\ncross-modal interaction and understanding within a unified space. To promote\nthe advancement of related research, we have compiled 47 relevant papers to\nprovide the community with a comprehensive introduction to Omni-MLLM. We first\nexplain the four core components of Omni-MLLM for unified modeling and\ninteraction of multiple modalities. Next, we introduce the effective\nintegration achieved through \"alignment pretraining\" and \"instruction\nfine-tuning,\" and discuss open-source datasets and testing of interaction\ncapabilities. Finally, we summarize the main challenges facing current\nOmni-MLLM and outline future directions.\n","authors":["Shixin Jiang","Jiafeng Liang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2412.11694v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2402.04902v5","updated":"2024-12-16T12:06:53Z","published":"2024-02-07T14:35:05Z","title":"L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models","summary":"  Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically apply post-training quantization\n(PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss.\nMeanwhile, this approach has limitations in recovering the accuracy loss. In\nthis paper, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA. By employing a memory-optimized layer design, L4Q\nsignificantly reduces QAT's memory overhead, making its training cost\ncomparable to LoRA, while preserving the advantage of QAT in producing fully\nquantized LLMs with high accuracy. Our experiments demonstrate that this\ncombined approach to quantization and fine-tuning achieves superior accuracy\ncompared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit\nquantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and\nMistral models with instructional datasets, we showcase L4Q's capabilities in\nlanguage tasks and few-shot learning.\n","authors":["Hyesung Jeon","Yulhwa Kim","Jae-joon Kim"],"pdf_url":"https://arxiv.org/pdf/2402.04902v5.pdf","comment":"8 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.11689v1","updated":"2024-12-16T12:02:12Z","published":"2024-12-16T12:02:12Z","title":"Just a Simple Transformation is Enough for Data Protection in Vertical\n  Federated Learning","summary":"  Vertical Federated Learning (VFL) aims to enable collaborative training of\ndeep learning models while maintaining privacy protection. However, the VFL\nprocedure still has components that are vulnerable to attacks by malicious\nparties. In our work, we consider feature reconstruction attacks, a common risk\ntargeting input data compromise. We theoretically claim that feature\nreconstruction attacks cannot succeed without knowledge of the prior\ndistribution on data. Consequently, we demonstrate that even simple model\narchitecture transformations can significantly impact the protection of input\ndata during VFL. Confirming these findings with experimental results, we show\nthat MLP-based models are resistant to state-of-the-art feature reconstruction\nattacks.\n","authors":["Andrei Semenov","Philip Zmushko","Alexander Pichugin","Aleksandr Beznosikov"],"pdf_url":"https://arxiv.org/pdf/2412.11689v1.pdf","comment":"29 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.11687v1","updated":"2024-12-16T12:01:08Z","published":"2024-12-16T12:01:08Z","title":"Dual Unscented Kalman Filter Architecture for Sensor Fusion in Water\n  Networks Leak Localization","summary":"  Leakage in water systems results in significant daily water losses, degrading\nservice quality, increasing costs, and aggravating environmental problems. Most\nleak localization methods rely solely on pressure data, missing valuable\ninformation from other sensor types. This article proposes a hydraulic state\nestimation methodology based on a dual Unscented Kalman Filter (UKF) approach,\nwhich enhances the estimation of both nodal hydraulic heads, critical in\nlocalization tasks, and pipe flows, useful for operational purposes. The\napproach enables the fusion of different sensor types, such as pressure, flow\nand demand meters. The strategy is evaluated in well-known open source case\nstudies, namely Modena and L-TOWN, showing improvements over other\nstate-of-the-art estimation approaches in terms of interpolation accuracy, as\nwell as more precise leak localization performance in L-TOWN.\n","authors":["Luis Romero-Ben","Paul Irofti","Florin Stoican","Vicenç Puig"],"pdf_url":"https://arxiv.org/pdf/2412.11687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10272v2","updated":"2024-12-16T12:00:34Z","published":"2024-11-15T15:28:42Z","title":"P$^2$ Law: Scaling Law for Post-Training After Model Pruning","summary":"  Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs.\n","authors":["Xiaodong Chen","Yuxuan Hu","Xiaokang Zhang","Yanling Wang","Cuiping Li","Hong Chen","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.10272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11683v1","updated":"2024-12-16T11:50:30Z","published":"2024-12-16T11:50:30Z","title":"Multimodal LLM for Intelligent Transportation Systems","summary":"  In the evolving landscape of transportation systems, integrating Large\nLanguage Models (LLMs) offers a promising frontier for advancing intelligent\ndecision-making across various applications. This paper introduces a novel\n3-dimensional framework that encapsulates the intersection of applications,\nmachine learning methodologies, and hardware devices, particularly emphasizing\nthe role of LLMs. Instead of using multiple machine learning algorithms, our\nframework uses a single, data-centric LLM architecture that can analyze time\nseries, images, and videos. We explore how LLMs can enhance data interpretation\nand decision-making in transportation. We apply this LLM framework to different\nsensor datasets, including time-series data and visual data from sources like\nOxford Radar RobotCar, D-Behavior (D-Set), nuScenes by Motional, and Comma2k19.\nThe goal is to streamline data processing workflows, reduce the complexity of\ndeploying multiple models, and make intelligent transportation systems more\nefficient and accurate. The study was conducted using state-of-the-art\nhardware, leveraging the computational power of AMD RTX 3060 GPUs and Intel\ni9-12900 processors. The experimental results demonstrate that our framework\nachieves an average accuracy of 91.33\\% across these datasets, with the highest\naccuracy observed in time-series data (92.7\\%), showcasing the model's\nproficiency in handling sequential information essential for tasks such as\nmotion planning and predictive maintenance. Through our exploration, we\ndemonstrate the versatility and efficacy of LLMs in handling multimodal data\nwithin the transportation sector, ultimately providing insights into their\napplication in real-world scenarios. Our findings align with the broader\nconference themes, highlighting the transformative potential of LLMs in\nadvancing transportation technologies.\n","authors":["Dexter Le","Aybars Yunusoglu","Karn Tiwari","Murat Isik","I. Can Dikmen"],"pdf_url":"https://arxiv.org/pdf/2412.11683v1.pdf","comment":"Accepted at IEEE Symposium Series on Computational Intelligence\n  (SSCI) 2025"},{"id":"http://arxiv.org/abs/2412.11682v1","updated":"2024-12-16T11:49:12Z","published":"2024-12-16T11:49:12Z","title":"NEST: A Neuromodulated Small-world Hypergraph Trajectory Prediction\n  Model for Autonomous Driving","summary":"  Accurate trajectory prediction is essential for the safety and efficiency of\nautonomous driving. Traditional models often struggle with real-time\nprocessing, capturing non-linearity and uncertainty in traffic environments,\nefficiency in dense traffic, and modeling temporal dynamics of interactions. We\nintroduce NEST (Neuromodulated Small-world Hypergraph Trajectory Prediction), a\nnovel framework that integrates Small-world Networks and hypergraphs for\nsuperior interaction modeling and prediction accuracy. This integration enables\nthe capture of both local and extended vehicle interactions, while the\nNeuromodulator component adapts dynamically to changing traffic conditions. We\nvalidate the NEST model on several real-world datasets, including nuScenes,\nMoCAD, and HighD. The results consistently demonstrate that NEST outperforms\nexisting methods in various traffic scenarios, showcasing its exceptional\ngeneralization capability, efficiency, and temporal foresight. Our\ncomprehensive evaluation illustrates that NEST significantly improves the\nreliability and operational efficiency of autonomous driving systems, making it\na robust solution for trajectory prediction in complex traffic environments.\n","authors":["Chengyue Wang","Haicheng Liao","Bonan Wang","Yanchen Guan","Bin Rao","Ziyuan Pu","Zhiyong Cui","Chengzhong Xu","Zhenning Li"],"pdf_url":"https://arxiv.org/pdf/2412.11682v1.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2412.09800v2","updated":"2024-12-16T11:48:07Z","published":"2024-12-13T02:39:04Z","title":"Infinite-dimensional next-generation reservoir computing","summary":"  Next-generation reservoir computing (NG-RC) has attracted much attention due\nto its excellent performance in spatio-temporal forecasting of complex systems\nand its ease of implementation. This paper shows that NG-RC can be encoded as a\nkernel ridge regression that makes training efficient and feasible even when\nthe space of chosen polynomial features is very large. Additionally, an\nextension to an infinite number of covariates is possible, which makes the\nmethodology agnostic with respect to the lags into the past that are considered\nas explanatory factors, as well as with respect to the number of polynomial\ncovariates, an important hyperparameter in traditional NG-RC. We show that this\napproach has solid theoretical backing and good behavior based on kernel\nuniversality properties previously established in the literature. Various\nnumerical illustrations show that these generalizations of NG-RC outperform the\ntraditional approach in several forecasting applications.\n","authors":["Lyudmila Grigoryeva","Hannah Lim Jing Ting","Juan-Pablo Ortega"],"pdf_url":"https://arxiv.org/pdf/2412.09800v2.pdf","comment":"13 pages, 2 figures, 3 tables; corrected typos, added github link,\n  added acknowledgments"},{"id":"http://arxiv.org/abs/2412.11674v1","updated":"2024-12-16T11:27:35Z","published":"2024-12-16T11:27:35Z","title":"UA-PDFL: A Personalized Approach for Decentralized Federated Learning","summary":"  Federated learning (FL) is a privacy preserving machine learning paradigm\ndesigned to collaboratively learn a global model without data leakage.\nSpecifically, in a typical FL system, the central server solely functions as an\ncoordinator to iteratively aggregate the collected local models trained by each\nclient, potentially introducing single-point transmission bottleneck and\nsecurity threats. To mitigate this issue, decentralized federated learning\n(DFL) has been proposed, where all participating clients engage in peer-to-peer\ncommunication without a central server. Nonetheless, DFL still suffers from\ntraining degradation as FL does due to the non-independent and identically\ndistributed (non-IID) nature of client data. And incorporating personalization\nlayers into DFL may be the most effective solutions to alleviate the side\neffects caused by non-IID data. Therefore, in this paper, we propose a novel\nunit representation aided personalized decentralized federated learning\nframework, named UA-PDFL, to deal with the non-IID challenge in DFL. By\nadaptively adjusting the level of personalization layers through the guidance\nof the unit representation, UA-PDFL is able to address the varying degrees of\ndata skew. Based on this scheme, client-wise dropout and layer-wise\npersonalization are proposed to further enhance the learning performance of\nDFL. Extensive experiments empirically prove the effectiveness of our proposed\nmethod.\n","authors":["Hangyu Zhu","Yuxiang Fan","Zhenping Xie"],"pdf_url":"https://arxiv.org/pdf/2412.11674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08707v7","updated":"2024-12-16T11:21:30Z","published":"2024-04-11T17:44:56Z","title":"CEM: A Data-Efficient Method for Large Language Models to Continue\n  Evolving From Mistakes","summary":"  As world knowledge advances and new task schemas emerge, Continual Learning\n(CL) becomes essential for keeping Large Language Models (LLMs) current and\naddressing their shortcomings. This process typically involves continual\ninstruction tuning (CIT) and continual pre-training (CPT) to enable these\nmodels to adapt to novel tasks and acquire critical knowledge. However,\ncollecting sufficient CPT data and efficiently bridging knowledge gaps remain\nsignificant challenges. Inspired by the 'summarizing mistakes' strategy, we\npropose the Continue Evolving from Mistakes (CEM) method, a data-efficient\napproach aiming to collect CPT data and continually improve LLMs' performance\nthrough iterative evaluation and supplementation with mistake-relevant\nknowledge. To further optimize data usage and mitigate forgetting, we introduce\na novel training paradigm that combines CIT and CPT. Experiments show that CEM\nsubstantially enhances multiple models' performance on both in-domain and\nout-of-domain QA tasks, achieving gains of up to 29.63%. Code and datasets are\navailable on https://anonymous.4open.science/r/cem-BB25.\n","authors":["Haokun Zhao","Haixia Han","Jie Shi","Chengyu Du","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.08707v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11664v1","updated":"2024-12-16T11:12:45Z","published":"2024-12-16T11:12:45Z","title":"C3oT: Generating Shorter Chain-of-Thought without Compromising\n  Effectiveness","summary":"  Generating Chain-of-Thought (CoT) before deriving the answer can effectively\nimprove the reasoning capabilities of large language models (LLMs) and\nsignificantly improve the accuracy of the generated answer. However, in most\ncases, the length of the generated CoT is much longer than the desired final\nanswer, which results in additional decoding costs. Furthermore, existing\nresearch has discovered that shortening the reasoning steps in CoT, even while\npreserving the key information, diminishes LLMs' abilities. These phenomena\nmake it difficult to use LLMs and CoT in many real-world applications that only\nrequire the final answer and are sensitive to latency, such as search and\nrecommendation. To reduce the costs of model decoding and shorten the length of\nthe generated CoT, this paper presents $\\textbf{C}$onditioned\n$\\textbf{C}$ompressed $\\textbf{C}$hain-of-$\\textbf{T}$hought (C3oT), a CoT\ncompression framework that involves a compressor to compress an original longer\nCoT into a shorter CoT while maintaining key information and interpretability,\na conditioned training method to train LLMs with both longer CoT and shorter\nCoT simultaneously to learn the corresponding relationships between them, and a\nconditioned inference method to gain the reasoning ability learned from longer\nCoT by generating shorter CoT. We conduct experiments over four datasets from\narithmetic and commonsense scenarios, showing that the proposed method is\ncapable of compressing the length of generated CoT by up to more than 50%\nwithout compromising its effectiveness.\n","authors":["Yu Kang","Xianghui Sun","Liangyu Chen","Wei Zou"],"pdf_url":"https://arxiv.org/pdf/2412.11664v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2409.19377v2","updated":"2024-12-16T11:09:58Z","published":"2024-09-28T15:03:49Z","title":"Interpretable, multi-dimensional Evaluation Framework for Causal\n  Discovery from observational i.i.d. Data","summary":"  Nonlinear causal discovery from observational data imposes strict\nidentifiability assumptions on the formulation of structural equations utilized\nin the data generating process. The evaluation of structure learning methods\nunder assumption violations requires a rigorous and interpretable approach,\nwhich quantifies both the structural similarity of the estimation with the\nground truth and the capacity of the discovered graphs to be used for causal\ninference. Motivated by the lack of unified performance assessment framework,\nwe introduce an interpretable, six-dimensional evaluation metric, i.e.,\ndistance to optimal solution (DOS), which is specifically tailored to the field\nof causal discovery. Furthermore, this is the first research to assess the\nperformance of structure learning algorithms from seven different families on\nincreasing percentage of non-identifiable, nonlinear causal patterns, inspired\nby real-world processes. Our large-scale simulation study, which incorporates\nseven experimental factors, shows that besides causal order-based methods,\namortized causal discovery delivers results with comparatively high proximity\nto the optimal solution.\n","authors":["Georg Velev","Stefan Lessmann"],"pdf_url":"https://arxiv.org/pdf/2409.19377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15795v3","updated":"2024-12-16T11:06:00Z","published":"2024-11-24T11:46:47Z","title":"Beyond adaptive gradient: Fast-Controlled Minibatch Algorithm for\n  large-scale optimization","summary":"  Adaptive gradient methods have been increasingly adopted by deep learning\ncommunity due to their fast convergence and reduced sensitivity to\nhyper-parameters. However, these methods come with limitations, such as\nincreased memory requirements for elements like moving averages and a poorly\nunderstood convergence theory. To overcome these challenges, we introduce\nF-CMA, a Fast-Controlled Mini-batch Algorithm with a random reshuffling method\nfeaturing a sufficient decrease condition and a line-search procedure to ensure\nloss reduction per epoch, along with its deterministic proof of global\nconvergence to a stationary point. To evaluate the F-CMA, we integrate it into\nconventional training protocols for classification tasks involving both\nconvolutional neural networks and vision transformer models, allowing for a\ndirect comparison with popular optimizers. Computational tests show significant\nimprovements, including a decrease in the overall training time by up to 68%,\nan increase in per-epoch efficiency by up to 20%, and in model accuracy by up\nto 5%.\n","authors":["Corrado Coppola","Lorenzo Papa","Irene Amerini","Laura Palagi"],"pdf_url":"https://arxiv.org/pdf/2411.15795v3.pdf","comment":"There is an error in the literature review, in section 1. In\n  particular, we noticed that there is a wrong citation, the [65], which has\n  been erroneously associated with another author's claims"},{"id":"http://arxiv.org/abs/2412.11660v1","updated":"2024-12-16T11:02:38Z","published":"2024-12-16T11:02:38Z","title":"Non-Convex Optimization in Federated Learning via Variance Reduction and\n  Adaptive Learning","summary":"  This paper proposes a novel federated algorithm that leverages momentum-based\nvariance reduction with adaptive learning to address non-convex settings across\nheterogeneous data. We intend to minimize communication and computation\noverhead, thereby fostering a sustainable federated learning system. We aim to\novercome challenges related to gradient variance, which hinders the model's\nefficiency, and the slow convergence resulting from learning rate adjustments\nwith heterogeneous data. The experimental results on the image classification\ntasks with heterogeneous data reveal the effectiveness of our suggested\nalgorithms in non-convex settings with an improved communication complexity of\n$\\mathcal{O}(\\epsilon^{-1})$ to converge to an $\\epsilon$-stationary point -\ncompared to the existing communication complexity $\\mathcal{O}(\\epsilon^{-2})$\nof most prior works. The proposed federated version maintains the trade-off\nbetween the convergence rate, number of communication rounds, and test accuracy\nwhile mitigating the client drift in heterogeneous settings. The experimental\nresults demonstrate the efficiency of our algorithms in image classification\ntasks (MNIST, CIFAR-10) with heterogeneous data.\n","authors":["Dipanwita Thakur","Antonella Guzzo","Giancarlo Fortino","Sajal K. Das"],"pdf_url":"https://arxiv.org/pdf/2412.11660v1.pdf","comment":"FLUID Workshop@AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11657v1","updated":"2024-12-16T11:00:02Z","published":"2024-12-16T11:00:02Z","title":"CNNtention: Can CNNs do better with Attention?","summary":"  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n","authors":["Julian Glattki","Nikhil Kapila","Tejas Rathi"],"pdf_url":"https://arxiv.org/pdf/2412.11657v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.11656v1","updated":"2024-12-16T10:59:49Z","published":"2024-12-16T10:59:49Z","title":"Private Yet Social: How LLM Chatbots Support and Challenge Eating\n  Disorder Recovery","summary":"  Eating disorders (ED) are complex mental health conditions that require\nlong-term management and support. Recent advancements in large language model\n(LLM)-based chatbots offer the potential to assist individuals in receiving\nimmediate support. Yet, concerns remain about their reliability and safety in\nsensitive contexts such as ED. We explore the opportunities and potential harms\nof using LLM-based chatbots for ED recovery. We observe the interactions\nbetween 26 participants with ED and an LLM-based chatbot, WellnessBot, designed\nto support ED recovery, over 10 days. We discovered that our participants have\nfelt empowered in recovery by discussing ED-related stories with the chatbot,\nwhich served as a personal yet social avenue. However, we also identified\nharmful chatbot responses, especially concerning individuals with ED, that went\nunnoticed partly due to participants' unquestioning trust in the chatbot's\nreliability. Based on these findings, we provide design implications for safe\nand effective LLM-based interventions in ED management.\n","authors":["Ryuhaerang Choi","Taehan Kim","Subin Park","Jennifer G Kim","Sung-Ju Lee"],"pdf_url":"https://arxiv.org/pdf/2412.11656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11654v1","updated":"2024-12-16T10:56:58Z","published":"2024-12-16T10:56:58Z","title":"Smoothness Really Matters: A Simple yet Effective Approach for\n  Unsupervised Graph Domain Adaptation","summary":"  Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution\nshifts between domains by transferring knowledge from labeled source graphs to\ngiven unlabeled target graphs. Existing UGDA methods primarily focus on\naligning features in the latent space learned by graph neural networks (GNNs)\nacross domains, often overlooking structural shifts, resulting in limited\neffectiveness when addressing structurally complex transfer scenarios. Given\nthe sensitivity of GNNs to local structural features, even slight discrepancies\nbetween source and target graphs could lead to significant shifts in node\nembeddings, thereby reducing the effectiveness of knowledge transfer. To\naddress this issue, we introduce a novel approach for UGDA called Target-Domain\nStructural Smoothing (TDSS). TDSS is a simple and effective method designed to\nperform structural smoothing directly on the target graph, thereby mitigating\nstructural distribution shifts and ensuring the consistency of node\nrepresentations. Specifically, by integrating smoothing techniques with\nneighborhood sampling, TDSS maintains the structural coherence of the target\ngraph while mitigating the risk of over-smoothing. Our theoretical analysis\nshows that TDSS effectively reduces target risk by improving model smoothness.\nEmpirical results on three real-world datasets demonstrate that TDSS\noutperforms recent state-of-the-art baselines, achieving significant\nimprovements across six transfer scenarios. The code is available in\nhttps://github.com/cwei01/TDSS.\n","authors":["Wei Chen","Guo Ye","Yakun Wang","Zhao Zhang","Libang Zhang","Daxin Wang","Zhiqiang Zhang","Fuzhen Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.11654v1.pdf","comment":"11 pages, Accpected by AAAI2025"},{"id":"http://arxiv.org/abs/2412.11646v1","updated":"2024-12-16T10:47:05Z","published":"2024-12-16T10:47:05Z","title":"BA-BFL: Barycentric Aggregation for Bayesian Federated Learning","summary":"  In this work, we study the problem of aggregation in the context of Bayesian\nFederated Learning (BFL). Using an information geometric perspective, we\ninterpret the BFL aggregation step as finding the barycenter of the trained\nposteriors for a pre-specified divergence metric. We study the barycenter\nproblem for the parametric family of $\\alpha$-divergences and, focusing on the\nstandard case of independent and Gaussian distributed parameters, we recover\nthe closed-form solution of the reverse Kullback-Leibler barycenter and develop\nthe analytical form of the squared Wasserstein-2 barycenter. Considering a\nnon-IID setup, where clients possess heterogeneous data, we analyze the\nperformance of the developed algorithms against state-of-the-art (SOTA)\nBayesian aggregation methods in terms of accuracy, uncertainty quantification\n(UQ), model calibration (MC), and fairness. Finally, we extend our analysis to\nthe framework of Hybrid Bayesian Deep Learning (HBDL), where we study how the\nnumber of Bayesian layers in the architecture impacts the considered\nperformance metrics. Our experimental results show that the proposed\nmethodology presents comparable performance with the SOTA while offering a\ngeometric interpretation of the aggregation phase.\n","authors":["Nour Jamoussi","Giuseppe Serra","Photios A. Stavrou","Marios Kountouris"],"pdf_url":"https://arxiv.org/pdf/2412.11646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v2","updated":"2024-12-16T10:28:02Z","published":"2024-10-11T15:50:31Z","title":"HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable\n  Deep Learning","summary":"  Prototype Learning methods provide an interpretable alternative to black-box\ndeep learning models. Approaches such as ProtoPNet learn, which part of a test\nimage \"look like\" known prototypical parts from training images, combining\npredictive power with the inherent interpretability of case-based reasoning.\nHowever, existing approaches have two main drawbacks: A) They rely solely on\ndeterministic similarity scores without statistical confidence. B) The\nprototypes are learned in a black-box manner without human input. This work\nintroduces HyperPg, a new prototype representation leveraging Gaussian\ndistributions on a hypersphere in latent space, with learnable mean and\nvariance. HyperPg prototypes adapt to the spread of clusters in the latent\nspace and output likelihood scores. The new architecture, HyperPgNet, leverages\nHyperPg to learn prototypes aligned with human concepts from pixel-level\nannotations. Consequently, each prototype represents a specific concept such as\ncolor, image texture, or part of the image subject. A concept extraction\npipeline built on foundation models provides pixel-level annotations,\nsignificantly reducing human labeling effort. Experiments on CUB-200-2011 and\nStanford Cars datasets demonstrate that HyperPgNet outperforms other prototype\nlearning architectures while using fewer parameters and training steps.\nAdditionally, the concept-aligned HyperPg prototypes are learned transparently,\nenhancing model interpretability.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11631v1","updated":"2024-12-16T10:16:54Z","published":"2024-12-16T10:16:54Z","title":"A Mapper Algorithm with implicit intervals and its optimization","summary":"  The Mapper algorithm is an essential tool for visualizing complex, high\ndimensional data in topology data analysis (TDA) and has been widely used in\nbiomedical research. It outputs a combinatorial graph whose structure implies\nthe shape of the data. However,the need for manual parameter tuning and fixed\nintervals, along with fixed overlapping ratios may impede the performance of\nthe standard Mapper algorithm. Variants of the standard Mapper algorithms have\nbeen developed to address these limitations, yet most of them still require\nmanual tuning of parameters. Additionally, many of these variants, including\nthe standard version found in the literature, were built within a deterministic\nframework and overlooked the uncertainty inherent in the data. To relax these\nlimitations, in this work, we introduce a novel framework that implicitly\nrepresents intervals through a hidden assignment matrix, enabling automatic\nparameter optimization via stochastic gradient descent. In this work, we\ndevelop a soft Mapper framework based on a Gaussian mixture model(GMM) for\nflexible and implicit interval construction. We further illustrate the\nrobustness of the soft Mapper algorithm by introducing the Mapper graph mode as\na point estimation for the output graph. Moreover, a stochastic gradient\ndescent algorithm with a specific topological loss function is proposed for\noptimizing parameters in the model. Both simulation and application studies\ndemonstrate its effectiveness in capturing the underlying topological\nstructures. In addition, the application to an RNA expression dataset obtained\nfrom the Mount Sinai/JJ Peters VA Medical Center Brain Bank (MSBB) successfully\nidentifies a distinct subgroup of Alzheimer's Disease.\n","authors":["Yuyang Tao","Shufei Ge"],"pdf_url":"https://arxiv.org/pdf/2412.11631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11629v1","updated":"2024-12-16T10:14:01Z","published":"2024-12-16T10:14:01Z","title":"QPruner: Probabilistic Decision Quantization for Structured Pruning in\n  Large Language Models","summary":"  The rise of large language models (LLMs) has significantly advanced various\nnatural language processing (NLP) tasks. However, the resource demands of these\nmodels pose substantial challenges. Structured pruning is an effective approach\nto reducing model size, but it often results in significant accuracy\ndegradation, necessitating parameter updates to adapt. Unfortunately, such\nfine-tuning requires substantial memory, which limits its applicability. To\naddress these challenges, we introduce quantization into the structured pruning\nframework to reduce memory consumption during both fine-tuning and inference.\nHowever, the combined errors from pruning and quantization increase the\ndifficulty of fine-tuning, requiring a more refined quantization scheme. To\nthis end, we propose QPruner, a novel framework that employs structured pruning\nto reduce model size, followed by a layer-wise mixed-precision quantization\nscheme. Quantization precisions are assigned to each layer based on their\nimportance to the target task, and Bayesian optimization is employed to refine\nprecision allocation strategies, ensuring a balance between model accuracy and\nmemory efficiency. Extensive experiments on benchmark datasets demonstrate that\nQPruner significantly outperforms existing methods in memory savings while\nmaintaining or improving model performance.\n","authors":["Changhai Zhou","Yuhua Zhou","Shijie Han","Qian Qiao","Hongguang Li"],"pdf_url":"https://arxiv.org/pdf/2412.11629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05092v2","updated":"2024-12-16T10:10:10Z","published":"2024-08-09T14:33:34Z","title":"PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural\n  Networks","summary":"  The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., facial or\nmedical images. In this work, we propose a method to perform the training phase\nof a deep learning model on both an edge device and a cloud server that\nprevents sensitive content being transmitted to the cloud while retaining the\ndesired information. The proposed privacy-preserving method uses adversarial\nearly exits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial and medical datasets with\ndiverse attributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box, deep and\nGAN-based reconstruction attacks. This approach is designed for\nresource-constrained edge devices, ensuring minimal memory usage and\ncomputational overhead.\n","authors":["Yamin Sepehri","Pedram Pad","Pascal Frossard","L. Andrea Dunbar"],"pdf_url":"https://arxiv.org/pdf/2408.05092v2.pdf","comment":"21 pages, 19 figures, 11 tables"},{"id":"http://arxiv.org/abs/2412.06926v2","updated":"2024-12-16T10:08:19Z","published":"2024-12-09T19:11:54Z","title":"When Every Token Counts: Optimal Segmentation for Low-Resource Language\n  Models","summary":"  Traditional greedy tokenization methods have been a critical step in Natural\nLanguage Processing (NLP), influencing how text is converted into tokens and\ndirectly impacting model performance. While subword tokenizers like Byte-Pair\nEncoding (BPE) are widely used, questions remain about their optimality across\nmodel scales and languages. In this work, we demonstrate through extensive\nexperiments that an optimal BPE configuration significantly reduces token count\ncompared to greedy segmentation, yielding improvements in token-saving\npercentages and performance benefits, particularly for smaller models. We\nevaluate tokenization performance across various intrinsic and extrinsic tasks,\nincluding generation and classification. Our findings suggest that\ncompression-optimized tokenization strategies could provide substantial\nadvantages for multilingual and low-resource language applications,\nhighlighting a promising direction for further research and inclusive NLP.\n","authors":["Bharath Raj S","Garvit Suri","Vikrant Dewangan","Raghav Sonavane"],"pdf_url":"https://arxiv.org/pdf/2412.06926v2.pdf","comment":"LoResLM @ COLING 2025"},{"id":"http://arxiv.org/abs/2308.14930v2","updated":"2024-12-16T10:04:38Z","published":"2023-08-28T23:08:32Z","title":"Application of Quantum Pre-Processing Filter for Binary Image\n  Classification with Small Samples","summary":"  Over the past few years, there has been significant interest in Quantum\nMachine Learning (QML) among researchers, as it has the potential to transform\nthe field of machine learning. Several models that exploit the properties of\nquantum mechanics have been developed for practical applications. In this\nstudy, we investigated the application of our previously proposed quantum\npre-processing filter (QPF) to binary image classification. We evaluated the\nQPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits\nand alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic\nsign images). Similar to our previous multi-class classification results, the\napplication of QPF improved the binary image classification accuracy using\nneural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8%\nto 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from\n93.5% to 92.0%. We then applied QPF in cases using a smaller number of training\nand testing samples, i.e. 80 and 20 samples per class, respectively. In order\nto derive statistically stable results, we conducted the experiment with 100\ntrials choosing randomly different training and testing samples and averaging\nthe results. The result showed that the application of QPF did not improve the\nimage classification accuracy against MNIST and EMNIST but improved it against\nCIFAR-10 and GTSRB from 65.8% to 67.2% and 90.5% to 91.8%, respectively.\nFurther research will be conducted as part of future work to investigate the\npotential of QPF to assess the scalability of the proposed approach to larger\nand complex datasets.\n","authors":["Farina Riaz","Shahab Abdulla","Hajime Suzuki","Srinjoy Ganguly","Ravinesh C. Deo","Susan Hopkins"],"pdf_url":"https://arxiv.org/pdf/2308.14930v2.pdf","comment":"This paper is accepted by Journal of Data Science and Intelligent\n  Systems (JDSIS)"},{"id":"http://arxiv.org/abs/2412.11618v1","updated":"2024-12-16T10:01:33Z","published":"2024-12-16T10:01:33Z","title":"EvoLlama: Enhancing LLMs' Understanding of Proteins via Multimodal\n  Structure and Sequence Representations","summary":"  Current Large Language Models (LLMs) for understanding proteins primarily\ntreats amino acid sequences as a text modality. Meanwhile, Protein Language\nModels (PLMs), such as ESM-2, have learned massive sequential evolutionary\nknowledge from the universe of natural protein sequences. Furthermore,\nstructure-based encoders like ProteinMPNN learn the structural information of\nproteins through Graph Neural Networks. However, whether the incorporation of\nprotein encoders can enhance the protein understanding of LLMs has not been\nexplored. To bridge this gap, we propose EvoLlama, a multimodal framework that\nconnects a structure-based encoder, a sequence-based protein encoder and an LLM\nfor protein understanding. EvoLlama consists of a ProteinMPNN structure\nencoder, an ESM-2 protein sequence encoder, a multimodal projector to align\nprotein and text representations and a Llama-3 text decoder. To train EvoLlama,\nwe fine-tune it on protein-oriented instructions and protein property\nprediction datasets verbalized via natural language instruction templates. Our\nexperiments show that EvoLlama's protein understanding capabilities have been\nsignificantly enhanced, outperforming other fine-tuned protein-oriented LLMs in\nzero-shot settings by an average of 1%-8% and surpassing the state-of-the-art\nbaseline with supervised fine-tuning by an average of 6%. On protein property\nprediction datasets, our approach achieves promising results that are\ncompetitive with state-of-the-art task-specific baselines. We will release our\ncode in a future version.\n","authors":["Nuowei Liu","Changzhi Sun","Tao Ji","Junfeng Tian","Jianxin Tang","Yuanbin Wu","Man Lan"],"pdf_url":"https://arxiv.org/pdf/2412.11618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08221v2","updated":"2024-12-16T09:54:46Z","published":"2024-12-11T09:17:39Z","title":"Generate Any Scene: Evaluating and Improving Text-to-Vision Generation\n  with Scene Graph Programming","summary":"  DALL-E and Sora have gained attention by producing implausible images, such\nas \"astronauts riding a horse in space.\" Despite the proliferation of\ntext-to-vision models that have inundated the internet with synthetic visuals,\nfrom images to 3D assets, current benchmarks predominantly evaluate these\nmodels on real-world scenes paired with captions. We introduce Generate Any\nScene, a framework that systematically enumerates scene graphs representing a\nvast array of visual scenes, spanning realistic to imaginative compositions.\nGenerate Any Scene leverages 'scene graph programming', a method for\ndynamically constructing scene graphs of varying complexity from a structured\ntaxonomy of visual elements. This taxonomy includes numerous objects,\nattributes, and relations, enabling the synthesis of an almost infinite variety\nof scene graphs. Using these structured representations, Generate Any Scene\ntranslates each scene graph into a caption, enabling scalable evaluation of\ntext-to-vision models through standard metrics. We conduct extensive\nevaluations across multiple text-to-image, text-to-video, and text-to-3D\nmodels, presenting key findings on model performance. We find that DiT-backbone\ntext-to-image models align more closely with input captions than UNet-backbone\nmodels. Text-to-video models struggle with balancing dynamics and consistency,\nwhile both text-to-video and text-to-3D models show notable gaps in human\npreference alignment. We demonstrate the effectiveness of Generate Any Scene by\nconducting three practical applications leveraging captions generated by\nGenerate Any Scene: 1) a self-improving framework where models iteratively\nenhance their performance using generated data, 2) a distillation process to\ntransfer specific strengths from proprietary models to open-source\ncounterparts, and 3) improvements in content moderation by identifying and\ngenerating challenging synthetic data.\n","authors":["Ziqi Gao","Weikai Huang","Jieyu Zhang","Aniruddha Kembhavi","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2412.08221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11608v1","updated":"2024-12-16T09:49:59Z","published":"2024-12-16T09:49:59Z","title":"Towards Adversarial Robustness of Model-Level Mixture-of-Experts\n  Architectures for Semantic Segmentation","summary":"  Vulnerability to adversarial attacks is a well-known deficiency of deep\nneural networks. Larger networks are generally more robust, and ensembling is\none method to increase adversarial robustness: each model's weaknesses are\ncompensated by the strengths of others. While an ensemble uses a deterministic\nrule to combine model outputs, a mixture of experts (MoE) includes an\nadditional learnable gating component that predicts weights for the outputs of\nthe expert models, thus determining their contributions to the final\nprediction. MoEs have been shown to outperform ensembles on specific tasks, yet\ntheir susceptibility to adversarial attacks has not been studied yet. In this\nwork, we evaluate the adversarial vulnerability of MoEs for semantic\nsegmentation of urban and highway traffic scenes. We show that MoEs are, in\nmost cases, more robust to per-instance and universal white-box adversarial\nattacks and can better withstand transfer attacks. Our code is available at\n\\url{https://github.com/KASTEL-MobilityLab/mixtures-of-experts/}.\n","authors":["Svetlana Pavlitska","Enrico Eisen","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2412.11608v1.pdf","comment":"Accepted for publication at ICMLA 2024"},{"id":"http://arxiv.org/abs/2412.11605v1","updated":"2024-12-16T09:47:43Z","published":"2024-12-16T09:47:43Z","title":"SPaR: Self-Play with Tree-Search Refinement to Improve\n  Instruction-Following in Large Language Models","summary":"  Instruction-following is a fundamental capability of language models,\nrequiring the model to recognize even the most subtle requirements in the\ninstructions and accurately reflect them in its output. Such an ability is\nwell-suited for and often optimized by preference learning. However, existing\nmethods often directly sample multiple independent responses from the model\nwhen creating preference pairs. Such practice can introduce content variations\nirrelevant to whether the instruction is precisely followed (e.g., different\nexpressions about the same semantic), interfering with the goal of teaching\nmodels to recognize the key differences that lead to improved instruction\nfollowing. In light of this, we introduce SPaR, a self-play framework\nintegrating tree-search self-refinement to yield valid and comparable\npreference pairs free from distractions. By playing against itself, an LLM\nemploys a tree-search strategy to refine its previous responses with respect to\nthe instruction while minimizing unnecessary variations. Our experiments show\nthat a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses\nGPT-4-Turbo on the IFEval benchmark without losing general capabilities.\nFurthermore, SPaR demonstrates promising scalability and transferability,\ngreatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how\ninference scaling in tree search would impact model performance. Our code and\ndata are publicly available at https://github.com/thu-coai/SPaR.\n","authors":["Jiale Cheng","Xiao Liu","Cunxiang Wang","Xiaotao Gu","Yida Lu","Dan Zhang","Yuxiao Dong","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01124v5","updated":"2024-12-16T09:37:17Z","published":"2024-05-02T09:38:07Z","title":"Investigating Self-Supervised Image Denoising with Denaturation","summary":"  Self-supervised learning for image denoising problems in the presence of\ndenaturation for noisy data is a crucial approach in machine learning. However,\ntheoretical understanding of the performance of the approach that uses\ndenatured data is lacking. To provide better understanding of the approach, in\nthis paper, we analyze a self-supervised denoising algorithm that uses\ndenatured data in depth through theoretical analysis and numerical experiments.\nThrough the theoretical analysis, we discuss that the algorithm finds desired\nsolutions to the optimization problem with the population risk, while the\nguarantee for the empirical risk depends on the hardness of the denoising task\nin terms of denaturation levels. We also conduct several experiments to\ninvestigate the performance of an extended algorithm in practice. The results\nindicate that the algorithm training with denatured images works, and the\nempirical performance aligns with the theoretical results. These results\nsuggest several insights for further improvement of self-supervised image\ndenoising that uses denatured data in future directions.\n","authors":["Hiroki Waida","Kimihiro Yamazaki","Atsushi Tokuhisa","Mutsuyo Wada","Yuichiro Wada"],"pdf_url":"https://arxiv.org/pdf/2405.01124v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16192v3","updated":"2024-12-16T09:23:47Z","published":"2023-12-22T13:17:00Z","title":"A Method for Auto-Differentiation of the Voronoi Tessellation","summary":"  Voronoi tessellation, also known as Voronoi diagram, is an important\ncomputational geometry technique that has applications in various scientific\ndisciplines. It involves dividing a given space into regions based on the\nproximity to a set of points. Autodifferentiation is a powerful tool for\nsolving optimization tasks. Autodifferentiation assumes constructing a\ncomputational graph that allows to compute gradients using backpropagation\nalgorithm. However, often the Voronoi tessellation remains the only\nnon-differentiable part of a pipeline, prohibiting end-to-end differentiation.\nWe present the method for autodifferentiation of the 2D Voronoi tessellation.\nThe method allows one to construct the Voronoi tessellation and pass gradients,\nmaking the construction end-to-end differentiable. We provide the\nimplementation details and present several important applications. To the best\nof our knowledge this is the first autodifferentiable realization of the\nVoronoi tessellation providing full set of Voronoi geometrical parameters in a\ndifferentiable way.\n","authors":["Sergei Shumilin","Alexander Ryabov","Serguei Barannikov","Evgeny Burnaev","Vladimir Vanovskii"],"pdf_url":"https://arxiv.org/pdf/2312.16192v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.02563v2","updated":"2024-12-16T09:23:46Z","published":"2021-12-05T13:23:10Z","title":"A Novel Approach to Solving Goal-Achieving Problems for Board Games","summary":"  Goal-achieving problems are puzzles that set up a specific situation with a\nclear objective. An example that is well-studied is the category of\nlife-and-death (L&D) problems for Go, which helps players hone their skill of\nidentifying region safety. Many previous methods like lambda search try null\nmoves first, then derive so-called relevance zones (RZs), outside of which the\nopponent does not need to search. This paper first proposes a novel RZ-based\napproach, called the RZ-Based Search (RZS), to solving L&D problems for Go. RZS\ntries moves before determining whether they are null moves post-hoc. This means\nwe do not need to rely on null move heuristics, resulting in a more elegant\nalgorithm, so that it can also be seamlessly incorporated into AlphaZero's\nsuper-human level play in our solver. To repurpose AlphaZero for solving, we\nalso propose a new training method called Faster to Life (FTL), which modifies\nAlphaZero to entice it to win more quickly. We use RZS and FTL to solve L&D\nproblems on Go, namely solving 68 among 106 problems from a professional L&D\nbook while a previous program solves 11 only. Finally, we discuss that the\napproach is generic in the sense that RZS is applicable to solving many other\ngoal-achieving problems for board games.\n","authors":["Chung-Chin Shih","Ti-Rong Wu","Ting Han Wei","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2112.02563v2.pdf","comment":"The main text is the final version to AAAI-22"},{"id":"http://arxiv.org/abs/2403.09755v3","updated":"2024-12-16T09:20:42Z","published":"2024-03-14T14:02:00Z","title":"Estimating the history of a random recursive tree","summary":"  This paper studies the problem of estimating the order of arrival of the\nvertices in a random recursive tree. Specifically, we study two fundamental\nmodels: the uniform attachment model and the linear preferential attachment\nmodel. We propose an order estimator based on the Jordan centrality measure and\ndefine a family of risk measures to quantify the quality of the ordering\nprocedure. Moreover, we establish a minimax lower bound for this problem, and\nprove that the proposed estimator is nearly optimal. Finally, we numerically\ndemonstrate that the proposed estimator outperforms degree-based and spectral\nordering procedures.\n","authors":["Simon Briend","Christophe Giraud","Gábor Lugosi","Déborah Sulem"],"pdf_url":"https://arxiv.org/pdf/2403.09755v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15395v2","updated":"2024-12-16T08:57:29Z","published":"2023-12-24T03:37:11Z","title":"Prompt Valuation Based on Shapley Values","summary":"  Large language models (LLMs) excel on new tasks without additional training,\nsimply by providing natural language prompts that demonstrate how the task\nshould be performed. Prompt ensemble methods comprehensively harness the\nknowledge of LLMs while mitigating individual biases and errors and further\nenhancing performance. However, more prompts do not necessarily lead to better\nresults, and not all prompts are beneficial. A small number of high-quality\nprompts often outperform many low-quality prompts. Currently, there is a lack\nof a suitable method for evaluating the impact of prompts on the results. In\nthis paper, we utilize the Shapley value to fairly quantify the contributions\nof prompts, helping to identify beneficial or detrimental prompts, and\npotentially guiding prompt valuation in data markets. Through extensive\nexperiments employing various ensemble methods and utility functions on diverse\ntasks, we validate the effectiveness of using the Shapley value method for\nprompts as it effectively distinguishes and quantifies the contributions of\neach prompt.\n","authors":["Hanxi Liu","Xiaokai Mao","Haocheng Xia","Jian Lou","Jinfei Liu","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2312.15395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10811v2","updated":"2024-12-16T08:55:12Z","published":"2024-01-19T16:56:11Z","title":"Simulation Based Bayesian Optimization","summary":"  Bayesian Optimization (BO) is a powerful method for optimizing black-box\nfunctions by combining prior knowledge with ongoing function evaluations. BO\nconstructs a probabilistic surrogate model of the objective function given the\ncovariates, which is in turn used to inform the selection of future evaluation\npoints through an acquisition function. For smooth continuous search spaces,\nGaussian Processes (GPs) are commonly used as the surrogate model as they offer\nanalytical access to posterior predictive distributions, thus facilitating the\ncomputation and optimization of acquisition functions. However, in complex\nscenarios involving optimization over categorical or mixed covariate spaces,\nGPs may not be ideal. This paper introduces Simulation Based Bayesian\nOptimization (SBBO) as a novel approach to optimizing acquisition functions\nthat only requires sampling-based access to posterior predictive distributions.\nSBBO allows the use of surrogate probabilistic models tailored for\ncombinatorial spaces with discrete variables. Any Bayesian model in which\nposterior inference is carried out through Markov chain Monte Carlo can be\nselected as the surrogate model in SBBO. We demonstrate empirically the\neffectiveness of SBBO using various choices of surrogate models in applications\ninvolving combinatorial optimization. choices of surrogate models.\n","authors":["Roi Naveiro","Becky Tang"],"pdf_url":"https://arxiv.org/pdf/2401.10811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11569v1","updated":"2024-12-16T08:55:03Z","published":"2024-12-16T08:55:03Z","title":"The dark side of the forces: assessing non-conservative force models for\n  atomistic machine learning","summary":"  The use of machine learning to estimate the energy of a group of atoms, and\nthe forces that drive them to more stable configurations, have revolutionized\nthe fields of computational chemistry and materials discovery. In this domain,\nrigorous enforcement of symmetry and conservation laws has traditionally been\nconsidered essential. For this reason, interatomic forces are usually computed\nas the derivatives of the potential energy, ensuring energy conservation.\nSeveral recent works have questioned this physically-constrained approach,\nsuggesting that using the forces as explicit learning targets yields a better\ntrade-off between accuracy and computational efficiency - and that energy\nconservation can be learned during training. The present work investigates the\napplicability of such non-conservative models in microscopic simulations. We\nidentify and demonstrate several fundamental issues, from ill-defined\nconvergence of geometry optimization to instability in various types of\nmolecular dynamics. Contrary to the case of rotational symmetry, lack of energy\nconservation is hard to learn, control, and correct. The best approach to\nexploit the acceleration afforded by direct force evaluation might be to use it\nin tandem with a conservative model, reducing - rather than eliminating - the\nadditional cost of backpropagation, but avoiding most of the pathological\nbehavior associated with non-conservative forces.\n","authors":["Filippo Bigi","Marcel Langer","Michele Ceriotti"],"pdf_url":"https://arxiv.org/pdf/2412.11569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05075v2","updated":"2024-12-16T08:50:59Z","published":"2024-05-08T14:18:13Z","title":"Towards Efficient Training and Evaluation of Robust Models against $l_0$\n  Bounded Adversarial Perturbations","summary":"  This work studies sparse adversarial perturbations bounded by $l_0$ norm. We\npropose a white-box PGD-like attack method named sparse-PGD to effectively and\nefficiently generate such perturbations. Furthermore, we combine sparse-PGD\nwith a black-box attack to comprehensively and more reliably evaluate the\nmodels' robustness against $l_0$ bounded adversarial perturbations. Moreover,\nthe efficiency of sparse-PGD enables us to conduct adversarial training to\nbuild robust models against sparse perturbations. Extensive experiments\ndemonstrate that our proposed attack algorithm exhibits strong performance in\ndifferent scenarios. More importantly, compared with other robust models, our\nadversarially trained model demonstrates state-of-the-art robustness against\nvarious sparse attacks. Codes are available at\nhttps://github.com/CityU-MLO/sPGD.\n","authors":["Xuyang Zhong","Yixiao Huang","Chen Liu"],"pdf_url":"https://arxiv.org/pdf/2405.05075v2.pdf","comment":"Accepted by ICML2024"},{"id":"http://arxiv.org/abs/2412.11554v1","updated":"2024-12-16T08:38:02Z","published":"2024-12-16T08:38:02Z","title":"Learning Massive-scale Partial Correlation Networks in Clinical\n  Multi-omics Studies with HP-ACCORD","summary":"  Graphical model estimation from modern multi-omics data requires a balance\nbetween statistical estimation performance and computational scalability. We\nintroduce a novel pseudolikelihood-based graphical model framework that\nreparameterizes the target precision matrix while preserving sparsity pattern\nand estimates it by minimizing an $\\ell_1$-penalized empirical risk based on a\nnew loss function. The proposed estimator maintains estimation and selection\nconsistency in various metrics under high-dimensional assumptions. The\nassociated optimization problem allows for a provably fast computation\nalgorithm using a novel operator-splitting approach and communication-avoiding\ndistributed matrix multiplication. A high-performance computing implementation\nof our framework was tested in simulated data with up to one million variables\ndemonstrating complex dependency structures akin to biological networks.\nLeveraging this scalability, we estimated partial correlation network from a\ndual-omic liver cancer data set. The co-expression network estimated from the\nultrahigh-dimensional data showed superior specificity in prioritizing key\ntranscription factors and co-activators by excluding the impact of epigenomic\nregulation, demonstrating the value of computational scalability in multi-omic\ndata analysis. %derived from the gene expression data.\n","authors":["Sungdong Lee","Joshua Bang","Youngrae Kim","Hyungwon Choi","Sang-Yun Oh","Joong-Ho Won"],"pdf_url":"https://arxiv.org/pdf/2412.11554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04074v2","updated":"2024-12-16T08:36:07Z","published":"2024-12-05T11:12:46Z","title":"Integrated Sensing and Communications for Low-Altitude Economy: A Deep\n  Reinforcement Learning Approach","summary":"  This paper studies an integrated sensing and communications (ISAC) system for\nlow-altitude economy (LAE), where a ground base station (GBS) provides\ncommunication and navigation services for authorized unmanned aerial vehicles\n(UAVs), while sensing the low-altitude airspace to monitor the unauthorized\nmobile target. The expected communication sum-rate over a given flight period\nis maximized by jointly optimizing the beamforming at the GBS and UAVs'\ntrajectories, subject to the constraints on the average signal-to-noise ratio\nrequirement for sensing, the flight mission and collision avoidance of UAVs, as\nwell as the maximum transmit power at the GBS. Typically, this is a sequential\ndecision-making problem with the given flight mission. Thus, we transform it to\na specific Markov decision process (MDP) model called episode task. Based on\nthis modeling, we propose a novel LAE-oriented ISAC scheme, referred to as Deep\nLAE-ISAC (DeepLSC), by leveraging the deep reinforcement learning (DRL)\ntechnique. In DeepLSC, a reward function and a new action selection policy\ntermed constrained noise-exploration policy are judiciously designed to fulfill\nvarious constraints. To enable efficient learning in episode tasks, we develop\na hierarchical experience replay mechanism, where the gist is to employ all\nexperiences generated within each episode to jointly train the neural network.\nBesides, to enhance the convergence speed of DeepLSC, a symmetric experience\naugmentation mechanism, which simultaneously permutes the indexes of all\nvariables to enrich available experience sets, is proposed. Simulation results\ndemonstrate that compared with benchmarks, DeepLSC yields a higher sum-rate\nwhile meeting the preset constraints, achieves faster convergence, and is more\nrobust against different settings.\n","authors":["Xiaowen Ye","Yuyi Mao","Xianghao Yu","Shu Sun","Liqun Fu","Jie Xu"],"pdf_url":"https://arxiv.org/pdf/2412.04074v2.pdf","comment":"submitted for an IEEE publication"},{"id":"http://arxiv.org/abs/2412.11550v1","updated":"2024-12-16T08:33:56Z","published":"2024-12-16T08:33:56Z","title":"THESAURUS: Contrastive Graph Clustering by Swapping Fused\n  Gromov-Wasserstein Couplings","summary":"  Graph node clustering is a fundamental unsupervised task. Existing methods\ntypically train an encoder through selfsupervised learning and then apply\nK-means to the encoder output. Some methods use this clustering result directly\nas the final assignment, while others initialize centroids based on this\ninitial clustering and then finetune both the encoder and these learnable\ncentroids. However, due to their reliance on K-means, these methods inherit its\ndrawbacks when the cluster separability of encoder output is low, facing\nchallenges from the Uniform Effect and Cluster Assimilation. We summarize three\nreasons for the low cluster separability in existing methods: (1) lack of\ncontextual information prevents discrimination between similar nodes from\ndifferent clusters; (2) training tasks are not sufficiently aligned with the\ndownstream clustering task; (3) the cluster information in the graph structure\nis not appropriately exploited. To address these issues, we propose conTrastive\ngrapH clustEring by SwApping fUsed gRomov-wasserstein coUplingS (THESAURUS).\nOur method introduces semantic prototypes to provide contextual information,\nand employs a cross-view assignment prediction pretext task that aligns well\nwith the downstream clustering task. Additionally, it utilizes\nGromov-Wasserstein Optimal Transport (GW-OT) along with the proposed prototype\ngraph to thoroughly exploit cluster information in the graph structure. To\nadapt to diverse real-world data, THESAURUS updates the prototype graph and the\nprototype marginal distribution in OT by using momentum. Extensive experiments\ndemonstrate that THESAURUS achieves higher cluster separability than the prior\nart, effectively mitigating the Uniform Effect and Cluster Assimilation issues\n","authors":["Bowen Deng","Tong Wang","Lele Fu","Sheng Huang","Chuan Chen","Tao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11550v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08114v3","updated":"2024-12-16T08:25:18Z","published":"2024-12-11T05:45:30Z","title":"Modeling Latent Non-Linear Dynamical System over Time Series","summary":"  We study the problem of modeling a non-linear dynamical system when given a\ntime series by deriving equations directly from the data. Despite the fact that\ntime series data are given as input, models for dynamics and estimation\nalgorithms that incorporate long-term temporal dependencies are largely absent\nfrom existing studies. In this paper, we introduce a latent state to allow\ntime-dependent modeling and formulate this problem as a dynamics estimation\nproblem in latent states. We face multiple technical challenges, including (1)\nmodeling latent non-linear dynamics and (2) solving circular dependencies\ncaused by the presence of latent states. To tackle these challenging problems,\nwe propose a new method, Latent Non-Linear equation modeling (LaNoLem), that\ncan model a latent non-linear dynamical system and a novel alternating\nminimization algorithm for effectively estimating latent states and model\nparameters. In addition, we introduce criteria to control model complexity\nwithout human intervention. Compared with the state-of-the-art model, LaNoLem\nachieves competitive performance for estimating dynamics while outperforming\nother methods in prediction.\n","authors":["Ren Fujiwara","Yasuko Matsubara","Yasushi Sakurai"],"pdf_url":"https://arxiv.org/pdf/2412.08114v3.pdf","comment":"accepted at AAAI'25"},{"id":"http://arxiv.org/abs/2412.11543v1","updated":"2024-12-16T08:23:50Z","published":"2024-12-16T08:23:50Z","title":"Error Diversity Matters: An Error-Resistant Ensemble Method for\n  Unsupervised Dependency Parsing","summary":"  We address unsupervised dependency parsing by building an ensemble of diverse\nexisting models through post hoc aggregation of their output dependency parse\nstructures. We observe that these ensembles often suffer from low robustness\nagainst weak ensemble components due to error accumulation. To tackle this\nproblem, we propose an efficient ensemble-selection approach that avoids error\naccumulation. Results demonstrate that our approach outperforms each individual\nmodel as well as previous ensemble techniques. Additionally, our experiments\nshow that the proposed ensemble-selection method significantly enhances the\nperformance and robustness of our ensemble, surpassing previously proposed\nstrategies, which have not accounted for error diversity.\n","authors":["Behzad Shayegh","Hobie H. -B. Lee","Xiaodan Zhu","Jackie Chi Kit Cheung","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2412.11543v1.pdf","comment":"Accepted by the AAAI Conference on Artificial Intelligence (AAAI)\n  2025"},{"id":"http://arxiv.org/abs/2412.11542v1","updated":"2024-12-16T08:22:23Z","published":"2024-12-16T08:22:23Z","title":"Meta Curvature-Aware Minimization for Domain Generalization","summary":"  Domain generalization (DG) aims to enhance the ability of models trained on\nsource domains to generalize effectively to unseen domains. Recently,\nSharpness-Aware Minimization (SAM) has shown promise in this area by reducing\nthe sharpness of the loss landscape to obtain more generalized models. However,\nSAM and its variants sometimes fail to guide the model toward a flat minimum,\nand their training processes exhibit limitations, hindering further\nimprovements in model generalization. In this paper, we first propose an\nimproved model training process aimed at encouraging the model to converge to a\nflat minima. To achieve this, we design a curvature metric that has a minimal\neffect when the model is far from convergence but becomes increasingly\ninfluential in indicating the curvature of the minima as the model approaches a\nlocal minimum. Then we derive a novel algorithm from this metric, called Meta\nCurvature-Aware Minimization (MeCAM), to minimize the curvature around the\nlocal minima. Specifically, the optimization objective of MeCAM simultaneously\nminimizes the regular training loss, the surrogate gap of SAM, and the\nsurrogate gap of meta-learning. We provide theoretical analysis on MeCAM's\ngeneralization error and convergence rate, and demonstrate its superiority over\nexisting DG methods through extensive experiments on five benchmark DG\ndatasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code\nwill be available on GitHub.\n","authors":["Ziyang Chen","Yiwen Ye","Feilong Tang","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11542v1.pdf","comment":"21 pages, 5 figures, 17 tables"},{"id":"http://arxiv.org/abs/2412.11526v1","updated":"2024-12-16T08:01:22Z","published":"2024-12-16T08:01:22Z","title":"Probability-Informed Machine Learning","summary":"  Machine learning (ML) has emerged as a powerful tool for tackling complex\nregression and classification tasks, yet its success often hinges on the\nquality of training data. This study introduces a novel ML paradigm inspired by\ndomain knowledge of the structure of output function, akin to physics-informed\nML, but rooted in probabilistic principles rather than physical laws. The\nproposed approach integrates the probabilistic structure of the target variable\n(such as its cumulative distribution function) into the training process. This\nprobabilistic information is obtained from historical data or estimated using\nstructural reliability methods during experimental design. By embedding\ndomain-specific probabilistic insights into the learning process, the method\nenhances model accuracy and mitigates risks of overfitting and underfitting.\nApplications in regression, image denoising, and classification demonstrate the\neffectiveness of the approach in addressing real-world problems.\n","authors":["Mohsen Rashki"],"pdf_url":"https://arxiv.org/pdf/2412.11526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11521v1","updated":"2024-12-16T07:56:54Z","published":"2024-12-16T07:56:54Z","title":"On the Ability of Deep Networks to Learn Symmetries from Data: A Neural\n  Kernel Theory","summary":"  Symmetries (transformations by group actions) are present in many datasets,\nand leveraging them holds significant promise for improving predictions in\nmachine learning. In this work, we aim to understand when and how deep networks\ncan learn symmetries from data. We focus on a supervised classification\nparadigm where data symmetries are only partially observed during training:\nsome classes include all transformations of a cyclic group, while others\ninclude only a subset. We ask: can deep networks generalize symmetry invariance\nto the partially sampled classes? In the infinite-width limit, where kernel\nanalogies apply, we derive a neural kernel theory of symmetry learning to\naddress this question. The group-cyclic nature of the dataset allows us to\nanalyze the spectrum of neural kernels in the Fourier domain; here we find a\nsimple characterization of the generalization error as a function of the\ninteraction between class separation (signal) and class-orbit density (noise).\nWe observe that generalization can only be successful when the local structure\nof the data prevails over its non-local, symmetric, structure, in the kernel\nspace defined by the architecture. This occurs when (1) classes are\nsufficiently distinct and (2) class orbits are sufficiently dense. Our\nframework also applies to equivariant architectures (e.g., CNNs), and recovers\ntheir success in the special case where the architecture matches the inherent\nsymmetry of the data. Empirically, our theory reproduces the generalization\nfailure of finite-width networks (MLP, CNN, ViT) trained on partially observed\nversions of rotated-MNIST. We conclude that conventional networks trained with\nsupervision lack a mechanism to learn symmetries that have not been explicitly\nembedded in their architecture a priori. Our framework could be extended to\nguide the design of architectures and training procedures able to learn\nsymmetries from data.\n","authors":["Andrea Perin","Stephane Deny"],"pdf_url":"https://arxiv.org/pdf/2412.11521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08160v3","updated":"2024-12-16T07:44:32Z","published":"2024-12-11T07:32:38Z","title":"DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with\n  Selective State Space Models","summary":"  Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns,\nwidely existing in the real world. Nevertheless, the structure incompleteness,\nnoise, and redundancy result in poor robustness for Dynamic Graph Neural\nNetworks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising\nway to optimize graph structures. However, aside from encountering unacceptable\nquadratic complexity, it overly relies on heuristic priors, making it hard to\ndiscover underlying predictive patterns. How to efficiently refine the dynamic\nstructures, capture intrinsic dependencies, and learn robust representations,\nremains under-explored. In this work, we propose the novel DG-Mamba, a robust\nand efficient Dynamic Graph structure learning framework with the Selective\nState Space Models (Mamba). To accelerate the spatio-temporal structure\nlearning, we propose a kernelized dynamic message-passing operator that reduces\nthe quadratic time complexity to linear. To capture global intrinsic dynamics,\nwe establish the dynamic graph as a self-contained system with State Space\nModel. By discretizing the system states with the cross-snapshot graph\nadjacency, we enable the long-distance dependencies capturing with the\nselective snapshot scan. To endow learned dynamic structures more expressive\nwith informativeness, we propose the self-supervised Principle of Relevant\nInformation for DGSL to regularize the most relevant yet least redundant\ninformation, enhancing global robustness. Extensive experiments demonstrate the\nsuperiority of the robustness and efficiency of our DG-Mamba compared with the\nstate-of-the-art baselines against adversarial attacks.\n","authors":["Haonan Yuan","Qingyun Sun","Zhaonan Wang","Xingcheng Fu","Cheng Ji","Yongjian Wang","Bo Jin","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2412.08160v3.pdf","comment":"Accepted by the Main Technical Track of the 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2403.07573v2","updated":"2024-12-16T07:42:33Z","published":"2024-03-12T12:03:16Z","title":"Towards a Dynamic Future with Adaptable Computing and Network\n  Convergence (ACNC)","summary":"  In the context of advancing 6G, a substantial paradigm shift is anticipated,\nhighlighting comprehensive everything-to-everything interactions characterized\nby numerous connections and stringent adherence to Quality of\nService/Experience (QoS/E) prerequisites. The imminent challenge stems from\nresource scarcity, prompting a deliberate transition to Computing-Network\nConvergence (CNC) as an auspicious approach for joint resource orchestration.\nWhile CNC-based mechanisms have garnered attention, their effectiveness in\nrealizing future services, particularly in use cases like the Metaverse, may\nencounter limitations due to the continually changing nature of users,\nservices, and resources. Hence, this paper presents the concept of Adaptable\nCNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for\nthe joint orchestration of computing and network resources, catering to dynamic\nand voluminous user requests with stringent requirements. ACNC encompasses two\nprimary functionalities: state recognition and context detection. Given the\nintricate nature of the user-service-computing-network space, the paper employs\ndimension reduction to generate live, holistic, abstract system states in a\nhierarchical structure. To address the challenges posed by dynamic changes,\nContinual Learning (CL) is employed, classifying the system state into contexts\ncontrolled by dedicated ML agents, enabling them to operate efficiently. These\ntwo functionalities are intricately linked within a closed loop overseen by the\nEnd-to-End (E2E) orchestrator to allocate resources. The paper introduces the\ncomponents of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in\nresource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow,\ndetails a numerical analysis for efficiency assessment, and concludes with\ndiscussions on relevant challenges and potential avenues for future research.\n","authors":["Masoud Shokrnezhad","Hao Yu","Tarik Taleb","Richard Li","Kyunghan Lee","Jaeseung Song","Cedric Westphal"],"pdf_url":"https://arxiv.org/pdf/2403.07573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11511v1","updated":"2024-12-16T07:39:46Z","published":"2024-12-16T07:39:46Z","title":"Constructing Confidence Intervals for Average Treatment Effects from\n  Multiple Datasets","summary":"  Constructing confidence intervals (CIs) for the average treatment effect\n(ATE) from patient records is crucial to assess the effectiveness and safety of\ndrugs. However, patient records typically come from different hospitals, thus\nraising the question of how multiple observational datasets can be effectively\ncombined for this purpose. In our paper, we propose a new method that estimates\nthe ATE from multiple observational datasets and provides valid CIs. Our method\nmakes little assumptions about the observational datasets and is thus widely\napplicable in medical practice. The key idea of our method is that we leverage\nprediction-powered inferences and thereby essentially `shrink' the CIs so that\nwe offer more precise uncertainty quantification as compared to na\\\"ive\napproaches. We further prove the unbiasedness of our method and the validity of\nour CIs. We confirm our theoretical results through various numerical\nexperiments. Finally, we provide an extension of our method for constructing\nCIs from combinations of experimental and observational datasets.\n","authors":["Yuxin Wang","Maresa Schröder","Dennis Frauen","Jonas Schweisthal","Konstantin Hess","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2412.11511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12728v4","updated":"2024-12-16T07:35:55Z","published":"2024-09-19T12:53:29Z","title":"PRAGA: Prototype-aware Graph Adaptive Aggregation for Spatial\n  Multi-modal Omics Analysis","summary":"  Spatial multi-modal omics technology, highlighted by Nature Methods as an\nadvanced biological technique in 2023, plays a critical role in resolving\nbiological regulatory processes with spatial context. Recently, graph neural\nnetworks based on K-nearest neighbor (KNN) graphs have gained prominence in\nspatial multi-modal omics methods due to their ability to model semantic\nrelations between sequencing spots. However, the fixed KNN graph fails to\ncapture the latent semantic relations hidden by the inevitable data\nperturbations during the biological sequencing process, resulting in the loss\nof semantic information. In addition, the common lack of spot annotation and\nclass number priors in practice further hinders the optimization of spatial\nmulti-modal omics models. Here, we propose a novel spatial multi-modal omics\nresolved framework, termed PRototype-Aware Graph Adaptative Aggregation for\nSpatial Multi-modal Omics Analysis (PRAGA). PRAGA constructs a dynamic graph to\ncapture latent semantic relations and comprehensively integrate spatial\ninformation and feature semantics. The learnable graph structure can also\ndenoise perturbations by learning cross-modal knowledge. Moreover, a dynamic\nprototype contrastive learning is proposed based on the dynamic adaptability of\nBayesian Gaussian Mixture Models to optimize the multi-modal omics\nrepresentations for unknown biological priors. Quantitative and qualitative\nexperiments on simulated and real datasets with 7 competing methods demonstrate\nthe superior performance of PRAGA. Code is available at\nhttps://github.com/Xubin-s-Lab/PRAGA.\n","authors":["Xinlei Huang","Zhiqi Ma","Dian Meng","Yanran Liu","Shiwei Ruan","Qingqiang Sun","Xubin Zheng","Ziyue Qiao"],"pdf_url":"https://arxiv.org/pdf/2409.12728v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11501v1","updated":"2024-12-16T07:23:22Z","published":"2024-12-16T07:23:22Z","title":"Explicit and Implicit Graduated Optimization in Deep Neural Networks","summary":"  Graduated optimization is a global optimization technique that is used to\nminimize a multimodal nonconvex function by smoothing the objective function\nwith noise and gradually refining the solution. This paper experimentally\nevaluates the performance of the explicit graduated optimization algorithm with\nan optimal noise scheduling derived from a previous study and discusses its\nlimitations. It uses traditional benchmark functions and empirical loss\nfunctions for modern neural network architectures for evaluating. In addition,\nthis paper extends the implicit graduated optimization algorithm, which is\nbased on the fact that stochastic noise in the optimization process of SGD\nimplicitly smooths the objective function, to SGD with momentum, analyzes its\nconvergence, and demonstrates its effectiveness through experiments on image\nclassification tasks with ResNet architectures.\n","authors":["Naoki Sato","Hideaki Iiduka"],"pdf_url":"https://arxiv.org/pdf/2412.11501v1.pdf","comment":"Accepted at AAAI-25"},{"id":"http://arxiv.org/abs/2402.11441v2","updated":"2024-12-16T07:18:06Z","published":"2024-02-18T03:36:26Z","title":"InfuserKI: Enhancing Large Language Models with Knowledge Graphs via\n  Infuser-Guided Knowledge Integration","summary":"  Large Language Models (LLMs) have achieved exceptional capabilities in open\ngeneration across various domains, yet they encounter difficulties with tasks\nthat require intensive knowledge. To address these challenges, methods for\nintegrating knowledge have been developed, which augment LLMs with\ndomain-specific knowledge graphs through external modules. These approaches,\nhowever, face data inefficiency issues as they necessitate the processing of\nboth known and unknown knowledge for fine-tuning. Thus, our research focuses on\na novel problem: efficiently integrating unknown knowledge into LLMs without\nunnecessary overlap of known knowledge. A risk of introducing new knowledge is\nthe potential forgetting of existing knowledge. To mitigate this risk, we\npropose the innovative {\\method} framework. This framework employs transformer\ninternal states to determine when to enrich LLM outputs with additional\ninformation, effectively preventing knowledge forgetting. Performance\nevaluations using the UMLS-2.5k and MetaQA domain knowledge graphs reveal that\n{\\method} not only successfully integrates new knowledge but also outperforms\nstate-of-the-art baselines, reducing knowledge forgetting by 9\\% and 6\\%,\nrespectively.\n","authors":["Fali Wang","Runxue Bao","Suhang Wang","Wenchao Yu","Yanchi Liu","Wei Cheng","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2402.11441v2.pdf","comment":"14 pages, 7 figures, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2106.07214v4","updated":"2024-12-16T07:08:33Z","published":"2021-06-14T08:00:48Z","title":"Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence\n  Functions","summary":"  Backdoor attacks inject poisoning samples during training, with the goal of\nforcing a machine learning model to output an attacker-chosen class when\npresented a specific trigger at test time. Although backdoor attacks have been\ndemonstrated in a variety of settings and against different models, the factors\naffecting their effectiveness are still not well understood. In this work, we\nprovide a unifying framework to study the process of backdoor learning under\nthe lens of incremental learning and influence functions. We show that the\neffectiveness of backdoor attacks depends on: (i) the complexity of the\nlearning algorithm, controlled by its hyperparameters; (ii) the fraction of\nbackdoor samples injected into the training set; and (iii) the size and\nvisibility of the backdoor trigger. These factors affect how fast a model\nlearns to correlate the presence of the backdoor trigger with the target class.\nOur analysis unveils the intriguing existence of a region in the hyperparameter\nspace in which the accuracy on clean test samples is still high while backdoor\nattacks are ineffective, thereby suggesting novel criteria to improve existing\ndefenses.\n","authors":["Antonio Emanuele Cinà","Kathrin Grosse","Sebastiano Vascon","Ambra Demontis","Battista Biggio","Fabio Roli","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2106.07214v4.pdf","comment":"Preprint; Paper accepted at International Journal of Machine Learning\n  and Cybernetics; 25 pages"},{"id":"http://arxiv.org/abs/2409.13474v2","updated":"2024-12-16T07:06:42Z","published":"2024-09-20T13:05:07Z","title":"Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models","summary":"  Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/AlternatePreference-Optimization\n","authors":["Anmol Mekala","Vineeth Dorna","Shreya Dubey","Abhishek Lalwani","David Koleczek","Mukund Rungta","Sadid Hasan","Elita Lobo"],"pdf_url":"https://arxiv.org/pdf/2409.13474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11489v1","updated":"2024-12-16T07:06:17Z","published":"2024-12-16T07:06:17Z","title":"HGSFusion: Radar-Camera Fusion with Hybrid Generation and\n  Synchronization for 3D Object Detection","summary":"  Millimeter-wave radar plays a vital role in 3D object detection for\nautonomous driving due to its all-weather and all-lighting-condition\ncapabilities for perception. However, radar point clouds suffer from pronounced\nsparsity and unavoidable angle estimation errors. To address these limitations,\nincorporating a camera may partially help mitigate the shortcomings.\nNevertheless, the direct fusion of radar and camera data can lead to negative\nor even opposite effects due to the lack of depth information in images and\nlow-quality image features under adverse lighting conditions. Hence, in this\npaper, we present the radar-camera fusion network with Hybrid Generation and\nSynchronization (HGSFusion), designed to better fuse radar potentials and image\nfeatures for 3D object detection. Specifically, we propose the Radar Hybrid\nGeneration Module (RHGM), which fully considers the Direction-Of-Arrival (DOA)\nestimation errors in radar signal processing. This module generates denser\nradar points through different Probability Density Functions (PDFs) with the\nassistance of semantic information. Meanwhile, we introduce the Dual Sync\nModule (DSM), comprising spatial sync and modality sync, to enhance image\nfeatures with radar positional information and facilitate the fusion of\ndistinct characteristics in different modalities. Extensive experiments\ndemonstrate the effectiveness of our approach, outperforming the\nstate-of-the-art methods in the VoD and TJ4DRadSet datasets by $6.53\\%$ and\n$2.03\\%$ in RoI AP and BEV AP, respectively. The code is available at\nhttps://github.com/garfield-cpp/HGSFusion.\n","authors":["Zijian Gu","Jianwei Ma","Yan Huang","Honghao Wei","Zhanye Chen","Hui Zhang","Wei Hong"],"pdf_url":"https://arxiv.org/pdf/2412.11489v1.pdf","comment":"12 pages, 8 figures, 7 tables. Accepted by AAAI 2025 , the 39th\n  Annual AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2404.08717v2","updated":"2024-12-16T06:56:28Z","published":"2024-04-12T07:32:57Z","title":"State-Space Systems as Dynamic Generative Models","summary":"  A probabilistic framework to study the dependence structure induced by\ndeterministic discrete-time state-space systems between input and output\nprocesses is introduced. General sufficient conditions are formulated under\nwhich output processes exist and are unique once an input process has been\nfixed, a property that in the deterministic state-space literature is known as\nthe echo state property. When those conditions are satisfied, the given\nstate-space system becomes a generative model for probabilistic dependences\nbetween two sequence spaces. Moreover, those conditions guarantee that the\noutput depends continuously on the input when using the Wasserstein metric. The\noutput processes whose existence is proved are shown to be causal in a specific\nsense and to generalize those studied in purely deterministic situations. The\nresults in this paper constitute a significant stochastic generalization of\nsufficient conditions for the deterministic echo state property to hold, in the\nsense that the stochastic echo state property can be satisfied under\ncontractivity conditions that are strictly weaker than those in deterministic\nsituations. This means that state-space systems can induce a purely\nprobabilistic dependence structure between input and output sequence spaces\neven when there is no functional relation between those two spaces.\n","authors":["Juan-Pablo Ortega","Florian Rossmannek"],"pdf_url":"https://arxiv.org/pdf/2404.08717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11483v1","updated":"2024-12-16T06:52:09Z","published":"2024-12-16T06:52:09Z","title":"\"They've Stolen My GPL-Licensed Model!\": Toward Standardized and\n  Transparent Model Licensing","summary":"  As model parameter sizes reach the billion-level range and their training\nconsumes zettaFLOPs of computation, components reuse and collaborative\ndevelopment are become increasingly prevalent in the Machine Learning (ML)\ncommunity. These components, including models, software, and datasets, may\noriginate from various sources and be published under different licenses, which\ngovern the use and distribution of licensed works and their derivatives.\nHowever, commonly chosen licenses, such as GPL and Apache, are\nsoftware-specific and are not clearly defined or bounded in the context of\nmodel publishing. Meanwhile, the reused components may also have free-content\nlicenses and model licenses, which pose a potential risk of license\nnoncompliance and rights infringement within the model production workflow. In\nthis paper, we propose addressing the above challenges along two lines: 1) For\nlicense analysis, we have developed a new vocabulary for ML workflow management\nand encoded license rules to enable ontological reasoning for analyzing rights\ngranting and compliance issues. 2) For standardized model publishing, we have\ndrafted a set of model licenses that provide flexible options to meet the\ndiverse needs of model publishing. Our analysis tool is built on Turtle\nlanguage and Notation3 reasoning engine, envisioned as a first step toward\nLinked Open Model Production Data. We have also encoded our proposed model\nlicenses into rules and demonstrated the effects of GPL and other commonly used\nlicenses in model publishing, along with the flexibility advantages of our\nlicenses, through comparisons and experiments.\n","authors":["Moming Duan","Rui Zhao","Linshan Jiang","Nigel Shadbolt","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2412.11483v1.pdf","comment":"12 pages, 6 figures. Under review"},{"id":"http://arxiv.org/abs/2412.10005v2","updated":"2024-12-16T06:48:34Z","published":"2024-12-13T09:42:42Z","title":"Matrix Completion via Residual Spectral Matching","summary":"  Noisy matrix completion has attracted significant attention due to its\napplications in recommendation systems, signal processing and image\nrestoration. Most existing works rely on (weighted) least squares methods under\nvarious low-rank constraints. However, minimizing the sum of squared residuals\nis not always efficient, as it may ignore the potential structural information\nin the residuals. In this study, we propose a novel residual spectral matching\ncriterion that incorporates not only the numerical but also locational\ninformation of residuals. This criterion is the first in noisy matrix\ncompletion to adopt the perspective of low-rank perturbation of random matrices\nand exploit the spectral properties of sparse random matrices. We derive\noptimal statistical properties by analyzing the spectral properties of sparse\nrandom matrices and bounding the effects of low-rank perturbations and partial\nobservations. Additionally, we propose algorithms that efficiently approximate\nsolutions by constructing easily computable pseudo-gradients. The iterative\nprocess of the proposed algorithms ensures convergence at a rate consistent\nwith the optimal statistical error bound. Our method and algorithms demonstrate\nimproved numerical performance in both simulated and real data examples,\nparticularly in environments with high noise levels.\n","authors":["Ziyuan Chen","Fang Yao"],"pdf_url":"https://arxiv.org/pdf/2412.10005v2.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.11477v1","updated":"2024-12-16T06:44:39Z","published":"2024-12-16T06:44:39Z","title":"NoteContrast: Contrastive Language-Diagnostic Pretraining for Medical\n  Text","summary":"  Accurate diagnostic coding of medical notes is crucial for enhancing patient\ncare, medical research, and error-free billing in healthcare organizations.\nManual coding is a time-consuming task for providers, and diagnostic codes\noften exhibit low sensitivity and specificity, whereas the free text in medical\nnotes can be a more precise description of a patients status. Thus, accurate\nautomated diagnostic coding of medical notes has become critical for a learning\nhealthcare system. Recent developments in long-document transformer\narchitectures have enabled attention-based deep-learning models to adjudicate\nmedical notes. In addition, contrastive loss functions have been used to\njointly pre-train large language and image models with noisy labels. To further\nimprove the automated adjudication of medical notes, we developed an approach\nbased on i) models for ICD-10 diagnostic code sequences using a large\nreal-world data set, ii) large language models for medical notes, and iii)\ncontrastive pre-training to build an integrated model of both ICD-10 diagnostic\ncodes and corresponding medical text. We demonstrate that a contrastive\napproach for pre-training improves performance over prior state-of-the-art\nmodels for the MIMIC-III-50, MIMIC-III-rare50, and MIMIC-III-full diagnostic\ncoding tasks.\n","authors":["Prajwal Kailas","Max Homilius","Rahul C. Deo","Calum A. MacRae"],"pdf_url":"https://arxiv.org/pdf/2412.11477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17406v3","updated":"2024-12-16T06:44:05Z","published":"2024-05-27T17:55:05Z","title":"Deep Learning Calabi-Yau four folds with hybrid and recurrent neural\n  network architectures","summary":"  In this work, we report the results of applying deep learning based on hybrid\nconvolutional-recurrent and purely recurrent neural network architectures to\nthe dataset of almost one million complete intersection Calabi-Yau four-folds\n(CICY4) to machine-learn their four Hodge numbers $h^{1,1}, h^{2,1}, h^{3,1},\nh^{2,2}$. In particular, we explored and experimented with twelve different\nneural network models, nine of which are convolutional-recurrent (CNN-RNN)\nhybrids with the RNN unit being either GRU (Gated Recurrent Unit) or Long Short\nTerm Memory (LSTM). The remaining four models are purely recurrent neural\nnetworks based on LSTM. In terms of the $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$\nprediction accuracies, at 72% training ratio, our best performing individual\nmodel is CNN-LSTM-400, a hybrid CNN-LSTM with the LSTM hidden size of 400,\nwhich obtained 99.74%, 98.07%, 95.19%, 81.01%, our second best performing\nindividual model is LSTM-448, an LSTM-based model with the hidden size of 448,\nwhich obtained 99.74%, 97.51%, 94.24%, and 78.63%. These results were improved\nby forming ensembles of the top two, three or even four models. Our best\nensemble, consisting of the top four models, achieved the accuracies of 99.84%,\n98.71%, 96.26%, 85.03%. At 80% training ratio, the top two performing models\nLSTM-448 and LSTM-424 are both LSTM-based with the hidden sizes of 448 and 424.\nCompared with the 72% training ratio, there is a significant improvement of\naccuracies, which reached 99.85%, 98.66%, 96.26%, 84.77% for the best\nindividual model and 99.90%, 99.03%, 97.97%, 87.34% for the best ensemble. By\nnature a proof of concept, the results of this work conclusively established\nthe utility of RNN-based architectures and demonstrated their effective\nperformances compared to the well-explored purely CNN-based architectures in\nthe problem of deep learning Calabi Yau manifolds.\n","authors":["H. L. Dao"],"pdf_url":"https://arxiv.org/pdf/2405.17406v3.pdf","comment":"v3: more discussions added, references added, typos corrected"},{"id":"http://arxiv.org/abs/2412.11476v1","updated":"2024-12-16T06:40:25Z","published":"2024-12-16T06:40:25Z","title":"Vertical Federated Unlearning via Backdoor Certification","summary":"  Vertical Federated Learning (VFL) offers a novel paradigm in machine\nlearning, enabling distinct entities to train models cooperatively while\nmaintaining data privacy. This method is particularly pertinent when entities\npossess datasets with identical sample identifiers but diverse attributes.\nRecent privacy regulations emphasize an individual's \\emph{right to be\nforgotten}, which necessitates the ability for models to unlearn specific\ntraining data. The primary challenge is to develop a mechanism to eliminate the\ninfluence of a specific client from a model without erasing all relevant data\nfrom other clients. Our research investigates the removal of a single client's\ncontribution within the VFL framework. We introduce an innovative modification\nto traditional VFL by employing a mechanism that inverts the typical learning\ntrajectory with the objective of extracting specific data contributions. This\napproach seeks to optimize model performance using gradient ascent, guided by a\npre-defined constrained model. We also introduce a backdoor mechanism to verify\nthe effectiveness of the unlearning procedure. Our method avoids fully\naccessing the initial training data and avoids storing parameter updates.\nEmpirical evidence shows that the results align closely with those achieved by\nretraining from scratch. Utilizing gradient ascent, our unlearning approach\naddresses key challenges in VFL, laying the groundwork for future advancements\nin this domain. All the code and implementations related to this paper are\npublicly available at https://github.com/mengde-han/VFL-unlearn.\n","authors":["Mengde Han","Tianqing Zhu","Lefeng Zhang","Huan Huo","Wanlei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.11476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16713v2","updated":"2024-12-16T06:37:01Z","published":"2024-10-22T05:49:24Z","title":"Collapse or Thrive? Perils and Promises of Synthetic Data in a\n  Self-Generating World","summary":"  The increasing presence of AI-generated content on the internet raises a\ncritical question: What happens when generative machine learning models are\npretrained on web-scale datasets containing data created by earlier models?\nSome authors prophesy \\textit{model collapse} under a `{\\it replace}' scenario:\na sequence of models, the first trained with real data and each later one\ntrained {\\it only on} synthetic data from its preceding model. In this\nscenario, models successively degrade. Others see collapse as avoidable; in an\n`{\\it accumulate}' scenario, a sequence of models is trained, but each training\nuses all real and synthetic data generated so far. In this work, we deepen and\nextend the study of these contrasting scenarios. First, collapse versus\navoidance of collapse is studied by comparing the replace and accumulate\nscenarios on each of three prominent generative modeling settings; we find the\nsame contrast emerges in all three settings. Second, we study a compromise\nscenario; the available data remains the same as in the {\\it accumulate}\nscenario -- but unlike {\\it accumulate} and like {\\it replace}, each model is\ntrained using a fixed compute budget; we demonstrate that model test loss on\nreal data is larger than in the {\\it accumulate} scenario, but apparently\nplateaus, unlike the divergence seen with {\\it replace}. Third, we study the\nrelative importance of cardinality and proportion of real data for avoiding\nmodel collapse. Surprisingly, we find a non-trivial interaction between real\nand synthetic data, where the value of synthetic data for reducing test loss\ndepends on the absolute quantity of real data. Our insights are particularly\nimportant when forecasting whether future frontier generative models will\ncollapse or thrive, and our results open avenues for empirically and\nmathematically studying the context-dependent value of synthetic data.\n","authors":["Joshua Kazdan","Rylan Schaeffer","Apratim Dey","Matthias Gerstgrasser","Rafael Rafailov","David L. Donoho","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2410.16713v2.pdf","comment":"Accepted at NeurIPS 2024 Workshops: Mathematics of Modern Machine\n  Learning (M3L) and Attributing Model Behavior at Scale (ATTRIB)"},{"id":"http://arxiv.org/abs/2412.11472v1","updated":"2024-12-16T06:19:35Z","published":"2024-12-16T06:19:35Z","title":"Leveraging Foundation Language Models (FLMs) for Automated Cohort\n  Extraction from Large EHR Databases","summary":"  A crucial step in cohort studies is to extract the required cohort from one\nor more study datasets. This step is time-consuming, especially when a\nresearcher is presented with a dataset that they have not previously worked\nwith. When the cohort has to be extracted from multiple datasets, cohort\nextraction can be extremely laborious. In this study, we present an approach\nfor partially automating cohort extraction from multiple electronic health\nrecord (EHR) databases. We formulate the guided multi-dataset cohort extraction\nproblem in which selection criteria are first converted into queries,\ntranslating them from natural language text to language that maps to database\nentities. Then, using FLMs, columns of interest identified from the queries are\nautomatically matched between the study databases. Finally, the generated\nqueries are run across all databases to extract the study cohort. We propose\nand evaluate an algorithm for automating column matching on two large, popular\nand publicly-accessible EHR databases -- MIMIC-III and eICU. Our approach\nachieves a high top-three accuracy of $92\\%$, correctly matching $12$ out of\nthe $13$ columns of interest, when using a small, pre-trained general purpose\nlanguage model. Furthermore, this accuracy is maintained even as the search\nspace (i.e., size of the database) increases.\n","authors":["Purity Mugambi","Alexandra Meliou","Madalina Fiterau"],"pdf_url":"https://arxiv.org/pdf/2412.11472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09003v5","updated":"2024-12-16T06:13:03Z","published":"2024-01-17T06:48:16Z","title":"Augmenting Math Word Problems via Iterative Question Composing","summary":"  Despite the advancements in large language models (LLMs) for mathematical\nreasoning, solving competition-level math problems remains a significant\nchallenge, especially for open-source LLMs without external tools. We introduce\nthe MMIQC dataset, comprising a mixture of processed web data and synthetic\nquestion-response pairs, aimed at enhancing the mathematical reasoning\ncapabilities of base language models. Models fine-tuned on MMIQC consistently\nsurpass their counterparts in performance on the MATH benchmark across various\nmodel sizes. Notably, Qwen-72B-MMIQC achieves a 45.0% accuracy, exceeding the\nprevious open-source state-of-the-art by 8.2% and outperforming the initial\nversion GPT-4 released in 2023. Extensive evaluation results on Hungarian high\nschool finals suggest that such improvement can generalize to unseen data. Our\nablation study on MMIQC reveals that a large part of the improvement can be\nattributed to our novel augmentation method, Iterative Question Composing\n(IQC), which involves iteratively composing new questions from seed problems\nusing an LLM and applying rejection sampling through another LLM.\n","authors":["Haoxiong Liu","Yifan Zhang","Yifan Luo","Andrew Chi-Chih Yao"],"pdf_url":"https://arxiv.org/pdf/2401.09003v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07972v2","updated":"2024-12-16T06:12:06Z","published":"2024-12-10T23:21:04Z","title":"Phase-aware Training Schedule Simplifies Learning in Flow-Based\n  Generative Models","summary":"  We analyze the training of a two-layer autoencoder used to parameterize a\nflow-based generative model for sampling from a high-dimensional Gaussian\nmixture. Previous work shows that the phase where the relative probability\nbetween the modes is learned disappears as the dimension goes to infinity\nwithout an appropriate time schedule. We introduce a time dilation that solves\nthis problem. This enables us to characterize the learned velocity field,\nfinding a first phase where the probability of each mode is learned and a\nsecond phase where the variance of each mode is learned. We find that the\nautoencoder representing the velocity field learns to simplify by estimating\nonly the parameters relevant to each phase. Turning to real data, we propose a\nmethod that, for a given feature, finds intervals of time where training\nimproves accuracy the most on that feature. Since practitioners take a uniform\ndistribution over training times, our method enables more efficient training.\nWe provide preliminary experiments validating this approach.\n","authors":["Santiago Aranguri","Francesco Insulla"],"pdf_url":"https://arxiv.org/pdf/2412.07972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19347v3","updated":"2024-12-16T05:56:32Z","published":"2024-05-21T06:27:07Z","title":"Near-Field Spot Beamfocusing: A Correlation-Aware Transfer Learning\n  Approach","summary":"  Three-dimensional (3D) spot beamfocusing (SBF), in contrast to conventional\nangular-domain beamforming, concentrates radiating power within a very small\nvolume in both radial and angular domains in the near-field zone. Recently the\nimplementation of channel-state-information (CSI)-independent machine learning\n(ML)-based approaches have been developed for effective SBF using extremely\nlarge-scale programmable metasurface (ELPMs). These methods involve dividing\nthe ELPMs into subarrays and independently training them with Deep\nReinforcement Learning to jointly focus the beam at the desired focal point\n(DFP).\n  This paper explores near-field SBF using ELPMs, addressing challenges\nassociated with lengthy training times resulting from independent training of\nsubarrays. To achieve a faster CSI-independent solution, inspired by the\ncorrelation between the beamfocusing matrices of the subarrays, we leverage\ntransfer learning techniques. First, we introduce a novel similarity criterion\nbased on the phase distribution image (PDI) of subarray apertures. Then we\ndevise a subarray policy propagation scheme that transfers the knowledge from\ntrained to untrained subarrays. We further enhance learning by introducing\nquasi-liquid layers as a revised version of the adaptive policy reuse\ntechnique. We show through simulations that the proposed scheme improves the\ntraining speed about 5 times. Furthermore, for dynamic DFP management, we\ndevised a DFP policy blending process, which augments the convergence rate up\nto 8-fold.\n","authors":["Mohammad Amir Fallah","Mehdi Monemi","Mehdi Rasti","Matti Latva-Aho"],"pdf_url":"https://arxiv.org/pdf/2405.19347v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11466v1","updated":"2024-12-16T05:47:35Z","published":"2024-12-16T05:47:35Z","title":"Mining In-distribution Attributes in Outliers for Out-of-distribution\n  Detection","summary":"  Out-of-distribution (OOD) detection is indispensable for deploying reliable\nmachine learning systems in real-world scenarios. Recent works, using auxiliary\noutliers in training, have shown good potential. However, they seldom concern\nthe intrinsic correlations between in-distribution (ID) and OOD data. In this\nwork, we discover an obvious correlation that OOD data usually possesses\nsignificant ID attributes. These attributes should be factored into the\ntraining process, rather than blindly suppressed as in previous approaches.\nBased on this insight, we propose a structured multi-view-based\nout-of-distribution detection learning (MVOL) framework, which facilitates\nrational handling of the intrinsic in-distribution attributes in outliers. We\nprovide theoretical insights on the effectiveness of MVOL for OOD detection.\nExtensive experiments demonstrate the superiority of our framework to others.\nMVOL effectively utilizes both auxiliary OOD datasets and even wild datasets\nwith noisy in-distribution data. Code is available at\nhttps://github.com/UESTC-nnLab/MVOL.\n","authors":["Yutian Lei","Luping Ji","Pei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11466v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.09265v3","updated":"2024-12-16T05:43:20Z","published":"2024-12-12T13:22:02Z","title":"Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation","summary":"  Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.\n","authors":["Bofang Jia","Pengxiang Ding","Can Cui","Mingyang Sun","Pengfang Qian","Siteng Huang","Zhaoxin Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09265v3.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2412.11463v1","updated":"2024-12-16T05:43:14Z","published":"2024-12-16T05:43:14Z","title":"FedCAR: Cross-client Adaptive Re-weighting for Generative Models in\n  Federated Learning","summary":"  Generative models trained on multi-institutional datasets can provide an\nenriched understanding through diverse data distributions. However, training\nthe models on medical images is often challenging due to hospitals' reluctance\nto share data for privacy reasons. Federated learning(FL) has emerged as a\nprivacy-preserving solution for training distributed datasets across data\ncenters by aggregating model weights from multiple clients instead of sharing\nraw data. Previous research has explored the adaptation of FL to generative\nmodels, yet effective aggregation algorithms specifically tailored for\ngenerative models remain unexplored. We hereby propose a novel algorithm aimed\nat improving the performance of generative models within FL. Our approach\nadaptively re-weights the contribution of each client, resulting in\nwell-trained shared parameters. In each round, the server side measures the\ndistribution distance between fake images generated by clients instead of\ndirectly comparing the Fr\\'echet Inception Distance per client, thereby\nenhancing efficiency of the learning. Experimental results on three public\nchest X-ray datasets show superior performance in medical image generation,\noutperforming both centralized learning and conventional FL algorithms. Our\ncode is available at https://github.com/danny0628/FedCAR.\n","authors":["Minjun Kim","Minjee Kim","Jinhoon Jeong"],"pdf_url":"https://arxiv.org/pdf/2412.11463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10942v4","updated":"2024-12-16T05:37:51Z","published":"2024-06-16T13:44:41Z","title":"Effective Generative AI: The Human-Algorithm Centaur","summary":"  Advanced analytics science methods have enabled combining the power of\nartificial and human intelligence, creating \\textit{centaurs} that allow\nsuperior decision-making. Centaurs are hybrid human-algorithm models that\ncombine both formal analytics and human intuition in a symbiotic manner within\ntheir learning and reasoning process. We argue that the future of AI\ndevelopment and use in many domains needs to focus more on centaurs as opposed\nto other AI approaches. This paradigm shift towards centaur-based AI methods\nraises some fundamental questions: How are centaurs different from other\nhuman-in-the-loop methods? What are the most effective methods for creating\ncentaurs? When should centaurs be used, and when should the lead be given to\npure AI models? Doesn't the incorporation of human intuition -- which at times\ncan be misleading -- in centaurs' decision-making process degrade its\nperformance compared to pure AI methods? This work aims to address these\nfundamental questions, focusing on recent advancements in generative AI, and\nespecially in Large Language Models (LLMs), as a main case study to illustrate\ncentaurs' critical essentiality to future AI endeavors.\n","authors":["Soroush Saghafian","Lihi Idan"],"pdf_url":"https://arxiv.org/pdf/2406.10942v4.pdf","comment":"To Appear in SI: Future Shock, Harvard Data Science Review\n  (https://hdsr.mitpress.mit.edu/specialissue5)"},{"id":"http://arxiv.org/abs/2412.11461v1","updated":"2024-12-16T05:35:58Z","published":"2024-12-16T05:35:58Z","title":"Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation","summary":"  Unsupervised anomaly detection (UAD) plays an important role in modern data\nanalytics and it is crucial to provide simple yet effective and guaranteed UAD\nalgorithms for real applications. In this paper, we present a novel UAD method\nfor tabular data by evaluating how much noise is in the data. Specifically, we\npropose to learn a deep neural network from the clean (normal) training dataset\nand a noisy dataset, where the latter is generated by adding highly diverse\nnoises to the clean data. The neural network can learn a reliable decision\nboundary between normal data and anomalous data when the diversity of the\ngenerated noisy data is sufficiently high so that the hard abnormal samples lie\nin the noisy region. Importantly, we provide theoretical guarantees, proving\nthat the proposed method can detect anomalous data successfully, although the\nmethod does not utilize any real anomalous data in the training stage.\nExtensive experiments through more than 60 benchmark datasets demonstrate the\neffectiveness of the proposed method in comparison to 12 baselines of UAD. Our\nmethod obtains a 92.27\\% AUC score and a 1.68 ranking score on average.\nMoreover, compared to the state-of-the-art UAD methods, our method is easier to\nimplement.\n","authors":["Wei Dai","Kai Hwang","Jicong Fan"],"pdf_url":"https://arxiv.org/pdf/2412.11461v1.pdf","comment":"The paper was accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2405.09004v2","updated":"2024-12-16T05:34:33Z","published":"2024-05-15T00:04:08Z","title":"Improving Sequential Market Coordination via Value-oriented Renewable\n  Energy Forecasting","summary":"  Large penetration of renewable energy sources (RESs) brings huge uncertainty\ninto the electricity markets. The current deterministic clearing approach in\nthe day-ahead (DA) market, where RESs participate based on expected production,\nhas been criticized for causing a lack of coordination between the DA and\nreal-time (RT) markets, leading to high overall operating costs. Previous works\nindicate that improving day-ahead RES entering quantities can significantly\nmitigate the drawbacks of deterministic clearing. In this work, we propose\nusing a trained forecasting model, referred to as value-oriented forecasting,\nto determine RES Improved Entering Quantities (RIEQ) more efficiently during\nthe operational phase. Unlike traditional models that minimize statistical\nforecasting errors, our approach trains model parameters to minimize the\nexpected overall operating costs across both DA and RT markets. We derive the\nexact form of the loss function used for training, which becomes piecewise\nlinear when market clearing is modeled by linear programs. Additionally, we\nprovide the analytical gradient of the loss function with respect to the\nforecast, enabling an efficient training strategy. Numerical studies\ndemonstrate that our forecasts significantly reduce overall operating costs for\ndeterministic market clearing compared to conventional forecasts based on\nexpected RES production.\n","authors":["Yufan Zhang","Honglin Wen","Yuexin Bian","Yuanyuan Shi"],"pdf_url":"https://arxiv.org/pdf/2405.09004v2.pdf","comment":"Submitted to IEEE Transactions on Energy Markets, Policy, and\n  Regulation"},{"id":"http://arxiv.org/abs/2412.11459v1","updated":"2024-12-16T05:33:05Z","published":"2024-12-16T05:33:05Z","title":"Understanding Knowledge Hijack Mechanism in In-context Learning through\n  Associative Memory","summary":"  In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks without fine-tuning by leveraging contextual information provided\nwithin a prompt. However, ICL relies not only on contextual clues but also on\nthe global knowledge acquired during pretraining for the next token prediction.\nAnalyzing this process has been challenging due to the complex computational\ncircuitry of LLMs. This paper investigates the balance between in-context\ninformation and pretrained bigram knowledge in token prediction, focusing on\nthe induction head mechanism, a key component in ICL. Leveraging the fact that\na two-layer transformer can implement the induction head mechanism with\nassociative memories, we theoretically analyze the logits when a two-layer\ntransformer is given prompts generated by a bigram model. In the experiments,\nwe design specific prompts to evaluate whether the outputs of a two-layer\ntransformer align with the theoretical results.\n","authors":["Shuo Wang","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2412.11459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11456v1","updated":"2024-12-16T05:23:01Z","published":"2024-12-16T05:23:01Z","title":"Regional Expected Improvement for Efficient Trust Region Selection in\n  High-Dimensional Bayesian Optimization","summary":"  Real-world optimization problems often involve complex objective functions\nwith costly evaluations. While Bayesian optimization (BO) with Gaussian\nprocesses is effective for these challenges, it suffers in high-dimensional\nspaces due to performance degradation from limited function evaluations. To\novercome this, simplification techniques like dimensionality reduction have\nbeen employed, yet they often rely on assumptions about the problem\ncharacteristics, potentially underperforming when these assumptions do not\nhold. Trust-region-based methods, which avoid such assumptions, focus on local\nsearch but risk stagnation in local optima. In this study, we propose a novel\nacquisition function, regional expected improvement (REI), designed to enhance\ntrust-region-based BO in medium to high-dimensional settings. REI identifies\nregions likely to contain the global optimum, improving performance without\nrelying on specific problem characteristics. We provide a theoretical proof\nthat REI effectively identifies optimal trust regions and empirically\ndemonstrate that incorporating REI into trust-region-based BO outperforms\nconventional BO and other high-dimensional BO methods in medium to\nhigh-dimensional real-world problems.\n","authors":["Nobuo Namura","Sho Takemori"],"pdf_url":"https://arxiv.org/pdf/2412.11456v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11452v1","updated":"2024-12-16T05:14:08Z","published":"2024-12-16T05:14:08Z","title":"Multilabel Classification for Lung Disease Detection: Integrating Deep\n  Learning and Natural Language Processing","summary":"  Classifying chest radiographs is a time-consuming and challenging task, even\nfor experienced radiologists. This provides an area for improvement due to the\ndifficulty in precisely distinguishing between conditions such as pleural\neffusion, pneumothorax, and pneumonia. We propose a novel transfer learning\nmodel for multi-label lung disease classification, utilizing the CheXpert\ndataset with over 12,617 images of frontal radiographs being analyzed. By\nintegrating RadGraph parsing for efficient annotation extraction, we enhance\nthe model's ability to accurately classify multiple lung diseases from complex\nmedical images. The proposed model achieved an F1 score of 0.69 and an AUROC of\n0.86, demonstrating its potential for clinical applications. Also explored was\nthe use of Natural Language Processing (NLP) to parse report metadata and\naddress uncertainties in disease classification. By comparing uncertain reports\nwith more certain cases, the NLP-enhanced model improves its ability to\nconclusively classify conditions. This research highlights the connection\nbetween deep learning and NLP, underscoring their potential to enhance\nradiological diagnostics and aid in the efficient analysis of chest\nradiographs.\n","authors":["Maria Efimovich","Jayden Lim","Vedant Mehta","Ethan Poon"],"pdf_url":"https://arxiv.org/pdf/2412.11452v1.pdf","comment":"All authors contributed equally"},{"id":"http://arxiv.org/abs/2412.11451v1","updated":"2024-12-16T05:10:58Z","published":"2024-12-16T05:10:58Z","title":"Data-Dependent Generalization Bounds for Parameterized Quantum Models\n  Under Noise","summary":"  Quantum machine learning offers a transformative approach to solving complex\nproblems, but the inherent noise hinders its practical implementation in\nnear-term quantum devices. This obstacle makes it challenging to understand the\ngeneralization capabilities of quantum circuit models. Designing robust quantum\nmachine learning models under noise requires a principled understanding of\ncomplexity and generalization, extending beyond classical capacity measures.\nThis study investigates the generalization properties of parameterized quantum\nmachine learning models under the influence of noise. We present a\ndata-dependent generalization bound grounded in the quantum Fisher information\nmatrix. We leverage statistical learning theory to relate the parameter space\nvolumes and training sizes to estimate the generalization capability of the\ntrained model. By integrating local parameter neighborhoods and effective\ndimensions defined through quantum Fisher information matrix eigenvalues, we\nprovide a structured characterization of complexity in quantum models. We\nanalyze the tightness of the bound and discuss the trade-off between model\nexpressiveness and generalization performance.\n","authors":["Bikram Khanal","Pablo Rivas"],"pdf_url":"https://arxiv.org/pdf/2412.11451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02144v2","updated":"2024-12-16T05:10:56Z","published":"2024-10-03T02:07:59Z","title":"SoundMorpher: Perceptually-Uniform Sound Morphing with Diffusion Model","summary":"  We present SoundMorpher, an open-world sound morphing method designed to\ngenerate perceptually uniform morphing trajectories. Traditional sound morphing\ntechniques typically assume a linear relationship between the morphing factor\nand sound perception, achieving smooth transitions by linearly interpolating\nthe semantic features of source and target sounds while gradually adjusting the\nmorphing factor. However, these methods oversimplify the complexities of sound\nperception, resulting in limitations in morphing quality. In contrast,\nSoundMorpher explores an explicit relationship between the morphing factor and\nthe perception of morphed sounds, leveraging log Mel-spectrogram features. This\napproach further refines the morphing sequence by ensuring a constant target\nperceptual difference for each transition and determining the corresponding\nmorphing factors using binary search. To address the lack of a formal\nquantitative evaluation framework for sound morphing, we propose a set of\nmetrics based on three established objective criteria. These metrics enable\ncomprehensive assessment of morphed results and facilitate direct comparisons\nbetween methods, fostering advancements in sound morphing research. Extensive\nexperiments demonstrate the effectiveness and versatility of SoundMorpher in\nreal-world scenarios, showcasing its potential in applications such as creative\nmusic composition, film post-production, and interactive audio technologies.\nOur demonstration and codes are available\nat~\\url{https://xinleiniu.github.io/SoundMorpher-demo/}.\n","authors":["Xinlei Niu","Jing Zhang","Charles Patrick Martin"],"pdf_url":"https://arxiv.org/pdf/2410.02144v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2402.12121v2","updated":"2024-12-16T16:09:47Z","published":"2024-02-19T13:16:10Z","title":"IRR: Image Review Ranking Framework for Evaluating Vision-Language\n  Models","summary":"  Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.\n","authors":["Kazuki Hayashi","Kazuma Onishi","Toma Suzuki","Yusuke Ide","Seiji Gobara","Shigeki Saito","Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2402.12121v2.pdf","comment":"18pages, Accepted at COLING25"},{"id":"http://arxiv.org/abs/2412.11851v1","updated":"2024-12-16T15:11:03Z","published":"2024-12-16T15:11:03Z","title":"A Benchmark and Robustness Study of In-Context-Learning with Large\n  Language Models in Music Entity Detection","summary":"  Detecting music entities such as song titles or artist names is a useful\napplication to help use cases like processing music search queries or analyzing\nmusic consumption on the web. Recent approaches incorporate smaller language\nmodels (SLMs) like BERT and achieve high results. However, further research\nindicates a high influence of entity exposure during pre-training on the\nperformance of the models. With the advent of large language models (LLMs),\nthese outperform SLMs in a variety of downstream tasks. However, researchers\nare still divided if this is applicable to tasks like entity detection in texts\ndue to issues like hallucination. In this paper, we provide a novel dataset of\nuser-generated metadata and conduct a benchmark and a robustness study using\nrecent LLMs with in-context-learning (ICL). Our results indicate that LLMs in\nthe ICL setting yield higher performance than SLMs. We further uncover the\nlarge impact of entity exposure on the best performing LLM in our study.\n","authors":["Simon Hachmeier","Robert Jäschke"],"pdf_url":"https://arxiv.org/pdf/2412.11851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11818v1","updated":"2024-12-16T14:35:32Z","published":"2024-12-16T14:35:32Z","title":"Leveraging User-Generated Metadata of Online Videos for Cover Song\n  Identification","summary":"  YouTube is a rich source of cover songs. Since the platform itself is\norganized in terms of videos rather than songs, the retrieval of covers is not\ntrivial. The field of cover song identification addresses this problem and\nprovides approaches that usually rely on audio content. However, including the\nuser-generated video metadata available on YouTube promises improved\nidentification results. In this paper, we propose a multi-modal approach for\ncover song identification on online video platforms. We combine the entity\nresolution models with audio-based approaches using a ranking model. Our\nfindings implicate that leveraging user-generated metadata can stabilize cover\nsong identification performance on YouTube.\n","authors":["Simon Hachmeier","Robert Jäschke"],"pdf_url":"https://arxiv.org/pdf/2412.11818v1.pdf","comment":"accepted for presentation at NLP for Music and Audio (NLP4MusA) 2024"},{"id":"http://arxiv.org/abs/2404.13282v2","updated":"2024-12-16T14:33:03Z","published":"2024-04-20T06:01:09Z","title":"Wills Aligner: Multi-Subject Collaborative Brain Visual Decoding","summary":"  Decoding visual information from human brain activity has seen remarkable\nadvancements in recent research. However, the diversity in cortical\nparcellation and fMRI patterns across individuals has prompted the development\nof deep learning models tailored to each subject. The personalization limits\nthe broader applicability of brain visual decoding in real-world scenarios. To\naddress this issue, we introduce Wills Aligner, a novel approach designed to\nachieve multi-subject collaborative brain visual decoding. Wills Aligner begins\nby aligning the fMRI data from different subjects at the anatomical level. It\nthen employs delicate mixture-of-brain-expert adapters and a meta-learning\nstrategy to account for individual fMRI pattern differences. Additionally,\nWills Aligner leverages the semantic relation of visual stimuli to guide the\nlearning of inter-subject commonality, enabling visual decoding for each\nsubject to draw insights from other subjects' data. We rigorously evaluate our\nWills Aligner across various visual decoding tasks, including classification,\ncross-modal retrieval, and image reconstruction. The experimental results\ndemonstrate that Wills Aligner achieves promising performance.\n","authors":["Guangyin Bao","Qi Zhang","Zixuan Gong","Jialei Zhou","Wei Fan","Kun Yi","Usman Naseem","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2404.13282v2.pdf","comment":"AAAI 2025, 16 pages"},{"id":"http://arxiv.org/abs/2404.12630v2","updated":"2024-12-16T13:59:51Z","published":"2024-04-19T05:12:04Z","title":"MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and\n  Semantic Correction","summary":"  Decoding natural visual scenes from brain activity has flourished, with\nextensive research in single-subject tasks and, however, less in cross-subject\ntasks. Reconstructing high-quality images in cross-subject tasks is a\nchallenging problem due to profound individual differences between subjects and\nthe scarcity of data annotation. In this work, we proposed MindTuner for\ncross-subject visual decoding, which achieves high-quality and rich semantic\nreconstructions using only 1 hour of fMRI training data benefiting from the\nphenomena of visual fingerprint in the human visual system and a novel\nfMRI-to-text alignment paradigm. Firstly, we pre-train a multi-subject model\namong 7 subjects and fine-tune it with scarce data on new subjects, where LoRAs\nwith Skip-LoRAs are utilized to learn the visual fingerprint. Then, we take the\nimage modality as the intermediate pivot modality to achieve fMRI-to-text\nalignment, which achieves impressive fMRI-to-text retrieval performance and\ncorrects fMRI-to-image reconstruction with fine-tuned semantics. The results of\nboth qualitative and quantitative analyses demonstrate that MindTuner surpasses\nstate-of-the-art cross-subject visual decoding models on the Natural Scenes\nDataset (NSD), whether using training data of 1 hour or 40 hours.\n","authors":["Zixuan Gong","Qi Zhang","Guangyin Bao","Lei Zhu","Ke Liu","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2404.12630v2.pdf","comment":"AAAI 2025, 14 pages"},{"id":"http://arxiv.org/abs/2412.11762v1","updated":"2024-12-16T13:26:52Z","published":"2024-12-16T13:26:52Z","title":"GS-ProCams: Gaussian Splatting-based Projector-Camera Systems","summary":"  We present GS-ProCams, the first Gaussian Splatting-based framework for\nprojector-camera systems (ProCams). GS-ProCams significantly enhances the\nefficiency of projection mapping (PM) that requires establishing geometric and\nradiometric mappings between the projector and the camera. Previous CNN-based\nProCams are constrained to a specific viewpoint, limiting their applicability\nto novel perspectives. In contrast, NeRF-based ProCams support view-agnostic\nprojection mapping, however, they require an additional colocated light source\nand demand significant computational and memory resources. To address this\nissue, we propose GS-ProCams that employs 2D Gaussian for scene\nrepresentations, and enables efficient view-agnostic ProCams applications. In\nparticular, we explicitly model the complex geometric and photometric mappings\nof ProCams using projector responses, the target surface's geometry and\nmaterials represented by Gaussians, and global illumination component. Then, we\nemploy differentiable physically-based rendering to jointly estimate them from\ncaptured multi-view projections. Compared to state-of-the-art NeRF-based\nmethods, our GS-ProCams eliminates the need for additional devices, achieving\nsuperior ProCams simulation quality. It is also 600 times faster and uses only\n1/10 of the GPU memory.\n","authors":["Qingyue Deng","Jijiang Li","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11715v1","updated":"2024-12-16T12:35:56Z","published":"2024-12-16T12:35:56Z","title":"Discrepancy-Aware Attention Network for Enhanced Audio-Visual Zero-Shot\n  Learning","summary":"  Audio-visual Zero-Shot Learning (ZSL) has attracted significant attention for\nits ability to identify unseen classes and perform well in video classification\ntasks. However, modal imbalance in (G)ZSL leads to over-reliance on the optimal\nmodality, reducing discriminative capabilities for unseen classes. Some studies\nhave attempted to address this issue by modifying parameter gradients, but two\nchallenges still remain: (a) Quality discrepancies, where modalities offer\ndiffering quantities and qualities of information for the same concept. (b)\nContent discrepancies, where sample contributions within a modality vary\nsignificantly. To address these challenges, we propose a Discrepancy-Aware\nAttention Network (DAAN) for Enhanced Audio-Visual ZSL. Our approach introduces\na Quality-Discrepancy Mitigation Attention (QDMA) unit to minimize redundant\ninformation in the high-quality modality and a Contrastive Sample-level\nGradient Modulation (CSGM) block to adjust gradient magnitudes and balance\ncontent discrepancies. We quantify modality contributions by integrating\noptimization and convergence rate for more precise gradient modulation in CSGM.\nExperiments demonstrates DAAN achieves state-of-the-art performance on\nbenchmark datasets, with ablation studies validating the effectiveness of\nindividual modules.\n","authors":["RunLin Yu","Yipu Gong","Wenrui Li","Aiwen Sun","Mengren Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.11715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11663v1","updated":"2024-12-16T11:11:23Z","published":"2024-12-16T11:11:23Z","title":"LMM-Regularized CLIP Embeddings for Image Classification","summary":"  In this paper we deal with image classification tasks using the powerful CLIP\nvision-language model. Our goal is to advance the classification performance\nusing the CLIP's image encoder, by proposing a novel Large Multimodal Model\n(LMM) based regularization method. The proposed method uses an LMM to extract\nsemantic descriptions for the images of the dataset. Then, it uses the CLIP's\ntext encoder, frozen, in order to obtain the corresponding text embeddings and\ncompute the mean semantic class descriptions. Subsequently, we adapt the CLIP's\nimage encoder by adding a classification head, and we train it along with the\nimage encoder output, apart from the main classification objective, with an\nadditional auxiliary objective. The additional objective forces the embeddings\nat the image encoder's output to become similar to their corresponding\nLMM-generated mean semantic class descriptions. In this way, it produces\nembeddings with enhanced discrimination ability, leading to improved\nclassification performance. The effectiveness of the proposed regularization\nmethod is validated through extensive experiments on three image classification\ndatasets.\n","authors":["Maria Tzelepi","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2412.11663v1.pdf","comment":"Accepted for publication, 26th Int. Symp. on Multimedia (IEEE ISM\n  2024), Tokyo, Japan, Dec. 2024. This is the authors' \"accepted version\""},{"id":"http://arxiv.org/abs/2412.11621v1","updated":"2024-12-16T10:08:38Z","published":"2024-12-16T10:08:38Z","title":"VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video\n  Prompting","summary":"  Large Language Model (LLM)-based agents have shown promise in procedural\ntasks, but the potential of multimodal instructions augmented by texts and\nvideos to assist users remains under-explored. To address this gap, we propose\nthe Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel\nLLM-empowered Multimodal Procedural Planning (MPP) framework. It generates\ncohesive text and video procedural plans given a specified high-level\nobjective. The main challenges are achieving textual and visual\ninformativeness, temporal coherence, and accuracy in procedural plans. VG-TVP\nleverages the zero-shot reasoning capability of LLMs, the video-to-text\ngeneration ability of the video captioning models, and the text-to-video\ngeneration ability of diffusion models. VG-TVP improves the interaction between\nmodalities by proposing a novel Fusion of Captioning (FoC) method and using\nText-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs\nto guide the generation of visually-grounded text plans and textual-grounded\nvideo plans. To address the scarcity of datasets suitable for MPP, we have\ncurated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We\nconduct comprehensive experiments and benchmarks to evaluate human preferences\n(regarding textual and visual informativeness, temporal coherence, and plan\naccuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP\ndataset.\n","authors":["Muhammet Furkan Ilaslan","Ali Koksal","Kevin Qinhong Lin","Burak Satar","Mike Zheng Shou","Qianli Xu"],"pdf_url":"https://arxiv.org/pdf/2412.11621v1.pdf","comment":"Accepted for The 39th Annual AAAI Conference on Artificial\n  Intelligence 2025 in Main Track, 19 pages, 24 figures"},{"id":"http://arxiv.org/abs/2410.06618v2","updated":"2024-12-16T09:52:32Z","published":"2024-10-09T07:14:49Z","title":"Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N\n  1-to-1 Relationships for Text-Video Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19180v3","updated":"2024-12-16T08:34:52Z","published":"2023-10-29T22:51:49Z","title":"JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music\n  Generation","summary":"  With rapid advances in generative artificial intelligence, the text-to-music\nsynthesis task has emerged as a promising direction for music generation.\nNevertheless, achieving precise control over multi-track generation remains an\nopen challenge. While existing models excel in directly generating multi-track\nmix, their limitations become evident when it comes to composing individual\ntracks and integrating them in a controllable manner. This departure from the\ntypical workflows of professional composers hinders the ability to refine\ndetails in specific tracks. To address this gap, we propose JEN-1 Composer, a\nunified framework designed to efficiently model marginal, conditional, and\njoint distributions over multi-track music using a single model. Building upon\nan audio latent diffusion model, JEN-1 Composer extends the versatility of\nmulti-track music generation. We introduce a progressive curriculum training\nstrategy, which gradually escalates the difficulty of training tasks while\nensuring the model's generalization ability and facilitating smooth transitions\nbetween different scenarios. During inference, users can iteratively generate\nand select music tracks, thus incrementally composing entire musical pieces in\naccordance with the Human-AI co-composition workflow. Our approach demonstrates\nstate-of-the-art performance in controllable and high-fidelity multi-track\nmusic synthesis, marking a significant advancement in interactive AI-assisted\nmusic creation. Our demo pages are available at www.jenmusic.ai/research.\n","authors":["Yao Yao","Peike Li","Boyu Chen","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19180v3.pdf","comment":"9 pages, 3 figures, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11409v1","updated":"2024-12-16T03:25:23Z","published":"2024-12-16T03:25:23Z","title":"Multi-modal and Multi-scale Spatial Environment Understanding for\n  Immersive Visual Text-to-Speech","summary":"  Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.\n","authors":["Rui Liu","Shuwei He","Yifan Hu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.11409v1.pdf","comment":"9 pages,2 figures, Accepted by AAAI'2025"},{"id":"http://arxiv.org/abs/2409.10958v2","updated":"2024-12-16T02:37:33Z","published":"2024-09-17T07:52:09Z","title":"Towards Effective User Attribution for Latent Diffusion Models via\n  Watermark-Informed Blending","summary":"  Rapid advancements in multimodal large language models have enabled the\ncreation of hyper-realistic images from textual descriptions. However, these\nadvancements also raise significant concerns about unauthorized use, which\nhinders their broader distribution. Traditional watermarking methods often\nrequire complex integration or degrade image quality. To address these\nchallenges, we introduce a novel framework Towards Effective user Attribution\nfor latent diffusion models via Watermark-Informed Blending (TEAWIB). TEAWIB\nincorporates a unique ready-to-use configuration approach that allows seamless\nintegration of user-specific watermarks into generative models. This approach\nensures that each user can directly apply a pre-configured set of parameters to\nthe model without altering the original model parameters or compromising image\nquality. Additionally, noise and augmentation operations are embedded at the\npixel level to further secure and stabilize watermarked images. Extensive\nexperiments validate the effectiveness of TEAWIB, showcasing the\nstate-of-the-art performance in perceptual quality and attribution accuracy.\n","authors":["Yongyang Pan","Xiaohong Liu","Siqi Luo","Yi Xin","Xiao Guo","Xiaoming Liu","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2409.10958v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.10719v3","updated":"2024-12-16T00:37:54Z","published":"2024-09-16T20:47:00Z","title":"Benchmarking VLMs' Reasoning About Persuasive Atypical Images","summary":"  Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available.\n","authors":["Sina Malakouti","Aysan Aghazadeh","Ashmit Khandelwal","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2409.10719v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12331v1","updated":"2024-12-16T20:01:35Z","published":"2024-12-16T20:01:35Z","title":"Efficient Object-centric Representation Learning with Pre-trained\n  Geometric Prior","summary":"  This paper addresses key challenges in object-centric representation learning\nof video. While existing approaches struggle with complex scenes, we propose a\nnovel weakly-supervised framework that emphasises geometric understanding and\nleverages pre-trained vision models to enhance object discovery. Our method\nintroduces an efficient slot decoder specifically designed for object-centric\nlearning, enabling effective representation of multi-object scenes without\nrequiring explicit depth information. Results on synthetic video benchmarks\nwith increasing complexity in terms of objects and their movement, object\nocclusion and camera motion demonstrate that our approach achieves comparable\nperformance to supervised methods while maintaining computational efficiency.\nThis advances the field towards more practical applications in complex\nreal-world scenarios.\n","authors":["Phúc H. Le Khac","Graham Healy","Alan F. Smeaton"],"pdf_url":"https://arxiv.org/pdf/2412.12331v1.pdf","comment":"6 pages, 4 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2412.12225v1","updated":"2024-12-16T10:03:44Z","published":"2024-12-16T10:03:44Z","title":"DLF: Disentangled-Language-Focused Multimodal Sentiment Analysis","summary":"  Multimodal Sentiment Analysis (MSA) leverages heterogeneous modalities, such\nas language, vision, and audio, to enhance the understanding of human\nsentiment. While existing models often focus on extracting shared information\nacross modalities or directly fusing heterogeneous modalities, such approaches\ncan introduce redundancy and conflicts due to equal treatment of all modalities\nand the mutual transfer of information between modality pairs. To address these\nissues, we propose a Disentangled-Language-Focused (DLF) multimodal\nrepresentation learning framework, which incorporates a feature disentanglement\nmodule to separate modality-shared and modality-specific information. To\nfurther reduce redundancy and enhance language-targeted features, four\ngeometric measures are introduced to refine the disentanglement process. A\nLanguage-Focused Attractor (LFA) is further developed to strengthen language\nrepresentation by leveraging complementary modality-specific information\nthrough a language-guided cross-attention mechanism. The framework also employs\nhierarchical predictions to improve overall accuracy. Extensive experiments on\ntwo popular MSA datasets, CMU-MOSI and CMU-MOSEI, demonstrate the significant\nperformance gains achieved by the proposed DLF framework. Comprehensive\nablation studies further validate the effectiveness of the feature\ndisentanglement module, language-focused attractor, and hierarchical\npredictions. Our code is available at https://github.com/pwang322/DLF.\n","authors":["Pan Wang","Qiang Zhou","Yawen Wu","Tianlong Chen","Jingtong Hu"],"pdf_url":"https://arxiv.org/pdf/2412.12225v1.pdf","comment":"AAAI 2025 accepted"}],"Performance":[{"id":"http://arxiv.org/abs/2412.11355v1","updated":"2024-12-16T01:09:12Z","published":"2024-12-16T01:09:12Z","title":"Chopin: An Open Source R-language Tool to Support Spatial Analysis on\n  Parallelizable Infrastructure","summary":"  An increasing volume of studies utilize geocomputation methods in large\nspatial data. There is a bottleneck in scalable computation for general\nscientific use as the existing solutions require high-performance computing\ndomain knowledge and are tailored for specific use cases. This study presents\nan R package `chopin` to reduce the technical burden for parallelization in\ngeocomputation. Supporting popular spatial analysis packages in R, `chopin`\nleverages parallel computing by partitioning data that are involved in a\ncomputation task. The partitioning is implemented at regular grids, data\nhierarchies, and multiple file inputs with flexible input types for\ninteroperability between different packages and efficiency. This approach makes\nthe geospatial covariate calculation to the scale of the available processing\npower in a wide range of computing assets from laptop computers to\nhigh-performance computing infrastructure. Testing use cases in environmental\nexposure assessment demonstrated that the package reduced the execution time by\norder of processing units used. The work is expected to provide broader\nresearch communities using geospatial data with an efficient tool to process\nlarge scale data.\n","authors":["Insang Song","Kyle P. Messier"],"pdf_url":"https://arxiv.org/pdf/2412.11355v1.pdf","comment":null}],"Database":[{"id":"http://arxiv.org/abs/2412.11828v1","updated":"2024-12-16T14:49:32Z","published":"2024-12-16T14:49:32Z","title":"The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey","summary":"  View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.\n","authors":["Sergey Zinchenko","Denis Ponomaryov"],"pdf_url":"https://arxiv.org/pdf/2412.11828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11827v1","updated":"2024-12-16T14:47:51Z","published":"2024-12-16T14:47:51Z","title":"Hyperparametric Robust and Dynamic Influence Maximization","summary":"  We study the problem of robust influence maximization in dynamic diffusion\nnetworks. In line with recent works, we consider the scenario where the network\ncan undergo insertion and removal of nodes and edges, in discrete time steps,\nand the influence weights are determined by the features of the corresponding\nnodes and a global hyperparameter. Given this, our goal is to find, at every\ntime step, the seed set maximizing the worst-case influence spread across all\npossible values of the hyperparameter. We propose an approximate solution using\nmultiplicative weight updates and a greedy algorithm, with provable quality\nguarantees. Our experiments validate the effectiveness and efficiency of the\nproposed methods.\n","authors":["Arkaprava Saha","Bogdan Cautis","Xiaokui Xiao","Laks V. S. Lakshmanan"],"pdf_url":"https://arxiv.org/pdf/2412.11827v1.pdf","comment":"AAAI Conference on Artificial Intelligence 2025 (Main Technical\n  Track)"},{"id":"http://arxiv.org/abs/2411.04920v3","updated":"2024-12-16T14:05:03Z","published":"2024-11-07T17:57:03Z","title":"GPTKB: Comprehensively Materializing Factual LLM Knowledge","summary":"  LLMs have majorly advanced NLP and AI, and next to their ability to perform a\nwide range of procedural tasks, a major success factor is their internalized\nfactual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has\ngained attention. However, most approaches investigate one question at a time\nvia modest-sized pre-defined samples, introducing an availability bias (Tversky\nand Kahnemann, 1973) that prevents the discovery of knowledge (or beliefs) of\nLLMs beyond the experimenter's predisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterializing an LLM's factual knowledge through recursive querying and result\nconsolidation.\n  As a prototype, we employ GPT-4o-mini to construct GPTKB, a large-scale\nknowledge base (KB) comprising 105 million triples for over 2.9 million\nentities - achieved at 1% of the cost of previous KB projects. This work marks\na milestone in two areas: For LLM research, for the first time, it provides\nconstructive insights into the scope and structure of LLMs' knowledge (or\nbeliefs). For KB construction, it pioneers new pathways for the long-standing\nchallenge of general-domain KB construction. GPTKB is accessible at\nhttps://gptkb.org.\n","authors":["Yujia Hu","Tuan-Phong Nguyen","Shrestha Ghosh","Simon Razniewski"],"pdf_url":"https://arxiv.org/pdf/2411.04920v3.pdf","comment":"13 pages, 4 tables, 10 figures"},{"id":"http://arxiv.org/abs/2406.12692v2","updated":"2024-12-16T13:52:51Z","published":"2024-06-18T15:06:06Z","title":"MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL","summary":"  Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhance the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. We make all agent interactions publicly available to\nthe research community, to foster further research in this area, offering a\nsynthetic dataset for future explorations into automatic self-correction\nguideline generation.\n","authors":["Arian Askari","Christian Poelitz","Xinye Tang"],"pdf_url":"https://arxiv.org/pdf/2406.12692v2.pdf","comment":"Accepted at Proceedings of the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.11669v1","updated":"2024-12-16T11:21:24Z","published":"2024-12-16T11:21:24Z","title":"Soft and Constrained Hypertree Width","summary":"  Hypertree decompositions provide a way to evaluate Conjunctive Queries (CQs)\nin polynomial time, where the exponent of this polynomial is determined by the\nwidth of the decomposition. In theory, the goal of efficient CQ evaluation\ntherefore has to be a minimisation of the width. However, in practical\nsettings, it turns out that there are also other properties of a decomposition\nthat influence the performance of query evaluation. It is therefore of interest\nto restrict the computation of decompositions by constraints and to guide this\ncomputation by preferences. To this end, we propose a novel framework based on\ncandidate tree decompositions, which allows us to introduce soft hypertree\nwidth (shw). This width measure is a relaxation of hypertree width (hw); it is\nnever greater than hw and, in some cases, shw may actually be lower than hw.\nost importantly, shw preserves the tractability of deciding if a given CQ is\nbelow some fixed bound, while offering more algorithmic flexibility. In\nparticular, it provides a natural way to incorporate preferences A prototype\nimplementation and preliminary experiments confirm that this novel framework\ncan indeed have a practical impact on query evaluation.\n","authors":["Matthias Lanzinger","Cem Okulmus","Reinhard Pichler","Alexander Selzer","Georg Gottlob"],"pdf_url":"https://arxiv.org/pdf/2412.11669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11661v1","updated":"2024-12-16T11:03:37Z","published":"2024-12-16T11:03:37Z","title":"Rewriting Consistent Answers on Annotated Data","summary":"  We embark on a study of the consistent answers of queries over databases\nannotated with values from a naturally ordered positive semiring. In this\nsetting, the consistent answers of a query are defined as the minimum of the\nsemiring values that the query takes over all repairs of an inconsistent\ndatabase. The main focus is on self-join free conjunctive queries and key\nconstraints, which is the most extensively studied case of consistent query\nanswering over standard databases. We introduce a variant of first-order logic\nwith a limited form of negation, define suitable semiring semantics, and then\nestablish the main result of the paper: the consistent query answers of a\nself-join free conjunctive query under key constraints are rewritable in this\nlogic if and only if the attack graph of the query contains no cycles. This\nresult generalizes an analogous result of Koutris and Wijsen for ordinary\ndatabases, but also yields new results for a multitude of semirings, including\nthe bag semiring, the tropical semiring, and the fuzzy semiring. We also show\nthat there are self-join free conjunctive queries with a cyclic attack graph\nwhose certain answers under bag semantics have no polynomial-time\nconstant-approximation algorithm, unless P = NP.\n","authors":["Phokion G. Kolaitis","Nina Pardal","Jonni Virtema"],"pdf_url":"https://arxiv.org/pdf/2412.11661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11617v1","updated":"2024-12-16T10:01:18Z","published":"2024-12-16T10:01:18Z","title":"Dung's Argumentation Framework: Unveiling the Expressive Power with\n  Inconsistent Databases","summary":"  The connection between inconsistent databases and Dung's abstract\nargumentation framework has recently drawn growing interest. Specifically, an\ninconsistent database, involving certain types of integrity constraints such as\nfunctional and inclusion dependencies, can be viewed as an argumentation\nframework in Dung's setting. Nevertheless, no prior work has explored the exact\nexpressive power of Dung's theory of argumentation when compared to\ninconsistent databases and integrity constraints. In this paper, we close this\ngap by arguing that an argumentation framework can also be viewed as an\ninconsistent database. We first establish a connection between subset-repairs\nfor databases and extensions for AFs, considering conflict-free, naive,\nadmissible, and preferred semantics. Further, we define a new family of\nattribute-based repairs based on the principle of maximal content preservation.\nThe effectiveness of these repairs is then highlighted by connecting them to\nstable, semi-stable, and stage semantics. Our main contributions include\ntranslating an argumentation framework into a database together with integrity\nconstraints. Moreover, this translation can be achieved in polynomial time,\nwhich is essential in transferring complexity results between the two\nformalisms.\n","authors":["Yasir Mahmood","Markus Hecher","Axel-Cyrille Ngonga Ngomo"],"pdf_url":"https://arxiv.org/pdf/2412.11617v1.pdf","comment":"Accepted at AAAI 2025, the current version contains full proof\n  details"},{"id":"http://arxiv.org/abs/2312.15395v2","updated":"2024-12-16T08:57:29Z","published":"2023-12-24T03:37:11Z","title":"Prompt Valuation Based on Shapley Values","summary":"  Large language models (LLMs) excel on new tasks without additional training,\nsimply by providing natural language prompts that demonstrate how the task\nshould be performed. Prompt ensemble methods comprehensively harness the\nknowledge of LLMs while mitigating individual biases and errors and further\nenhancing performance. However, more prompts do not necessarily lead to better\nresults, and not all prompts are beneficial. A small number of high-quality\nprompts often outperform many low-quality prompts. Currently, there is a lack\nof a suitable method for evaluating the impact of prompts on the results. In\nthis paper, we utilize the Shapley value to fairly quantify the contributions\nof prompts, helping to identify beneficial or detrimental prompts, and\npotentially guiding prompt valuation in data markets. Through extensive\nexperiments employing various ensemble methods and utility functions on diverse\ntasks, we validate the effectiveness of using the Shapley value method for\nprompts as it effectively distinguishes and quantifies the contributions of\neach prompt.\n","authors":["Hanxi Liu","Xiaokai Mao","Haocheng Xia","Jian Lou","Jinfei Liu","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2312.15395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11557v1","updated":"2024-12-16T08:42:43Z","published":"2024-12-16T08:42:43Z","title":"Enhancing Healthcare Recommendation Systems with a Multimodal LLMs-based\n  MOE Architecture","summary":"  With the increasing availability of multimodal data, many fields urgently\nrequire advanced architectures capable of effectively integrating these diverse\ndata sources to address specific problems. This study proposes a hybrid\nrecommendation model that combines the Mixture of Experts (MOE) framework with\nlarge language models to enhance the performance of recommendation systems in\nthe healthcare domain. We built a small dataset for recommending healthy food\nbased on patient descriptions and evaluated the model's performance on several\nkey metrics, including Precision, Recall, NDCG, and MAP@5. The experimental\nresults show that the hybrid model outperforms the baseline models, which use\nMOE or large language models individually, in terms of both accuracy and\npersonalized recommendation effectiveness. The paper finds image data provided\nrelatively limited improvement in the performance of the personalized\nrecommendation system, particularly in addressing the cold start problem. Then,\nthe issue of reclassification of images also affected the recommendation\nresults, especially when dealing with low-quality images or changes in the\nappearance of items, leading to suboptimal performance. The findings provide\nvaluable insights into the development of powerful, scalable, and\nhigh-performance recommendation systems, advancing the application of\npersonalized recommendation technologies in real-world domains such as\nhealthcare.\n","authors":["Jingyu Xu","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11557v1.pdf","comment":"10 page, accpted by Conf-SMPL conference"}]},"2024-12-15T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.11216v1","updated":"2024-12-15T15:13:14Z","published":"2024-12-15T15:13:14Z","title":"Distribution-Consistency-Guided Multi-modal Hashing","summary":"  Multi-modal hashing methods have gained popularity due to their fast speed\nand low storage requirements. Among them, the supervised methods demonstrate\nbetter performance by utilizing labels as supervisory signals compared with\nunsupervised methods. Currently, for almost all supervised multi-modal hashing\nmethods, there is a hidden assumption that training sets have no noisy labels.\nHowever, labels are often annotated incorrectly due to manual labeling in\nreal-world scenarios, which will greatly harm the retrieval performance. To\naddress this issue, we first discover a significant distribution consistency\npattern through experiments, i.e., the 1-0 distribution of the presence or\nabsence of each category in the label is consistent with the high-low\ndistribution of similarity scores of the hash codes relative to category\ncenters. Then, inspired by this pattern, we propose a novel\nDistribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to\nfilter and reconstruct noisy labels to enhance retrieval performance.\nSpecifically, the proposed method first randomly initializes several category\ncenters, which are used to compute the high-low distribution of similarity\nscores; Noisy and clean labels are then separately filtered out via the\ndiscovered distribution consistency pattern to mitigate the impact of noisy\nlabels; Subsequently, a correction strategy, which is indirectly designed via\nthe distribution consistency pattern, is applied to the filtered noisy labels,\ncorrecting high-confidence ones while treating low-confidence ones as unlabeled\nfor unsupervised learning, thereby further enhancing the model's performance.\nExtensive experiments on three widely used datasets demonstrate the superiority\nof the proposed method compared to state-of-the-art baselines in multi-modal\nretrieval tasks. The code is available at\nhttps://github.com/LiuJinyu1229/DCGMH.\n","authors":["Jin-Yu Liu","Xian-Ling Mao","Tian-Yi Che","Rong-Cheng Tu"],"pdf_url":"https://arxiv.org/pdf/2412.11216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11203v1","updated":"2024-12-15T14:35:49Z","published":"2024-12-15T14:35:49Z","title":"Task-Oriented Dialog Systems for the Senegalese Wolof Language","summary":"  In recent years, we are seeing considerable interest in conversational agents\nwith the rise of large language models (LLMs). Although they offer considerable\nadvantages, LLMs also present significant risks, such as hallucination, which\nhinder their widespread deployment in industry. Moreover, low-resource\nlanguages such as African ones are still underrepresented in these systems\nlimiting their performance in these languages. In this paper, we illustrate a\nmore classical approach based on modular architectures of Task-oriented Dialog\nSystems (ToDS) offering better control over outputs. We propose a chatbot\ngeneration engine based on the Rasa framework and a robust methodology for\nprojecting annotations onto the Wolof language using an in-house machine\ntranslation system. After evaluating a generated chatbot trained on the Amazon\nMassive dataset, our Wolof Intent Classifier performs similarly to the one\nobtained for French, which is a resource-rich language. We also show that this\napproach is extensible to other low-resource languages, thanks to the intent\nclassifier's language-agnostic pipeline, simplifying the design of chatbots in\nthese languages.\n","authors":["Derguene Mbaye","Moussa Diallo"],"pdf_url":"https://arxiv.org/pdf/2412.11203v1.pdf","comment":"10 pages, 3 tables, 6 figures, The 31st International Conference on\n  Computational Linguistics (COLING 2025)"},{"id":"http://arxiv.org/abs/2412.11127v1","updated":"2024-12-15T09:17:45Z","published":"2024-12-15T09:17:45Z","title":"Modeling the Heterogeneous Duration of User Interest in Time-Dependent\n  Recommendation: A Hidden Semi-Markov Approach","summary":"  Recommender systems are widely used for suggesting books, education\nmaterials, and products to users by exploring their behaviors. In reality,\nusers' preferences often change over time, leading to studies on time-dependent\nrecommender systems. However, most existing approaches that deal with time\ninformation remain primitive. In this paper, we extend existing methods and\npropose a hidden semi-Markov model to track the change of users' interests.\nParticularly, this model allows for capturing the different durations of user\nstays in a (latent) interest state, which can better model the heterogeneity of\nuser interests and focuses. We derive an expectation maximization algorithm to\nestimate the parameters of the framework and predict users' actions.\nExperiments on three real-world datasets show that our model significantly\noutperforms the state-of-the-art time-dependent and static benchmark methods.\nFurther analyses of the experiment results indicate that the performance\nimprovement is related to the heterogeneity of state durations and the drift of\nuser interests in the dataset.\n","authors":["Haidong Zhang","Wancheng Ni","Xin Li","Yiping Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14548v2","updated":"2024-12-15T08:36:19Z","published":"2023-09-25T21:45:30Z","title":"Algorithmic Collusion or Competition: the Role of Platforms' Recommender\n  Systems","summary":"  Recent scholarly work has extensively examined the phenomenon of algorithmic\ncollusion driven by AI-enabled pricing algorithms. However, online platforms\ncommonly deploy recommender systems that influence how consumers discover and\npurchase products, thereby shaping the reward structures faced by pricing\nalgorithms and ultimately affecting competition dynamics and equilibrium\noutcomes. To address this gap in the literature and elucidate the role of\nrecommender systems, we propose a novel repeated game framework that integrates\nseveral key components. We first develop a structural search model to\ncharacterize consumers' decision-making processes in response to varying\nrecommendation sets. This model incorporates both observable and unobservable\nheterogeneity in utility and search cost functions, and is estimated using\nreal-world data. Building on the resulting consumer model, we formulate\npersonalized recommendation algorithms designed to maximize either platform\nrevenue or consumer utility. We further introduce pricing algorithms for\nsellers and integrate all these elements to facilitate comprehensive numerical\nexperiments. Our experimental findings reveal that a revenue-maximizing\nrecommender system intensifies algorithmic collusion, whereas a\nutility-maximizing recommender system encourages more competitive pricing\nbehavior among sellers. Intriguingly, and contrary to conventional insights\nfrom the industrial organization and choice modeling literature, increasing the\nsize of recommendation sets under a utility-maximizing regime does not\nconsistently enhance consumer utility. Moreover, the degree of horizontal\ndifferentiation moderates this phenomenon in unexpected ways. The \"more is\nless\" effect does not arise at low levels of differentiation, but becomes\nincreasingly pronounced as horizontal differentiation increases.\n","authors":["Xingchen Xu","Stephanie Lee","Yong Tan"],"pdf_url":"https://arxiv.org/pdf/2309.14548v2.pdf","comment":"54 pages, 4 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.11105v1","updated":"2024-12-15T08:08:07Z","published":"2024-12-15T08:08:07Z","title":"Multi-Graph Co-Training for Capturing User Intent in Session-based\n  Recommendation","summary":"  Session-based recommendation focuses on predicting the next item a user will\ninteract with based on sequences of anonymous user sessions. A significant\nchallenge in this field is data sparsity due to the typically short-term\ninteractions. Most existing methods rely heavily on users' current\ninteractions, overlooking the wealth of auxiliary information available. To\naddress this, we propose a novel model, the Multi-Graph Co-Training model\n(MGCOT), which leverages not only the current session graph but also similar\nsession graphs and a global item relation graph. This approach allows for a\nmore comprehensive exploration of intrinsic relationships and better captures\nuser intent from multiple views, enabling session representations to complement\neach other. Additionally, MGCOT employs multi-head attention mechanisms to\neffectively capture relevant session intent and uses contrastive learning to\nform accurate and robust session representations. Extensive experiments on\nthree datasets demonstrate that MGCOT significantly enhances the performance of\nsession-based recommendations, particularly on the Diginetica dataset,\nachieving improvements up to 2.00% in P@20 and 10.70% in MRR@20. Resources have\nbeen made publicly available in our GitHub repository\nhttps://github.com/liang-tian-tian/MGCOT.\n","authors":["Zhe Yang","Tiantian Liang"],"pdf_url":"https://arxiv.org/pdf/2412.11105v1.pdf","comment":"COLING 2025 Main Conference"},{"id":"http://arxiv.org/abs/2412.11087v1","updated":"2024-12-15T07:09:02Z","published":"2024-12-15T07:09:02Z","title":"Leveraging Large Vision-Language Model as User Intent-aware Encoder for\n  Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve target images from candidate\nset using a hybrid-modality query consisting of a reference image and a\nrelative caption that describes the user intent. Recent studies attempt to\nutilize Vision-Language Pre-training Models (VLPMs) with various fusion\nstrategies for addressing the task.However, these methods typically fail to\nsimultaneously meet two key requirements of CIR: comprehensively extracting\nvisual information and faithfully following the user intent. In this work, we\npropose CIR-LVLM, a novel framework that leverages the large vision-language\nmodel (LVLM) as the powerful user intent-aware encoder to better meet these\nrequirements. Our motivation is to explore the advanced reasoning and\ninstruction-following capabilities of LVLM for accurately understanding and\nresponding the user intent. Furthermore, we design a novel hybrid intent\ninstruction module to provide explicit intent guidance at two levels: (1) The\ntask prompt clarifies the task requirement and assists the model in discerning\nuser intent at the task level. (2) The instance-specific soft prompt, which is\nadaptively selected from the learnable prompt pool, enables the model to better\ncomprehend the user intent at the instance level compared to a universal prompt\nfor all instances. CIR-LVLM achieves state-of-the-art performance across three\nprominent benchmarks with acceptable inference efficiency. We believe this\nstudy provides fundamental insights into CIR-related fields.\n","authors":["Zelong Sun","Dong Jing","Guoxing Yang","Nanyi Fei","Zhiwu Lu"],"pdf_url":"https://arxiv.org/pdf/2412.11087v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11068v1","updated":"2024-12-15T05:57:36Z","published":"2024-12-15T05:57:36Z","title":"RecSys Arena: Pair-wise Recommender System Evaluation with Large\n  Language Models","summary":"  Evaluating the quality of recommender systems is critical for algorithm\ndesign and optimization. Most evaluation methods are computed based on offline\nmetrics for quick algorithm evolution, since online experiments are usually\nrisky and time-consuming. However, offline evaluation usually cannot fully\nreflect users' preference for the outcome of different recommendation\nalgorithms, and the results may not be consistent with online A/B test.\nMoreover, many offline metrics such as AUC do not offer sufficient information\nfor comparing the subtle differences between two competitive recommender\nsystems in different aspects, which may lead to substantial performance\ndifferences in long-term online serving. Fortunately, due to the strong\ncommonsense knowledge and role-play capability of large language models (LLMs),\nit is possible to obtain simulated user feedback on offline recommendation\nresults. Motivated by the idea of LLM Chatbot Arena, in this paper we present\nthe idea of RecSys Arena, where the recommendation results given by two\ndifferent recommender systems in each session are evaluated by an LLM judger to\nobtain fine-grained evaluation feedback. More specifically, for each sample we\nuse LLM to generate a user profile description based on user behavior history\nor off-the-shelf profile features, which is used to guide LLM to play the role\nof this user and evaluate the relative preference for two recommendation\nresults generated by different models. Through extensive experiments on two\nrecommendation datasets in different scenarios, we demonstrate that many\ndifferent LLMs not only provide general evaluation results that are highly\nconsistent with canonical offline metrics, but also provide rich insight in\nmany subjective aspects. Moreover, it can better distinguish different\nalgorithms with comparable performance in terms of AUC and nDCG.\n","authors":["Zhuo Wu","Qinglin Jia","Chuhan Wu","Zhaocheng Du","Shuai Wang","Zan Wang","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2412.11068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12202v1","updated":"2024-12-15T09:23:14Z","published":"2024-12-15T09:23:14Z","title":"A multi-theoretical kernel-based approach to social network-based\n  recommendation","summary":"  Recommender systems are a critical component of e-commercewebsites. The rapid\ndevelopment of online social networking services provides an opportunity to\nexplore social networks together with information used in traditional\nrecommender systems, such as customer demographics, product characteristics,\nand transactions. It also provides more applications for recommender systems.\nTo tackle this social network-based recommendation problem, previous studies\ngenerally built trust models in light of the social influence theory. This\nstudy inspects a spectrumof social network theories to systematicallymodel\nthemultiple facets of a social network and infer user preferences. In order to\neffectively make use of these heterogonous theories, we take a kernel-based\nmachine learning paradigm, design and select kernels describing individual\nsimilarities according to social network theories, and employ a non-linear\nmultiple kernel learning algorithm to combine the kernels into a unified model.\nThis design also enables us to consider multiple theories' interactions in\nassessing individual behaviors. We evaluate our proposed approach on a\nreal-world movie review data set. The experiments show that our approach\nprovides more accurate recommendations than trust-based methods and the\ncollaborative filtering approach. Further analysis shows that kernels derived\nfrom contagion theory and homophily theory contribute a larger portion of the\nmodel.\n","authors":["Xin Li","Mengyue Wang","T. -P. Liang"],"pdf_url":"https://arxiv.org/pdf/2412.12202v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.05050v4","updated":"2024-12-15T20:29:34Z","published":"2024-03-08T04:53:53Z","title":"DyRoNet: Dynamic Routing and Low-Rank Adapters for Autonomous Driving\n  Streaming Perception","summary":"  The advancement of autonomous driving systems hinges on the ability to\nachieve low-latency and high-accuracy perception. To address this critical\nneed, this paper introduces Dynamic Routing Network (DyRoNet), a low-rank\nenhanced dynamic routing framework designed for streaming perception in\nautonomous driving systems. DyRoNet integrates a suite of pre-trained branch\nnetworks, each meticulously fine-tuned to function under distinct environmental\nconditions. At its core, the framework offers a speed router module, developed\nto assess and route input data to the most suitable branch for processing. This\napproach not only addresses the inherent limitations of conventional models in\nadapting to diverse driving conditions but also ensures the balance between\nperformance and efficiency. Extensive experimental evaluations demonstrate the\nadaptability of DyRoNet to diverse branch selection strategies, resulting in\nsignificant performance enhancements across different scenarios. This work\nestablishes a new benchmark for streaming perception and provides valuable\nengineering insights for future work.\n","authors":["Xiang Huang","Zhi-Qi Cheng","Jun-Yan He","Chenyang Li","Wangmeng Xiang","Baigui Sun"],"pdf_url":"https://arxiv.org/pdf/2403.05050v4.pdf","comment":"Accepted to WACV 2025. 17 pages, 8 figures. Project:\n  https://tastevision.github.io/DyRoNet/"},{"id":"http://arxiv.org/abs/2412.11248v1","updated":"2024-12-15T16:54:53Z","published":"2024-12-15T16:54:53Z","title":"Multimodal Class-aware Semantic Enhancement Network for Audio-Visual\n  Video Parsing","summary":"  The Audio-Visual Video Parsing task aims to recognize and temporally localize\nall events occurring in either the audio or visual stream, or both. Capturing\naccurate event semantics for each audio/visual segment is vital. Prior works\ndirectly utilize the extracted holistic audio and visual features for intra-\nand cross-modal temporal interactions. However, each segment may contain\nmultiple events, resulting in semantically mixed holistic features that can\nlead to semantic interference during intra- or cross-modal interactions: the\nevent semantics of one segment may incorporate semantics of unrelated events\nfrom other segments. To address this issue, our method begins with a\nClass-Aware Feature Decoupling (CAFD) module, which explicitly decouples the\nsemantically mixed features into distinct class-wise features, including\nmultiple event-specific features and a dedicated background feature. The\ndecoupled class-wise features enable our model to selectively aggregate useful\nsemantics for each segment from clearly matched classes contained in other\nsegments, preventing semantic interference from irrelevant classes.\nSpecifically, we further design a Fine-Grained Semantic Enhancement module for\nencoding intra- and cross-modal relations. It comprises a Segment-wise Event\nCo-occurrence Modeling (SECM) block and a Local-Global Semantic Fusion (LGSF)\nblock. The SECM exploits inter-class dependencies of concurrent events within\nthe same timestamp with the aid of a new event co-occurrence loss. The LGSF\nfurther enhances the event semantics of each segment by incorporating relevant\nsemantics from more informative global video features. Extensive experiments\nvalidate the effectiveness of the proposed modules and loss functions,\nresulting in a new state-of-the-art parsing performance.\n","authors":["Pengcheng Zhao","Jinxing Zhou","Dan Guo","Yang Zhao","Yanxiang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11248v1.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2404.12900v2","updated":"2024-12-15T14:53:00Z","published":"2024-04-19T14:13:46Z","title":"Training-and-Prompt-Free General Painterly Harmonization via Zero-Shot\n  Disentenglement on Style and Content References","summary":"  Painterly image harmonization aims at seamlessly blending disparate visual\nelements within a single image. However, previous approaches often struggle due\nto limitations in training data or reliance on additional prompts, leading to\ninharmonious and content-disrupted output. To surmount these hurdles, we design\na Training-and-prompt-Free General Painterly Harmonization method (TF-GPH).\nTF-GPH incorporates a novel ``Similarity Disentangle Mask'', which disentangles\nthe foreground content and background image by redirecting their attention to\ncorresponding reference images, enhancing the attention mechanism for\nmulti-image inputs. Additionally, we propose a ``Similarity Reweighting''\nmechanism to balance harmonization between stylization and content\npreservation. This mechanism minimizes content disruption by prioritizing the\ncontent-similar features within the given background style reference. Finally,\nwe address the deficiencies in existing benchmarks by proposing novel\nrange-based evaluation metrics and a new benchmark to better reflect real-world\napplications. Extensive experiments demonstrate the efficacy of our method in\nall benchmarks. More detailed in https://github.com/BlueDyee/TF-GPH.\n","authors":["Teng-Fang Hsiao","Bo-Kai Ruan","Hong-Han Shuai"],"pdf_url":"https://arxiv.org/pdf/2404.12900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19723v2","updated":"2024-12-15T14:24:47Z","published":"2024-03-28T03:20:54Z","title":"HeGTa: Leveraging Heterogeneous Graph-enhanced Large Language Models for\n  Few-shot Complex Table Understanding","summary":"  Table understanding (TU) has achieved promising advancements, but it faces\nthe challenges of the scarcity of manually labeled tables and the presence of\ncomplex table structures.To address these challenges, we propose HGT, a\nframework with a heterogeneous graph (HG)-enhanced large language model (LLM)\nto tackle few-shot TU tasks.It leverages the LLM by aligning the table\nsemantics with the LLM's parametric knowledge through soft prompts and\ninstruction turning and deals with complex tables by a multi-task pre-training\nscheme involving three novel multi-granularity self-supervised HG pre-training\nobjectives.We empirically demonstrate the effectiveness of HGT, showing that it\noutperforms the SOTA for few-shot complex TU on several benchmarks.\n","authors":["Rihui Jin","Yu Li","Guilin Qi","Nan Hu","Yuan-Fang Li","Jiaoyan Chen","Jianan Wang","Yongrui Chen","Dehai Min","Sheng Bi"],"pdf_url":"https://arxiv.org/pdf/2403.19723v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2403.08505v3","updated":"2024-12-15T02:07:57Z","published":"2024-03-13T13:12:57Z","title":"CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code\nis available at https://github.com/Xinjie-Q/CAMSIC.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12206v1","updated":"2024-12-15T16:10:10Z","published":"2024-12-15T16:10:10Z","title":"Provably Secure Robust Image Steganography via Cross-Modal Error\n  Correction","summary":"  The rapid development of image generation models has facilitated the\nwidespread dissemination of generated images on social networks, creating\nfavorable conditions for provably secure image steganography. However, existing\nmethods face issues such as low quality of generated images and lack of\nsemantic control in the generation process. To leverage provably secure\nsteganography with more effective and high-performance image generation models,\nand to ensure that stego images can accurately extract secret messages even\nafter being uploaded to social networks and subjected to lossy processing such\nas JPEG compression, we propose a high-quality, provably secure, and robust\nimage steganography method based on state-of-the-art autoregressive (AR) image\ngeneration models using Vector-Quantized (VQ) tokenizers. Additionally, we\nemploy a cross-modal error-correction framework that generates stego text from\nstego images to aid in restoring lossy images, ultimately enabling the\nextraction of secret messages embedded within the images. Extensive experiments\nhave demonstrated that the proposed method provides advantages in stego\nquality, embedding capacity, and robustness, while ensuring provable\nundetectability.\n","authors":["Yuang Qi","Kejiang Chen","Na Zhao","Zijin Yang","Weiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12206v1.pdf","comment":"7 pages. Accepted by AAAI 2025"}],"Performance":[{"id":"http://arxiv.org/abs/2412.05784v3","updated":"2024-12-15T14:21:17Z","published":"2024-12-08T02:30:23Z","title":"ASC-Hook: fast and transparent system call hook for Arm","summary":"  Intercepting system calls is crucial for tools that aim to modify or monitor\napplication behavior. However, existing system call interception tools on the\nARM platform still suffer from limitations in terms of performance and\ncompleteness. This paper presents an efficient and comprehensive binary\nrewriting framework, ASC-Hook, specifically designed for intercepting system\ncalls on the ARM platform. ASC-Hook addresses two key challenges on the ARM\narchitecture: the misalignment of the target address caused by directly\nreplacing the SVC instruction with br x8, and the return to the original\ncontrol flow after system call interception. This is achieved through a hybrid\nreplacement strategy and our specially designed trampoline mechanism. By\nimplementing multiple completeness strategies specifically for system calls, we\nensured comprehensive and thorough interception. Experimental results show that\nASC-Hook reduces overhead to at least 1/29 of that of existing system call\ninterception tools. We conducted extensive performance evaluations of ASC-Hook,\nand the average performance loss for system call-intensive applications is\n3.7\\% .\n","authors":["Yang Shen","Min Xie","Wenzhe Zhang","Tao Wu"],"pdf_url":"https://arxiv.org/pdf/2412.05784v3.pdf","comment":"11 pages (including appendix), 6 figures, not yet published"}],"Database":[{"id":"http://arxiv.org/abs/2404.00776v2","updated":"2024-12-15T22:49:53Z","published":"2024-03-31T19:15:09Z","title":"PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning","summary":"  We present PyTorch Frame, a PyTorch-based framework for deep learning over\nmulti-modal tabular data. PyTorch Frame makes tabular deep learning easy by\nproviding a PyTorch-based data structure to handle complex tabular data,\nintroducing a model abstraction to enable modular implementation of tabular\nmodels, and allowing external foundation models to be incorporated to handle\ncomplex columns (e.g., LLMs for text columns). We demonstrate the usefulness of\nPyTorch Frame by implementing diverse tabular models in a modular way,\nsuccessfully applying these models to complex multi-modal tabular data, and\nintegrating our framework with PyTorch Geometric, a PyTorch library for Graph\nNeural Networks (GNNs), to perform end-to-end learning over relational\ndatabases.\n","authors":["Weihua Hu","Yiwen Yuan","Zecheng Zhang","Akihiro Nitta","Kaidi Cao","Vid Kocijan","Jinu Sunil","Jure Leskovec","Matthias Fey"],"pdf_url":"https://arxiv.org/pdf/2404.00776v2.pdf","comment":"https://github.com/pyg-team/pytorch-frame"},{"id":"http://arxiv.org/abs/2403.19723v2","updated":"2024-12-15T14:24:47Z","published":"2024-03-28T03:20:54Z","title":"HeGTa: Leveraging Heterogeneous Graph-enhanced Large Language Models for\n  Few-shot Complex Table Understanding","summary":"  Table understanding (TU) has achieved promising advancements, but it faces\nthe challenges of the scarcity of manually labeled tables and the presence of\ncomplex table structures.To address these challenges, we propose HGT, a\nframework with a heterogeneous graph (HG)-enhanced large language model (LLM)\nto tackle few-shot TU tasks.It leverages the LLM by aligning the table\nsemantics with the LLM's parametric knowledge through soft prompts and\ninstruction turning and deals with complex tables by a multi-task pre-training\nscheme involving three novel multi-granularity self-supervised HG pre-training\nobjectives.We empirically demonstrate the effectiveness of HGT, showing that it\noutperforms the SOTA for few-shot complex TU on several benchmarks.\n","authors":["Rihui Jin","Yu Li","Guilin Qi","Nan Hu","Yuan-Fang Li","Jiaoyan Chen","Jianan Wang","Yongrui Chen","Dehai Min","Sheng Bi"],"pdf_url":"https://arxiv.org/pdf/2403.19723v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2408.06134v3","updated":"2024-12-15T12:16:19Z","published":"2024-08-12T13:23:49Z","title":"Learned Indexes with Distribution Smoothing via Virtual Points","summary":"  Recent research on learned indexes has created a new perspective for indexes\nas models that map keys to their respective storage locations. These learned\nindexes are created to approximate the cumulative distribution function of the\nkey set, where using only a single model may have limited accuracy. To overcome\nthis limitation, a typical method is to use multiple models, arranged in a\nhierarchical manner, where the query performance depends on two aspects: (i)\ntraversal time to find the correct model and (ii) search time to find the key\nin the selected model. Such a method may cause some key space regions that are\ndifficult to model to be placed at deeper levels in the hierarchy. To address\nthis issue, we propose an alternative method that modifies the key space as\nopposed to any structural or model modifications. This is achieved through\nmaking the key set more learnable (i.e., smoothing the distribution) by\ninserting virtual points. Furthermore, we develop an algorithm named CSV to\nintegrate our virtual point insertion method into existing learned indexes,\nreducing both their traversal and search time. We implement CSV on\nstate-of-the-art learned indexes and evaluate them on real-world datasets.\nExtensive experimental results show significant query performance improvement\nfor the keys in deeper levels of the index structures at a low storage cost.\n","authors":["Kasun Amarasinghe","Farhana Choudhury","Jianzhong Qi","James Bailey"],"pdf_url":"https://arxiv.org/pdf/2408.06134v3.pdf","comment":"Modified the template"}]},"2024-12-14T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.10858v1","updated":"2024-12-14T15:14:39Z","published":"2024-12-14T15:14:39Z","title":"CRENER: A Character Relation Enhanced Chinese NER Model","summary":"  Chinese Named Entity Recognition (NER) is an important task in information\nextraction, which has a significant impact on downstream applications. Due to\nthe lack of natural separators in Chinese, previous NER methods mostly relied\non external dictionaries to enrich the semantic and boundary information of\nChinese words. However, such methods may introduce noise that affects the\naccuracy of named entity recognition. To this end, we propose a character\nrelation enhanced Chinese NER model (CRENER). This model defines four types of\ntags that reflect the relationships between characters, and proposes a\nfine-grained modeling of the relationships between characters based on three\ntypes of relationships: adjacency relations between characters, relations\nbetween characters and tags, and relations between tags, to more accurately\nidentify entity boundaries and improve Chinese NER accuracy. Specifically, we\ntransform the Chinese NER task into a character-character relationship\nclassification task, ensuring the accuracy of entity boundary recognition\nthrough joint modeling of relation tags. To enhance the model's ability to\nunderstand contextual information, WRENER further constructed an adapted\ntransformer encoder that combines unscaled direction-aware and distance-aware\nmasked self-attention mechanisms. Moreover, a relationship representation\nenhancement module was constructed to model predefined relationship tags,\neffectively mining the relationship representations between characters and\ntags. Experiments conducted on four well-known Chinese NER benchmark datasets\nhave shown that the proposed model outperforms state-of-the-art baselines. The\nablation experiment also demonstrated the effectiveness of the proposed model.\n","authors":["Yaqiong Qiao","Shixuan Peng"],"pdf_url":"https://arxiv.org/pdf/2412.10858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09560v3","updated":"2024-12-14T14:14:26Z","published":"2023-04-19T10:59:34Z","title":"An Offline Metric for the Debiasedness of Click Models","summary":"  A well-known problem when learning from user clicks are inherent biases\nprevalent in the data, such as position or trust bias. Click models are a\ncommon method for extracting information from user clicks, such as document\nrelevance in web search, or to estimate click biases for downstream\napplications such as counterfactual learning-to-rank, ad placement, or fair\nranking. Recent work shows that the current evaluation practices in the\ncommunity fail to guarantee that a well-performing click model generalizes well\nto downstream tasks in which the ranking distribution differs from the training\ndistribution, i.e., under covariate shift. In this work, we propose an\nevaluation metric based on conditional independence testing to detect a lack of\nrobustness to covariate shift in click models. We introduce the concept of\ndebiasedness in click modeling and derive a metric for measuring it. In\nextensive semi-synthetic experiments, we show that our proposed metric helps to\npredict the downstream performance of click models under covariate shift and is\nuseful in an off-policy model selection setting.\n","authors":["Romain Deffayet","Philipp Hager","Jean-Michel Renders","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2304.09560v3.pdf","comment":"SIGIR23 - Full paper"},{"id":"http://arxiv.org/abs/2412.10787v1","updated":"2024-12-14T10:49:00Z","published":"2024-12-14T10:49:00Z","title":"Why Not Together? A Multiple-Round Recommender System for Queries and\n  Items","summary":"  A fundamental technique of recommender systems involves modeling user\npreferences, where queries and items are widely used as symbolic\nrepresentations of user interests. Queries delineate user needs at an abstract\nlevel, providing a high-level description, whereas items operate on a more\nspecific and concrete level, representing the granular facets of user\npreference. While practical, both query and item recommendations encounter the\nchallenge of sparse user feedback. To this end, we propose a novel approach\nnamed Multiple-round Auto Guess-and-Update System (MAGUS) that capitalizes on\nthe synergies between both types, allowing us to leverage both query and item\ninformation to form user interests. This integrated system introduces a\nrecursive framework that could be applied to any recommendation method to\nexploit queries and items in historical interactions and to provide\nrecommendations for both queries and items in each interaction round. Empirical\nresults from testing 12 different recommendation methods demonstrate that\nintegrating queries into item recommendations via MAGUS significantly enhances\nthe efficiency, with which users can identify their preferred items during\nmultiple-round interactions.\n","authors":["Jiarui Jin","Xianyu Chen","Weinan Zhang","Yong Yu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10787v1.pdf","comment":"KDD 2025"},{"id":"http://arxiv.org/abs/2412.10770v1","updated":"2024-12-14T09:47:21Z","published":"2024-12-14T09:47:21Z","title":"Learned Data Compression: Challenges and Opportunities for the Future","summary":"  Compressing integer keys is a fundamental operation among multiple\ncommunities, such as database management (DB), information retrieval (IR), and\nhigh-performance computing (HPC). Recent advances in \\emph{learned indexes}\nhave inspired the development of \\emph{learned compressors}, which leverage\nsimple yet compact machine learning (ML) models to compress large-scale sorted\nkeys. The core idea behind learned compressors is to \\emph{losslessly} encode\nsorted keys by approximating them with \\emph{error-bounded} ML models (e.g.,\npiecewise linear functions) and using a \\emph{residual array} to guarantee\naccurate key reconstruction.\n  While the concept of learned compressors remains in its early stages of\nexploration, our benchmark results demonstrate that an SIMD-optimized learned\ncompressor can significantly outperform state-of-the-art CPU-based compressors.\nDrawing on our preliminary experiments, this vision paper explores the\npotential of learned data compression to enhance critical areas in DBMS and\nrelated domains. Furthermore, we outline the key technical challenges that\nexisting systems must address when integrating this emerging methodology.\n","authors":["Qiyu Liu","Siyuan Han","Jianwei Liao","Jin Li","Jingshu Peng","Jun Du","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10745v1","updated":"2024-12-14T08:28:52Z","published":"2024-12-14T08:28:52Z","title":"Enhancing Event Extraction from Short Stories through Contextualized\n  Prompts","summary":"  Event extraction is an important natural language processing (NLP) task of\nidentifying events in an unstructured text. Although a plethora of works deal\nwith event extraction from new articles, clinical text etc., only a few works\nfocus on event extraction from literary content. Detecting events in short\nstories presents several challenges to current systems, encompassing a\ndifferent distribution of events as compared to other domains and the portrayal\nof diverse emotional conditions. This paper presents \\texttt{Vrittanta-EN}, a\ncollection of 1000 English short stories annotated for real events. Exploring\nthis field could result in the creation of techniques and resources that\nsupport literary scholars in improving their effectiveness. This could\nsimultaneously influence the field of Natural Language Processing. Our\nobjective is to clarify the intricate idea of events in the context of short\nstories. Towards the objective, we collected 1,000 short stories written mostly\nfor children in the Indian context. Further, we present fresh guidelines for\nannotating event mentions and their categories, organized into \\textit{seven\ndistinct classes}. The classes are {\\tt{COGNITIVE-MENTAL-STATE(CMS),\nCOMMUNICATION(COM), CONFLICT(CON), GENERAL-ACTIVITY(GA), LIFE-EVENT(LE),\nMOVEMENT(MOV), and OTHERS(OTH)}}. Subsequently, we apply these guidelines to\nannotate the short story dataset. Later, we apply the baseline methods for\nautomatically detecting and categorizing events. We also propose a prompt-based\nmethod for event detection and classification. The proposed method outperforms\nthe baselines, while having significant improvement of more than 4\\% for the\nclass \\texttt{CONFLICT} in event classification task.\n","authors":["Chaitanya Kirti","Ayon Chattopadhyay","Ashish Anand","Prithwijit Guha"],"pdf_url":"https://arxiv.org/pdf/2412.10745v1.pdf","comment":"47 pages, 8 figures, Planning to submit in Elsevier (Computer Speech\n  and Language Journal)"},{"id":"http://arxiv.org/abs/2412.10737v1","updated":"2024-12-14T08:18:23Z","published":"2024-12-14T08:18:23Z","title":"Sentiment and Hashtag-aware Attentive Deep Neural Network for Multimodal\n  Post Popularity Prediction","summary":"  Social media users articulate their opinions on a broad spectrum of subjects\nand share their experiences through posts comprising multiple modes of\nexpression, leading to a notable surge in such multimodal content on social\nmedia platforms. Nonetheless, accurately forecasting the popularity of these\nposts presents a considerable challenge. Prevailing methodologies primarily\ncenter on the content itself, thereby overlooking the wealth of information\nencapsulated within alternative modalities such as visual demographics,\nsentiments conveyed through hashtags and adequately modeling the intricate\nrelationships among hashtags, texts, and accompanying images. This oversight\nlimits the ability to capture emotional connection and audience relevance,\nsignificantly influencing post popularity. To address these limitations, we\npropose a seNtiment and hAshtag-aware attentive deep neuRal netwoRk for\nmultimodAl posT pOpularity pRediction, herein referred to as NARRATOR that\nextracts visual demographics from faces appearing in images and discerns\nsentiment from hashtag usage, providing a more comprehensive understanding of\nthe factors influencing post popularity Moreover, we introduce a hashtag-guided\nattention mechanism that leverages hashtags as navigational cues, guiding the\nmodels focus toward the most pertinent features of textual and visual\nmodalities, thus aligning with target audience interests and broader social\nmedia context. Experimental results demonstrate that NARRATOR outperforms\nexisting methods by a significant margin on two real-world datasets.\nFurthermore, ablation studies underscore the efficacy of integrating visual\ndemographics, sentiment analysis of hashtags, and hashtag-guided attention\nmechanisms in enhancing the performance of post popularity prediction, thereby\nfacilitating increased audience relevance, emotional engagement, and aesthetic\nappeal.\n","authors":["Shubhi Bansal","Mohit Kumar","Chandravardhan Singh Raghaw","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2412.10737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10714v1","updated":"2024-12-14T06:56:46Z","published":"2024-12-14T06:56:46Z","title":"Movie Recommendation using Web Crawling","summary":"  In today's digital world, streaming platforms offer a vast array of movies,\nmaking it hard for users to find content matching their preferences. This paper\nexplores integrating real time data from popular movie websites using advanced\nHTML scraping techniques and APIs. It also incorporates a recommendation system\ntrained on a static Kaggle dataset, enhancing the relevance and freshness of\nsuggestions. By combining content based filtering, collaborative filtering, and\na hybrid model, we create a system that utilizes both historical and real time\ndata for more personalized suggestions. Our methodology shows that\nincorporating dynamic data not only boosts user satisfaction but also aligns\nrecommendations with current viewing trends.\n","authors":["Pronit Raj","Chandrashekhar Kumar","Harshit Shekhar","Amit Kumar","Kritibas Paul","Debasish Jana"],"pdf_url":"https://arxiv.org/pdf/2412.10714v1.pdf","comment":"12 pages, 3 figures, Accepted and to be published in Proceedings of\n  2025 International Conference on Applied Algorithms (ICAA), Kolkata, India,\n  Dec 8-10, 2025"},{"id":"http://arxiv.org/abs/2412.10701v1","updated":"2024-12-14T06:18:19Z","published":"2024-12-14T06:18:19Z","title":"Beyond Quantile Methods: Improved Top-K Threshold Estimation for\n  Traditional and Learned Sparse Indexes","summary":"  Top-k threshold estimation is the problem of estimating the score of the k-th\nhighest ranking result of a search query. A good estimate can be used to speed\nup many common top-k query processing algorithms, and thus a number of\nresearchers have recently studied the problem. Among the various approaches\nthat have been proposed, quantile methods appear to give the best estimates\noverall at modest computational costs, followed by sampling-based methods in\ncertain cases. In this paper, we make two main contributions. First, we study\nhow to get even better estimates than the state of the art. Starting from\nquantile-based methods, we propose a series of enhancements that give improved\nestimates in terms of the commonly used mean under-prediction fraction (MUF).\nSecond, we study the threshold estimation problem on recently proposed learned\nsparse index structures, showing that our methods also work well for these\ncases. Our best methods substantially narrow the gap between the state of the\nart and the ideal MUF of 1.0, at some additional cost in time and space.\n","authors":["Jinrui Gou","Yifan Liu","Minghao Shao","Torsten Suel"],"pdf_url":"https://arxiv.org/pdf/2412.10701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18069v2","updated":"2024-12-14T05:56:10Z","published":"2024-11-27T05:43:00Z","title":"Overview of TREC 2024 Biomedical Generative Retrieval (BioGen) Track","summary":"  With the advancement of large language models (LLMs), the biomedical domain\nhas seen significant progress and improvement in multiple tasks such as\nbiomedical question answering, lay language summarization of the biomedical\nliterature, clinical note summarization, etc. However, hallucinations or\nconfabulations remain one of the key challenges when using LLMs in the\nbiomedical and other domains. Inaccuracies may be particularly harmful in\nhigh-risk situations, such as medical question answering, making clinical\ndecisions, or appraising biomedical research. Studies on the evaluation of the\nLLMs abilities to ground generated statements in verifiable sources have shown\nthat models perform significantly worse on lay-user-generated questions, and\noften fail to reference relevant sources. This can be problematic when those\nseeking information want evidence from studies to back up the claims from LLMs.\nUnsupported statements are a major barrier to using LLMs in any applications\nthat may affect health. Methods for grounding generated statements in reliable\nsources along with practical evaluation approaches are needed to overcome this\nbarrier. Towards this, in our pilot task organized at TREC 2024, we introduced\nthe task of reference attribution as a means to mitigate the generation of\nfalse statements by LLMs answering biomedical questions.\n","authors":["Deepak Gupta","Dina Demner-Fushman","William Hersh","Steven Bedrick","Kirk Roberts"],"pdf_url":"https://arxiv.org/pdf/2411.18069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10680v1","updated":"2024-12-14T04:59:38Z","published":"2024-12-14T04:59:38Z","title":"UCDR-Adapter: Exploring Adaptation of Pre-Trained Vision-Language Models\n  for Universal Cross-Domain Retrieval","summary":"  Universal Cross-Domain Retrieval (UCDR) retrieves relevant images from unseen\ndomains and classes without semantic labels, ensuring robust generalization.\nExisting methods commonly employ prompt tuning with pre-trained vision-language\nmodels but are inherently limited by static prompts, reducing adaptability. We\npropose UCDR-Adapter, which enhances pre-trained models with adapters and\ndynamic prompt generation through a two-phase training strategy. First, Source\nAdapter Learning integrates class semantics with domain-specific visual\nknowledge using a Learnable Textual Semantic Template and optimizes Class and\nDomain Prompts via momentum updates and dual loss functions for robust\nalignment. Second, Target Prompt Generation creates dynamic prompts by\nattending to masked source prompts, enabling seamless adaptation to unseen\ndomains and classes. Unlike prior approaches, UCDR-Adapter dynamically adapts\nto evolving data distributions, enhancing both flexibility and generalization.\nDuring inference, only the image branch and generated prompts are used,\neliminating reliance on textual inputs for highly efficient retrieval.\nExtensive benchmark experiments show that UCDR-Adapter consistently outperforms\nProS in most cases and other state-of-the-art methods on UCDR, U(c)CDR, and\nU(d)CDR settings.\n","authors":["Haoyu Jiang","Zhi-Qi Cheng","Gabriel Moreira","Jiawen Zhu","Jingdong Sun","Bukun Ren","Jun-Yan He","Qi Dai","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2412.10680v1.pdf","comment":"Accepted to WACV 2025. Project link:\n  https://github.com/fine68/UCDR2024"},{"id":"http://arxiv.org/abs/2412.10674v1","updated":"2024-12-14T04:22:09Z","published":"2024-12-14T04:22:09Z","title":"USM: Unbiased Survey Modeling for Limiting Negative User Experiences in\n  Recommendation Systems","summary":"  Negative feedback signals are crucial to guardrail content recommendations\nand improve user experience. When these signals are effectively integrated into\nrecommendation systems, they play a vital role in preventing the promotion of\nharmful or undesirable content, thereby contributing to a healthier online\nenvironment. However, the challenges associated with negative signals are\nnoteworthy. Due to the limited visibility of options for users to express\nnegative feedback, these signals are often sparse compared to positive signals.\nThis imbalance can lead to a skewed understanding of user preferences,\nresulting in recommendations that prioritize short-term engagement over\nlong-term satisfaction. Moreover, an over-reliance on positive signals can\ncreate a filter bubble, where users are continuously exposed to content that\naligns with their immediate preferences but may not be beneficial in the long\nrun. This scenario can ultimately lead to user attrition as audiences become\ndisillusioned with the quality of the content provided. Additionally, existing\nuser signals frequently fail to meet specific customized requirements, such as\nunderstanding the underlying reasons for a user's likes or dislikes regarding a\nvideo. This lack of granularity hinders our ability to tailor content\nrecommendations effectively, as we cannot identify the particular attributes of\ncontent that resonate with individual users.\n","authors":["Chenghui Yu","Peiyi Li","Haoze Wu","Bingfeng Deng","Hongyu Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.10674v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.04506v2","updated":"2024-12-14T00:13:09Z","published":"2024-12-03T22:59:36Z","title":"Arctic-Embed 2.0: Multilingual Retrieval Without Compromise","summary":"  This paper presents the training methodology of Arctic-Embed 2.0, a set of\nopen-source text embedding models built for accurate and efficient multilingual\nretrieval. While prior works have suffered from degraded English retrieval\nquality, Arctic-Embed 2.0 delivers competitive retrieval quality on\nmultilingual and English-only benchmarks, and supports Matryoshka\nRepresentation Learning (MRL) for efficient embedding storage with\nsignificantly lower compressed quality degradation compared to alternatives. We\ndetail the design and implementation, presenting several important open\nresearch questions that arose during model development. We conduct experiments\nexploring these research questions and include extensive discussion aimed at\nfostering further discussion in this field.\n","authors":["Puxuan Yu","Luke Merrick","Gaurav Nuti","Daniel Campos"],"pdf_url":"https://arxiv.org/pdf/2412.04506v2.pdf","comment":"10 pages, 5 figures, 3 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.08174v2","updated":"2024-12-14T10:34:35Z","published":"2024-10-10T17:50:42Z","title":"Sample then Identify: A General Framework for Risk Control and\n  Assessment in Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) exhibit promising advancements\nacross various tasks, yet they still encounter significant trustworthiness\nissues. Prior studies apply Split Conformal Prediction (SCP) in language\nmodeling to construct prediction sets with statistical guarantees. However,\nthese methods typically rely on internal model logits or are restricted to\nmultiple-choice settings, which hampers their generalizability and adaptability\nin dynamic, open-ended environments. In this paper, we introduce TRON, a\ntwo-step framework for risk control and assessment, applicable to any MLLM that\nsupports sampling in both open-ended and closed-ended scenarios. TRON comprises\ntwo main components: (1) a novel conformal score to sample response sets of\nminimum size, and (2) a nonconformity score to identify high-quality responses\nbased on self-consistency theory, controlling the error rates by two specific\nrisk levels. Furthermore, we investigate semantic redundancy in prediction sets\nwithin open-ended contexts for the first time, leading to a promising\nevaluation metric for MLLMs based on average set size. Our comprehensive\nexperiments across four Video Question-Answering (VideoQA) datasets utilizing\neight MLLMs show that TRON achieves desired error rates bounded by two\nuser-specified risk levels. Additionally, deduplicated prediction sets maintain\nadaptiveness while being more efficient and stable for risk assessment under\ndifferent risk levels.\n","authors":["Qingni Wang","Tiantian Geng","Zhiyuan Wang","Teng Wang","Bo Fu","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.08174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10768v1","updated":"2024-12-14T09:36:10Z","published":"2024-12-14T09:36:10Z","title":"VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation","summary":"  Recent advances in audio generation have focused on text-to-audio (T2A) and\nvideo-to-audio (V2A) tasks. However, T2A or V2A methods cannot generate\nholistic sounds (onscreen and off-screen). This is because T2A cannot generate\nsounds aligning with onscreen objects, while V2A cannot generate semantically\ncomplete (offscreen sounds missing). In this work, we address the task of\nholistic audio generation: given a video and a text prompt, we aim to generate\nboth onscreen and offscreen sounds that are temporally synchronized with the\nvideo and semantically aligned with text and video. Previous approaches for\njoint text and video-to-audio generation often suffer from modality bias,\nfavoring one modality over the other. To overcome this limitation, we introduce\nVinTAGe, a flow-based transformer model that jointly considers text and video\nto guide audio generation. Our framework comprises two key components: a\nVisual-Text Encoder and a Joint VT-SiT model. To reduce modality bias and\nimprove generation quality, we employ pretrained uni-modal text-to-audio and\nvideo-to-audio generation models for additional guidance. Due to the lack of\nappropriate benchmarks, we also introduce VinTAGe-Bench, a dataset of 636\nvideo-text-audio pairs containing both onscreen and offscreen sounds. Our\ncomprehensive experiments on VinTAGe-Bench demonstrate that joint text and\nvisual interaction is necessary for holistic audio generation. Furthermore,\nVinTAGe achieves state-of-the-art results on the VGGSound benchmark. Our source\ncode and pre-trained models will be released. Demo is available at:\nhttps://www.youtube.com/watch?v=QmqWhUjPkJI.\n","authors":["Saksham Singh Kushwaha","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.10768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10749v1","updated":"2024-12-14T08:34:44Z","published":"2024-12-14T08:34:44Z","title":"Patch-level Sounding Object Tracking for Audio-Visual Question Answering","summary":"  Answering questions related to audio-visual scenes, i.e., the AVQA task, is\nbecoming increasingly popular. A critical challenge is accurately identifying\nand tracking sounding objects related to the question along the timeline. In\nthis paper, we present a new Patch-level Sounding Object Tracking (PSOT)\nmethod. It begins with a Motion-driven Key Patch Tracking (M-KPT) module, which\nrelies on visual motion information to identify salient visual patches with\nsignificant movements that are more likely to relate to sounding objects and\nquestions. We measure the patch-wise motion intensity map between neighboring\nvideo frames and utilize it to construct and guide a motion-driven graph\nnetwork. Meanwhile, we design a Sound-driven KPT (S-KPT) module to explicitly\ntrack sounding patches. This module also involves a graph network, with the\nadjacency matrix regularized by the audio-visual correspondence map. The M-KPT\nand S-KPT modules are performed in parallel for each temporal segment, allowing\nbalanced tracking of salient and sounding objects. Based on the tracked\npatches, we further propose a Question-driven KPT (Q-KPT) module to retain\npatches highly relevant to the question, ensuring the model focuses on the most\ninformative clues. The audio-visual-question features are updated during the\nprocessing of these modules, which are then aggregated for final answer\nprediction. Extensive experiments on standard datasets demonstrate the\neffectiveness of our method, achieving competitive performance even compared to\nrecent large-scale pretraining-based approaches.\n","authors":["Zhangbin Li","Jinxing Zhou","Jing Zhang","Shengeng Tang","Kun Li","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2412.10749v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10707v1","updated":"2024-12-14T06:33:53Z","published":"2024-12-14T06:33:53Z","title":"MambaPro: Multi-Modal Object Re-Identification with Mamba Aggregation\n  and Synergistic Prompt","summary":"  Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects\nby utilizing complementary image information from different modalities.\nRecently, large-scale pre-trained models like CLIP have demonstrated impressive\nperformance in traditional single-modal object ReID tasks. However, they remain\nunexplored for multi-modal object ReID. Furthermore, current multi-modal\naggregation methods have obvious limitations in dealing with long sequences\nfrom different modalities. To address above issues, we introduce a novel\nframework called MambaPro for multi-modal object ReID. To be specific, we first\nemploy a Parallel Feed-Forward Adapter (PFA) for adapting CLIP to multi-modal\nobject ReID. Then, we propose the Synergistic Residual Prompt (SRP) to guide\nthe joint learning of multi-modal features. Finally, leveraging Mamba's\nsuperior scalability for long sequences, we introduce Mamba Aggregation (MA) to\nefficiently model interactions between different modalities. As a result,\nMambaPro could extract more robust features with lower complexity. Extensive\nexperiments on three multi-modal object ReID benchmarks (i.e., RGBNT201,\nRGBNT100 and MSVR310) validate the effectiveness of our proposed methods. The\nsource code is available at https://github.com/924973292/MambaPro.\n","authors":["Yuhao Wang","Xuehu Liu","Tianyu Yan","Yang Liu","Aihua Zheng","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10707v1.pdf","comment":"This work is accepted by AAAI2025. More modifications may be\n  performed"},{"id":"http://arxiv.org/abs/2407.20592v2","updated":"2024-12-14T06:15:40Z","published":"2024-07-30T06:57:00Z","title":"EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos","summary":"  We introduce EgoSonics, a method to generate semantically meaningful and\nsynchronized audio tracks conditioned on silent egocentric videos. Generating\naudio for silent egocentric videos could open new applications in virtual\nreality, assistive technologies, or for augmenting existing datasets. Existing\nwork has been limited to domains like speech, music, or impact sounds and\ncannot capture the broad range of audio frequencies found in egocentric videos.\nEgoSonics addresses these limitations by building on the strengths of latent\ndiffusion models for conditioned audio synthesis. We first encode and process\npaired audio-video data to make them suitable for generation. The encoded data\nis then used to train a model that can generate an audio track that captures\nthe semantics of the input video. Our proposed SyncroNet builds on top of\nControlNet to provide control signals that enables generation of temporally\nsynchronized audio. Extensive evaluations and a comprehensive user study show\nthat our model outperforms existing work in audio quality, and in our proposed\nsynchronization evaluation method. Furthermore, we demonstrate downstream\napplications of our model in improving video summarization.\n","authors":["Aashish Rai","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2407.20592v2.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2412.10680v1","updated":"2024-12-14T04:59:38Z","published":"2024-12-14T04:59:38Z","title":"UCDR-Adapter: Exploring Adaptation of Pre-Trained Vision-Language Models\n  for Universal Cross-Domain Retrieval","summary":"  Universal Cross-Domain Retrieval (UCDR) retrieves relevant images from unseen\ndomains and classes without semantic labels, ensuring robust generalization.\nExisting methods commonly employ prompt tuning with pre-trained vision-language\nmodels but are inherently limited by static prompts, reducing adaptability. We\npropose UCDR-Adapter, which enhances pre-trained models with adapters and\ndynamic prompt generation through a two-phase training strategy. First, Source\nAdapter Learning integrates class semantics with domain-specific visual\nknowledge using a Learnable Textual Semantic Template and optimizes Class and\nDomain Prompts via momentum updates and dual loss functions for robust\nalignment. Second, Target Prompt Generation creates dynamic prompts by\nattending to masked source prompts, enabling seamless adaptation to unseen\ndomains and classes. Unlike prior approaches, UCDR-Adapter dynamically adapts\nto evolving data distributions, enhancing both flexibility and generalization.\nDuring inference, only the image branch and generated prompts are used,\neliminating reliance on textual inputs for highly efficient retrieval.\nExtensive benchmark experiments show that UCDR-Adapter consistently outperforms\nProS in most cases and other state-of-the-art methods on UCDR, U(c)CDR, and\nU(d)CDR settings.\n","authors":["Haoyu Jiang","Zhi-Qi Cheng","Gabriel Moreira","Jiawen Zhu","Jingdong Sun","Bukun Ren","Jun-Yan He","Qi Dai","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2412.10680v1.pdf","comment":"Accepted to WACV 2025. Project link:\n  https://github.com/fine68/UCDR2024"},{"id":"http://arxiv.org/abs/2412.10649v1","updated":"2024-12-14T02:36:45Z","published":"2024-12-14T02:36:45Z","title":"Hidden Echoes Survive Training in Audio To Audio Generative Instrument\n  Models","summary":"  As generative techniques pervade the audio domain, there has been increasing\ninterest in tracing back through these complicated models to understand how\nthey draw on their training data to synthesize new examples, both to ensure\nthat they use properly licensed data and also to elucidate their black box\nbehavior. In this paper, we show that if imperceptible echoes are hidden in the\ntraining data, a wide variety of audio to audio architectures (differentiable\ndigital signal processing (DDSP), Realtime Audio Variational autoEncoder\n(RAVE), and ``Dance Diffusion'') will reproduce these echoes in their outputs.\nHiding a single echo is particularly robust across all architectures, but we\nalso show promising results hiding longer time spread echo patterns for an\nincreased information capacity. We conclude by showing that echoes make their\nway into fine tuned models, that they survive mixing/demixing, and that they\nsurvive pitch shift augmentation during training. Hence, this simple, classical\nidea in watermarking shows significant promise for tagging generative audio\nmodels.\n","authors":["Christopher J. Tralie","Matt Amery","Benjamin Douglas","Ian Utz"],"pdf_url":"https://arxiv.org/pdf/2412.10649v1.pdf","comment":"8 pages, 11 Figures, Proceedings of 2025 AAAI Workshop on AI for\n  Music"}],"Performance":[{"id":"http://arxiv.org/abs/2412.10856v1","updated":"2024-12-14T15:11:07Z","published":"2024-12-14T15:11:07Z","title":"RWKV-edge: Deeply Compressed RWKV for Resource-Constrained Devices","summary":"  To deploy LLMs on resource-contained platforms such as mobile robotics and\nwearables, non-transformers LLMs have achieved major breakthroughs. Recently, a\nnovel RNN-based LLM family, Repentance Weighted Key Value (RWKV) models have\nshown promising results in text generation on resource-constrained devices\nthanks to their computational efficiency. However, these models remain too\nlarge to be deployed on embedded devices due to their high parameter count. In\nthis paper, we propose an efficient suite of compression techniques, tailored\nto the RWKV architecture. These techniques include low-rank approximation,\nsparsity predictors, and clustering head, designed to align with the model\nsize. Our methods compress the RWKV models by 4.95--3.8x with only 2.95pp loss\nin accuracy.\n","authors":["Wonkyo Choe","Yangfeng Ji","Felix Lin"],"pdf_url":"https://arxiv.org/pdf/2412.10856v1.pdf","comment":null}],"Database":[{"id":"http://arxiv.org/abs/2412.10770v1","updated":"2024-12-14T09:47:21Z","published":"2024-12-14T09:47:21Z","title":"Learned Data Compression: Challenges and Opportunities for the Future","summary":"  Compressing integer keys is a fundamental operation among multiple\ncommunities, such as database management (DB), information retrieval (IR), and\nhigh-performance computing (HPC). Recent advances in \\emph{learned indexes}\nhave inspired the development of \\emph{learned compressors}, which leverage\nsimple yet compact machine learning (ML) models to compress large-scale sorted\nkeys. The core idea behind learned compressors is to \\emph{losslessly} encode\nsorted keys by approximating them with \\emph{error-bounded} ML models (e.g.,\npiecewise linear functions) and using a \\emph{residual array} to guarantee\naccurate key reconstruction.\n  While the concept of learned compressors remains in its early stages of\nexploration, our benchmark results demonstrate that an SIMD-optimized learned\ncompressor can significantly outperform state-of-the-art CPU-based compressors.\nDrawing on our preliminary experiments, this vision paper explores the\npotential of learned data compression to enhance critical areas in DBMS and\nrelated domains. Furthermore, we outline the key technical challenges that\nexisting systems must address when integrating this emerging methodology.\n","authors":["Qiyu Liu","Siyuan Han","Jianwei Liao","Jin Li","Jingshu Peng","Jun Du","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10770v1.pdf","comment":null}]},"2024-12-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.06215v2","updated":"2024-12-17T18:54:45Z","published":"2024-10-08T17:20:37Z","title":"DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback","summary":"  The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.\n","authors":["Zaid Khan","Elias Stengel-Eskin","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.06215v2.pdf","comment":"Project Page: https://DataEnvGym.github.io"},{"id":"http://arxiv.org/abs/2412.13175v1","updated":"2024-12-17T18:54:01Z","published":"2024-12-17T18:54:01Z","title":"DnDScore: Decontextualization and Decomposition for Factuality\n  Verification in Long-Form Text Generation","summary":"  The decompose-then-verify strategy for verification of Large Language Model\n(LLM) generations decomposes claims that are then independently verified.\nDecontextualization augments text (claims) to ensure it can be verified outside\nof the original context, enabling reliable verification. While decomposition\nand decontextualization have been explored independently, their interactions in\na complete system have not been investigated. Their conflicting purposes can\ncreate tensions: decomposition isolates atomic facts while decontextualization\ninserts relevant information. Furthermore, a decontextualized subclaim presents\na challenge to the verification step: what part of the augmented text should be\nverified as it now contains multiple atomic facts? We conduct an evaluation of\ndifferent decomposition, decontextualization, and verification strategies and\nfind that the choice of strategy matters in the resulting factuality scores.\nAdditionally, we introduce DnDScore, a decontextualization aware verification\nmethod which validates subclaims in the context of contextual information.\n","authors":["Miriam Wanner","Benjamin Van Durme","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2412.13175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13171v1","updated":"2024-12-17T18:50:33Z","published":"2024-12-17T18:50:33Z","title":"Compressed Chain of Thought: Efficient Reasoning Through Dense\n  Representations","summary":"  Chain-of-thought (CoT) decoding enables language models to improve reasoning\nperformance at the cost of high generation latency in decoding. Recent\nproposals have explored variants of contemplation tokens, a term we introduce\nthat refers to special tokens used during inference to allow for extra\ncomputation. Prior work has considered fixed-length sequences drawn from a\ndiscrete set of embeddings as contemplation tokens. Here we propose Compressed\nChain-of-Thought (CCoT), a framework to generate contentful and continuous\ncontemplation tokens of variable sequence length. The generated contemplation\ntokens are compressed representations of explicit reasoning chains, and our\nmethod can be applied to off-the-shelf decoder language models. Through\nexperiments, we illustrate how CCoT enables additional reasoning over dense\ncontentful representations to achieve corresponding improvements in accuracy.\nMoreover, the reasoning improvements can be adaptively modified on demand by\ncontrolling the number of contemplation tokens generated.\n","authors":["Jeffrey Cheng","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2412.13171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13169v1","updated":"2024-12-17T18:46:32Z","published":"2024-12-17T18:46:32Z","title":"Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study","summary":"  In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness.\n","authors":["Bolei Ma","Berk Yoztyurk","Anna-Carolina Haensch","Xinpeng Wang","Markus Herklotz","Frauke Kreuter","Barbara Plank","Matthias Assenmacher"],"pdf_url":"https://arxiv.org/pdf/2412.13169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04619v2","updated":"2024-12-17T18:42:57Z","published":"2024-12-05T21:12:37Z","title":"Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization","summary":"  Language models (LMs), like other neural networks, often favor shortcut\nheuristics based on surface-level patterns. Although LMs behave like n-gram\nmodels early in training, they must eventually learn hierarchical syntactic\nrepresentations to correctly apply grammatical rules out-of-distribution (OOD).\nIn this work, we use case studies of English grammar to explore how complex,\ndiverse training data drives models to generalize OOD. We construct a framework\nthat unifies our understanding of random variation with training dynamics, rule\nselection with memorization, and data diversity with complexity. We show that\nthese factors are nuanced, and that intermediate levels of diversity and\ncomplexity lead to inconsistent behavior across random seeds and to unstable\ntraining dynamics. Our findings emphasize the critical role of training data in\nshaping generalization patterns and illuminate how competing model strategies\nlead to inconsistent generalization outcomes across random seeds. Code is\navailable at https://github.com/sunnytqin/concept_comp.git.\n","authors":["Tian Qin","Naomi Saphra","David Alvarez-Melis"],"pdf_url":"https://arxiv.org/pdf/2412.04619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13161v1","updated":"2024-12-17T18:39:10Z","published":"2024-12-17T18:39:10Z","title":"BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce","summary":"  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n","authors":["Mohammad Nazmush Shamael","Sabila Nawshin","Swakkhar Shatabda","Salekul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.13161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13147v1","updated":"2024-12-17T18:12:47Z","published":"2024-12-17T18:12:47Z","title":"Are Your LLMs Capable of Stable Reasoning?","summary":"  The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.\n","authors":["Junnan Liu","Hongwei Liu","Linchen Xiao","Ziyi Wang","Kuikun Liu","Songyang Gao","Wenwei Zhang","Songyang Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13147v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.13146v1","updated":"2024-12-17T18:12:33Z","published":"2024-12-17T18:12:33Z","title":"Syntactic Transfer to Kyrgyz Using the Treebank Translation Method","summary":"  The Kyrgyz language, as a low-resource language, requires significant effort\nto create high-quality syntactic corpora. This study proposes an approach to\nsimplify the development process of a syntactic corpus for Kyrgyz. We present a\ntool for transferring syntactic annotations from Turkish to Kyrgyz based on a\ntreebank translation method. The effectiveness of the proposed tool was\nevaluated using the TueCL treebank. The results demonstrate that this approach\nachieves higher syntactic annotation accuracy compared to a monolingual model\ntrained on the Kyrgyz KTMU treebank. Additionally, the study introduces a\nmethod for assessing the complexity of manual annotation for the resulting\nsyntactic trees, contributing to further optimization of the annotation\nprocess.\n","authors":["Anton Alekseev","Alina Tillabaeva","Gulnara Dzh. Kabaeva","Sergey I. Nikolenko"],"pdf_url":"https://arxiv.org/pdf/2412.13146v1.pdf","comment":"To be published in the Journal of Math. Sciences. Zapiski version (in\n  Russian): http://www.pdmi.ras.ru/znsl/2024/v540/abs252.html"},{"id":"http://arxiv.org/abs/2412.10400v2","updated":"2024-12-17T18:05:11Z","published":"2024-12-05T16:10:42Z","title":"Reinforcement Learning Enhanced LLMs: A Survey","summary":"  This paper surveys research in the rapidly growing field of enhancing large\nlanguage models (LLMs) with reinforcement learning (RL), a technique that\nenables LLMs to improve their performance by receiving feedback in the form of\nrewards based on the quality of their outputs, allowing them to generate more\naccurate, coherent, and contextually appropriate responses. In this work, we\nmake a systematic review of the most up-to-date state of knowledge on\nRL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing\nresearch in this field, helping researchers understand the current challenges\nand advancements. Specifically, we (1) detail the basics of RL; (2) introduce\npopular RL-enhanced LLMs; (3) review researches on two widely-used reward\nmodel-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)\nand Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct\nPreference Optimization (DPO), a set of methods that bypass the reward model to\ndirectly use human preference data for aligning LLM outputs with human\nexpectations. We will also point out current challenges and deficiencies of\nexisting methods and suggest some avenues for further improvements. Project\npage of this work can be found at:\n\\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}.\n","authors":["Shuhe Wang","Shengyu Zhang","Jie Zhang","Runyi Hu","Xiaoya Li","Tianwei Zhang","Jiwei Li","Fei Wu","Guoyin Wang","Eduard Hovy"],"pdf_url":"https://arxiv.org/pdf/2412.10400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13474v3","updated":"2024-12-17T17:45:07Z","published":"2024-09-20T13:05:07Z","title":"Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models","summary":"  Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/Alternate-Preference-Optimization.\n","authors":["Anmol Mekala","Vineeth Dorna","Shreya Dubey","Abhishek Lalwani","David Koleczek","Mukund Rungta","Sadid Hasan","Elita Lobo"],"pdf_url":"https://arxiv.org/pdf/2409.13474v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16383v4","updated":"2024-12-17T17:42:18Z","published":"2024-09-24T18:35:09Z","title":"RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation","summary":"  Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings.\n","authors":["Ioannis Panagiotopoulos","Giorgos Filandrianos","Maria Lymperaiou","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2409.16383v4.pdf","comment":"Accepted at COLING 2025"},{"id":"http://arxiv.org/abs/2412.13110v1","updated":"2024-12-17T17:31:17Z","published":"2024-12-17T17:31:17Z","title":"Improving Explainability of Sentence-level Metrics via Edit-level\n  Attribution for Grammatical Error Correction","summary":"  Various evaluation metrics have been proposed for Grammatical Error\nCorrection (GEC), but many, particularly reference-free metrics, lack\nexplainability. This lack of explainability hinders researchers from analyzing\nthe strengths and weaknesses of GEC models and limits the ability to provide\ndetailed feedback for users. To address this issue, we propose attributing\nsentence-level scores to individual edits, providing insight into how specific\ncorrections contribute to the overall performance. For the attribution method,\nwe use Shapley values, from cooperative game theory, to compute the\ncontribution of each edit. Experiments with existing sentence-level metrics\ndemonstrate high consistency across different edit granularities and show\napproximately 70\\% alignment with human evaluations. In addition, we analyze\nbiases in the metrics based on the attribution results, revealing trends such\nas the tendency to ignore orthographic edits. Our implementation is available\nat \\url{https://github.com/naist-nlp/gec-attribute}.\n","authors":["Takumi Goto","Justin Vasselli","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2412.13110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04049v3","updated":"2024-12-17T17:17:21Z","published":"2024-02-06T14:51:55Z","title":"Systematic Biases in LLM Simulations of Debates","summary":"  The emergence of Large Language Models (LLMs), has opened exciting\npossibilities for constructing computational simulations designed to replicate\nhuman behavior accurately. Current research suggests that LLM-based agents\nbecome increasingly human-like in their performance, sparking interest in using\nthese AI agents as substitutes for human participants in behavioral studies.\nHowever, LLMs are complex statistical learners without straightforward\ndeductive rules, making them prone to unexpected behaviors. Hence, it is\ncrucial to study and pinpoint the key behavioral distinctions between humans\nand LLM-based agents. In this study, we highlight the limitations of LLMs in\nsimulating human interactions, particularly focusing on LLMs' ability to\nsimulate political debates on topics that are important aspects of people's\nday-to-day lives and decision-making processes. Our findings indicate a\ntendency for LLM agents to conform to the model's inherent social biases\ndespite being directed to debate from certain political perspectives. This\ntendency results in behavioral patterns that seem to deviate from\nwell-established social dynamics among humans. We reinforce these observations\nusing an automatic self-fine-tuning method, which enables us to manipulate the\nbiases within the LLM and demonstrate that agents subsequently align with the\naltered biases. These results underscore the need for further research to\ndevelop methods that help agents overcome these biases, a critical step toward\ncreating more realistic simulations.\n","authors":["Amir Taubenfeld","Yaniv Dover","Roi Reichart","Ariel Goldstein"],"pdf_url":"https://arxiv.org/pdf/2402.04049v3.pdf","comment":"Published as a conference paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2412.13103v1","updated":"2024-12-17T17:17:03Z","published":"2024-12-17T17:17:03Z","title":"AI PERSONA: Towards Life-long Personalization of LLMs","summary":"  In this work, we introduce the task of life-long personalization of large\nlanguage models. While recent mainstream efforts in the LLM community mainly\nfocus on scaling data and compute for improved capabilities of LLMs, we argue\nthat it is also very important to enable LLM systems, or language agents, to\ncontinuously adapt to the diverse and ever-changing profiles of every distinct\nuser and provide up-to-date personalized assistance. We provide a clear task\nformulation and introduce a simple, general, effective, and scalable framework\nfor life-long personalization of LLM systems and language agents. To facilitate\nfuture research on LLM personalization, we also introduce methods to synthesize\nrealistic benchmarks and robust evaluation metrics. We will release all codes\nand data for building and benchmarking life-long personalized LLM systems.\n","authors":["Tiannan Wang","Meiling Tao","Ruoyu Fang","Huilin Wang","Shuai Wang","Yuchen Eleanor Jiang","Wangchunshu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13103v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2412.13102v1","updated":"2024-12-17T17:15:21Z","published":"2024-12-17T17:15:21Z","title":"AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark","summary":"  Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.\n","authors":["Jianlyu Chen","Nan Wang","Chaofan Li","Bo Wang","Shitao Xiao","Han Xiao","Hao Liao","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13102v1.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13098v1","updated":"2024-12-17T17:08:35Z","published":"2024-12-17T17:08:35Z","title":"Uchaguzi-2022: A Dataset of Citizen Reports on the 2022 Kenyan Election","summary":"  Online reporting platforms have enabled citizens around the world to\ncollectively share their opinions and report in real time on events impacting\ntheir local communities. Systematically organizing (e.g., categorizing by\nattributes) and geotagging large amounts of crowdsourced information is crucial\nto ensuring that accurate and meaningful insights can be drawn from this data\nand used by policy makers to bring about positive change. These tasks, however,\ntypically require extensive manual annotation efforts. In this paper we present\nUchaguzi-2022, a dataset of 14k categorized and geotagged citizen reports\nrelated to the 2022 Kenyan General Election containing mentions of\nelection-related issues such as official misconduct, vote count irregularities,\nand acts of violence. We use this dataset to investigate whether language\nmodels can assist in scalably categorizing and geotagging reports, thus\nhighlighting its potential application in the AI for Social Good space.\n","authors":["Roberto Mondini","Neema Kotonya","Robert L. Logan IV","Elizabeth M Olson","Angela Oduor Lungati","Daniel Duke Odongo","Tim Ombasa","Hemank Lamba","Aoife Cahill","Joel R. Tetreault","Alejandro Jaimes"],"pdf_url":"https://arxiv.org/pdf/2412.13098v1.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2412.06651v4","updated":"2024-12-17T17:06:01Z","published":"2024-12-09T16:50:02Z","title":"Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben","summary":"  [Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts.\n","authors":["Rainer Muehlhoff","Marte Henningsen"],"pdf_url":"https://arxiv.org/pdf/2412.06651v4.pdf","comment":"33 pages, in German language"},{"id":"http://arxiv.org/abs/2412.13091v1","updated":"2024-12-17T17:01:15Z","published":"2024-12-17T17:01:15Z","title":"LMUnit: Fine-grained Evaluation with Natural Language Unit Tests","summary":"  As language models become integral to critical workflows, assessing their\nbehavior remains a fundamental challenge -- human evaluation is costly and\nnoisy, while automated metrics provide only coarse, difficult-to-interpret\nsignals. We introduce natural language unit tests, a paradigm that decomposes\nresponse quality into explicit, testable criteria, along with a unified scoring\nmodel, LMUnit, which combines multi-objective training across preferences,\ndirect ratings, and natural language rationales. Through controlled human\nstudies, we show this paradigm significantly improves inter-annotator agreement\nand enables more effective LLM development workflows. LMUnit achieves\nstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and\ncompetitive results on RewardBench. These results validate both our proposed\nparadigm and scoring model, suggesting a promising path forward for language\nmodel evaluation and development.\n","authors":["Jon Saad-Falcon","Rajan Vivek","William Berrios","Nandita Shankar Naik","Matija Franklin","Bertie Vidgen","Amanpreet Singh","Douwe Kiela","Shikib Mehri"],"pdf_url":"https://arxiv.org/pdf/2412.13091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09739v2","updated":"2024-12-17T16:52:12Z","published":"2024-09-15T14:10:01Z","title":"PersonaMark: Personalized LLM watermarking for model protection and user\n  attribution","summary":"  The rapid advancement of customized Large Language Models (LLMs) offers\nconsiderable convenience. However, it also intensifies concerns regarding the\nprotection of copyright/confidential information. With the extensive adoption\nof private LLMs, safeguarding model copyright and ensuring data privacy have\nbecome critical. Text watermarking has emerged as a viable solution for\ndetecting AI-generated content and protecting models. However, existing methods\nfall short in providing individualized watermarks for each user, a critical\nfeature for enhancing accountability and traceability. In this paper, we\nintroduce PersonaMark, a novel personalized text watermarking scheme designed\nto protect LLMs' copyrights and bolster accountability. PersonaMark leverages\nsentence structure as a subtle carrier of watermark information and optimizes\nthe generation process to maintain the natural output of the model. By\nemploying a personalized hashing function, unique watermarks are embedded for\neach user, enabling high-quality text generation without compromising the\nmodel's performance. This approach is both time-efficient and scalable, capable\nof handling large numbers of users through a multi-user hashing mechanism. To\nthe best of our knowledge, this is a pioneer study to explore personalized\nwatermarking in LLMs. We conduct extensive evaluations across four LLMs,\nanalyzing various metrics such as perplexity, sentiment, alignment, and\nreadability. The results validate that PersonaMark preserves text quality,\nensures unbiased watermark insertion, and offers robust watermark detection\ncapabilities, all while maintaining the model's behavior with minimal\ndisruption.\n","authors":["Yuehan Zhang","Peizhuo Lv","Yinpeng Liu","Yongqiang Ma","Wei Lu","Xiaofeng Wang","Xiaozhong Liu","Jiawei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.09739v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2412.11985v2","updated":"2024-12-17T16:47:49Z","published":"2024-12-16T17:05:18Z","title":"Speak & Improve Challenge 2025: Tasks and Baseline Systems","summary":"  This paper presents the \"Speak & Improve Challenge 2025: Spoken Language\nAssessment and Feedback\" -- a challenge associated with the ISCA SLaTE 2025\nWorkshop. The goal of the challenge is to advance research on spoken language\nassessment and feedback, with tasks associated with both the underlying\ntechnology and language learning feedback. Linked with the challenge, the Speak\n& Improve (S&I) Corpus 2025 is being pre-released, a dataset of L2 learner\nEnglish data with holistic scores and language error annotation, collected from\nopen (spontaneous) speaking tests on the Speak & Improve learning platform. The\ncorpus consists of approximately 315 hours of audio data from second language\nEnglish learners with holistic scores, and a 55-hour subset with manual\ntranscriptions and error labels. The Challenge has four shared tasks: Automatic\nSpeech Recognition (ASR), Spoken Language Assessment (SLA), Spoken Grammatical\nError Correction (SGEC), and Spoken Grammatical Error Correction Feedback\n(SGECF). Each of these tasks has a closed track where a predetermined set of\nmodels and data sources are allowed to be used, and an open track where any\npublic resource may be used. Challenge participants may do one or more of the\ntasks. This paper describes the challenge, the S&I Corpus 2025, and the\nbaseline systems released for the Challenge.\n","authors":["Mengjie Qian","Kate Knill","Stefano Banno","Siyuan Tang","Penny Karanasou","Mark J. F. Gales","Diane Nicholls"],"pdf_url":"https://arxiv.org/pdf/2412.11985v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06527v2","updated":"2024-12-17T16:44:16Z","published":"2024-08-12T23:19:02Z","title":"Rethinking the Alignment of Psychotherapy Dialogue Generation with\n  Motivational Interviewing Strategies","summary":"  Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, particularly in the context of\nmotivational interviewing (MI). However, the inherent lack of transparency in\nLLM outputs presents significant challenges given the sensitive nature of\npsychotherapy. Applying MI strategies, a set of MI skills, to generate more\ncontrollable therapeutic-adherent conversations with explainability provides a\npossible solution. In this work, we explore the alignment of LLMs with MI\nstrategies by first prompting the LLMs to predict the appropriate strategies as\nreasoning and then utilizing these strategies to guide the subsequent dialogue\ngeneration. We seek to investigate whether such alignment leads to more\ncontrollable and explainable generations. Multiple experiments including\nautomatic and human evaluations are conducted to validate the effectiveness of\nMI strategies in aligning psychotherapy dialogue generation. Our findings\ndemonstrate the potential of LLMs in producing strategically aligned dialogues\nand suggest directions for practical applications in psychotherapeutic\nsettings.\n","authors":["Xin Sun","Xiao Tang","Abdallah El Ali","Zhuying Li","Pengjie Ren","Jan de Wit","Jiahuan Pei","Jos A. Bosch"],"pdf_url":"https://arxiv.org/pdf/2408.06527v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11986v2","updated":"2024-12-17T16:40:27Z","published":"2024-12-16T17:07:26Z","title":"Speak & Improve Corpus 2025: an L2 English Speech Corpus for Language\n  Assessment and Feedback","summary":"  We introduce the Speak & Improve Corpus 2025, a dataset of L2 learner English\ndata with holistic scores and language error annotation, collected from open\n(spontaneous) speaking tests on the Speak & Improve learning platform. The aim\nof the corpus release is to address a major challenge to developing L2 spoken\nlanguage processing systems, the lack of publicly available data with\nhigh-quality annotations. It is being made available for non-commercial use on\nthe ELiT website. In designing this corpus we have sought to make it cover a\nwide-range of speaker attributes, from their L1 to their speaking ability, as\nwell as providing manual annotations. This enables a range of language-learning\ntasks to be examined, such as assessing speaking proficiency or providing\nfeedback on grammatical errors in a learner's speech. Additionally the data\nsupports research into the underlying technology required for these tasks\nincluding automatic speech recognition (ASR) of low resource L2 learner\nEnglish, disfluency detection or spoken grammatical error correction (GEC). The\ncorpus consists of around 315 hours of L2 English learners audio with holistic\nscores, and a subset of audio annotated with transcriptions and error labels.\n","authors":["Kate Knill","Diane Nicholls","Mark J. F. Gales","Mengjie Qian","Pawel Stroinski"],"pdf_url":"https://arxiv.org/pdf/2412.11986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13071v1","updated":"2024-12-17T16:38:10Z","published":"2024-12-17T16:38:10Z","title":"CLASP: Contrastive Language-Speech Pretraining for Multilingual\n  Multimodal Information Retrieval","summary":"  This study introduces CLASP (Contrastive Language-Speech Pretraining), a\nmultilingual, multimodal representation tailored for audio-text information\nretrieval. CLASP leverages the synergy between spoken content and textual data.\nDuring training, we utilize our newly introduced speech-text dataset, which\nencompasses 15 diverse categories ranging from fiction to religion. CLASP's\naudio component integrates audio spectrograms with a pre-trained\nself-supervised speech model, while its language encoding counterpart employs a\nsentence encoder pre-trained on over 100 languages. This unified lightweight\nmodel bridges the gap between various modalities and languages, enhancing its\neffectiveness in handling and retrieving multilingual and multimodal data. Our\nevaluations across multiple languages demonstrate that CLASP establishes new\nbenchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional\nASR-based retrieval approaches in specific scenarios.\n","authors":["Mohammad Mahdi Abootorabi","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2412.13071v1.pdf","comment":"accepted at ECIR 2025"},{"id":"http://arxiv.org/abs/2409.12059v3","updated":"2024-12-17T16:30:39Z","published":"2024-09-18T15:32:48Z","title":"MeTHanol: Modularized Thinking Language Models with Intermediate Layer\n  Thinking, Decoding and Bootstrapping Reasoning","summary":"  Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.\n","authors":["Ningyuan Xi","Xiaoyu Wang","Yetao Wu","Teng Chen","Qingqing Gu","Yue Zhao","Jinxian Qu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.12059v3.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.13050v1","updated":"2024-12-17T16:13:56Z","published":"2024-12-17T16:13:56Z","title":"Modality-Inconsistent Continual Learning of Multimodal Large Language\n  Models","summary":"  In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),\na new continual learning scenario for Multimodal Large Language Models (MLLMs)\nthat involves tasks with inconsistent modalities (image, audio, or video) and\nvarying task types (captioning or question-answering). Unlike existing\nvision-only or modality-incremental settings, MICL combines modality and task\ntype shifts, both of which drive catastrophic forgetting. To address these\nchallenges, we propose MoInCL, which employs a Pseudo Targets Generation Module\nto mitigate forgetting caused by task type shifts in previously seen\nmodalities. It also incorporates Instruction-based Knowledge Distillation to\npreserve the model's ability to handle previously learned modalities when new\nones are introduced. We benchmark MICL using a total of six tasks and conduct\nexperiments to validate the effectiveness of our proposed MoInCL. The\nexperimental results highlight the superiority of MoInCL, showing significant\nimprovements over representative and state-of-the-art continual learning\nbaselines.\n","authors":["Weiguo Pian","Shijian Deng","Shentong Mo","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02738v3","updated":"2024-12-17T16:10:26Z","published":"2024-03-05T07:47:34Z","title":"Causal Prompting: Debiasing Large Language Model Prompting based on\n  Front-Door Adjustment","summary":"  Despite the notable advancements of existing prompting methods, such as\nIn-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they\nstill face challenges related to various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including approaches based on data\naugmentation and reweighting, yet they struggle with the complex biases\ninherent in LLMs. To address such limitations, the causal relationship behind\nthe prompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate LLMs biases. In specific, causal intervention is achieved\nby designing the prompts without accessing the parameters and logits of LLMs.\nThe chain-of-thought generated by LLM is employed as the mediator variable and\nthe causal effect between input prompts and output answers is calculated\nthrough front-door adjustment to mitigate model biases. Moreover, to accurately\nrepresent the chain-of-thoughts and estimate the causal effects, contrastive\nlearning is used to fine-tune the encoder of chain-of-thought by aligning its\nspace with that of the LLM. Experimental results show that the proposed causal\nprompting approach achieves excellent performance across seven natural language\nprocessing datasets on both open-source and closed-source LLMs.\n","authors":["Congzhi Zhang","Linhai Zhang","Jialong Wu","Yulan He","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.02738v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13041v1","updated":"2024-12-17T16:05:30Z","published":"2024-12-17T16:05:30Z","title":"Harnessing Event Sensory Data for Error Pattern Prediction in Vehicles:\n  A Language Model Approach","summary":"  In this paper, we draw an analogy between processing natural languages and\nprocessing multivariate event streams from vehicles in order to predict\n$\\textit{when}$ and $\\textit{what}$ error pattern is most likely to occur in\nthe future for a given car. Our approach leverages the temporal dynamics and\ncontextual relationships of our event data from a fleet of cars. Event data is\ncomposed of discrete values of error codes as well as continuous values such as\ntime and mileage. Modelled by two causal Transformers, we can anticipate\nvehicle failures and malfunctions before they happen. Thus, we introduce\n$\\textit{CarFormer}$, a Transformer model trained via a new self-supervised\nlearning strategy, and $\\textit{EPredictor}$, an autoregressive Transformer\ndecoder model capable of predicting $\\textit{when}$ and $\\textit{what}$ error\npattern will most likely occur after some error code apparition. Despite the\nchallenges of high cardinality of event types, their unbalanced frequency of\nappearance and limited labelled data, our experimental results demonstrate the\nexcellent predictive ability of our novel model. Specifically, with sequences\nof $160$ error codes on average, our model is able with only half of the error\ncodes to achieve $80\\%$ F1 score for predicting $\\textit{what}$ error pattern\nwill occur and achieves an average absolute error of $58.4 \\pm 13.2$h\n$\\textit{when}$ forecasting the time of occurrence, thus enabling confident\npredictive maintenance and enhancing vehicle safety.\n","authors":["Hugo Math","Rainer Lienhart","Robin Schön"],"pdf_url":"https://arxiv.org/pdf/2412.13041v1.pdf","comment":"10 pages, 8 figures, accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.02819v4","updated":"2024-12-17T16:03:43Z","published":"2024-12-03T20:35:57Z","title":"CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels","summary":"  Large Language Models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of high-quality long-context\nsummarization datasets has hindered further advancements in this area. To\naddress this, we introduce CNNSum, a multi-scale long-context summarization\nbenchmark based on Chinese novels, featuring human-driven annotations, which\ncomprises four subsets totaling 695 samples, with lengths ranging from 16k to\n128k. We evaluate numerous LLMs and conduct detailed case analyses.\nFurthermore, we conduct extensive fine-tuning experiments to explore and\nimprove long-context summarization. In our study: (1) Advanced LLMs like GPT-4o\nmay still generate subjective commentary, leading to vague summaries. (2)\nCurrently, long-context summarization mainly relies on memory ability afforded\nby longer context lengths. The advantages of Large LLMs are hard to utilize,\nthus small LLMs are the most cost-effective. (3) Different prompt templates\npaired with various version models may cause large performance gaps. In further\nfine-tuning, these can be mitigated, and the Base version models perform\nbetter. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential;\nusing short-context data can significantly improve long-context summarization\nperformance. However, further applying other interpolation methods requires\ncareful selection. (5) CNNSum provides more reliable and insightful evaluation\nresults than other benchmarks. We release CNNSum to advance future research in\nthis field. https://github.com/CxsGhost/CNNSum\n","authors":["Lingxiao Wei","He Yan","Xiangju Lu","Junmin Zhu","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.02819v4.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2412.10582v2","updated":"2024-12-17T15:56:50Z","published":"2024-12-13T21:48:54Z","title":"WHAT-IF: Exploring Branching Narratives by Meta-Prompting Large Language\n  Models","summary":"  WHAT-IF -- Writing a Hero's Alternate Timeline through Interactive Fiction --\nis a system that uses zero-shot meta-prompting to create branching narratives\nfrom a prewritten story. Played as an interactive fiction (IF) game, WHAT-IF\nlets the player choose between decisions that the large language model (LLM)\nGPT-4 generates as possible branches in the story. Starting with an existing\nlinear plot as input, a branch is created at each key decision taken by the\nmain character. By meta-prompting the LLM to consider the major plot points\nfrom the story, the system produces coherent and well-structured alternate\nstorylines. WHAT-IF stores the branching plot tree in a graph which helps it to\nboth keep track of the story for prompting and maintain the structure for the\nfinal IF system. A video demo of our system can be found here:\nhttps://youtu.be/8vBqjqtupcc.\n","authors":["Runsheng \"Anson\" Huang","Lara J. Martin","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2412.10582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13026v1","updated":"2024-12-17T15:48:25Z","published":"2024-12-17T15:48:25Z","title":"NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation","summary":"  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n","authors":["Karan Wanchoo","Xiaoye Zuo","Hannah Gonzalez","Soham Dan","Georgios Georgakis","Dan Roth","Kostas Daniilidis","Eleni Miltsakaki"],"pdf_url":"https://arxiv.org/pdf/2412.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13018v1","updated":"2024-12-17T15:38:42Z","published":"2024-12-17T15:38:42Z","title":"OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\n  Financial Domain","summary":"  As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.\n","authors":["Shuting Wang","Jiejun Tan","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.13018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12957v2","updated":"2024-12-17T15:38:23Z","published":"2024-04-19T15:40:39Z","title":"Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt\n  Many-Shot Based Factual Knowledge Extraction","summary":"  In this paper, we focus on the challenging task of reliably estimating\nfactual knowledge that is embedded inside large language models (LLMs). To\navoid reliability concerns with prior approaches, we propose to eliminate\nprompt engineering when probing LLMs for factual knowledge. Our approach,\ncalled Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the\nin-context learning ability of LLMs to communicate both the factual knowledge\nquestion as well as the expected answer format. Our knowledge estimator is both\nconceptually simpler (i.e., doesn't depend on meta-linguistic judgments of\nLLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that\nit can surface more of the latent knowledge embedded in LLMs. We also\ninvestigate how different design choices affect the performance of ZP-LKE.\nUsing the proposed estimator, we perform a large-scale evaluation of the\nfactual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2),\nMistral, Gemma, etc. over a large set of relations and facts from the Wikidata\nknowledge base. We observe differences in the factual knowledge between\ndifferent model families and models of different sizes, that some relations are\nconsistently better known than others but that models differ in the precise\nfacts they know, and differences in the knowledge of base models and their\nfinetuned counterparts. Code available at:\nhttps://github.com/QinyuanWu0710/ZeroPrompt_LKE\n","authors":["Qinyuan Wu","Mohammad Aflah Khan","Soumi Das","Vedant Nanda","Bishwamittra Ghosh","Camila Kolling","Till Speicher","Laurent Bindschaedler","Krishna P. Gummadi","Evimaria Terzi"],"pdf_url":"https://arxiv.org/pdf/2404.12957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13008v1","updated":"2024-12-17T15:29:31Z","published":"2024-12-17T15:29:31Z","title":"RCLMuFN: Relational Context Learning and Multiplex Fusion Network for\n  Multimodal Sarcasm Detection","summary":"  Sarcasm typically conveys emotions of contempt or criticism by expressing a\nmeaning that is contrary to the speaker's true intent. Accurate detection of\nsarcasm aids in identifying and filtering undesirable information on the\nInternet, thereby reducing malicious defamation and rumor-mongering.\nNonetheless, the task of automatic sarcasm detection remains highly challenging\nfor machines, as it critically depends on intricate factors such as relational\ncontext. Most existing multimodal sarcasm detection methods focus on\nintroducing graph structures to establish entity relationships between text and\nimages while neglecting to learn the relational context between text and\nimages, which is crucial evidence for understanding the meaning of sarcasm. In\naddition, the meaning of sarcasm changes with the evolution of different\ncontexts, but existing methods may not be accurate in modeling such dynamic\nchanges, limiting the generalization ability of the models. To address the\nabove issues, we propose a relational context learning and multiplex fusion\nnetwork (RCLMuFN) for multimodal sarcasm detection. Firstly, we employ four\nfeature extractors to comprehensively extract features from raw text and\nimages, aiming to excavate potential features that may have been previously\noverlooked. Secondly, we utilize the relational context learning module to\nlearn the contextual information of text and images and capture the dynamic\nproperties through shallow and deep interactions. Finally, we employ a\nmultiplex feature fusion module to enhance the generalization of the model by\npenetratingly integrating multimodal features derived from various interaction\ncontexts. Extensive experiments on two multimodal sarcasm detection datasets\nshow that our proposed method achieves state-of-the-art performance.\n","authors":["Tongguan Wang","Junkai Li","Guixin Su","Yongcheng Zhang","Dongyu Su","Yuxue Hu","Ying Sha"],"pdf_url":"https://arxiv.org/pdf/2412.13008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12997v1","updated":"2024-12-17T15:21:28Z","published":"2024-12-17T15:21:28Z","title":"Enabling Low-Resource Language Retrieval: Establishing Baselines for\n  Urdu MS MARCO","summary":"  As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. This paper introduces the first large-scale\nUrdu IR dataset, created by translating the MS MARCO dataset through machine\ntranslation. We establish baseline results through zero-shot learning for IR in\nUrdu and subsequently apply the mMARCO multilingual IR methodology to this\nnewly translated dataset. Our findings demonstrate that the fine-tuned model\n(Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a\nRecall@10 of 0.439, representing significant improvements over zero-shot\nresults and showing the potential for expanding IR access for Urdu speakers. By\nbridging access gaps for speakers of low-resource languages, this work not only\nadvances multilingual IR research but also emphasizes the ethical and societal\nimportance of inclusive IR technologies. This work provides valuable insights\ninto the challenges and solutions for improving language representation and\nlays the groundwork for future research, especially in South Asian languages,\nwhich can benefit from the adaptable methods used in this study.\n","authors":["Umer Butt","Stalin Veranasi","Günter Neumann"],"pdf_url":"https://arxiv.org/pdf/2412.12997v1.pdf","comment":"6 pages, ECIR 2025, conference submission version"},{"id":"http://arxiv.org/abs/2412.12981v1","updated":"2024-12-17T15:01:07Z","published":"2024-12-17T15:01:07Z","title":"Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental\n  Health","summary":"  Large language models (LLMs) have shown promising capabilities in healthcare\nanalysis but face several challenges like hallucinations, parroting, and bias\nmanifestation. These challenges are exacerbated in complex, sensitive, and\nlow-resource domains. Therefore, in this work we introduce IC-AnnoMI, an\nexpert-annotated motivational interviewing (MI) dataset built upon AnnoMI by\ngenerating in-context conversational dialogues leveraging LLMs, particularly\nChatGPT. IC-AnnoMI employs targeted prompts accurately engineered through cues\nand tailored information, taking into account therapy style (empathy,\nreflection), contextual relevance, and false semantic change. Subsequently, the\ndialogues are annotated by experts, strictly adhering to the Motivational\nInterviewing Skills Code (MISC), focusing on both the psychological and\nlinguistic dimensions of MI dialogues. We comprehensively evaluate the\nIC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding\nof domain intricacies by modeling novel classification tasks employing several\nclassical machine learning and current state-of-the-art transformer approaches.\nFinally, we discuss the effects of progressive prompting strategies and the\nimpact of augmented data in mitigating the biases manifested in IC-AnnoM. Our\ncontributions provide the MI community with not only a comprehensive dataset\nbut also valuable insights for using LLMs in empathetic text generation for\nconversational therapy in supervised settings.\n","authors":["Vivek Kumar","Eirini Ntoutsi","Pushpraj Singh Rajawat","Giacomo Medda","Diego Reforgiato Recupero"],"pdf_url":"https://arxiv.org/pdf/2412.12981v1.pdf","comment":"International Conference on Natural Language Processing and\n  Artificial Intelligence for Cyber Security (NLPAICS) 2024"},{"id":"http://arxiv.org/abs/2412.00876v3","updated":"2024-12-17T14:45:12Z","published":"2024-12-01T16:32:31Z","title":"Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification","summary":"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n","authors":["Wenxuan Huang","Zijie Zhai","Yunhang Shen","Shaosheng Cao","Fei Zhao","Xiangfeng Xu","Zheyu Ye","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.00876v3.pdf","comment":"Code is available at https://github.com/Osilly/dynamic_llava"},{"id":"http://arxiv.org/abs/2412.12961v1","updated":"2024-12-17T14:44:27Z","published":"2024-12-17T14:44:27Z","title":"Adaptations of AI models for querying the LandMatrix database in natural\n  language","summary":"  The Land Matrix initiative (https://landmatrix.org) and its global\nobservatory aim to provide reliable data on large-scale land acquisitions to\ninform debates and actions in sectors such as agriculture, extraction, or\nenergy in low- and middle-income countries. Although these data are recognized\nin the academic world, they remain underutilized in public policy, mainly due\nto the complexity of access and exploitation, which requires technical\nexpertise and a good understanding of the database schema.\n  The objective of this work is to simplify access to data from different\ndatabase systems. The methods proposed in this article are evaluated using data\nfrom the Land Matrix. This work presents various comparisons of Large Language\nModels (LLMs) as well as combinations of LLM adaptations (Prompt Engineering,\nRAG, Agents) to query different database systems (GraphQL and REST queries).\nThe experiments are reproducible, and a demonstration is available online:\nhttps://github.com/tetis-nlp/landmatrix-graphql-python.\n","authors":["Fatiha Ait Kbir","Jérémy Bourgoin","Rémy Decoupes","Marie Gradeler","Roberto Interdonato"],"pdf_url":"https://arxiv.org/pdf/2412.12961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12956v1","updated":"2024-12-17T14:38:21Z","published":"2024-12-17T14:38:21Z","title":"SnakModel: Lessons Learned from Training an Open Danish Large Language\n  Model","summary":"  We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,\nwhich we continuously pre-train on 13.6B Danish words, and further tune on 3.7M\nDanish instructions. As best practices for creating LLMs for smaller language\ncommunities have yet to be established, we examine the effects of early\nmodeling and training decisions on downstream performance throughout the entire\ntraining pipeline, including (1) the creation of a strictly curated corpus of\nDanish text from diverse sources; (2) the language modeling and\ninstruction-tuning training process itself, including the analysis of\nintermediate training dynamics, and ablations across different hyperparameters;\n(3) an evaluation on eight language and culturally-specific tasks. Across these\nexperiments SnakModel achieves the highest overall performance, outperforming\nmultiple contemporary Llama2-7B-based models. By making SnakModel, the majority\nof our pre-training corpus, and the associated code available under open\nlicenses, we hope to foster further research and development in Danish Natural\nLanguage Processing, and establish training guidelines for languages with\nsimilar resource constraints.\n","authors":["Mike Zhang","Max Müller-Eberstein","Elisa Bassignana","Rob van der Goot"],"pdf_url":"https://arxiv.org/pdf/2412.12956v1.pdf","comment":"Accepted at NoDaLiDa 2025 (oral)"},{"id":"http://arxiv.org/abs/2412.12955v1","updated":"2024-12-17T14:37:50Z","published":"2024-12-17T14:37:50Z","title":"Learning from Noisy Labels via Self-Taught On-the-Fly Meta Loss\n  Rescaling","summary":"  Correct labels are indispensable for training effective machine learning\nmodels. However, creating high-quality labels is expensive, and even\nprofessionally labeled data contains errors and ambiguities. Filtering and\ndenoising can be applied to curate labeled data prior to training, at the cost\nof additional processing and loss of information. An alternative is on-the-fly\nsample reweighting during the training process to decrease the negative impact\nof incorrect or ambiguous labels, but this typically requires clean seed data.\nIn this work we propose unsupervised on-the-fly meta loss rescaling to reweight\ntraining samples. Crucially, we rely only on features provided by the model\nbeing trained, to learn a rescaling function in real time without knowledge of\nthe true clean data distribution. We achieve this via a novel meta learning\nsetup that samples validation data for the meta update directly from the noisy\ntraining corpus by employing the rescaling function being trained. Our proposed\nmethod consistently improves performance across various NLP tasks with minimal\ncomputational overhead. Further, we are among the first to attempt on-the-fly\ntraining data reweighting on the challenging task of dialogue modeling, where\nnoisy and ambiguous labels are common. Our strategy is robust in the face of\nnoisy and clean data, handles class imbalance, and prevents overfitting to\nnoisy labels. Our self-taught loss rescaling improves as the model trains,\nshowing the ability to keep learning from the model's own signals. As training\nprogresses, the impact of correctly labeled data is scaled up, while the impact\nof wrongly labeled data is suppressed.\n","authors":["Michael Heck","Christian Geishauser","Nurul Lubis","Carel van Niekerk","Shutong Feng","Hsien-Chin Lin","Benjamin Matthias Ruppik","Renato Vukovic","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2412.12955v1.pdf","comment":"10 pages, 3 figures, accepted at AAAI'25"},{"id":"http://arxiv.org/abs/2412.12954v1","updated":"2024-12-17T14:35:33Z","published":"2024-12-17T14:35:33Z","title":"Recipient Profiling: Predicting Characteristics from Messages","summary":"  It has been shown in the field of Author Profiling that texts may\ninadvertently reveal sensitive information about their authors, such as gender\nor age. This raises important privacy concerns that have been extensively\naddressed in the literature, in particular with the development of methods to\nhide such information. We argue that, when these texts are in fact messages\nexchanged between individuals, this is not the end of the story. Indeed, in\nthis case, a second party, the intended recipient, is also involved and should\nbe considered. In this work, we investigate the potential privacy leaks\naffecting them, that is we propose and address the problem of Recipient\nProfiling. We provide empirical evidence that such a task is feasible on\nseveral publicly accessible datasets\n(https://huggingface.co/datasets/sileod/recipient_profiling). Furthermore, we\nshow that the learned models can be transferred to other datasets, albeit with\na loss in accuracy.\n","authors":["Martin Borquez","Mikaela Keller","Michael Perrot","Damien Sileo"],"pdf_url":"https://arxiv.org/pdf/2412.12954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12948v1","updated":"2024-12-17T14:28:14Z","published":"2024-12-17T14:28:14Z","title":"MOPO: Multi-Objective Prompt Optimization for Affective Text Generation","summary":"  How emotions are expressed depends on the context and domain. On X (formerly\nTwitter), for instance, an author might simply use the hashtag #anger, while in\na news headline, emotions are typically written in a more polite, indirect\nmanner. To enable conditional text generation models to create emotionally\nconnotated texts that fit a domain, users need to have access to a parameter\nthat allows them to choose the appropriate way to express an emotion. To\nachieve this, we introduce MOPO, a Multi-Objective Prompt Optimization\nmethodology. MOPO optimizes prompts according to multiple objectives (which\ncorrespond here to the output probabilities assigned by emotion classifiers\ntrained for different domains). In contrast to single objective optimization,\nMOPO outputs a set of prompts, each with a different weighting of the multiple\nobjectives. Users can then choose the most appropriate prompt for their\ncontext. We evaluate MOPO using three objectives, determined by various\ndomain-specific emotion classifiers. MOPO improves performance by up to 15 pp\nacross all objectives with a minimal loss (1-2 pp) for any single objective\ncompared to single-objective optimization. These minor performance losses are\noffset by a broader generalization across multiple objectives - which is not\npossible with single-objective optimization. Additionally, MOPO reduces\ncomputational requirements by simultaneously optimizing for multiple\nobjectives, eliminating separate optimization procedures for each objective.\n","authors":["Yarik Menchaca Resendiz","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2412.12948v1.pdf","comment":"accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.12940v1","updated":"2024-12-17T14:18:50Z","published":"2024-12-17T14:18:50Z","title":"Improving Fine-grained Visual Understanding in VLMs through Text-Only\n  Training","summary":"  Visual-Language Models (VLMs) have become a powerful tool for bridging the\ngap between visual and linguistic understanding. However, the conventional\nlearning approaches for VLMs often suffer from limitations, such as the high\nresource requirements of collecting and training image-text paired data. Recent\nresearch has suggested that language understanding plays a crucial role in the\nperformance of VLMs, potentially indicating that text-only training could be a\nviable approach. In this work, we investigate the feasibility of enhancing\nfine-grained visual understanding in VLMs through text-only training. Inspired\nby how humans develop visual concept understanding, where rich textual\ndescriptions can guide visual recognition, we hypothesize that VLMs can also\nbenefit from leveraging text-based representations to improve their visual\nrecognition abilities. We conduct comprehensive experiments on two distinct\ndomains: fine-grained species classification and cultural visual understanding\ntasks. Our findings demonstrate that text-only training can be comparable to\nconventional image-text training while significantly reducing computational\ncosts. This suggests a more efficient and cost-effective pathway for advancing\nVLM capabilities, particularly valuable in resource-constrained environments.\n","authors":["Dasol Choi","Guijin Son","Soo Yong Kim","Gio Paik","Seunghyeok Hong"],"pdf_url":"https://arxiv.org/pdf/2412.12940v1.pdf","comment":"AAAI25 workshop accepted"},{"id":"http://arxiv.org/abs/2412.11974v2","updated":"2024-12-17T14:12:56Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v2.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2406.11497v3","updated":"2024-12-17T14:11:19Z","published":"2024-06-17T13:01:12Z","title":"CrAM: Credibility-Aware Attention Modification in LLMs for Combating\n  Misinformation in RAG","summary":"  Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large\nLanguage Models (LLMs) by referencing external documents. However, the\nmisinformation in external documents may mislead LLMs' generation. To address\nthis issue, we explore the task of \"credibility-aware RAG\", in which LLMs\nautomatically adjust the influence of retrieved documents based on their\ncredibility scores to counteract misinformation. To this end, we introduce a\nplug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention\n$\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in\nLLMs and adjusts their attention weights based on the credibility of the\ndocuments, thereby reducing the impact of low-credibility documents.\nExperiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and\nQwen1.5-7B show that CrAM improves the RAG performance of LLMs against\nmisinformation pollution by over 20%, even surpassing supervised fine-tuning\nmethods.\n","authors":["Boyi Deng","Wenjie Wang","Fengbin Zhu","Qifan Wang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2406.11497v3.pdf","comment":"AAAI25 camera-ready"},{"id":"http://arxiv.org/abs/2412.12928v1","updated":"2024-12-17T14:07:01Z","published":"2024-12-17T14:07:01Z","title":"Truthful Text Sanitization Guided by Inference Attacks","summary":"  The purpose of text sanitization is to rewrite those text spans in a document\nthat may directly or indirectly identify an individual, to ensure they no\nlonger disclose personal information. Text sanitization must strike a balance\nbetween preventing the leakage of personal information (privacy protection)\nwhile also retaining as much of the document's original content as possible\n(utility preservation). We present an automated text sanitization strategy\nbased on generalizations, which are more abstract (but still informative) terms\nthat subsume the semantic content of the original text spans. The approach\nrelies on instruction-tuned large language models (LLMs) and is divided into\ntwo stages. The LLM is first applied to obtain truth-preserving replacement\ncandidates and rank them according to their abstraction level. Those candidates\nare then evaluated for their ability to protect privacy by conducting inference\nattacks with the LLM. Finally, the system selects the most informative\nreplacement shown to be resistant to those attacks. As a consequence of this\ntwo-stage process, the chosen replacements effectively balance utility and\nprivacy. We also present novel metrics to automatically evaluate these two\naspects without the need to manually annotate data. Empirical results on the\nText Anonymization Benchmark show that the proposed approach leads to enhanced\nutility, with only a marginal increase in the risk of re-identifying protected\nindividuals compared to fully suppressing the original information.\nFurthermore, the selected replacements are shown to be more truth-preserving\nand abstractive than previous methods.\n","authors":["Ildikó Pilán","Benet Manzanares-Salor","David Sánchez","Pierre Lison"],"pdf_url":"https://arxiv.org/pdf/2412.12928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12841v2","updated":"2024-12-17T13:31:18Z","published":"2024-07-04T12:59:10Z","title":"Black-box Model Ensembling for Textual and Visual Question Answering via\n  Information Fusion","summary":"  A diverse range of large language models (LLMs), e.g., ChatGPT, and visual\nquestion answering (VQA) models, e.g., BLIP, have been developed for solving\ntextual and visual question answering tasks. However, fine-tuning these models\nis either difficult, as it requires access via APIs, rendering them as\nblack-boxes, or costly due to the need of tuning a large number of parameters.\nTo address this, we introduce InfoSel, a data-efficient ensemble method that\nlearns to dynamically pick the winner from existing black-box models for\npredictions on both textual and multimodal visual question answering tasks.\nUnlike traditional ensemble models, InfoSel does not rely on prediction\nprobabilities or confidences, which typically are not available in black-box\nmodels. Experimental results on four datasets demonstrate that our approach\nachieves an absolute increase of up to +5.19\\% in the F1-score compared to\nstandalone LLMs using only 1K training instances.\n","authors":["Yuxi Xia","Kilm Zaporojets","Benjamin Roth"],"pdf_url":"https://arxiv.org/pdf/2407.12841v2.pdf","comment":"15 pages, 6 figures, 9 tables"},{"id":"http://arxiv.org/abs/2412.12898v1","updated":"2024-12-17T13:21:26Z","published":"2024-12-17T13:21:26Z","title":"An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural\n  Language Descriptions","summary":"  The Piping and Instrumentation Diagrams (P&IDs) are foundational to the\ndesign, construction, and operation of workflows in the engineering and process\nindustries. However, their manual creation is often labor-intensive,\nerror-prone, and lacks robust mechanisms for error detection and correction.\nWhile recent advancements in Generative AI, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), have demonstrated significant\npotential across various domains, their application in automating generation of\nengineering workflows remains underexplored. In this work, we introduce a novel\ncopilot for automating the generation of P&IDs from natural language\ndescriptions. Leveraging a multi-step agentic workflow, our copilot provides a\nstructured and iterative approach to diagram creation directly from Natural\nLanguage prompts. We demonstrate the feasibility of the generation process by\nevaluating the soundness and completeness of the workflow, and show improved\nresults compared to vanilla zero-shot and few-shot generation approaches.\n","authors":["Shreeyash Gowaikar","Srinivasan Iyengar","Sameer Segal","Shivkumar Kalyanaraman"],"pdf_url":"https://arxiv.org/pdf/2412.12898v1.pdf","comment":"Accepted at the AAAI'25 Workshop on AI to Accelerate Science and\n  Engineering (AI2ASE)"},{"id":"http://arxiv.org/abs/2412.12893v1","updated":"2024-12-17T13:19:38Z","published":"2024-12-17T13:19:38Z","title":"Question: How do Large Language Models perform on the Question Answering\n  tasks? Answer:","summary":"  Large Language Models (LLMs) have been showing promising results for various\nNLP-tasks without the explicit need to be trained for these tasks by using\nfew-shot or zero-shot prompting techniques. A common NLP-task is\nquestion-answering (QA). In this study, we propose a comprehensive performance\ncomparison between smaller fine-tuned models and out-of-the-box\ninstruction-following LLMs on the Stanford Question Answering Dataset 2.0\n(SQuAD2), specifically when using a single-inference prompting technique. Since\nthe dataset contains unanswerable questions, previous work used a double\ninference method. We propose a prompting style which aims to elicit the same\nability without the need for double inference, saving compute time and\nresources. Furthermore, we investigate their generalization capabilities by\ncomparing their performance on similar but different QA datasets, without\nfine-tuning neither model, emulating real-world uses where the context and\nquestions asked may differ from the original training distribution, for example\nswapping Wikipedia for news articles.\n  Our results show that smaller, fine-tuned models outperform current\nState-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are\nable to close this gap on the out-of-distribution test and even outperform the\nfine-tuned models on 3 of the 5 tested QA datasets.\n","authors":["Kevin Fischer","Darren Fürst","Sebastian Steindl","Jakob Lindner","Ulrich Schäfer"],"pdf_url":"https://arxiv.org/pdf/2412.12893v1.pdf","comment":"Accepted at SAI Computing Conference 2025"},{"id":"http://arxiv.org/abs/2412.12881v1","updated":"2024-12-17T13:05:36Z","published":"2024-12-17T13:05:36Z","title":"RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented\n  Verification and Refinement","summary":"  Existing large language models (LLMs) show exceptional problem-solving\ncapabilities but might struggle with complex reasoning tasks. Despite the\nsuccesses of chain-of-thought and tree-based search methods, they mainly depend\non the internal knowledge of LLMs to search over intermediate reasoning steps,\nlimited to dealing with simple tasks involving fewer reasoning steps. In this\npaper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the\nretrieved information to guide the tree-based deliberative reasoning process\nthat relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree\nSearch, RAG-Star iteratively plans intermediate sub-queries and answers for\nreasoning based on the LLM itself. To consolidate internal and external\nknowledge, we propose an retrieval-augmented verification that utilizes query-\nand answer-aware reward modeling to provide feedback for the inherent reasoning\nof LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate\nthat RAG-Star significantly outperforms previous RAG and reasoning methods.\n","authors":["Jinhao Jiang","Jiayi Chen","Junyi Li","Ruiyang Ren","Shijie Wang","Wayne Xin Zhao","Yang Song","Tao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12881v1.pdf","comment":"LLM;RAG;MCTS"},{"id":"http://arxiv.org/abs/2412.12865v1","updated":"2024-12-17T12:49:14Z","published":"2024-12-17T12:49:14Z","title":"Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over\n  Aligned Large Language Models","summary":"  Alignment, endowing a pre-trained Large language model (LLM) with the ability\nto follow instructions, is crucial for its real-world applications.\nConventional supervised fine-tuning (SFT) methods formalize it as causal\nlanguage modeling typically with a cross-entropy objective, requiring a large\namount of high-quality instruction-response pairs. However, the quality of\nwidely used SFT datasets can not be guaranteed due to the high cost and\nintensive labor for the creation and maintenance in practice. To overcome the\nlimitations associated with the quality of SFT datasets, we introduce a novel\n\\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning\napproach, namely PoFT. The intuition is to boost SFT by imposing a particular\npreference: \\textit{favoring the target model over aligned LLMs on the same SFT\ndata.} This preference encourages the target model to predict a higher\nlikelihood than that predicted by the aligned LLMs, incorporating assessment\ninformation on data quality (i.e., predicted likelihood by the aligned LLMs)\ninto the training process. Extensive experiments are conducted, and the results\nvalidate the effectiveness of the proposed method. PoFT achieves stable and\nconsistent improvements over the SFT baselines across different training\ndatasets and base models. Moreover, we prove that PoFT can be integrated with\nexisting SFT data filtering methods to achieve better performance, and further\nimproved by following preference optimization procedures, such as DPO.\n","authors":["Yuchen Fan","Yuzhong Hong","Qiushi Wang","Junwei Bao","Hongfei Jiang","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2412.12865v1.pdf","comment":"AAAI2025, 12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2402.15061v2","updated":"2024-12-17T12:45:20Z","published":"2024-02-23T02:24:15Z","title":"Fine-tuning Large Language Models for Domain-specific Machine\n  Translation","summary":"  Large language models (LLMs) have shown great potential in domain-specific\nmachine translation (MT). However, one major issue is that LLMs pre-trained on\ngeneral domain corpus might not generalize well to specific domains due to the\nlack of domain-specific knowledge. To address this issue, this paper focuses on\nenhancing the domain-specific MT capability of LLMs, by providing high-quality\ntraining datasets and proposing a novel fine-tuning framework denoted by\nDragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced\nprompting integrates dictionary information into prompts to improve the\ntranslation of domain-specific terminology.; (ii) RAG-based few-shot example\nselection provides high-quality examples that simulate both the domain and\nstyle characteristics; (iii) Fine-tuning with few-shot examples further\nenhances performance when using in-domain examples. We deploy DragFT on three\nwell-known LLM backbones with 13B training parameters to validate its\neffectiveness. The results on three domain-specific datasets show that DragFT\nachieves a significant performance boost and shows superior performance\ncompared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance\nimprovement of DragFT over existing LLMs can be attributed to incorporating\nrelevant knowledge while mitigating noise.\n","authors":["Jiawei Zheng","Hanghai Hong","Feiyan Liu","Xiaoli Wang","Jingsong Su","Yonggui Liang","Shikai Wu"],"pdf_url":"https://arxiv.org/pdf/2402.15061v2.pdf","comment":"13 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2412.12863v1","updated":"2024-12-17T12:44:06Z","published":"2024-12-17T12:44:06Z","title":"DISC: Plug-and-Play Decoding Intervention with Similarity of Characters\n  for Chinese Spelling Check","summary":"  One key characteristic of the Chinese spelling check (CSC) task is that\nincorrect characters are usually similar to the correct ones in either\nphonetics or glyph. To accommodate this, previous works usually leverage\nconfusion sets, which suffer from two problems, i.e., difficulty in determining\nwhich character pairs to include and lack of probabilities to distinguish items\nin the set. In this paper, we propose a light-weight plug-and-play DISC (i.e.,\ndecoding intervention with similarity of characters) module for CSC models.DISC\nmeasures phonetic and glyph similarities between characters and incorporates\nthis similarity information only during the inference phase. This method can be\neasily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and\nReLM, without additional training costs. Experiments on three CSC benchmarks\ndemonstrate that our proposed method significantly improves model performance,\napproaching and even surpassing the current state-of-the-art models.\n","authors":["Ziheng Qiao","Houquan Zhou","Yumeng Liu","Zhenghua Li","Min Zhang","Bo Zhang","Chen Li","Ji Zhang","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17679v3","updated":"2024-12-17T12:37:47Z","published":"2024-11-26T18:44:39Z","title":"Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning","summary":"  Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness.\n","authors":["Zhu Xu","Zhiqiang Zhao","Zihan Zhang","Yuchi Liu","Quanwei Shen","Fei Liu","Yu Kuang","Jian He","Conglin Liu"],"pdf_url":"https://arxiv.org/pdf/2411.17679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12852v1","updated":"2024-12-17T12:26:14Z","published":"2024-12-17T12:26:14Z","title":"Selective Shot Learning for Code Explanation","summary":"  Code explanation plays a crucial role in the software engineering domain,\naiding developers in grasping code functionality efficiently. Recent work shows\nthat the performance of LLMs for code explanation improves in a few-shot\nsetting, especially when the few-shot examples are selected intelligently.\nState-of-the-art approaches for such Selective Shot Learning (SSL) include\ntoken-based and embedding-based methods. However, these SSL approaches have\nbeen evaluated on proprietary LLMs, without much exploration on open-source\nCode-LLMs. Additionally, these methods lack consideration for programming\nlanguage syntax. To bridge these gaps, we present a comparative study and\npropose a novel SSL method (SSL_ner) that utilizes entity information for\nfew-shot example selection. We present several insights and show the\neffectiveness of SSL_ner approach over state-of-the-art methods across two\ndatasets. To the best of our knowledge, this is the first systematic\nbenchmarking of open-source Code-LLMs while assessing the performances of the\nvarious few-shot examples selection approaches for the code explanation task.\n","authors":["Paheli Bhattacharya","Rishabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2412.12852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09056v3","updated":"2024-12-17T12:20:34Z","published":"2024-06-13T12:43:40Z","title":"Towards Reliable Detection of LLM-Generated Texts: A Comprehensive\n  Evaluation Framework with CUDRT","summary":"  The increasing prevalence of large language models (LLMs) has significantly\nadvanced text generation, but the human-like quality of LLM outputs presents\nmajor challenges in reliably distinguishing between human-authored and\nLLM-generated texts. Existing detection benchmarks are constrained by their\nreliance on static datasets, scenario-specific tasks (e.g., question answering\nand text refinement), and a primary focus on English, overlooking the diverse\nlinguistic and operational subtleties of LLMs. To address these gaps, we\npropose CUDRT, a comprehensive evaluation framework and bilingual benchmark in\nChinese and English, categorizing LLM activities into five key operations:\nCreate, Update, Delete, Rewrite, and Translate. CUDRT provides extensive\ndatasets tailored to each operation, featuring outputs from state-of-the-art\nLLMs to assess the reliability of LLM-generated text detectors. This framework\nsupports scalable, reproducible experiments and enables in-depth analysis of\nhow operational diversity, multilingual training sets, and LLM architectures\ninfluence detection performance. Our extensive experiments demonstrate the\nframework's capacity to optimize detection systems, providing critical insights\nto enhance reliability, cross-linguistic adaptability, and detection accuracy.\nBy advancing robust methodologies for identifying LLM-generated texts, this\nwork contributes to the development of intelligent systems capable of meeting\nreal-world multilingual detection challenges. Source code and dataset are\navailable at GitHub.\n","authors":["Zhen Tao","Yanfang Chen","Dinghao Xi","Zhiyu Li","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2406.09056v3.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2412.12841v1","updated":"2024-12-17T12:10:38Z","published":"2024-12-17T12:10:38Z","title":"Benchmarking and Understanding Compositional Relational Reasoning of\n  LLMs","summary":"  Compositional relational reasoning (CRR) is a hallmark of human intelligence,\nbut we lack a clear understanding of whether and how existing transformer large\nlanguage models (LLMs) can solve CRR tasks. To enable systematic exploration of\nthe CRR capability of LLMs, we first propose a new synthetic benchmark called\nGeneralized Associative Recall (GAR) by integrating and generalizing the\nessence of several tasks in mechanistic interpretability (MI) study in a\nunified framework. Evaluation shows that GAR is challenging enough for existing\nLLMs, revealing their fundamental deficiency in CRR. Meanwhile, it is easy\nenough for systematic MI study. Then, to understand how LLMs solve GAR tasks,\nwe use attribution patching to discover the core circuits reused by Vicuna-33B\nacross different tasks and a set of vital attention heads. Intervention\nexperiments show that the correct functioning of these heads significantly\nimpacts task performance. Especially, we identify two classes of heads whose\nactivations represent the abstract notion of true and false in GAR tasks\nrespectively. They play a fundamental role in CRR across various models and\ntasks. The dataset and code are available at https://github.com/Caiyun-AI/GAR.\n","authors":["Ruikang Ni","Da Xiao","Qingye Meng","Xiangyu Li","Shihui Zheng","Hongliang Liang"],"pdf_url":"https://arxiv.org/pdf/2412.12841v1.pdf","comment":"Accepted to the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.12832v1","updated":"2024-12-17T11:54:16Z","published":"2024-12-17T11:54:16Z","title":"DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction\n  in the Era of Large Language Models","summary":"  Evaluating the performance of Grammatical Error Correction (GEC) models has\nbecome increasingly challenging, as large language model (LLM)-based GEC\nsystems often produce corrections that diverge from provided gold references.\nThis discrepancy undermines the reliability of traditional reference-based\nevaluation metrics. In this study, we propose a novel evaluation framework for\nGEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,\nand utilizing a dynamic weighting mechanism. Our framework employs the Analytic\nHierarchy Process (AHP) in conjunction with large language models to ascertain\nthe relative importance of various evaluation criteria. Additionally, we\ndevelop a dataset incorporating human annotations and LLM-simulated sentences\nto validate our algorithms and fine-tune more cost-effective models.\nExperimental results indicate that our proposed approach enhances the\neffectiveness of GEC model evaluations.\n","authors":["Jinxiang Xie","Yilin Li","Xunjian Yin","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2412.12832v1.pdf","comment":"Extended version of a paper to appear in AAAI-25"},{"id":"http://arxiv.org/abs/2410.04468v2","updated":"2024-12-17T11:51:51Z","published":"2024-10-06T12:50:15Z","title":"Revisiting In-context Learning Inference Circuit in Large Language\n  Models","summary":"  In-context Learning (ICL) is an emerging few-shot learning paradigm on\nLanguage Models (LMs) with inner mechanisms un-explored. There are already\nexisting works describing the inner processing of ICL, while they struggle to\ncapture all the inference phenomena in large language models. Therefore, this\npaper proposes a comprehensive circuit to model the inference dynamics and try\nto explain the observed phenomena of ICL. In detail, we divide ICL inference\ninto 3 major operations: (1) Input Text Encode: LMs encode every input text\n(demonstrations and queries) into linear representation in the hidden states\nwith sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge\nthe encoded representations of demonstrations with their corresponding label\ntokens to produce joint representations of labels and demonstrations. (3)\nFeature Retrieval and Copy: LMs search the joint representations similar to the\nquery representation on a task subspace, and copy the searched representations\ninto the query. Then, language model heads capture these copied label\nrepresentations to a certain extent and decode them into predicted labels. The\nproposed inference circuit successfully captured many phenomena observed during\nthe ICL process, making it a comprehensive and practical explanation of the ICL\ninference process. Moreover, ablation analysis by disabling the proposed steps\nseriously damages the ICL performance, suggesting the proposed inference\ncircuit is a dominating mechanism. Additionally, we confirm and list some\nbypass mechanisms that solve ICL tasks in parallel with the proposed circuit.\n","authors":["Hakaze Cho","Mariko Kato","Yoshihiro Sakai","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2410.04468v2.pdf","comment":"37 pages, 41 figures, 8 tables"},{"id":"http://arxiv.org/abs/2412.12808v1","updated":"2024-12-17T11:25:55Z","published":"2024-12-17T11:25:55Z","title":"Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning","summary":"  This paper focuses on sarcasm detection, which aims to identify whether given\nstatements convey criticism, mockery, or other negative sentiment opposite to\nthe literal meaning. To detect sarcasm, humans often require a comprehensive\nunderstanding of the semantics in the statement and even resort to external\ncommonsense to infer the fine-grained incongruity. However, existing methods\nlack commonsense inferential ability when they face complex real-world\nscenarios, leading to unsatisfactory performance. To address this problem, we\npropose a novel framework for sarcasm detection, which conducts incongruity\nreasoning based on commonsense augmentation, called EICR. Concretely, we first\nemploy retrieval-augmented large language models to supplement the missing but\nindispensable commonsense background knowledge. To capture complex contextual\nassociations, we construct a dependency graph and obtain the optimized topology\nvia graph refinement. We further introduce an adaptive reasoning skeleton that\nintegrates prior rules to extract sentiment-inconsistent subgraphs explicitly.\nTo eliminate the possible spurious relations between words and labels, we\nemploy adversarial contrastive learning to enhance the robustness of the\ndetector. Experiments conducted on five datasets demonstrate the effectiveness\nof EICR.\n","authors":["Ziqi Qiu","Jianxing Yu","Yufeng Zhang","Hanjiang Lai","Yanghui Rao","Qinliang Su","Jian Yin"],"pdf_url":"https://arxiv.org/pdf/2412.12808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12806v1","updated":"2024-12-17T11:21:09Z","published":"2024-12-17T11:21:09Z","title":"Cross-Dialect Information Retrieval: Information Access in Low-Resource\n  and High-Variance Languages","summary":"  A large amount of local and culture-specific knowledge (e.g., people,\ntraditions, food) can only be found in documents written in dialects. While\nthere has been extensive research conducted on cross-lingual information\nretrieval (CLIR), the field of cross-dialect retrieval (CDIR) has received\nlimited attention. Dialect retrieval poses unique challenges due to the limited\navailability of resources to train retrieval models and the high variability in\nnon-standardized languages. We study these challenges on the example of German\ndialects and introduce the first German dialect retrieval dataset, dubbed\nWikiDIR, which consists of seven German dialects extracted from Wikipedia.\nUsing WikiDIR, we demonstrate the weakness of lexical methods in dealing with\nhigh lexical variation in dialects. We further show that commonly used\nzero-shot cross-lingual transfer approach with multilingual encoders do not\ntransfer well to extremely low-resource setups, motivating the need for\nresource-lean and dialect-specific retrieval models. We finally demonstrate\nthat (document) translation is an effective way to reduce the dialect gap in\nCDIR.\n","authors":["Robert Litschko","Oliver Kraus","Verena Blaschke","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2412.12806v1.pdf","comment":"Accepted at COLING 2025"},{"id":"http://arxiv.org/abs/2412.12797v1","updated":"2024-12-17T11:00:34Z","published":"2024-12-17T11:00:34Z","title":"Is it the end of (generative) linguistics as we know it?","summary":"  A significant debate has emerged in response to a paper written by Steven\nPiantadosi (Piantadosi, 2023) and uploaded to the LingBuzz platform, the open\narchive for generative linguistics. Piantadosi's dismissal of Chomsky's\napproach is ruthless, but generative linguists deserve it. In this paper, I\nwill adopt three idealized perspectives -- computational, theoretical, and\nexperimental -- to focus on two fundamental issues that lend partial support to\nPiantadosi's critique: (a) the evidence challenging the Poverty of Stimulus\n(PoS) hypothesis and (b) the notion of simplicity as conceived within\nmainstream Minimalism. In conclusion, I argue that, to reclaim a central role\nin language studies, generative linguistics -- representing a prototypical\ntheoretical perspective on language -- needs a serious update leading to (i)\nmore precise, consistent, and complete formalizations of foundational\nintuitions and (ii) the establishment and utilization of a standardized dataset\nof crucial empirical evidence to evaluate the theory's adequacy. On the other\nhand, ignoring the formal perspective leads to major drawbacks in both\ncomputational and experimental approaches. Neither descriptive nor explanatory\nadequacy can be easily achieved without the precise formulation of general\nprinciples that can be challenged empirically.\n","authors":["Cristiano Chesi"],"pdf_url":"https://arxiv.org/pdf/2412.12797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12767v1","updated":"2024-12-17T10:31:21Z","published":"2024-12-17T10:31:21Z","title":"A Survey of Calibration Process for Black-Box LLMs","summary":"  Large Language Models (LLMs) demonstrate remarkable performance in semantic\nunderstanding and generation, yet accurately assessing their output reliability\nremains a significant challenge. While numerous studies have explored\ncalibration techniques, they primarily focus on White-Box LLMs with accessible\nparameters. Black-Box LLMs, despite their superior performance, pose heightened\nrequirements for calibration techniques due to their API-only interaction\nconstraints. Although recent researches have achieved breakthroughs in\nblack-box LLMs calibration, a systematic survey of these methodologies is still\nlacking. To bridge this gap, we presents the first comprehensive survey on\ncalibration techniques for black-box LLMs. We first define the Calibration\nProcess of LLMs as comprising two interrelated key steps: Confidence Estimation\nand Calibration. Second, we conduct a systematic review of applicable methods\nwithin black-box settings, and provide insights on the unique challenges and\nconnections in implementing these key steps. Furthermore, we explore typical\napplications of Calibration Process in black-box LLMs and outline promising\nfuture research directions, providing new perspectives for enhancing\nreliability and human-machine alignment. This is our GitHub link:\nhttps://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs\n","authors":["Liangru Xie","Hui Liu","Jingying Zeng","Xianfeng Tang","Yan Han","Chen Luo","Jing Huang","Zhen Li","Suhang Wang","Qi He"],"pdf_url":"https://arxiv.org/pdf/2412.12767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12761v1","updated":"2024-12-17T10:26:54Z","published":"2024-12-17T10:26:54Z","title":"Revealing the impact of synthetic native samples and multi-tasking\n  strategies in Hindi-English code-mixed humour and sarcasm detection","summary":"  In this paper, we reported our experiments with various strategies to improve\ncode-mixed humour and sarcasm detection. We did all of our experiments for\nHindi-English code-mixed scenario, as we have the linguistic expertise for the\nsame. We experimented with three approaches, namely (i) native sample mixing,\n(ii) multi-task learning (MTL), and (iii) prompting very large multilingual\nlanguage models (VMLMs). In native sample mixing, we added monolingual task\nsamples in code-mixed training sets. In MTL learning, we relied on native and\ncode-mixed samples of a semantically related task (hate detection in our case).\nFinally, in our third approach, we evaluated the efficacy of VMLMs via few-shot\ncontext prompting. Some interesting findings we got are (i) adding native\nsamples improved humor (raising the F1-score up to 6.76%) and sarcasm (raising\nthe F1-score up to 8.64%) detection, (ii) training MLMs in an MTL framework\nboosted performance for both humour (raising the F1-score up to 10.67%) and\nsarcasm (increment up to 12.35% in F1-score) detection, and (iii) prompting\nVMLMs couldn't outperform the other approaches. Finally, our ablation studies\nand error analysis discovered the cases where our model is yet to improve. We\nprovided our code for reproducibility.\n","authors":["Debajyoti Mazumder","Aakash Kumar","Jasabanta Patro"],"pdf_url":"https://arxiv.org/pdf/2412.12761v1.pdf","comment":"26 pages; under review"},{"id":"http://arxiv.org/abs/2412.12744v1","updated":"2024-12-17T10:08:57Z","published":"2024-12-17T10:08:57Z","title":"Your Next State-of-the-Art Could Come from Another Domain: A\n  Cross-Domain Analysis of Hierarchical Text Classification","summary":"  Text classification with hierarchical labels is a prevalent and challenging\ntask in natural language processing. Examples include assigning ICD codes to\npatient records, tagging patents into IPC classes, assigning EUROVOC\ndescriptors to European legal texts, and more. Despite its widespread\napplications, a comprehensive understanding of state-of-the-art methods across\ndifferent domains has been lacking. In this paper, we provide the first\ncomprehensive cross-domain overview with empirical analysis of state-of-the-art\nmethods. We propose a unified framework that positions each method within a\ncommon structure to facilitate research. Our empirical analysis yields key\ninsights and guidelines, confirming the necessity of learning across different\nresearch areas to design effective methods. Notably, under our unified\nevaluation pipeline, we achieved new state-of-the-art results by applying\ntechniques beyond their original domains.\n","authors":["Nan Li","Bo Kang","Tijl De Bie"],"pdf_url":"https://arxiv.org/pdf/2412.12744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19585v2","updated":"2024-12-17T10:06:51Z","published":"2024-09-29T07:04:50Z","title":"Two-stage Framework for Robust Speech Emotion Recognition Using Target\n  Speaker Extraction in Human Speech Noise Conditions","summary":"  Developing a robust speech emotion recognition (SER) system in noisy\nconditions faces challenges posed by different noise properties. Most previous\nstudies have not considered the impact of human speech noise, thus limiting the\napplication scope of SER. In this paper, we propose a novel two-stage framework\nfor the problem by cascading target speaker extraction (TSE) method and SER. We\nfirst train a TSE model to extract the speech of target speaker from a mixture.\nThen, in the second stage, we utilize the extracted speech for SER training.\nAdditionally, we explore a joint training of TSE and SER models in the second\nstage. Our developed system achieves a 14.33% improvement in unweighted\naccuracy (UA) compared to a baseline without using TSE method, demonstrating\nthe effectiveness of our framework in mitigating the impact of human speech\nnoise. Moreover, we conduct experiments considering speaker gender, showing\nthat our framework performs particularly well in different-gender mixture.\n","authors":["Jinyi Mi","Xiaohan Shi","Ding Ma","Jiajun He","Takuya Fujimura","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2409.19585v2.pdf","comment":"This is the preprint version of the paper accepted at APSIPA ASC 2024"},{"id":"http://arxiv.org/abs/2412.12735v1","updated":"2024-12-17T09:57:21Z","published":"2024-12-17T09:57:21Z","title":"GIRAFFE: Design Choices for Extending the Context Length of Visual\n  Language Models","summary":"  Visual Language Models (VLMs) demonstrate impressive capabilities in\nprocessing multimodal inputs, yet applications such as visual agents, which\nrequire handling multiple images and high-resolution videos, demand enhanced\nlong-range modeling. Moreover, existing open-source VLMs lack systematic\nexploration into extending their context length, and commercial models often\nprovide limited details. To tackle this, we aim to establish an effective\nsolution that enhances long context performance of VLMs while preserving their\ncapacities in short context scenarios. Towards this goal, we make the best\ndesign choice through extensive experiment settings from data curation to\ncontext window extending and utilizing: (1) we analyze data sources and length\ndistributions to construct ETVLM - a data recipe to balance the performance\nacross scenarios; (2) we examine existing position extending methods, identify\ntheir limitations and propose M-RoPE++ as an enhanced approach; we also choose\nto solely instruction-tune the backbone with mixed-source data; (3) we discuss\nhow to better utilize extended context windows and propose hybrid-resolution\ntraining. Built on the Qwen-VL series model, we propose Giraffe, which is\neffectively extended to 128K lengths. Evaluated on extensive long context VLM\nbenchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves\nstate-of-the-art performance among similarly sized open-source long VLMs and is\ncompetitive with commercial model GPT-4V. We will open-source the code, data,\nand models.\n","authors":["Mukai Li","Lei Li","Shansan Gong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12735v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.12733v1","updated":"2024-12-17T09:55:41Z","published":"2024-12-17T09:55:41Z","title":"EventFull: Complete and Consistent Event Relation Annotation","summary":"  Event relation detection is a fundamental NLP task, leveraged in many\ndownstream applications, whose modeling requires datasets annotated with event\nrelations of various types. However, systematic and complete annotation of\nthese relations is costly and challenging, due to the quadratic number of event\npairs that need to be considered. Consequently, many current event relation\ndatasets lack systematicity and completeness. In response, we introduce\n\\textit{EventFull}, the first tool that supports consistent, complete and\nefficient annotation of temporal, causal and coreference relations via a\nunified and synergetic process. A pilot study demonstrates that EventFull\naccelerates and simplifies the annotation process while yielding high\ninter-annotator agreement.\n","authors":["Alon Eirew","Eviatar Nachshoni","Aviv Slobodkin","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2412.12733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12731v1","updated":"2024-12-17T09:54:17Z","published":"2024-12-17T09:54:17Z","title":"SentiQNF: A Novel Approach to Sentiment Analysis Using Quantum\n  Algorithms and Neuro-Fuzzy Systems","summary":"  Sentiment analysis is an essential component of natural language processing,\nused to analyze sentiments, attitudes, and emotional tones in various contexts.\nIt provides valuable insights into public opinion, customer feedback, and user\nexperiences. Researchers have developed various classical machine learning and\nneuro-fuzzy approaches to address the exponential growth of data and the\ncomplexity of language structures in sentiment analysis. However, these\napproaches often fail to determine the optimal number of clusters, interpret\nresults accurately, handle noise or outliers efficiently, and scale effectively\nto high-dimensional data. Additionally, they are frequently insensitive to\ninput variations. In this paper, we propose a novel hybrid approach for\nsentiment analysis called the Quantum Fuzzy Neural Network (QFNN), which\nleverages quantum properties and incorporates a fuzzy layer to overcome the\nlimitations of classical sentiment analysis algorithms. In this study, we test\nthe proposed approach on two Twitter datasets: the Coronavirus Tweets Dataset\n(CVTD) and the General Sentimental Tweets Dataset (GSTD), and compare it with\nclassical and hybrid algorithms. The results demonstrate that QFNN outperforms\nall classical, quantum, and hybrid algorithms, achieving 100% and 90% accuracy\nin the case of CVTD and GSTD, respectively. Furthermore, QFNN demonstrates its\nrobustness against six different noise models, providing the potential to\ntackle the computational complexity associated with sentiment analysis on a\nlarge scale in a noisy environment. The proposed approach expedites sentiment\ndata processing and precisely analyses different forms of textual data, thereby\nenhancing sentiment classification and insights associated with sentiment\nanalysis.\n","authors":["Kshitij Dave","Nouhaila Innan","Bikash K. Behera","Zahid Mumtaz","Saif Al-Kuwari","Ahmed Farouk"],"pdf_url":"https://arxiv.org/pdf/2412.12731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18264v2","updated":"2024-12-17T09:53:41Z","published":"2024-02-28T11:51:56Z","title":"WIKIGENBENCH: Exploring Full-length Wikipedia Generation under\n  Real-World Scenario","summary":"  It presents significant challenges to generate comprehensive and accurate\nWikipedia articles for newly emerging events under a real-world scenario.\nExisting attempts fall short either by focusing only on short snippets or by\nusing metrics that are insufficient to evaluate real-world scenarios. In this\npaper, we construct WIKIGENBENCH, a new benchmark consisting of 1,320 entries,\ndesigned to align with real-world scenarios in both generation and evaluation.\nFor generation, we explore a real-world scenario where structured, full-length\nWikipedia articles with citations are generated for new events using input\ndocuments from web sources. For evaluation, we integrate systematic metrics and\nLLM-based metrics to assess the verifiability, organization, and other aspects\naligned with real-world scenarios. Based on this benchmark, we conduct\nextensive experiments using various models within three commonly used\nframeworks: direct RAG, hierarchical structure-based RAG, and RAG with a\nfine-tuned generation model. Experimental results show that hierarchical-based\nmethods can generate more comprehensive content, while fine-tuned methods\nachieve better verifiability. However, even the best methods still show a\nsignificant gap compared to existing Wikipedia content, indicating that further\nresearch is necessary.\n","authors":["Jiebin Zhang","Eugene J. Yu","Qinyu Chen","Chenhao Xiong","Dawei Zhu","Han Qian","Mingbo Song","Weimin Xiong","Xiaoguang Li","Qun Liu","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2402.18264v2.pdf","comment":"COLING 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2406.13362v2","updated":"2024-12-17T09:46:19Z","published":"2024-06-19T09:07:31Z","title":"VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models","summary":"  Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.\n","authors":["Haowen Hou","Peigen Zeng","Fei Ma","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2406.13362v2.pdf","comment":"Accepted at COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2411.08599v2","updated":"2024-12-17T09:45:45Z","published":"2024-11-13T13:30:21Z","title":"XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL","summary":"  To tackle the challenges of large language model performance in natural\nlanguage to SQL tasks, we introduce XiYan-SQL, an innovative framework that\nemploys a multi-generator ensemble strategy to improve candidate generation. We\nintroduce M-Schema, a semi-structured schema representation method designed to\nenhance the understanding of database structures. To enhance the quality and\ndiversity of generated candidate SQL queries, XiYan-SQL integrates the\nsignificant potential of in-context learning (ICL) with the precise control of\nsupervised fine-tuning. On one hand, we propose a series of training strategies\nto fine-tune models to generate high-quality candidates with diverse\npreferences. On the other hand, we implement the ICL approach with an example\nselection method based on named entity recognition to prevent overemphasis on\nentities. The refiner optimizes each candidate by correcting logical or\nsyntactical errors. To address the challenge of identifying the best candidate,\nwe fine-tune a selection model to distinguish nuances of candidate SQL queries.\nThe experimental results on multiple dialect datasets demonstrate the\nrobustness of XiYan-SQL in addressing challenges across different scenarios.\nOverall, our proposed XiYan-SQL achieves the state-of-the-art execution\naccuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on\nSQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the\nquality and diversity of SQL queries but also outperforms previous methods.\n","authors":["Yingqi Gao","Yifu Liu","Xiaoxia Li","Xiaorong Shi","Yin Zhu","Yiming Wang","Shiqi Li","Wei Li","Yuntao Hong","Zhiling Luo","Jinyang Gao","Liyu Mou","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2411.08599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12710v1","updated":"2024-12-17T09:25:44Z","published":"2024-12-17T09:25:44Z","title":"Enhancing Naturalness in LLM-Generated Utterances through Disfluency\n  Insertion","summary":"  Disfluencies are a natural feature of spontaneous human speech but are\ntypically absent from the outputs of Large Language Models (LLMs). This absence\ncan diminish the perceived naturalness of synthesized speech, which is an\nimportant criteria when building conversational agents that aim to mimick human\nbehaviours. We show how the insertion of disfluencies can alleviate this\nshortcoming. The proposed approach involves (1) fine-tuning an LLM with\nLow-Rank Adaptation (LoRA) to incorporate various types of disfluencies into\nLLM-generated utterances and (2) synthesizing those utterances using a\ntext-to-speech model that supports the generation of speech phenomena such as\ndisfluencies. We evaluated the quality of the generated speech across two\nmetrics: intelligibility and perceived spontaneity. We demonstrate through a\nuser study that the insertion of disfluencies significantly increase the\nperceived spontaneity of the generated speech. This increase came, however,\nalong with a slight reduction in intelligibility.\n","authors":["Syed Zohaib Hassan","Pierre Lison","Pål Halvorsen"],"pdf_url":"https://arxiv.org/pdf/2412.12710v1.pdf","comment":"4 pages short paper, references and appendix are additional"},{"id":"http://arxiv.org/abs/2412.12706v1","updated":"2024-12-17T09:20:31Z","published":"2024-12-17T09:20:31Z","title":"More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression","summary":"  As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.\n","authors":["Jiebin Zhang","Dawei Zhu","Yifan Song","Wenhao Wu","Chuqiao Kuang","Xiaoguang Li","Lifeng Shang","Qun Liu","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2412.12706v1.pdf","comment":"13pages,7 figures"},{"id":"http://arxiv.org/abs/2408.09429v2","updated":"2024-12-17T09:19:46Z","published":"2024-08-18T10:07:02Z","title":"Reefknot: A Comprehensive Benchmark for Relation Hallucination\n  Evaluation, Analysis and Mitigation in Multimodal Large Language Models","summary":"  Hallucination issues continue to affect multimodal large language models\n(MLLMs), with existing research mainly addressing object-level or\nattribute-level hallucinations, neglecting the more complex relation\nhallucinations that require advanced reasoning. Current benchmarks for relation\nhallucinations lack detailed evaluation and effective mitigation, and their\ndatasets often suffer from biases due to systematic annotation processes. To\naddress these challenges, we introduce Reefknot, a comprehensive benchmark\ntargeting relation hallucinations, comprising over 20,000 real-world samples.\nWe provide a systematic definition of relation hallucinations, integrating\nperceptive and cognitive perspectives, and construct a relation-based corpus\nusing the Visual Genome scene graph dataset. Our comparative evaluation reveals\nsignificant limitations in current MLLMs' ability to handle relation\nhallucinations. Additionally, we propose a novel confidence-based mitigation\nstrategy, which reduces the hallucination rate by an average of 9.75% across\nthree datasets, including Reefknot. Our work offers valuable insights for\nachieving trustworthy multimodal intelligence.\n","authors":["Kening Zheng","Junkai Chen","Yibo Yan","Xin Zou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2408.09429v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12701v1","updated":"2024-12-17T09:16:54Z","published":"2024-12-17T09:16:54Z","title":"Trigger$^3$: Refining Query Correction via Adaptive Model Selector","summary":"  In search scenarios, user experience can be hindered by erroneous queries due\nto typos, voice errors, or knowledge gaps. Therefore, query correction is\ncrucial for search engines. Current correction models, usually small models\ntrained on specific data, often struggle with queries beyond their training\nscope or those requiring contextual understanding. While the advent of Large\nLanguage Models (LLMs) offers a potential solution, they are still limited by\ntheir pre-training data and inference cost, particularly for complex queries,\nmaking them not always effective for query correction. To tackle these, we\npropose Trigger$^3$, a large-small model collaboration framework that\nintegrates the traditional correction model and LLM for query correction,\ncapable of adaptively choosing the appropriate correction method based on the\nquery and the correction results from the traditional correction model and LLM.\nTrigger$^3$ first employs a correction trigger to filter out correct queries.\nIncorrect queries are then corrected by the traditional correction model. If\nthis fails, an LLM trigger is activated to call the LLM for correction.\nFinally, for queries that no model can correct, a fallback trigger decides to\nreturn the original query. Extensive experiments demonstrate Trigger$^3$\noutperforms correction baselines while maintaining efficiency.\n","authors":["Kepu Zhang","Zhongxiang Sun","Xiao Zhang","Xiaoxue Zang","Kai Zheng","Yang Song","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2412.12701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12513v2","updated":"2024-12-17T09:11:47Z","published":"2024-10-16T12:45:35Z","title":"FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction","summary":"  Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.\n","authors":["Akriti Jain","Saransh Sharma","Koyel Mukherjee","Soumyabrata Pal"],"pdf_url":"https://arxiv.org/pdf/2410.12513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12686v1","updated":"2024-12-17T09:05:30Z","published":"2024-12-17T09:05:30Z","title":"XTransplant: A Probe into the Upper Bound Performance of Multilingual\n  Capability and Culture Adaptability in LLMs via Mutual Cross-lingual\n  Feed-forward Transplantation","summary":"  Current large language models (LLMs) often exhibit imbalances in multilingual\ncapabilities and cultural adaptability, largely due to their English-centric\npretraining data. To address this imbalance, we propose a probing method named\nXTransplant that explores cross-lingual latent interactions via cross-lingual\nfeed-forward transplantation during inference stage, with the hope of enabling\nthe model to leverage the strengths of both English and non-English languages.\nThrough extensive pilot experiments, we empirically prove that both the\nmultilingual capabilities and cultural adaptability of LLMs hold the potential\nto be significantly improved by XTransplant, respectively from En -> non-En and\nnon-En -> En, highlighting the underutilization of current LLMs' multilingual\npotential. And the patterns observed in these pilot experiments further\nmotivate an offline scaling inference strategy, which demonstrates consistent\nperformance improvements in multilingual and culture-aware tasks, sometimes\neven surpassing multilingual supervised fine-tuning. And we do hope our further\nanalysis and discussion could help gain deeper insights into XTransplant\nmechanism.\n","authors":["Yangfan Ye","Xiaocheng Feng","Xiachong Feng","Libo Qin","Yichong Huang","Lei Huang","Weitao Ma","Zhirui Zhang","Yunfei Lu","Xiaohui Yan","Duyu Tang","Dandan Tu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2412.12686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00667v3","updated":"2024-12-17T08:57:43Z","published":"2023-02-01T18:53:42Z","title":"Does Vision Accelerate Hierarchical Generalization in Neural Language\n  Learners?","summary":"  Neural language models (LMs) are arguably less data-efficient than humans\nfrom a language acquisition perspective. One fundamental question is why this\nhuman-LM gap arises. This study explores the advantage of grounded language\nacquisition, specifically the impact of visual information -- which humans can\nusually rely on but LMs largely do not have access to during language\nacquisition -- on syntactic generalization in LMs. Our experiments, following\nthe poverty of stimulus paradigm under two scenarios (using artificial vs.\nnaturalistic images), demonstrate that if the alignments between the linguistic\nand visual components are clear in the input, access to vision data does help\nwith the syntactic generalization of LMs, but if not, visual input does not\nhelp. This highlights the need for additional biases or signals, such as mutual\ngaze, to enhance cross-modal alignment and enable efficient syntactic\ngeneralization in multimodal LMs.\n","authors":["Tatsuki Kuribayashi","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2302.00667v3.pdf","comment":"COLING 2025; 15 pages"},{"id":"http://arxiv.org/abs/2412.12679v1","updated":"2024-12-17T08:47:41Z","published":"2024-12-17T08:47:41Z","title":"Detecting Document-level Paraphrased Machine Generated Content:\n  Mimicking Human Writing Style and Involving Discourse Features","summary":"  The availability of high-quality APIs for Large Language Models (LLMs) has\nfacilitated the widespread creation of Machine-Generated Content (MGC), posing\nchallenges such as academic plagiarism and the spread of misinformation.\nExisting MGC detectors often focus solely on surface-level information,\noverlooking implicit and structural features. This makes them susceptible to\ndeception by surface-level sentence patterns, particularly for longer texts and\nin texts that have been subsequently paraphrased.\n  To overcome these challenges, we introduce novel methodologies and datasets.\nBesides the publicly available dataset Plagbench, we developed the paraphrased\nLong-Form Question and Answer (paraLFQA) and paraphrased Writing Prompts\n(paraWP) datasets using GPT and DIPPER, a discourse paraphrasing tool, by\nextending artifacts from their original versions. To address the challenge of\ndetecting highly similar paraphrased texts, we propose MhBART, an\nencoder-decoder model designed to emulate human writing style while\nincorporating a novel difference score mechanism. This model outperforms strong\nclassifier baselines and identifies deceptive sentence patterns. To better\ncapture the structure of longer texts at document level, we propose\nDTransformer, a model that integrates discourse analysis through PDTB\npreprocessing to encode structural features. It results in substantial\nperformance gains across both datasets -- 15.5\\% absolute improvement on\nparaLFQA, 4\\% absolute improvement on paraWP, and 1.5\\% absolute improvement on\nM4 compared to SOTA approaches.\n","authors":["Yupei Li","Manuel Milling","Lucia Specia","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2412.12679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12674v1","updated":"2024-12-17T08:44:00Z","published":"2024-12-17T08:44:00Z","title":"Train More Parameters But Mind Their Placement: Insights into Language\n  Adaptation with PEFT","summary":"  Smaller LLMs still face significant challenges even in medium-resourced\nlanguages, particularly when it comes to language-specific knowledge -- a\nproblem not easily resolved with machine-translated data. In this case study on\nIcelandic, we aim to enhance the generation performance of an LLM by\nspecialising it using unstructured text corpora. A key focus is on preventing\ninterference with the models' capabilities of handling longer context during\nthis adaptation. Through ablation studies using various parameter-efficient\nfine-tuning (PEFT) methods and setups, we find that increasing the number of\ntrainable parameters leads to better and more robust language adaptation. LoRAs\nplaced in the feed-forward layers and bottleneck adapters show promising\nresults with sufficient parameters, while prefix tuning and (IA)3 are not\nsuitable. Although improvements are consistent in 0-shot summarisation, some\nadapted models struggle with longer context lengths, an issue that can be\nmitigated by adapting only the final layers.\n","authors":["Jenny Kunz"],"pdf_url":"https://arxiv.org/pdf/2412.12674v1.pdf","comment":"To appear at NoDaLiDa 2025"},{"id":"http://arxiv.org/abs/2408.04662v2","updated":"2024-12-17T08:37:34Z","published":"2024-08-06T02:13:15Z","title":"Citekit: A Modular Toolkit for Large Language Model Citation Generation","summary":"  Enabling Large Language Models (LLMs) to generate citations in\nQuestion-Answering (QA) tasks is an emerging paradigm aimed at enhancing the\nverifiability of their responses when LLMs are utilizing external references to\ngenerate an answer. However, there is currently no unified framework to\nstandardize and fairly compare different citation generation methods, leading\nto difficulties in reproducing different methods and a comprehensive\nassessment. To cope with the problems above, we introduce \\name, an open-source\nand modular toolkit designed to facilitate the implementation and evaluation of\nexisting citation generation methods, while also fostering the development of\nnew approaches to improve citation quality in LLM outputs. This tool is highly\nextensible, allowing users to utilize 4 main modules and 14 components to\nconstruct a pipeline, evaluating an existing method or innovative designs. Our\nexperiments with two state-of-the-art LLMs and 11 citation generation baselines\ndemonstrate varying strengths of different modules in answer accuracy and\ncitation quality improvement, as well as the challenge of enhancing\ngranularity. Based on our analysis of the effectiveness of components, we\npropose a new method, self-RAG \\snippet, obtaining a balanced answer accuracy\nand citation quality. Citekit is released at\nhttps://github.com/SjJ1017/Citekit.\n","authors":["Jiajun Shen","Tong Zhou","Yubo Chen","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04662v2.pdf","comment":"7 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.12661v1","updated":"2024-12-17T08:30:00Z","published":"2024-12-17T08:30:00Z","title":"MedMax: Mixed-Modal Instruction Tuning for Training Biomedical\n  Assistants","summary":"  Recent advancements in mixed-modal generative models have enabled flexible\nintegration of information across image-text content. These models have opened\nnew avenues for developing unified biomedical assistants capable of analyzing\nbiomedical images, answering complex questions about them, and predicting the\nimpact of medical procedures on a patient's health. However, existing resources\nface challenges such as limited data availability, narrow domain coverage, and\nrestricted sources (e.g., medical papers). To address these gaps, we present\nMedMax, the first large-scale multimodal biomedical instruction-tuning dataset\nfor mixed-modal foundation models. With 1.47 million instances, MedMax\nencompasses a diverse range of tasks, including multimodal content generation\n(interleaved image-text data), biomedical image captioning and generation,\nvisual chatting, and report understanding. These tasks span diverse medical\ndomains such as radiology and histopathology. Subsequently, we fine-tune a\nmixed-modal foundation model on the MedMax dataset, achieving significant\nperformance improvements: a 26% gain over the Chameleon model and an 18.3%\nimprovement over GPT-4o across 12 downstream biomedical visual\nquestion-answering tasks. Additionally, we introduce a unified evaluation suite\nfor biomedical tasks, providing a robust framework to guide the development of\nnext-generation mixed-modal biomedical AI assistants.\n","authors":["Hritik Bansal","Daniel Israel","Siyan Zhao","Shufan Li","Tung Nguyen","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2412.12661v1.pdf","comment":"12 figures, 15 tables"},{"id":"http://arxiv.org/abs/2412.12649v1","updated":"2024-12-17T08:16:04Z","published":"2024-12-17T08:16:04Z","title":"ClustEm4Ano: Clustering Text Embeddings of Nominal Textual Attributes\n  for Microdata Anonymization","summary":"  This work introduces ClustEm4Ano, an anonymization pipeline that can be used\nfor generalization and suppression-based anonymization of nominal textual\ntabular data. It automatically generates value generalization hierarchies\n(VGHs) that, in turn, can be used to generalize attributes in\nquasi-identifiers. The pipeline leverages embeddings to generate semantically\nclose value generalizations through iterative clustering. We applied KMeans and\nHierarchical Agglomerative Clustering on $13$ different predefined text\nembeddings (both open and closed-source (via APIs)). Our approach is\nexperimentally tested on a well-known benchmark dataset for anonymization: The\nUCI Machine Learning Repository's Adult dataset. ClustEm4Ano supports\nanonymization procedures by offering more possibilities compared to using\narbitrarily chosen VGHs. Experiments demonstrate that these VGHs can outperform\nmanually constructed ones in terms of downstream efficacy (especially for small\n$k$-anonymity ($2 \\leq k \\leq 30$)) and therefore can foster the quality of\nanonymized datasets. Our implementation is made public.\n","authors":["Robert Aufschläger","Sebastian Wilhelm","Michael Heigl","Martin Schramm"],"pdf_url":"https://arxiv.org/pdf/2412.12649v1.pdf","comment":"16 pages, 5 figures, accepted for presentation at IDEAS: 2024 28th\n  International Symposium on Database Engineered Applications, Bayonne, France,\n  August 26-29, 2024"},{"id":"http://arxiv.org/abs/2412.12644v1","updated":"2024-12-17T08:09:15Z","published":"2024-12-17T08:09:15Z","title":"iPrOp: Interactive Prompt Optimization for Large Language Models with a\n  Human in the Loop","summary":"  Prompt engineering has made significant contributions to the era of large\nlanguage models, yet its effectiveness depends on the skills of a prompt\nauthor. Automatic prompt optimization can support the prompt development\nprocess, but requires annotated data. This paper introduces $\\textit{iPrOp}$, a\nnovel Interactive Prompt Optimization system, to bridge manual prompt\nengineering and automatic prompt optimization. With human intervention in the\noptimization loop, $\\textit{iPrOp}$ offers users the flexibility to assess\nevolving prompts. We present users with prompt variations, selected instances,\nlarge language model predictions accompanied by corresponding explanations, and\nperformance metrics derived from a subset of the training data. This approach\nempowers users to choose and further refine the provided prompts based on their\nindividual preferences and needs. This system not only assists non-technical\ndomain experts in generating optimal prompts tailored to their specific tasks\nor domains, but also enables to study the intrinsic parameters that influence\nthe performance of prompt optimization. Our evaluation shows that our system\nhas the capability to generate improved prompts, leading to enhanced task\nperformance.\n","authors":["Jiahui Li","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2412.12644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12643v1","updated":"2024-12-17T08:07:16Z","published":"2024-12-17T08:07:16Z","title":"LLM-based Discriminative Reasoning for Knowledge Graph Question\n  Answering","summary":"  Large language models (LLMs) based on generative pre-trained Transformer have\nachieved remarkable performance on knowledge graph question-answering (KGQA)\ntasks. However, LLMs often produce ungrounded subgraph planning or reasoning\nresults in KGQA due to the hallucinatory behavior brought by the generative\nparadigm, which may hinder the advancement of the LLM-based KGQA model. To deal\nwith the issue, we propose a novel LLM-based Discriminative Reasoning (LDR)\nmethod to explicitly model the subgraph retrieval and answer inference process.\nBy adopting discriminative strategies, the proposed LDR method not only\nenhances the capability of LLMs to retrieve question-related subgraphs but also\nalleviates the issue of ungrounded reasoning brought by the generative paradigm\nof LLMs. Experimental results show that the proposed approach outperforms\nmultiple strong comparison methods, along with achieving state-of-the-art\nperformance on two widely used WebQSP and CWQ benchmarks.\n","authors":["Mufan Xu","Kehai Chen","Xuefeng Bai","Muyun Yang","Tiejun Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10984v3","updated":"2024-12-17T08:03:38Z","published":"2024-06-16T15:44:37Z","title":"Revisiting Cosine Similarity via Normalized ICA-transformed Embeddings","summary":"  Cosine similarity is widely used to measure the similarity between two\nembeddings, while interpretations based on angle and correlation coefficient\nare common. In this study, we focus on the interpretable axes of embeddings\ntransformed by Independent Component Analysis (ICA), and propose a novel\ninterpretation of cosine similarity as the sum of semantic similarities over\naxes. The normalized ICA-transformed embeddings exhibit sparsity, enhancing the\ninterpretability of each axis, and the semantic similarity defined by the\nproduct of the components represents the shared meaning between the two\nembeddings along each axis. The effectiveness of this approach is demonstrated\nthrough intuitive numerical examples and thorough numerical experiments. By\nderiving the probability distributions that govern each component and the\nproduct of components, we propose a method for selecting statistically\nsignificant axes.\n","authors":["Hiroaki Yamagiwa","Momose Oyama","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2406.10984v3.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2410.18652v4","updated":"2024-12-17T08:03:10Z","published":"2024-10-24T11:32:00Z","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","summary":"  Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.\n","authors":["Woosung Koh","Jang Han Yoon","MinHyung Lee","Youngjin Song","Jaegwan Cho","Jaehyun Kang","Taehyeon Kim","Se-young Yun","Youngjae Yu","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18652v4.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.12639v1","updated":"2024-12-17T08:02:08Z","published":"2024-12-17T08:02:08Z","title":"Falcon: Faster and Parallel Inference of Large Language Models through\n  Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree","summary":"  Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers.\n","authors":["Xiangxiang Gao","Weisheng Xie","Yiwei Xiang","Feng Ji"],"pdf_url":"https://arxiv.org/pdf/2412.12639v1.pdf","comment":"AAAI 2025 Accepted"},{"id":"http://arxiv.org/abs/2412.12632v1","updated":"2024-12-17T07:49:49Z","published":"2024-12-17T07:49:49Z","title":"What External Knowledge is Preferred by LLMs? Characterizing and\n  Exploring Chain of Evidence in Imperfect Context","summary":"  Incorporating external knowledge into large language models (LLMs) has\nemerged as a promising approach to mitigate outdated knowledge and\nhallucination in LLMs. However, external knowledge is often imperfect. In\naddition to useful knowledge, external knowledge is rich in irrelevant or\nmisinformation in the context that can impair the reliability of LLM responses.\nThis paper focuses on LLMs' preferred external knowledge in imperfect contexts\nwhen handling multi-hop QA. Inspired by criminal procedural law's Chain of\nEvidence (CoE), we characterize that knowledge preferred by LLMs should\nmaintain both relevance to the question and mutual support among knowledge\npieces. Accordingly, we propose an automated CoE discrimination approach and\nexplore LLMs' preferences from their effectiveness, faithfulness and\nrobustness, as well as CoE's usability in a naive Retrieval-Augmented\nGeneration (RAG) case. The evaluation on five LLMs reveals that CoE enhances\nLLMs through more accurate generation, stronger answer faithfulness, better\nrobustness against knowledge conflict, and improved performance in a popular\nRAG case.\n","authors":["Zhiyuan Chang","Mingyang Li","Xiaojun Jia","Junjie Wang","Yuekai Huang","Qing Wang","Yihao Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12632v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.12627v1","updated":"2024-12-17T07:41:23Z","published":"2024-12-17T07:41:23Z","title":"Make Imagination Clearer! Stable Diffusion-based Visual Imagination for\n  Multimodal Machine Translation","summary":"  Visual information has been introduced for enhancing machine translation\n(MT), and its effectiveness heavily relies on the availability of large amounts\nof bilingual parallel sentence pairs with manual image annotations. In this\npaper, we introduce a stable diffusion-based imagination network into a\nmultimodal large language model (MLLM) to explicitly generate an image for each\nsource sentence, thereby advancing the multimodel MT. Particularly, we build\nheuristic human feedback with reinforcement learning to ensure the consistency\nof the generated image with the source sentence without the supervision of\nimage annotation, which breaks the bottleneck of using visual information in\nMT. Furthermore, the proposed method enables imaginative visual information to\nbe integrated into large-scale text-only MT in addition to multimodal MT.\nExperimental results show that our model significantly outperforms existing\nmultimodal MT and text-only MT, especially achieving an average improvement of\nmore than 14 BLEU points on Multi30K multimodal MT benchmarks.\n","authors":["Andong Chen","Yuchen Song","Kehai Chen","Muyun Yang","Tiejun Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12627v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2412.12621v1","updated":"2024-12-17T07:33:41Z","published":"2024-12-17T07:33:41Z","title":"Jailbreaking? One Step Is Enough!","summary":"  Large language models (LLMs) excel in various tasks but remain vulnerable to\njailbreak attacks, where adversaries manipulate prompts to generate harmful\noutputs. Examining jailbreak prompts helps uncover the shortcomings of LLMs.\nHowever, current jailbreak methods and the target model's defenses are engaged\nin an independent and adversarial process, resulting in the need for frequent\nattack iterations and redesigning attacks for different models. To address\nthese gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that\ndisguises the attack intention as the \"defense\". intention against harmful\ncontent. Specifically, REDA starts from the target response, guiding the model\nto embed harmful content within its defensive measures, thereby relegating\nharmful content to a secondary role and making the model believe it is\nperforming a defensive task. The attacking model considers that it is guiding\nthe target model to deal with harmful content, while the target model thinks it\nis performing a defensive task, creating an illusion of cooperation between the\ntwo. Additionally, to enhance the model's confidence and guidance in\n\"defensive\" intentions, we adopt in-context learning (ICL) with a small number\nof attack examples and construct a corresponding dataset of attack examples.\nExtensive evaluations demonstrate that the REDA method enables cross-model\nattacks without the need to redesign attack strategies for different models,\nenables successful jailbreak in one iteration, and outperforms existing methods\non both open-source and closed-source models.\n","authors":["Weixiong Zheng","Peijian Zeng","Yiwei Li","Hongyan Wu","Nankai Lin","Junhao Chen","Aimin Yang","Yongmei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12621v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2408.08072v3","updated":"2024-12-17T07:30:54Z","published":"2024-08-15T10:44:38Z","title":"I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm","summary":"  Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.\n","authors":["Yiming Liang","Ge Zhang","Xingwei Qu","Tianyu Zheng","Jiawei Guo","Xinrun Du","Zhenzhu Yang","Jiaheng Liu","Chenghua Lin","Lei Ma","Wenhao Huang","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.08072v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05026v2","updated":"2024-12-17T07:26:39Z","published":"2024-10-30T09:35:35Z","title":"Deep Learning and Machine Learning -- Natural Language Processing: From\n  Theory to Application","summary":"  With a focus on natural language processing (NLP) and the role of large\nlanguage models (LLMs), we explore the intersection of machine learning, deep\nlearning, and artificial intelligence. As artificial intelligence continues to\nrevolutionize fields from healthcare to finance, NLP techniques such as\ntokenization, text classification, and entity recognition are essential for\nprocessing and understanding human language. This paper discusses advanced data\npreprocessing techniques and the use of frameworks like Hugging Face for\nimplementing transformer-based models. Additionally, it highlights challenges\nsuch as handling multilingual data, reducing bias, and ensuring model\nrobustness. By addressing key aspects of data processing and model fine-tuning,\nthis work aims to provide insights into deploying effective and ethically sound\nAI solutions.\n","authors":["Keyu Chen","Cheng Fei","Ziqian Bi","Junyu Liu","Benji Peng","Sen Zhang","Xuanhe Pan","Jiawei Xu","Jinlang Wang","Caitlyn Heqi Yin","Yichao Zhang","Pohsun Feng","Yizhu Wen","Tianyang Wang","Ming Li","Jintao Ren","Qian Niu","Silin Chen","Weiche Hsieh","Lawrence K. Q. Yan","Chia Xin Liang","Han Xu","Hong-Ming Tseng","Xinyuan Song","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2411.05026v2.pdf","comment":"252 pages"},{"id":"http://arxiv.org/abs/2412.12612v1","updated":"2024-12-17T07:21:25Z","published":"2024-12-17T07:21:25Z","title":"SynthCypher: A Fully Synthetic Data Generation Framework for\n  Text-to-Cypher Querying in Knowledge Graphs","summary":"  Cypher, the query language for Neo4j graph databases, plays a critical role\nin enabling graph-based analytics and data exploration. While substantial\nresearch has been dedicated to natural language to SQL query generation\n(Text2SQL), the analogous problem for graph databases referred to as\nText2Cypher remains underexplored. In this work, we introduce SynthCypher, a\nfully synthetic and automated data generation pipeline designed to address this\ngap. SynthCypher employs a novel LLMSupervised Generation-Verification\nframework, ensuring syntactically and semantically correct Cypher queries\nacross diverse domains and query complexities. Using this pipeline, we create\nSynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher\ninstances. Fine-tuning open-source large language models (LLMs), including\nLLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant\nperformance improvements of up to 40% on the Text2Cypher test set and 30% on\nthe SPIDER benchmark adapted for graph databases. This work demonstrates that\nhigh-quality synthetic data can effectively advance the state-of-the-art in\nText2Cypher tasks.\n","authors":["Aman Tiwari","Shiva Krishna Reddy Malay","Vikas Yadav","Masoud Hashemi","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2412.12612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18200v2","updated":"2024-12-17T07:18:53Z","published":"2024-06-26T09:33:41Z","title":"SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative\n  Decoding","summary":"  Large Language Models (LLMs) demonstrate remarkable emergent abilities across\nvarious tasks, yet fall short of complex reasoning and planning tasks. The\ntree-search-based reasoning methods address this by surpassing the capabilities\nof chain-of-thought prompting, encouraging exploration of intermediate steps.\nHowever, such methods introduce significant inference latency due to the\nsystematic exploration and evaluation of multiple thought paths. This paper\nintroduces SeeD, a novel and efficient inference framework to optimize runtime\nspeed and GPU memory management concurrently. By employing a scheduled\nspeculative execution, SeeD efficiently handles multiple iterations for the\nthought generation and the state evaluation, leveraging a rounds-scheduled\nstrategy to manage draft model dispatching. Extensive experimental evaluations\non three reasoning datasets demonstrate superior speedup performance of SeeD,\nproviding a viable path for batched inference in training-free speculative\ndecoding.\n","authors":["Zhenglin Wang","Jialong Wu","Yilong Lai","Congzhi Zhang","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.18200v2.pdf","comment":"Accepted by COLING2025"},{"id":"http://arxiv.org/abs/2409.00222v5","updated":"2024-12-17T07:15:38Z","published":"2024-08-30T19:26:15Z","title":"Can Large Language Models Address Open-Target Stance Detection?","summary":"  Stance detection (SD) identifies the text position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs) from\nGPT, Gemini, Llama, and Mistral families, comparing their performance to the\nonly existing work, Target-Stance Extraction (TSE), which benefits from\npredefined targets. Unlike TSE, OTSD removes the dependency of a predefined\nlist, making target generation and evaluation more challenging. We also provide\na metric for evaluating target quality that correlates well with human\njudgment. Our experiments reveal that LLMs outperform TSE in target generation,\nboth when the real target is explicitly and not explicitly mentioned in the\ntext. Similarly, LLMs overall surpass TSE in stance detection for both explicit\nand non-explicit cases. However, LLMs struggle in both target generation and\nstance detection when the target is not explicit.\n","authors":["Abu Ubaida Akash","Ahmed Fahmy","Amine Trabelsi"],"pdf_url":"https://arxiv.org/pdf/2409.00222v5.pdf","comment":"13 pages; currently under submission"},{"id":"http://arxiv.org/abs/2412.12609v1","updated":"2024-12-17T07:14:03Z","published":"2024-12-17T07:14:03Z","title":"MultiLingPoT: Enhancing Mathematical Reasoning with Multilingual Program\n  Fine-tuning","summary":"  Program-of-Thought (PoT), which aims to use programming language instead of\nnatural language as an intermediate step in reasoning, is an important way for\nLLMs to solve mathematical problems. Since different programming languages\nexcel in different areas, it is natural to use the most suitable language for\nsolving specific problems. However, current PoT research only focuses on single\nlanguage PoT, ignoring the differences between different programming languages.\nTherefore, this paper proposes an multilingual program reasoning method,\nMultiLingPoT. This method allows the model to answer questions using multiple\nprogramming languages by fine-tuning on multilingual data. Additionally, prior\nand posterior hybrid methods are used to help the model select the most\nsuitable language for each problem. Our experimental results show that the\ntraining of MultiLingPoT improves each program's mathematical reasoning by\nabout 2.5\\%. Moreover, with proper mixing, the performance of MultiLingPoT can\nbe further improved, achieving a 6\\% increase compared to the single-language\nPoT with the data augmentation.Resources of this paper can be found at\nhttps://github.com/Nianqi-Li/MultiLingPoT.\n","authors":["Nianqi Li","Zujie Liang","Siyu Yuan","Jiaqing Liang","Feng Wei","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.12609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11253v2","updated":"2024-12-17T07:07:52Z","published":"2024-09-17T15:02:23Z","title":"Norm of Mean Contextualized Embeddings Determines their Variance","summary":"  Contextualized embeddings vary by context, even for the same token, and form\na distribution in the embedding space. To analyze this distribution, we focus\non the norm of the mean embedding and the variance of the embeddings. In this\nstudy, we first demonstrate that these values follow the well-known formula for\nvariance in statistics and provide an efficient sequential computation method.\nThen, by observing embeddings from intermediate layers of several Transformer\nmodels, we found a strong trade-off relationship between the norm and the\nvariance: as the mean embedding becomes closer to the origin, the variance\nincreases. This trade-off is likely influenced by the layer normalization\nmechanism used in Transformer models. Furthermore, when the sets of token\nembeddings are treated as clusters, we show that the variance of the entire\nembedding set can theoretically be decomposed into the within-cluster variance\nand the between-cluster variance. We found experimentally that as the layers of\nTransformer models deepen, the embeddings move farther from the origin, the\nbetween-cluster variance relatively decreases, and the within-cluster variance\nrelatively increases. These results are consistent with existing studies on the\nanisotropy of the embedding spaces across layers.\n","authors":["Hiroaki Yamagiwa","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2409.11253v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2412.12606v1","updated":"2024-12-17T07:06:10Z","published":"2024-12-17T07:06:10Z","title":"Multi-Dimensional Insights: Benchmarking Real-World Personalization in\n  Large Multimodal Models","summary":"  The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to\nevaluate the model's ability to analyze and reason beyond basic content. (2)\nRecognizing that people of different age groups have varying needs and\nperspectives when faced with the same scenario, our benchmark stratifies\nquestions into three age categories: young people, middle-aged people, and\nolder people. This design allows for a detailed assessment of LMMs'\ncapabilities in meeting the preferences and needs of different age groups. With\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\ntasks, indicating that existing LMMs still have considerable room for\nimprovement in addressing real-world applications. Looking ahead, we anticipate\nthat the MDI-Benchmark will open new pathways for aligning real-world\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\navailable at https://mdi-benchmark.github.io/\n","authors":["YiFan Zhang","Shanglin Lei","Runqi Qiao","Zhuoma GongQue","Xiaoshuai Song","Guanting Dong","Qiuna Tan","Zhe Wei","Peiqing Yang","Ye Tian","Yadong Xue","Xiaofei Wang","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12606v1.pdf","comment":"33 pages, 33 figures, Work in progress"},{"id":"http://arxiv.org/abs/2410.12608v2","updated":"2024-12-17T06:55:00Z","published":"2024-10-16T14:24:55Z","title":"Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning","summary":"  Large language models (LLMs) have shown increasing competence in solving\nmathematical reasoning problems. However, many open-source LLMs still struggle\nwith errors in calculation and semantic understanding during intermediate\nreasoning steps. In this work, we introduce Prove, a simple yet effective\nframework that leverages translated programs derived from natural language\nsolutions as a verification mechanism to filter out potentially incorrect\nreasoning paths before aggregating final answers. Unlike vanilla majority\nvoting, our approach filters out solutions whose corresponding program output\nis inconsistent with the generated solution, aggregating only those that pass\nverification. We conducted extensive experiments using 13 open-source LLMs from\nvarious model families and sizes, ranging from 0.5B to 13B parameters, across\neight mathematical benchmarks. Our results show that Prove consistently\noutperforms vanilla majority voting as a heuristic for solving mathematical\nreasoning tasks across all model sizes and datasets, achieving improvements of\nup to 18% on GSM8K and 8% on MATH-500. Our codes are available at\nhttps://github.com/declare-lab/prove.\n","authors":["Vernon Y. H. Toh","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2410.12608v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13532v2","updated":"2024-12-17T06:54:32Z","published":"2024-02-21T05:03:07Z","title":"Whispers in Grammars: Injecting Covert Backdoors to Compromise Dense\n  Retrieval Systems","summary":"  Dense retrieval systems have been widely used in various NLP applications.\nHowever, their vulnerabilities to potential attacks have been underexplored.\nThis paper investigates a novel attack scenario where the attackers aim to\nmislead the retrieval system into retrieving the attacker-specified contents.\nThose contents, injected into the retrieval corpus by attackers, can include\nharmful text like hate speech or spam. Unlike prior methods that rely on model\nweights and generate conspicuous, unnatural outputs, we propose a covert\nbackdoor attack triggered by grammar errors. Our approach ensures that the\nattacked models can function normally for standard queries while covertly\ntriggering the retrieval of the attacker's contents in response to minor\nlinguistic mistakes. Specifically, dense retrievers are trained with\ncontrastive loss and hard negative sampling. Surprisingly, our findings\ndemonstrate that contrastive loss is notably sensitive to grammatical errors,\nand hard negative sampling can exacerbate susceptibility to backdoor attacks.\nOur proposed method achieves a high attack success rate with a minimal corpus\npoisoning rate of only 0.048%, while preserving normal retrieval performance.\nThis indicates that the method has negligible impact on user experience for\nerror-free queries. Furthermore, evaluations across three real-world defense\nstrategies reveal that the malicious passages embedded within the corpus remain\nhighly resistant to detection and filtering, underscoring the robustness and\nsubtlety of the proposed attack.\n","authors":["Quanyu Long","Yue Deng","LeiLei Gan","Wenya Wang","Sinno Jialin Pan"],"pdf_url":"https://arxiv.org/pdf/2402.13532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16657v2","updated":"2024-12-17T06:52:46Z","published":"2024-11-25T18:41:56Z","title":"DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation","summary":"  Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.\n","authors":["Zun Wang","Jialu Li","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.16657v2.pdf","comment":"Project website: https://zunwang1.github.io/DreamRunner"},{"id":"http://arxiv.org/abs/2412.12591v1","updated":"2024-12-17T06:48:24Z","published":"2024-12-17T06:48:24Z","title":"LLMs are Also Effective Embedding Models: An In-depth Overview","summary":"  Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods, such as handling longer texts, and multilingual and cross-modal data.\nFurthermore, we discuss factors affecting choices of embedding models, such as\nperformance/efficiency comparisons, dense vs sparse embeddings, pooling\nstrategies, and scaling law. Lastly, the survey highlights the limitations and\nchallenges in adapting LLMs for embeddings, including cross-task embedding\nquality, trade-offs between efficiency and accuracy, low-resource,\nlong-context, data bias, robustness, etc. This survey serves as a valuable\nresource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models.\n","authors":["Chongyang Tao","Tao Shen","Shen Gao","Junshuo Zhang","Zhen Li","Zhengwei Tao","Shuai Ma"],"pdf_url":"https://arxiv.org/pdf/2412.12591v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2412.12588v1","updated":"2024-12-17T06:44:06Z","published":"2024-12-17T06:44:06Z","title":"PerSphere: A Comprehensive Framework for Multi-Faceted Perspective\n  Retrieval and Summarization","summary":"  As online platforms and recommendation algorithms evolve, people are\nincreasingly trapped in echo chambers, leading to biased understandings of\nvarious issues. To combat this issue, we have introduced PerSphere, a benchmark\ndesigned to facilitate multi-faceted perspective retrieval and summarization,\nthus breaking free from these information silos. For each query within\nPerSphere, there are two opposing claims, each supported by distinct,\nnon-overlapping perspectives drawn from one or more documents. Our goal is to\naccurately summarize these documents, aligning the summaries with the\nrespective claims and their underlying perspectives. This task is structured as\na two-step end-to-end pipeline that includes comprehensive document retrieval\nand multi-faceted summarization. Furthermore, we propose a set of metrics to\nevaluate the comprehensiveness of the retrieval and summarization content.\nExperimental results on various counterparts for the pipeline show that recent\nmodels struggle with such a complex task. Analysis shows that the main\nchallenge lies in long context and perspective extraction, and we propose a\nsimple but effective multi-agent summarization system, offering a promising\nsolution to enhance performance on PerSphere.\n","authors":["Yun Luo","Yingjie Li","Xiangkun Hu","Qinglin Qi","Fang Guo","Qipeng Guo","Zheng Zhang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18279v4","updated":"2024-12-17T06:35:56Z","published":"2024-11-27T12:13:39Z","title":"Large Language Model-Brained GUI Agents: A Survey","summary":"  GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.\n","authors":["Chaoyun Zhang","Shilin He","Jiaxu Qian","Bowen Li","Liqun Li","Si Qin","Yu Kang","Minghua Ma","Guyue Liu","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.18279v4.pdf","comment":"The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration"},{"id":"http://arxiv.org/abs/2411.13425v2","updated":"2024-12-17T06:30:32Z","published":"2024-11-20T16:09:22Z","title":"WaterPark: A Robustness Assessment of Language Model Watermarking","summary":"  Various watermarking methods (``watermarkers'') have been proposed to\nidentify LLM-generated texts; yet, due to the lack of unified evaluation\nplatforms, many critical questions remain under-explored: i) What are the\nstrengths/limitations of various watermarkers, especially their attack\nrobustness? ii) How do various design choices impact their robustness? iii) How\nto optimally operate watermarkers in adversarial environments? To fill this\ngap, we systematize existing LLM watermarkers and watermark removal attacks,\nmapping out their design spaces. We then develop WaterPark, a unified platform\nthat integrates 10 state-of-the-art watermarkers and 12 representative attacks.\nMore importantly, by leveraging WaterPark, we conduct a comprehensive\nassessment of existing watermarkers, unveiling the impact of various design\nchoices on their attack robustness. We further explore the best practices to\noperate watermarkers in adversarial environments. We believe our study sheds\nlight on current LLM watermarking techniques while WaterPark serves as a\nvaluable testbed to facilitate future research.\n","authors":["Jiacheng Liang","Zian Wang","Lauren Hong","Shouling Ji","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13425v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2407.04368v2","updated":"2024-12-17T06:28:55Z","published":"2024-07-05T09:13:24Z","title":"Romanization Encoding For Multilingual ASR","summary":"  We introduce romanization encoding for script-heavy languages to optimize\nmultilingual and code-switching Automatic Speech Recognition (ASR) systems. By\nadopting romanization encoding alongside a balanced concatenated tokenizer\nwithin a FastConformer-RNNT framework equipped with a Roman2Char module, we\nsignificantly reduce vocabulary and output dimensions, enabling larger training\nbatches and reduced memory consumption. Our method decouples acoustic modeling\nand language modeling, enhancing the flexibility and adaptability of the\nsystem. In our study, applying this method to Mandarin-English ASR resulted in\na remarkable 63.51% vocabulary reduction and notable performance gains of\n13.72% and 15.03% on SEAME code-switching benchmarks. Ablation studies on\nMandarin-Korean and Mandarin-Japanese highlight our method's strong capability\nto address the complexities of other script-heavy languages, paving the way for\nmore versatile and effective multilingual ASR systems.\n","authors":["Wen Ding","Fei Jia","Hainan Xu","Yu Xi","Junjie Lai","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2407.04368v2.pdf","comment":"Accepted by IEEE SLT2024"},{"id":"http://arxiv.org/abs/2412.12583v1","updated":"2024-12-17T06:24:34Z","published":"2024-12-17T06:24:34Z","title":"Process-Supervised Reward Models for Clinical Note Generation: A\n  Scalable Approach Guided by Domain Expertise","summary":"  Process-supervised reward models (PRMs), which verify large language model\n(LLM) outputs step-by-step, have achieved significant success in mathematical\nand coding problems. However, their application to other domains remains\nlargely unexplored. In this work, we train a PRM to provide step-level reward\nsignals for clinical notes generated by LLMs from patient-doctor dialogues.\nGuided by real-world clinician expertise, we carefully designed step\ndefinitions for clinical notes and utilized Gemini-Pro 1.5 to automatically\ngenerate process supervision data at scale. Our proposed PRM, trained on the\nLLaMA-3.1 8B instruct model, demonstrated superior performance compared to\nGemini-Pro 1.5 and an outcome-supervised reward model (ORM) across two key\nevaluations: (1) the accuracy of selecting gold-reference samples from\nerror-containing samples, achieving 98.8% (versus 61.3% for ORM and 93.8% for\nGemini-Pro 1.5), and (2) the accuracy of selecting physician-preferred notes,\nachieving 56.2% (compared to 51.2% for ORM and 50.0% for Gemini-Pro 1.5).\nAdditionally, we conducted ablation studies to determine optimal loss functions\nand data selection strategies, along with physician reader studies to explore\npredictors of downstream Best-of-N performance. Our promising results suggest\nthe potential of PRMs to extend beyond the clinical domain, offering a scalable\nand effective solution for diverse generative tasks.\n","authors":["Hanyin Wang","Qiping Xu","Bolun Liu","Guleid Hussein","Hariprasad Korsapati","Mohamad El Labban","Kingsley Iheasirim","Mohamed Hassan","Gokhan Anil","Brian Bartlett","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09998v4","updated":"2024-12-17T06:01:52Z","published":"2023-07-19T14:13:02Z","title":"Controlling Equational Reasoning in Large Language Models with Prompt\n  Interventions","summary":"  This paper investigates how hallucination rates in Large Language Models\n(LLMs) may be controlled and mitigated via a symbolic data generation\nframework, and explores a fundamental relationship between the rate of certain\nmathematical errors and interventions. Specifically, we systematically generate\ndata for a derivation generation task, and apply targeted interventions on\nprompts to perturb aspects such as the surface forms of symbols, equational\ntree structures, and mathematical context, and evaluate the effect of prompt\ninterventions across a range of LLMs including fine-tuned T5 models, GPT, and\nothers. Experiments suggest that T5-Large can outperform the few-shot\nperformance of GPT-4 on various evaluation sets generated via the framework,\nhowever, an extensive evaluation based on human analysis, template-based error\ndetection, and various text generation metrics reveals fine-tuned model\nweaknesses beyond what the reference-based metrics singularly describe. We use\nthese results to tie characteristic distributional footprints of interventions\nto the human evaluation of LLM derivation quality, potentially leading to\nsignificant control over fine-grained mathematical capabilities of language\nmodels with respect to specific types of errors.\n","authors":["Jordan Meadows","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2307.09998v4.pdf","comment":"AAAI 2025 (7 pages)"},{"id":"http://arxiv.org/abs/2412.12569v1","updated":"2024-12-17T06:00:54Z","published":"2024-12-17T06:00:54Z","title":"Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport","summary":"  Lexical semantic change detection aims to identify shifts in word meanings\nover time. While existing methods using embeddings from a diachronic corpus\npair estimate the degree of change for target words, they offer limited insight\ninto changes at the level of individual usage instances. To address this, we\napply Unbalanced Optimal Transport (UOT) to sets of contextualized word\nembeddings, capturing semantic change through the excess and deficit in the\nalignment between usage instances. In particular, we propose Sense Usage Shift\n(SUS), a measure that quantifies changes in the usage frequency of a word sense\nat each usage instance. By leveraging SUS, we demonstrate that several\nchallenges in semantic change detection can be addressed in a unified manner,\nincluding quantifying instance-level semantic change and word-level tasks such\nas measuring the magnitude of semantic change and the broadening or narrowing\nof meaning.\n","authors":["Ryo Kishino","Hiroaki Yamagiwa","Ryo Nagata","Sho Yokoi","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2412.12569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16469v2","updated":"2024-12-17T05:51:01Z","published":"2024-06-24T09:18:15Z","title":"Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark\n  with Human-VLM Collaboration","summary":"  To create culturally inclusive vision-language models (VLMs), developing a\nbenchmark that tests their ability to address culturally relevant questions is\nessential. Existing approaches typically rely on human annotators, making the\nprocess labor-intensive and creating a cognitive burden in generating diverse\nquestions. To address this, we propose a semi-automated framework for\nconstructing cultural VLM benchmarks, specifically targeting multiple-choice\nQA. This framework combines human-VLM collaboration, where VLMs generate\nquestions based on guidelines, a small set of annotated examples, and relevant\nknowledge, followed by a verification process by native speakers. We\ndemonstrate the effectiveness of this framework through the creation of\nK-Viscuit, a dataset focused on Korean culture. Our experiments on this dataset\nreveal that open-source models lag behind proprietary ones in understanding\nKorean culture, highlighting key areas for improvement. We also present a\nseries of further analyses, including human evaluation, augmenting VLMs with\nexternal knowledge, and the evaluation beyond multiple-choice QA. Our dataset\nis available at https://huggingface.co/datasets/ddehun/k-viscuit.\n","authors":["Yujin Baek","ChaeHun Park","Jaeseok Kim","Yu-Jung Heo","Du-Seong Chang","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.16469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12567v1","updated":"2024-12-17T05:50:55Z","published":"2024-12-17T05:50:55Z","title":"FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning","summary":"  Real-world decision-making often requires integrating and reasoning over\ninformation from multiple modalities. While recent multimodal large language\nmodels (MLLMs) have shown promise in such tasks, their ability to perform\nmulti-hop reasoning across diverse sources remains insufficiently evaluated.\nExisting benchmarks, such as MMQA, face challenges due to (1) data\ncontamination and (2) a lack of complex queries that necessitate operations\nacross more than two modalities, hindering accurate performance assessment. To\naddress this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a\nbenchmark created to analyze the reasoning capabilities of MLLMs by urging them\nto combine information from textual reports, tables, and charts within the\nfinancial domain. FCMR is categorized into three difficulty levels-Easy,\nMedium, and Hard-facilitating a step-by-step evaluation. In particular,\nproblems at the Hard level require precise cross-modal three-hop reasoning and\nare designed to prevent the disregard of any modality. Experiments on this new\nbenchmark reveal that even state-of-the-art MLLMs struggle, with the\nbest-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the\nmost challenging tier. We also conduct analysis to provide insights into the\ninner workings of the models, including the discovery of a critical bottleneck\nin the information retrieval phase.\n","authors":["Seunghee Kim","Changhyeon Kim","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12564v1","updated":"2024-12-17T05:48:48Z","published":"2024-12-17T05:48:48Z","title":"Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with\n  Large Language Models","summary":"  Aspect-based sentiment analysis (ABSA), a sequence labeling task, has\nattracted increasing attention in multilingual contexts. While previous\nresearch has focused largely on fine-tuning or training models specifically for\nABSA, we evaluate large language models (LLMs) under zero-shot conditions to\nexplore their potential to tackle this challenge with minimal task-specific\nadaptation. We conduct a comprehensive empirical evaluation of a series of LLMs\non multilingual ABSA tasks, investigating various prompting strategies,\nincluding vanilla zero-shot, chain-of-thought (CoT), self-improvement,\nself-debate, and self-consistency, across nine different models. Results\nindicate that while LLMs show promise in handling multilingual ABSA, they\ngenerally fall short of fine-tuned, task-specific models. Notably, simpler\nzero-shot prompts often outperform more complex strategies, especially in\nhigh-resource languages like English. These findings underscore the need for\nfurther refinement of LLM-based approaches to effectively address ABSA task\nacross diverse languages.\n","authors":["Chengyan Wu","Bolei Ma","Zheyu Zhang","Ningyuan Deng","Yanqing He","Yun Xue"],"pdf_url":"https://arxiv.org/pdf/2412.12564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12563v1","updated":"2024-12-17T05:46:50Z","published":"2024-12-17T05:46:50Z","title":"Task-Agnostic Language Model Watermarking via High Entropy Passthrough\n  Layers","summary":"  In the era of costly pre-training of large language models, ensuring the\nintellectual property rights of model owners, and insuring that said models are\nresponsibly deployed, is becoming increasingly important. To this end, we\npropose model watermarking via passthrough layers, which are added to existing\npre-trained networks and trained using a self-supervised loss such that the\nmodel produces high-entropy output when prompted with a unique private key, and\nacts normally otherwise. Unlike existing model watermarking methods, our method\nis fully task-agnostic, and can be applied to both classification and\nsequence-to-sequence tasks without requiring advanced access to downstream\nfine-tuning datasets. We evaluate the proposed passthrough layers on a wide\nrange of downstream tasks, and show experimentally our watermarking method\nachieves a near-perfect watermark extraction accuracy and false-positive rate\nin most cases without damaging original model performance. Additionally, we\nshow our method is robust to both downstream fine-tuning, fine-pruning, and\nlayer removal attacks, and can be trained in a fraction of the time required to\ntrain the original model. Code is available in the paper.\n","authors":["Vaden Masrani","Mohammad Akbari","David Ming Xuan Yue","Ahmad Rezaei","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12563v1.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2412.12559v1","updated":"2024-12-17T05:38:27Z","published":"2024-12-17T05:38:27Z","title":"EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation","summary":"  We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT\n","authors":["Taeho Hwang","Sukmin Cho","Soyeong Jeong","Hoyun Song","SeungYoon Han","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2412.12559v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.13191v3","updated":"2024-12-17T05:36:53Z","published":"2024-10-17T03:38:29Z","title":"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback","summary":"  Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.\n","authors":["Zonghai Yao","Aditya Parashar","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13191v3.pdf","comment":"Equal contribution for the first two authors. Keywords: Question\n  Generation, USMLE, Self-Refine, Self-Critique, and Self-Correction,\n  LLM-as-Judge, AI for Medical Education"},{"id":"http://arxiv.org/abs/2406.12307v3","updated":"2024-12-17T05:33:04Z","published":"2024-06-18T06:28:06Z","title":"Can Tool-augmented Large Language Models be Aware of Incomplete\n  Conditions?","summary":"  Recent advancements in integrating large language models (LLMs) with tools\nhave allowed the models to interact with real-world environments. However,\nthese tool-augmented LLMs often encounter incomplete scenarios when users\nprovide partial information or the necessary tools are unavailable. Recognizing\nand managing such scenarios is crucial for LLMs to ensure their reliability,\nbut this exploration remains understudied. This study examines whether LLMs can\nidentify incomplete conditions and appropriately determine when to refrain from\nusing tools. To this end, we address a dataset by manipulating instances from\ntwo datasets by removing necessary tools or essential information for tool\ninvocation. Our experiments show that LLMs often struggle to identify the\nabsence of information required to utilize specific tools and recognize the\nabsence of appropriate tools. We further analyze model behaviors in different\nenvironments and compare their performance against humans. Our research can\ncontribute to advancing reliable LLMs by addressing common scenarios during\ninteractions between humans and LLMs. Our code and dataset will be publicly\navailable.\n","authors":["Seungbin Yang","ChaeHun Park","Taehee Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.12307v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12541v1","updated":"2024-12-17T05:09:07Z","published":"2024-12-17T05:09:07Z","title":"LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven\n  Curriculum Learning","summary":"  While large-scale language models (LLMs) have demonstrated remarkable\ncapabilities in specific natural language processing (NLP) tasks, they may\nstill lack proficiency compared to specialized models in certain domains, such\nas grammatical error correction (GEC). Drawing inspiration from the concept of\ncurriculum learning, we have delved into refining LLMs into proficient GEC\nexperts by devising effective curriculum learning (CL) strategies. In this\npaper, we introduce a novel approach, termed LLM-based curriculum learning,\nwhich capitalizes on the robust semantic comprehension and discriminative\nprowess inherent in LLMs to gauge the complexity of GEC training data. Unlike\ntraditional curriculum learning techniques, our method closely mirrors human\nexpert-designed curriculums. Leveraging the proposed LLM-based CL method, we\nsequentially select varying levels of curriculums ranging from easy to hard,\nand iteratively train and refine using the pretrianed T5 and LLaMA series\nmodels. Through rigorous testing and analysis across diverse benchmark\nassessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19\ndevelopment sets, our approach showcases a significant performance boost over\nbaseline models and conventional curriculum learning methodologies.\n","authors":["Tao Fang","Derek F. Wong","Lusheng Zhang","Keyan Jin","Qiang Zhang","Tianjiao Li","Jinlong Hou","Lidia S. Chao"],"pdf_url":"https://arxiv.org/pdf/2412.12541v1.pdf","comment":"Derek F. Wong is the corresponding author. The preprint version\n  consists of 15 Pages, 5 Figures, 5 Tables, and 3 Appendices"},{"id":"http://arxiv.org/abs/2412.12527v1","updated":"2024-12-17T04:38:08Z","published":"2024-12-17T04:38:08Z","title":"When to Speak, When to Abstain: Contrastive Decoding with Abstention","summary":"  Large Language Models (LLMs) demonstrate exceptional performance across\ndiverse tasks by leveraging both pre-trained knowledge (i.e., parametric\nknowledge) and external knowledge (i.e., contextual knowledge). While\nsubstantial efforts have been made to leverage both forms of knowledge,\nscenarios in which the model lacks any relevant knowledge remain underexplored.\nSuch limitations can result in issues like hallucination, causing reduced\nreliability and potential risks in high-stakes applications. To address such\nlimitations, this paper extends the task scope to encompass cases where the\nuser's request cannot be fulfilled due to the lack of relevant knowledge. To\nthis end, we introduce Contrastive Decoding with Abstention (CDA), a\ntraining-free decoding method that empowers LLMs to generate responses when\nrelevant knowledge is available and to abstain otherwise. CDA evaluates the\nrelevance of each knowledge for a given query, adaptively determining which\nknowledge to prioritize or which to completely ignore. Extensive experiments\nwith four LLMs on three question-answering datasets demonstrate that CDA can\neffectively perform accurate generation and abstention simultaneously. These\nfindings highlight CDA's potential to broaden the applicability of LLMs,\nenhancing reliability and preserving user trust.\n","authors":["Hyuhng Joon Kim","Youna Kim","Sang-goo Lee","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12527v1.pdf","comment":"under-review"},{"id":"http://arxiv.org/abs/2410.09097v2","updated":"2024-12-17T04:34:32Z","published":"2024-10-09T01:35:38Z","title":"Recent advancements in LLM Red-Teaming: Techniques, Defenses, and\n  Ethical Considerations","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks, but their vulnerability to jailbreak attacks\nposes significant security risks. This survey paper presents a comprehensive\nanalysis of recent advancements in attack strategies and defense mechanisms\nwithin the field of Large Language Model (LLM) red-teaming. We analyze various\nattack methods, including gradient-based optimization, reinforcement learning,\nand prompt engineering approaches. We discuss the implications of these attacks\non LLM safety and the need for improved defense mechanisms. This work aims to\nprovide a thorough understanding of the current landscape of red-teaming\nattacks and defenses on LLMs, enabling the development of more secure and\nreliable language models.\n","authors":["Tarun Raheja","Nilay Pochhi","F. D. C. M. Curie"],"pdf_url":"https://arxiv.org/pdf/2410.09097v2.pdf","comment":"16 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.10882v7","updated":"2024-12-17T04:33:12Z","published":"2024-06-16T10:10:37Z","title":"SCAR: Efficient Instruction-Tuning for Large Language Models via Style\n  Consistency-Aware Response Ranking","summary":"  Recent studies emphasize that manually ensuring a consistent response style\nand maintaining high data quality in training sets can significantly improve\nthe performance of fine-tuned Large Language Models (LLMs) while reducing the\nnumber of training examples needed. However, the precise definition of style\nand the relationship between style, data quality, and LLM performance remains\nunclear. This research identifies two key stylistic elements in responses:\nlinguistic form and instructional surprisal. We find that, among training data\nof comparable quality, higher consistency in these response elements leads to\nbetter LLM performance. Inspired by this, we introduce Style Consistency-Aware\nResponse Ranking (SCAR), which automatically prioritizes instruction-response\npairs in the training set based on their response stylistic consistency. By\nselecting the most style-consistent examples, sometimes as few as 0.7% of the\nfull dataset, the fine-tuned LLMs can match or even surpass the performance of\nmodels trained on the entire dataset in coding and open-ended\nquestion-answering benchmarks. Code and data are available at\nhttps://github.com/zhuang-li/SCAR .\n","authors":["Zhuang Li","Yuncheng Hua","Thuy-Trang Vu","Haolan Zhan","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.10882v7.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2401.11791v3","updated":"2024-12-17T04:27:31Z","published":"2024-01-22T09:41:05Z","title":"Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation","summary":"  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each object category.\nIn this way, SemPLeS can perform better semantic alignment between object\nregions and class labels, resulting in desired pseudo masks for training\nsegmentation models. The proposed SemPLeS framework achieves competitive\nperformance on standard WSSS benchmarks, PASCAL VOC 2012 and MS COCO2014, and\nshows compatibility with other WSSS methods.\n","authors":["Ci-Siang Lin","Chien-Yi Wang","Yu-Chiang Frank Wang","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11791v3.pdf","comment":"WACV 2025. Project page: https://projectdisr.github.io/semples"},{"id":"http://arxiv.org/abs/2412.12522v1","updated":"2024-12-17T04:22:22Z","published":"2024-12-17T04:22:22Z","title":"Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust\n  Text-to-SQL","summary":"  Recently, large language models (LLMs) have significantly improved the\nperformance of text-to-SQL systems. Nevertheless, many state-of-the-art (SOTA)\napproaches have overlooked the critical aspect of system robustness. Our\nexperiments reveal that while LLM-driven methods excel on standard datasets,\ntheir accuracy is notably compromised when faced with adversarial\nperturbations. To address this challenge, we propose a robust text-to-SQL\nsolution, called Solid-SQL, designed to integrate with various LLMs. We focus\non the pre-processing stage, training a robust schema-linking model enhanced by\nLLM-based data augmentation. Additionally, we design a two-round, structural\nsimilarity-based example retrieval strategy for in-context learning. Our method\nachieves SOTA SQL execution accuracy levels of 82.1% and 58.9% on the general\nSpider and Bird benchmarks, respectively. Furthermore, experimental results\nshow that Solid-SQL delivers an average improvement of 11.6% compared to\nbaselines on the perturbed Spider-Syn, Spider-Realistic, and Dr. Spider\nbenchmarks.\n","authors":["Geling Liu","Yunzhi Tan","Ruichao Zhong","Yuanzhen Xie","Lingchen Zhao","Qian Wang","Bo Hu","Zang Li"],"pdf_url":"https://arxiv.org/pdf/2412.12522v1.pdf","comment":"Accepted at COLING 2025 Main"},{"id":"http://arxiv.org/abs/2408.11869v2","updated":"2024-12-17T03:59:22Z","published":"2024-08-19T02:27:00Z","title":"ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA","summary":"  Large language models (LLMs) require model editing to efficiently update\nspecific knowledge within them and avoid factual errors. Most model editing\nmethods are solely designed for single-time use and result in a significant\nforgetting effect in lifelong editing scenarios, where sequential edits are\nconducted over time. Previous approaches manage sequential edits by freezing\noriginal parameters and discretely allocating new parameters for each knowledge\nupdate. However, these methods lack robustness to minor input variations due to\nthe discrete mapping between data and parameters. To overcome this challenge,\nwe propose ELDER, a novel approach to create a continuous association between\ndata and adapters. ELDER integrates multiple LoRAs through a router network and\nis trained to establish a smooth data-adapter association, thereby enhancing\nthe edit robustness and generalization of semantically equivalent inputs. To\nensure inputs containing the same knowledge will be processed by the same\nLoRAs, we design a novel loss to guide the model link LoRA allocations with\nedit knowledge. Furthermore, we propose a deferral mechanism to retain the\noriginal LLM capabilities post-edit. Extensive experiments on GPT-2 XL and\nLLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong\nsetting, outperforming eight baselines while exhibiting strong scalability and\npreserving LLMs' general abilities on downstream tasks.\n","authors":["Jiaang Li","Quan Wang","Zhongnan Wang","Yongdong Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2408.11869v2.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2410.16179v3","updated":"2024-12-17T03:56:26Z","published":"2024-10-21T16:44:51Z","title":"MagicPIG: LSH Sampling for Efficient LLM Generation","summary":"  Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.\n","authors":["Zhuoming Chen","Ranajoy Sadhukhan","Zihao Ye","Yang Zhou","Jianyu Zhang","Niklas Nolte","Yuandong Tian","Matthijs Douze","Leon Bottou","Zhihao Jia","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16179v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12510v1","updated":"2024-12-17T03:46:51Z","published":"2024-12-17T03:46:51Z","title":"Can Large Language Models Understand You Better? An MBTI Personality\n  Detection Dataset Aligned with Population Traits","summary":"  The Myers-Briggs Type Indicator (MBTI) is one of the most influential\npersonality theories reflecting individual differences in thinking, feeling,\nand behaving. MBTI personality detection has garnered considerable research\ninterest and has evolved significantly over the years. However, this task tends\nto be overly optimistic, as it currently does not align well with the natural\ndistribution of population personality traits. Specifically, (1) the\nself-reported labels in existing datasets result in incorrect labeling issues,\nand (2) the hard labels fail to capture the full range of population\npersonality distributions. In this paper, we optimize the task by constructing\nMBTIBench, the first manually annotated high-quality MBTI personality detection\ndataset with soft labels, under the guidance of psychologists. As for the first\nchallenge, MBTIBench effectively solves the incorrect labeling issues, which\naccount for 29.58% of the data. As for the second challenge, we estimate soft\nlabels by deriving the polarity tendency of samples. The obtained soft labels\nconfirm that there are more people with non-extreme personality traits.\nExperimental results not only highlight the polarized predictions and biases in\nLLMs as key directions for future research, but also confirm that soft labels\ncan provide more benefits to other psychological tasks than hard labels. The\ncode and data are available at https://github.com/Personality-NLP/MbtiBench.\n","authors":["Bohan Li","Jiannan Guan","Longxu Dou","Yunlong Feng","Dingzirui Wang","Yang Xu","Enbo Wang","Qiguang Chen","Bichen Wang","Xiao Xu","Yimeng Zhang","Libo Qin","Yanyan Zhao","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2412.12510v1.pdf","comment":"Accepted by COLING 2025. 28 papges, 20 figures, 10 tables"},{"id":"http://arxiv.org/abs/2412.12509v1","updated":"2024-12-17T03:37:31Z","published":"2024-12-17T03:37:31Z","title":"Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge","summary":"  Large Language Models (LLMs) have become increasingly powerful and\nubiquitous, but their stochastic nature poses challenges to the reliability of\ntheir outputs. While deterministic settings can improve consistency, they do\nnot guarantee reliability, as a single sample from the model's probability\ndistribution can still be misleading. Building upon the concept of\nLLM-as-a-judge, we introduce a novel framework for rigorously evaluating the\nreliability of LLM judgments, leveraging McDonald's omega. We evaluate the\nreliability of LLMs when judging the outputs of other LLMs on standard\nsingle-turn and multi-turn benchmarks, simultaneously investigating the impact\nof temperature on reliability. By analyzing these results, we demonstrate the\nlimitations of fixed randomness and the importance of considering multiple\nsamples, which we show has significant implications for downstream\napplications. Our findings highlight the need for a nuanced understanding of\nLLM reliability and the potential risks associated with over-reliance on\nsingle-shot evaluations. This work provides a crucial step towards building\nmore trustworthy and reliable LLM-based systems and applications.\n","authors":["Kayla Schroeder","Zach Wood-Doughty"],"pdf_url":"https://arxiv.org/pdf/2412.12509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16416v2","updated":"2024-12-17T03:22:34Z","published":"2024-06-24T08:06:56Z","title":"Multilingual Knowledge Editing with Language-Agnostic Factual Neurons","summary":"  Multilingual knowledge editing (MKE) aims to simultaneously update factual\nknowledge across multiple languages within large language models (LLMs).\nPrevious research indicates that the same knowledge across different languages\nwithin LLMs exhibits a degree of shareability. However, most existing MKE\nmethods overlook the connections of the same knowledge between different\nlanguages, resulting in knowledge conflicts and limited edit performance. To\naddress this issue, we first investigate how LLMs process multilingual factual\nknowledge and discover that the same factual knowledge in different languages\ngenerally activates a shared set of neurons, which we call language-agnostic\nfactual neurons (LAFNs). These neurons represent the same factual knowledge\nshared across languages and imply the semantic connections among multilingual\nknowledge. Inspired by this finding, we propose a new MKE method by Locating\nand Updating Language-Agnostic Factual Neurons (LU-LAFNs) to edit multilingual\nknowledge simultaneously, which avoids knowledge conflicts and thus improves\nedit performance. Experimental results on Bi-ZsRE and MzsRE benchmarks\ndemonstrate that our method achieves the best edit performance, indicating the\neffectiveness and importance of modeling the semantic connections among\nmultilingual knowledge.\n","authors":["Xue Zhang","Yunlong Liang","Fandong Meng","Songming Zhang","Yufeng Chen","Jinan Xu","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.16416v2.pdf","comment":"COLING 2025 (14 pages, 3 figures, 12 tables)"},{"id":"http://arxiv.org/abs/2412.12505v1","updated":"2024-12-17T03:20:00Z","published":"2024-12-17T03:20:00Z","title":"DocFusion: A Unified Framework for Document Parsing Tasks","summary":"  Document parsing is essential for analyzing complex document structures and\nextracting fine-grained information, supporting numerous downstream\napplications. However, existing methods often require integrating multiple\nindependent models to handle various parsing tasks, leading to high complexity\nand maintenance overhead. To address this, we propose DocFusion, a lightweight\ngenerative model with only 0.28B parameters. It unifies task representations\nand achieves collaborative training through an improved objective function.\nExperiments reveal and leverage the mutually beneficial interaction among\nrecognition tasks, and integrating recognition data significantly enhances\ndetection performance. The final results demonstrate that DocFusion achieves\nstate-of-the-art (SOTA) performance across four key tasks.\n","authors":["Mingxu Chai","Ziyu Shen","Chong Zhang","Yue Zhang","Xiao Wang","Shihan Dou","Jihua Kang","Jiazheng Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12501v1","updated":"2024-12-17T03:05:27Z","published":"2024-12-17T03:05:27Z","title":"Unleashing the Potential of Model Bias for Generalized Category\n  Discovery","summary":"  Generalized Category Discovery is a significant and complex task that aims to\nidentify both known and undefined novel categories from a set of unlabeled\ndata, leveraging another labeled dataset containing only known categories. The\nprimary challenges stem from model bias induced by pre-training on only known\ncategories and the lack of precise supervision for novel ones, leading to\ncategory bias towards known categories and category confusion among different\nnovel categories, which hinders models' ability to identify novel categories\neffectively. To address these challenges, we propose a novel framework named\nSelf-Debiasing Calibration (SDC). Unlike prior methods that regard model bias\ntowards known categories as an obstacle to novel category identification, SDC\nprovides a novel insight into unleashing the potential of the bias to\nfacilitate novel category learning. Specifically, the output of the biased\nmodel serves two key purposes. First, it provides an accurate modeling of\ncategory bias, which can be utilized to measure the degree of bias and debias\nthe output of the current training model. Second, it offers valuable insights\nfor distinguishing different novel categories by transferring knowledge between\nsimilar categories. Based on these insights, SDC dynamically adjusts the output\nlogits of the current training model using the output of the biased model. This\napproach produces less biased logits to effectively address the issue of\ncategory bias towards known categories, and generates more accurate pseudo\nlabels for unlabeled data, thereby mitigating category confusion for novel\ncategories. Experiments on three benchmark datasets show that SDC outperforms\nSOTA methods, especially in the identification of novel categories. Our code\nand data are available at \\url{https://github.com/Lackel/SDC}.\n","authors":["Wenbin An","Haonan Lin","Jiahao Nie","Feng Tian","Wenkai Shi","Yaqiang Wu","Qianying Wang","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12501v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12500v1","updated":"2024-12-17T03:05:26Z","published":"2024-12-17T03:05:26Z","title":"Beyond Data Quantity: Key Factors Driving Performance in Multilingual\n  Language Models","summary":"  Multilingual language models (MLLMs) are crucial for handling text across\nvarious languages, yet they often show performance disparities due to\ndifferences in resource availability and linguistic characteristics. While the\nimpact of pre-train data percentage and model size on performance is\nwell-known, our study reveals additional critical factors that significantly\ninfluence MLLM effectiveness. Analyzing a wide range of features, including\ngeographical, linguistic, and resource-related aspects, we focus on the SIB-200\ndataset for classification and the Flores-200 dataset for machine translation,\nusing regression models and SHAP values across 204 languages. Our findings\nidentify token similarity and country similarity as pivotal factors, alongside\npre-train data and model size, in enhancing model performance. Token similarity\nfacilitates cross-lingual transfer, while country similarity highlights the\nimportance of shared cultural and linguistic contexts. These insights offer\nvaluable guidance for developing more equitable and effective multilingual\nlanguage models, particularly for underrepresented languages.\n","authors":["Sina Bagheri Nezhad","Ameeta Agrawal","Rhitabrat Pokharel"],"pdf_url":"https://arxiv.org/pdf/2412.12500v1.pdf","comment":"Accepted at The First Workshop on Language Models for Low-Resource\n  Languages @ COLING 2025"},{"id":"http://arxiv.org/abs/2412.12499v1","updated":"2024-12-17T03:03:17Z","published":"2024-12-17T03:03:17Z","title":"LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for\n  Low-Resource Language Tasks","summary":"  Large language models (LLMs) have demonstrated impressive multilingual\nunderstanding and reasoning capabilities, driven by extensive pre-training\nmultilingual corpora and fine-tuning instruction data. However, a performance\ngap persists between high-resource and low-resource language tasks due to\nlanguage imbalance in the pre-training corpus, even using more low-resource\ndata during fine-tuning. To alleviate this issue, we propose LinguaLIFT, a\ntwo-stage instruction tuning framework for advancing low-resource language\ntasks. An additional language alignment layer is first integrated into the LLM\nto adapt a pre-trained multilingual encoder, thereby enhancing multilingual\nalignment through code-switched fine-tuning. The second stage fine-tunes LLM\nwith English-only instruction data while freezing the language alignment layer,\nallowing LLM to transfer task-specific capabilities from English to\nlow-resource language tasks. Additionally, we introduce the Multilingual Math\nWorld Problem (MMWP) benchmark, which spans 21 low-resource, 17\nmedium-resource, and 10 high-resource languages, enabling comprehensive\nevaluation of multilingual reasoning. Experimental results show that LinguaLIFT\noutperforms several competitive baselines across MMWP and other widely used\nbenchmarks.\n","authors":["Hongbin Zhang","Kehai Chen","Xuefeng Bai","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12497v1","updated":"2024-12-17T02:59:04Z","published":"2024-12-17T02:59:04Z","title":"NLSR: Neuron-Level Safety Realignment of Large Language Models Against\n  Harmful Fine-Tuning","summary":"  The emergence of finetuning-as-a-service has revealed a new vulnerability in\nlarge language models (LLMs). A mere handful of malicious data uploaded by\nusers can subtly manipulate the finetuning process, resulting in an\nalignment-broken model. Existing methods to counteract fine-tuning attacks\ntypically require substantial computational resources. Even with\nparameter-efficient techniques like LoRA, gradient updates remain essential. To\naddress these challenges, we propose \\textbf{N}euron-\\textbf{L}evel\n\\textbf{S}afety \\textbf{R}ealignment (\\textbf{NLSR}), a training-free framework\nthat restores the safety of LLMs based on the similarity difference of\nsafety-critical neurons before and after fine-tuning. The core of our framework\nis first to construct a safety reference model from an initially aligned model\nto amplify safety-related features in neurons. We then utilize this reference\nmodel to identify safety-critical neurons, which we prepare as patches.\nFinally, we selectively restore only those neurons that exhibit significant\nsimilarity differences by transplanting these prepared patches, thereby\nminimally altering the fine-tuned model. Extensive experiments demonstrate\nsignificant safety enhancements in fine-tuned models across multiple downstream\ntasks, while greatly maintaining task-level accuracy. Our findings suggest\nregions of some safety-critical neurons show noticeable differences after\nfine-tuning, which can be effectively corrected by transplanting neurons from\nthe reference model without requiring additional training. The code will be\navailable at \\url{https://github.com/xinykou/NLSR}\n","authors":["Xin Yi","Shunfan Zheng","Linlin Wang","Gerard de Melo","Xiaoling Wang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2412.12497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12486v1","updated":"2024-12-17T02:43:54Z","published":"2024-12-17T02:43:54Z","title":"Boosting Long-Context Information Seeking via Query-Guided Activation\n  Refilling","summary":"  Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.\n","authors":["Hongjin Qian","Zheng Liu","Peitian Zhang","Zhicheng Dou","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2412.12486v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2405.13820v2","updated":"2024-12-17T02:35:30Z","published":"2024-05-22T16:51:07Z","title":"Towards Comprehensive Post Safety Alignment of Large Language Models via\n  Safety Patching","summary":"  Safety alignment of large language models (LLMs) has been gaining increasing\nattention. However, current safety-aligned LLMs suffer from the fragile and\nimbalanced safety mechanisms, which can still be induced to generate unsafe\nresponses, exhibit over-safety by rejecting safe user inputs, and fail to\npreserve general utility after safety alignment. To this end, we propose a\nnovel post safety alignment (PSA) method to address these inherent and emerging\nsafety challenges, including safety enhancement, over-safety mitigation, and\nutility preservation. In specific, we introduce \\textsc{SafePatching}, a novel\nframework for comprehensive PSA, where two distinct safety patches are\ndeveloped on the harmful data to enhance safety and mitigate over-safety\nconcerns, and then seamlessly integrated into the target LLM backbone without\ncompromising its utility. Extensive experiments on four representative aligned\nLLMs, including LLaMA-2/3, Gemma and Mistral, show that \\textsc{SafePatching}\nachieves a more comprehensive PSA than baseline methods, further optimizing the\nbalance between being helpful and harmless in current aligned LLMs. Also,\n\\textsc{SafePatching} demonstrates its superiority in continual PSA scenarios.\n","authors":["Weixiang Zhao","Yulin Hu","Zhuojun Li","Yang Deng","Jiahe Guo","Xingyu Sui","Yanyan Zhao","Bing Qin","Tat-Seng Chua","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2405.13820v2.pdf","comment":"25 pages, 10 figures and 14 tables"},{"id":"http://arxiv.org/abs/2412.12478v1","updated":"2024-12-17T02:29:54Z","published":"2024-12-17T02:29:54Z","title":"Human-in-the-Loop Generation of Adversarial Texts: A Case Study on\n  Tibetan Script","summary":"  DNN-based language models perform excellently on various tasks, but even SOTA\nLLMs are susceptible to textual adversarial attacks. Adversarial texts play\ncrucial roles in multiple subfields of NLP. However, current research has the\nfollowing issues. (1) Most textual adversarial attack methods target\nrich-resourced languages. How do we generate adversarial texts for less-studied\nlanguages? (2) Most textual adversarial attack methods are prone to generating\ninvalid or ambiguous adversarial texts. How do we construct high-quality\nadversarial robustness benchmarks? (3) New language models may be immune to\npart of previously generated adversarial texts. How do we update adversarial\nrobustness benchmarks? To address the above issues, we introduce HITL-GAT, a\nsystem based on a general approach to human-in-the-loop generation of\nadversarial texts. HITL-GAT contains four stages in one pipeline: victim model\nconstruction, adversarial example generation, high-quality benchmark\nconstruction, and adversarial robustness evaluation. Additionally, we utilize\nHITL-GAT to make a case study on Tibetan script which can be a reference for\nthe adversarial research of other less-studied languages.\n","authors":["Xi Cao","Yuan Sun","Jiajun Li","Quzong Gesang","Nuo Qun","Tashi Nyima"],"pdf_url":"https://arxiv.org/pdf/2412.12478v1.pdf","comment":"Review Version; Submitted to NAACL 2025 Demo Track"},{"id":"http://arxiv.org/abs/2411.08534v2","updated":"2024-12-17T02:27:04Z","published":"2024-11-13T11:31:02Z","title":"Neural Topic Modeling with Large Language Models in the Loop","summary":"  Topic modeling is a fundamental task in natural language processing, allowing\nthe discovery of latent thematic structures in text corpora. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities in topic\ndiscovery, their direct application to topic modeling suffers from issues such\nas incomplete topic coverage, misalignment of topics, and inefficiency. To\naddress these limitations, we propose LLM-ITL, a novel LLM-in-the-loop\nframework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL,\nglobal topics and document representations are learned through the NTM.\nMeanwhile, an LLM refines these topics using an Optimal Transport (OT)-based\nalignment objective, where the refinement is dynamically adjusted based on the\nLLM's confidence in suggesting topical words for each set of input words. With\nthe flexibility of being integrated into many existing NTMs, the proposed\napproach enhances the interpretability of topics while preserving the\nefficiency of NTMs in learning topics and document representations. Extensive\nexperiments demonstrate that LLM-ITL helps NTMs significantly improve their\ntopic interpretability while maintaining the quality of document\nrepresentation. Our code and datasets will be available at Github.\n","authors":["Xiaohao Yang","He Zhao","Weijie Xu","Yuanyuan Qi","Jueqing Lu","Dinh Phung","Lan Du"],"pdf_url":"https://arxiv.org/pdf/2411.08534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12475v1","updated":"2024-12-17T02:22:24Z","published":"2024-12-17T02:22:24Z","title":"RareAgents: Autonomous Multi-disciplinary Team for Rare Disease\n  Diagnosis and Treatment","summary":"  Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the huge number of diseases. The\ncomplexity of symptoms and the shortage of specialized doctors with relevant\nexperience make diagnosing and treating rare diseases more challenging than\ncommon diseases. Recently, agents powered by large language models (LLMs) have\ndemonstrated notable improvements across various domains. In the medical field,\nsome agent methods have outperformed direct prompts in question-answering tasks\nfrom medical exams. However, current agent frameworks lack adaptation for\nreal-world clinical scenarios, especially those involving the intricate demands\nof rare diseases. To address these challenges, we present RareAgents, the first\nmulti-disciplinary team of LLM-based agents tailored to the complex clinical\ncontext of rare diseases. RareAgents integrates advanced planning capabilities,\nmemory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B\nas the base model. Experimental results show that RareAgents surpasses\nstate-of-the-art domain-specific models, GPT-4o, and existing agent frameworks\nin both differential diagnosis and medication recommendation for rare diseases.\nFurthermore, we contribute a novel dataset, MIMIC-IV-Ext-Rare, derived from\nMIMIC-IV, to support further advancements in this field.\n","authors":["Xuanzhong Chen","Ye Jin","Xiaohao Mao","Lun Wang","Shuyang Zhang","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12472v1","updated":"2024-12-17T02:14:02Z","published":"2024-12-17T02:14:02Z","title":"Knowledge Boundary of Large Language Models: A Survey","summary":"  Although large language models (LLMs) store vast amount of knowledge in their\nparameters, they still have limitations in the memorization and utilization of\ncertain knowledge, leading to undesired behaviors such as generating untruthful\nand inaccurate responses. This highlights the critical need to understand the\nknowledge boundary of LLMs, a concept that remains inadequately defined in\nexisting research. In this survey, we propose a comprehensive definition of the\nLLM knowledge boundary and introduce a formalized taxonomy categorizing\nknowledge into four distinct types. Using this foundation, we systematically\nreview the field through three key lenses: the motivation for studying LLM\nknowledge boundaries, methods for identifying these boundaries, and strategies\nfor mitigating the challenges they present. Finally, we discuss open challenges\nand potential research directions in this area. We aim for this survey to offer\nthe community a comprehensive overview, facilitate access to key issues, and\ninspire further advancements in LLM knowledge research.\n","authors":["Moxin Li","Yong Zhao","Yang Deng","Wenxuan Zhang","Shuaiyi Li","Wenya Xie","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2412.12472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10994v3","updated":"2024-12-17T02:05:27Z","published":"2024-09-17T08:56:27Z","title":"Less is More: A Simple yet Effective Token Reduction Method for\n  Efficient Multi-modal LLMs","summary":"  The rapid advancement of Multimodal Large Language Models (MLLMs) has led to\nremarkable performances across various domains. However, this progress is\naccompanied by a substantial surge in the resource consumption of these models.\nWe address this pressing issue by introducing a new approach, Token Reduction\nusing CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without\nsacrificing their performance. Inspired by human attention patterns in Visual\nQuestion Answering (VQA) tasks, TRIM presents a fresh perspective on the\nselection and reduction of image tokens. The TRIM method has been extensively\ntested across 12 datasets, and the results demonstrate a significant reduction\nin computational overhead while maintaining a consistent level of performance.\nThis research marks a critical stride in efficient MLLM development, promoting\ngreater accessibility and sustainability of high-performing models.\n","authors":["Dingjie Song","Wenjun Wang","Shunian Chen","Xidong Wang","Michael Guan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.10994v3.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.12465v1","updated":"2024-12-17T01:54:08Z","published":"2024-12-17T01:54:08Z","title":"Core Context Aware Attention for Long Context Language Modeling","summary":"  Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in various natural language processing tasks primarily attributed to\nself-attention mechanism, which requires a token to consider all preceding\ntokens as its context to compute the attention score. However, when the context\nlength L becomes very large (e.g., 32K), more redundant context information\nwill be included w.r.t. any tokens, making the self-attention suffer from two\nmain limitations: 1) The computational and memory complexity scales\nquadratically w.r.t. L; 2) The presence of redundant context information may\nhamper the model to capture dependencies among crucial tokens, which may\ndegrade the representation performance. In this paper, we propose a\nplug-and-play Core Context Aware (CCA) Attention for efficient long-range\ncontext modeling, which consists of two components: 1) Globality-pooling\nattention that divides input tokens into groups and then dynamically merges\ntokens within each group into one core token based on their significance; 2)\nLocality-preserved attention that incorporates neighboring tokens into the\nattention calculation. The two complementary attentions will then be fused to\nthe final attention, maintaining comprehensive modeling ability as the full\nself-attention. In this way, the core context information w.r.t. a given token\nwill be automatically focused and strengthened, while the context information\nin redundant groups will be diminished during the learning process. As a\nresult, the computational and memory complexity will be significantly reduced.\nMore importantly, the CCA-Attention can improve the long-context modeling\nability by diminishing the redundant context information. Extensive\nexperimental results demonstrate that our CCA-Attention significantly\noutperforms state-of-the-art models in terms of computational efficiency and\nlong-context modeling ability.\n","authors":["Yaofo Chen","Zeng You","Shuhai Zhang","Haokun Li","Yirui Li","Yaowei Wang","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2412.12465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12459v1","updated":"2024-12-17T01:43:44Z","published":"2024-12-17T01:43:44Z","title":"LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework","summary":"  Topic modeling is widely used for uncovering thematic structures within text\ncorpora, yet traditional models often struggle with specificity and coherence\nin domain-focused applications. Guided approaches, such as SeededLDA and CorEx,\nincorporate user-provided seed words to improve relevance but remain\nlabor-intensive and static. Large language models (LLMs) offer potential for\ndynamic topic refinement and discovery, yet their application often incurs high\nAPI costs. To address these challenges, we propose the LLM-assisted Iterative\nTopic Augmentation framework (LITA), an LLM-assisted approach that integrates\nuser-provided seeds with embedding-based clustering and iterative refinement.\nLITA identifies a small number of ambiguous documents and employs an LLM to\nreassign them to existing or new topics, minimizing API costs while enhancing\ntopic quality. Experiments on two datasets across topic quality and clustering\nperformance metrics demonstrate that LITA outperforms five baseline models,\nincluding LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an\nefficient and adaptable framework for advancing topic modeling and text\nclustering.\n","authors":["Chia-Hsuan Chang","Jui-Tse Tsai","Yi-Hang Tsai","San-Yih Hwang"],"pdf_url":"https://arxiv.org/pdf/2412.12459v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.12456v1","updated":"2024-12-17T01:41:17Z","published":"2024-12-17T01:41:17Z","title":"Graph Learning in the Era of LLMs: A Survey from the Perspective of\n  Data, Models, and Tasks","summary":"  With the increasing prevalence of cross-domain Text-Attributed Graph (TAG)\nData (e.g., citation networks, recommendation systems, social networks, and\nai4science), the integration of Graph Neural Networks (GNNs) and Large Language\nModels (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as\ncollaborators, LLM as predictor) has emerged as a promising technological\nparadigm. The core of this new graph learning paradigm lies in the synergistic\ncombination of GNNs' ability to capture complex structural relationships and\nLLMs' proficiency in understanding informative contexts from the rich textual\ndescriptions of graphs. Therefore, we can leverage graph description texts with\nrich semantic context to fundamentally enhance Data quality, thereby improving\nthe representational capacity of model-centric approaches in line with\ndata-centric machine learning principles. By leveraging the strengths of these\ndistinct neural network architectures, this integrated approach addresses a\nwide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph\nquestion answering), particularly in complex industrial scenarios (e.g.,\nsupervised, few-shot, and zero-shot settings). In other words, we can treat\ntext as a medium to enable cross-domain generalization of graph learning Model,\nallowing a single graph model to effectively handle the diversity of downstream\ngraph-based Task across different data domains. This work serves as a\nfoundational reference for researchers and practitioners looking to advance\ngraph learning methodologies in the rapidly evolving landscape of LLM. We\nconsistently maintain the related open-source materials at\n\\url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.\n","authors":["Xunkai Li","Zhengyu Wu","Jiayi Wu","Hanwen Cui","Jishuo Jia","Rong-Hua Li","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12456v1.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2412.12447v1","updated":"2024-12-17T01:23:45Z","published":"2024-12-17T01:23:45Z","title":"PERC: Plan-As-Query Example Retrieval for Underrepresented Code\n  Generation","summary":"  Code generation with large language models has shown significant promise,\nespecially when employing retrieval-augmented generation (RAG) with few-shot\nexamples. However, selecting effective examples that enhance generation quality\nremains a challenging task, particularly when the target programming language\n(PL) is underrepresented. In this study, we present two key findings: (1)\nretrieving examples whose presented algorithmic plans can be referenced for\ngenerating the desired behavior significantly improves generation accuracy, and\n(2) converting code into pseudocode effectively captures such algorithmic\nplans, enhancing retrieval quality even when the source and the target PLs are\ndifferent. Based on these findings, we propose Plan-as-query Example Retrieval\nfor few-shot prompting in Code generation (PERC), a novel framework that\nutilizes algorithmic plans to identify and retrieve effective examples. We\nvalidate the effectiveness of PERC through extensive experiments on the\nCodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms\nthe state-of-the-art RAG methods in code generation, both when the source and\ntarget programming languages match or differ, highlighting its adaptability and\nrobustness in diverse coding environments.\n","authors":["Jaeseok Yoo","Hojae Han","Youngwon Lee","Jaejin Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2412.12447v1.pdf","comment":"Accepted by COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2412.12445v1","updated":"2024-12-17T01:15:40Z","published":"2024-12-17T01:15:40Z","title":"Persona-SQ: A Personalized Suggested Question Generation Framework For\n  Real-world Documents","summary":"  Suggested questions (SQs) provide an effective initial interface for users to\nengage with their documents in AI-powered reading applications. In practical\nreading sessions, users have diverse backgrounds and reading goals, yet current\nSQ features typically ignore such user information, resulting in homogeneous or\nineffective questions. We introduce a pipeline that generates personalized SQs\nby incorporating reader profiles (professions and reading goals) and\ndemonstrate its utility in two ways: 1) as an improved SQ generation pipeline\nthat produces higher quality and more diverse questions compared to current\nbaselines, and 2) as a data generator to fine-tune extremely small models that\nperform competitively with much larger models on SQ generation. Our approach\ncan not only serve as a drop-in replacement in current SQ systems to\nimmediately improve their performance but also help develop on-device SQ models\nthat can run locally to deliver fast and private SQ experience.\n","authors":["Zihao Lin","Zichao Wang","Yuanting Pan","Varun Manjunatha","Ryan Rossi","Angela Lau","Lifu Huang","Tong Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12445v1.pdf","comment":"38 pages, 26 figures"},{"id":"http://arxiv.org/abs/2412.09900v2","updated":"2024-12-17T00:59:26Z","published":"2024-12-13T06:35:55Z","title":"Analyzing Fairness of Computer Vision and Natural Language Processing\n  Models","summary":"  Machine learning (ML) algorithms play a crucial role in decision making\nacross diverse fields such as healthcare, finance, education, and law\nenforcement. Despite their widespread adoption, these systems raise ethical and\nsocial concerns due to potential biases and fairness issues. This study focuses\non evaluating and improving the fairness of Computer Vision and Natural\nLanguage Processing (NLP) models applied to unstructured datasets, emphasizing\nhow biased predictions can reinforce existing systemic inequalities. A publicly\navailable dataset from Kaggle was utilized to simulate a practical scenario for\nexamining fairness in ML workflows. To address and mitigate biases, the study\nemployed two leading fairness libraries: Fairlearn by Microsoft, and AIF360 by\nIBM. These tools offer comprehensive frameworks for fairness analysis,\nincluding metrics evaluation, result visualization, and bias mitigation\ntechniques. The research aims to measure bias levels in ML models, compare the\neffectiveness of these fairness libraries, and provide actionable\nrecommendations for practitioners. The results demonstrate that each library\npossesses distinct strengths and limitations in evaluating and mitigating\nfairness. By systematically analyzing these tools, the study contributes\nvaluable insights to the growing field of ML fairness, offering practical\nguidance for integrating fairness solutions into real world applications. This\nresearch underscores the importance of building more equitable and responsible\nmachine learning systems.\n","authors":["Ahmed Rashed","Abdelkrim Kallich","Mohamed Eltayeb"],"pdf_url":"https://arxiv.org/pdf/2412.09900v2.pdf","comment":"16 pages, 1 table, 4 figures"},{"id":"http://arxiv.org/abs/2412.12433v1","updated":"2024-12-17T00:50:23Z","published":"2024-12-17T00:50:23Z","title":"Refining Dimensions for Improving Clustering-based Cross-lingual Topic\n  Models","summary":"  Recent works in clustering-based topic models perform well in monolingual\ntopic identification by introducing a pipeline to cluster the contextualized\nrepresentations. However, the pipeline is suboptimal in identifying topics\nacross languages due to the presence of language-dependent dimensions (LDDs)\ngenerated by multilingual language models. To address this issue, we introduce\na novel, SVD-based dimension refinement component into the pipeline of the\nclustering-based topic model. This component effectively neutralizes the\nnegative impact of LDDs, enabling the model to accurately identify topics\nacross languages. Our experiments on three datasets demonstrate that the\nupdated pipeline with the dimension refinement component generally outperforms\nother state-of-the-art cross-lingual topic models.\n","authors":["Chia-Hsuan Chang","Tien-Yuan Huang","Yi-Hang Tsai","Chia-Ming Chang","San-Yih Hwang"],"pdf_url":"https://arxiv.org/pdf/2412.12433v1.pdf","comment":"Accepted to 18th BUCC Workshop at COLING 2025"},{"id":"http://arxiv.org/abs/2409.12468v2","updated":"2024-12-17T00:25:46Z","published":"2024-09-19T05:14:55Z","title":"Familiarity-Aware Evidence Compression for Retrieval-Augmented\n  Generation","summary":"  Retrieval-augmented generation (RAG) improves large language models (LMs) by\nincorporating non-parametric knowledge through evidence retrieved from external\nsources. However, it often struggles to cope with inconsistent and irrelevant\ninformation that can distract the LM from its tasks, especially when multiple\nevidence pieces are required. While compressing the retrieved evidence with a\ncompression model aims to address this issue, the compressed evidence may still\nbe unfamiliar to the target model used for downstream tasks, potentially\nfailing to utilize the evidence effectively. We propose FaviComp\n(Familarity-Aware Evidence Compression), a novel training-free evidence\ncompression technique that makes retrieved evidence more familiar to the target\nmodel, while seamlessly integrating parametric knowledge from the model.\nExperimental results show that FaviComp consistently outperforms most recent\nevidence compression baselines across multiple open-domain QA datasets,\nimproving accuracy by up to 28.1% while achieving high compression rates.\nAdditionally, we demonstrate the effective integration of both parametric and\nnon-parametric knowledge during evidence compression.\n","authors":["Dongwon Jung","Qin Liu","Tenghao Huang","Ben Zhou","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2409.12468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12422v1","updated":"2024-12-17T00:07:05Z","published":"2024-12-17T00:07:05Z","title":"Assessing the Limitations of Large Language Models in Clinical Fact\n  Decomposition","summary":"  Verifying factual claims is critical for using large language models (LLMs)\nin healthcare. Recent work has proposed fact decomposition, which uses LLMs to\nrewrite source text into concise sentences conveying a single piece of\ninformation, as an approach for fine-grained fact verification. Clinical\ndocumentation poses unique challenges for fact decomposition due to dense\nterminology and diverse note types. To explore these challenges, we present\nFactEHR, a dataset consisting of full document fact decompositions for 2,168\nclinical notes spanning four types from three hospital systems. Our evaluation,\nincluding review by clinicians, highlights significant variability in the\nquality of fact decomposition for four commonly used LLMs, with some LLMs\ngenerating 2.6x more facts per sentence than others. The results underscore the\nneed for better LLM capabilities to support factual verification in clinical\ntext. To facilitate future research in this direction, we plan to release our\ncode at \\url{https://github.com/som-shahlab/factehr}.\n","authors":["Monica Munnangi","Akshay Swaminathan","Jason Alan Fries","Jenelle Jindal","Sanjana Narayanan","Ivan Lopez","Lucia Tu","Philip Chung","Jesutofunmi A. Omiye","Mehr Kashyap","Nigam Shah"],"pdf_url":"https://arxiv.org/pdf/2412.12422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13381v1","updated":"2024-12-17T23:29:18Z","published":"2024-12-17T23:29:18Z","title":"An Automated Explainable Educational Assessment System Built on LLMs","summary":"  In this demo, we present AERA Chat, an automated and explainable educational\nassessment system designed for interactive and visual evaluations of student\nresponses. This system leverages large language models (LLMs) to generate\nautomated marking and rationale explanations, addressing the challenge of\nlimited explainability in automated educational assessment and the high costs\nassociated with annotation. Our system allows users to input questions and\nstudent answers, providing educators and researchers with insights into\nassessment accuracy and the quality of LLM-assessed rationales. Additionally,\nit offers advanced visualization and robust evaluation tools, enhancing the\nusability for educational assessment and facilitating efficient rationale\nverification. Our demo video can be found at https://youtu.be/qUSjz-sxlBc.\n","authors":["Jiazheng Li","Artem Bobrov","David West","Cesare Aloisi","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2412.13381v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13378v1","updated":"2024-12-17T23:26:44Z","published":"2024-12-17T23:26:44Z","title":"SummExecEdit: A Factual Consistency Benchmark in Summarization with\n  Executable Edits","summary":"  Detecting factual inconsistencies in summarization is critical, yet existing\nbenchmarks lack the necessary challenge and interpretability for robust\nevaluation. In this paper, we introduce SummExecEdit, a novel benchmark\nleveraging executable edits to assess models on their ability to both detect\nfactual errors and provide accurate explanations. The top-performing model,\nClaude3-Opus, achieves a joint detection and explanation score of only 0.49 in\nour benchmark, with individual scores of 0.67 for detection and 0.73 for\nexplanation. Furthermore, we identify four primary types of explanation errors,\nwith 45.4% of errors focusing on completely unrelated parts of the summary.\n","authors":["Onkar Thorat","Philippe Laban","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2412.13378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13377v1","updated":"2024-12-17T23:25:47Z","published":"2024-12-17T23:25:47Z","title":"DateLogicQA: Benchmarking Temporal Biases in Large Language Models","summary":"  This paper introduces DateLogicQA, a benchmark with 190 questions covering\ndiverse date formats, temporal contexts, and reasoning types. We propose the\nSemantic Integrity Metric to assess tokenization quality and analyse two\nbiases: Representation-Level Bias, affecting embeddings, and Logical-Level\nBias, influencing reasoning outputs. Our findings provide a comprehensive\nevaluation of LLMs' capabilities and limitations in temporal reasoning,\nhighlighting key challenges in handling temporal data accurately. The GitHub\nrepository for our work is available at\nhttps://github.com/gagan3012/EAIS-Temporal-Bias\n","authors":["Gagan Bhatia","MingZe Tang","Cristina Mahanta","Madiha Kazi"],"pdf_url":"https://arxiv.org/pdf/2412.13377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13375v1","updated":"2024-12-17T23:18:06Z","published":"2024-12-17T23:18:06Z","title":"Extending LLMs to New Languages: A Case Study of Llama and Persian\n  Adaptation","summary":"  Large language models (LLMs) have made great progress in classification and\ntext generation tasks. However, they are mainly trained on English data and\noften struggle with low-resource languages. In this study, we explore adding a\nnew language, i.e., Persian, to Llama (a model with a limited understanding of\nPersian) using parameter-efficient fine-tuning. We employ a multi-stage\napproach involving pretraining on monolingual Persian data, aligning\nrepresentations through bilingual pretraining and instruction datasets, and\ninstruction-tuning with task-specific datasets. We evaluate the model's\nperformance at each stage on generation and classification tasks. Our findings\nsuggest that incorporating the Persian language, through bilingual data\nalignment, can enhance classification accuracy for Persian tasks, with no\nadverse impact and sometimes even improvements on English tasks. Additionally,\nthe results highlight the model's initial strength as a critical factor when\nworking with limited training data, with cross-lingual alignment offering\nminimal benefits for the low-resource language. Knowledge transfer from English\nto Persian has a marginal effect, primarily benefiting simple classification\ntasks.\n","authors":["Samin Mahdizadeh Sani","Pouya Sadeghi","Thuy-Trang Vu","Yadollah Yaghoobzadeh","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2412.13375v1.pdf","comment":"accepted at COLING 2025"},{"id":"http://arxiv.org/abs/2410.23022v2","updated":"2024-12-17T22:29:46Z","published":"2024-10-30T13:52:43Z","title":"Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback","summary":"  Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples, due to requiring LLM annotations for each observation, or\nthey require a diverse offline dataset, which may not exist or be impossible to\ncollect. In this work, we address these limitations through a combination of\nalgorithmic and systems-level contributions. We propose \\oni, a distributed\narchitecture that simultaneously learns an RL policy and an intrinsic reward\nfunction using LLM feedback. Our approach annotates the agent's collected\nexperience via an asynchronous LLM server, which is then distilled into an\nintrinsic reward model. We explore a range of algorithmic choices for reward\nmodeling with varying complexity, including hashing, classification, and\nranking models. By studying their relative tradeoffs, we shed light on\nquestions regarding intrinsic reward design for sparse reward problems. Our\napproach achieves state-of-the-art performance across a range of challenging,\nsparse reward tasks from the NetHack Learning Environment in a simple unified\nprocess, solely using the agent's gathered experience, without requiring\nexternal datasets. We make our code available at\n\\url{https://github.com/facebookresearch/oni}.\n","authors":["Qinqing Zheng","Mikael Henaff","Amy Zhang","Aditya Grover","Brandon Amos"],"pdf_url":"https://arxiv.org/pdf/2410.23022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14881v2","updated":"2024-12-17T22:07:18Z","published":"2024-10-18T22:07:36Z","title":"Class-RAG: Real-Time Content Moderation with Retrieval Augmented\n  Generation","summary":"  Robust content moderation classifiers are essential for the safety of\nGenerative AI systems. In this task, differences between safe and unsafe inputs\nare often extremely subtle, making it difficult for classifiers (and indeed,\neven humans) to properly distinguish violating vs. benign samples without\ncontext or explanation. Scaling risk discovery and mitigation through\ncontinuous model fine-tuning is also slow, challenging and costly, preventing\ndevelopers from being able to respond quickly and effectively to emergent\nharms. We propose a Classification approach employing Retrieval-Augmented\nGeneration (Class-RAG). Class-RAG extends the capability of its base LLM\nthrough access to a retrieval library which can be dynamically updated to\nenable semantic hotfixing for immediate, flexible risk mitigation. Compared to\nmodel fine-tuning, Class-RAG demonstrates flexibility and transparency in\ndecision-making, outperforms on classification and is more robust against\nadversarial attack, as evidenced by empirical studies. Our findings also\nsuggest that Class-RAG performance scales with retrieval library size,\nindicating that increasing the library size is a viable and low-cost approach\nto improve content moderation.\n","authors":["Jianfa Chen","Emily Shen","Trupti Bavalatti","Xiaowen Lin","Yongkai Wang","Shuming Hu","Harihar Subramanyam","Ksheeraj Sai Vepuri","Ming Jiang","Ji Qi","Li Chen","Nan Jiang","Ankit Jain"],"pdf_url":"https://arxiv.org/pdf/2410.14881v2.pdf","comment":"11 pages, submit to ACL"},{"id":"http://arxiv.org/abs/2310.00074v3","updated":"2024-12-17T21:56:45Z","published":"2023-09-29T18:25:46Z","title":"SocREval: Large Language Models with the Socratic Method for\n  Reference-Free Reasoning Evaluation","summary":"  To comprehensively gauge the capacity of current models for complex\nreasoning, it is crucial to assess their step-by-step reasoning in a scalable\nmanner. Established reference-based evaluation metrics rely on human-annotated\nreasoning chains as references to assess the model-derived chains. However,\nsuch \"gold-standard\" human-written reasoning chains may not be unique and their\nacquisition is often labor-intensive. Existing reference-free reasoning\nevaluation metrics, while eliminating the need for human-crafted reasoning\nchains as references, often require fine-tuning with human-derived chains\nbefore evaluation, complicating the process and questioning their adaptability\nto other datasets. To address these challenges, we harness GPT-4 to\nautomatically evaluate reasoning chain quality, thereby removing the dependency\non human-written reasoning chains for both model fine-tuning and evaluative\npurposes. Leveraging the Socratic method, we develop SocREval ({\\bf Soc}ratic\nMethod-Inspired {\\bf R}easoning {\\bf Eval}uation), a novel approach for prompt\ndesign in reference-free reasoning evaluation. Empirical results from four\nhuman annotated datasets reveal that SocREval significantly improves GPT-4's\nperformance, surpassing existing reference-free and reference-based reasoning\nevaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be\nboth cost-efficient and robust to prompt writing and example selection, as\nsubstantiated by our in-depth analysis.\n","authors":["Hangfeng He","Hongming Zhang","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2310.00074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13335v1","updated":"2024-12-17T21:15:52Z","published":"2024-12-17T21:15:52Z","title":"Experience of Training a 1.7B-Parameter LLaMa Model From Scratch","summary":"  Pretraining large language models is a complex endeavor influenced by\nmultiple factors, including model architecture, data quality, training\ncontinuity, and hardware constraints. In this paper, we share insights gained\nfrom the experience of training DMaS-LLaMa-Lite, a fully open source,\n1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of\ncarefully curated data. We chronicle the full training trajectory, documenting\nhow evolving validation loss levels and downstream benchmarks reflect\ntransitions from incoherent text to fluent, contextually grounded output.\nBeyond standard quantitative metrics, we highlight practical considerations\nsuch as the importance of restoring optimizer states when resuming from\ncheckpoints, and the impact of hardware changes on training stability and\nthroughput. While qualitative evaluation provides an intuitive understanding of\nmodel improvements, our analysis extends to various performance benchmarks,\ndemonstrating how high-quality data and thoughtful scaling enable competitive\nresults with significantly fewer training tokens. By detailing these\nexperiences and offering training logs, checkpoints, and sample outputs, we aim\nto guide future researchers and practitioners in refining their pretraining\nstrategies. The training script is available on Github at\nhttps://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code. The model\ncheckpoints are available on Huggingface at\nhttps://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.\n","authors":["Miles Q. Li","Benjamin C. M. Fung","Shih-Chia Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11404v3","updated":"2024-12-17T21:15:26Z","published":"2024-09-17T17:59:25Z","title":"AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs","summary":"  Arabic, with its rich diversity of dialects, remains significantly\nunderrepresented in Large Language Models, particularly in dialectal\nvariations. We address this gap by introducing seven synthetic datasets in\ndialects alongside Modern Standard Arabic (MSA), created using Machine\nTranslation (MT) combined with human post-editing. We present AraDiCE, a\nbenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on\ndialect comprehension and generation, focusing specifically on low-resource\nArabic dialects. Additionally, we introduce the first-ever fine-grained\nbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, and\nLevant regions, providing a novel dimension to LLM evaluation. Our findings\ndemonstrate that while Arabic-specific models like Jais and AceGPT outperform\nmultilingual models on dialectal tasks, significant challenges persist in\ndialect identification, generation, and translation. This work contributes\n$\\approx$45K post-edited samples, a cultural benchmark, and highlights the\nimportance of tailored training to improve LLM performance in capturing the\nnuances of diverse Arabic dialects and cultural contexts. We have released the\ndialectal translation models and benchmarks developed in this study\n(https://huggingface.co/datasets/QCRI/AraDiCE).\n","authors":["Basel Mousi","Nadir Durrani","Fatema Ahmad","Md. Arid Hasan","Maram Hasanain","Tameem Kabbani","Fahim Dalvi","Shammur Absar Chowdhury","Firoj Alam"],"pdf_url":"https://arxiv.org/pdf/2409.11404v3.pdf","comment":"Benchmarking, Culturally Informed, Large Language Models, Arabic NLP,\n  LLMs, Arabic Dialect, Dialectal Benchmarking"},{"id":"http://arxiv.org/abs/2401.15713v3","updated":"2024-12-17T20:58:26Z","published":"2024-01-28T17:34:42Z","title":"Contrastive Learning and Mixture of Experts Enables Precise Vector\n  Embeddings","summary":"  The advancement of transformer neural networks has significantly elevated the\ncapabilities of sentence similarity models, but they still struggle with highly\ndiscriminative tasks and may produce sub-optimal representations of important\ndocuments like scientific literature. With the increased reliance on retrieval\naugmentation and search, representing diverse documents as concise and\ndescriptive vectors is crucial. This paper improves upon the vectors embeddings\nof scientific text by assembling niche datasets using co-citations as a\nsimilarity metric, focusing on biomedical domains. We apply a novel Mixture of\nExperts (MoE) extension pipeline to pretrained BERT models, where every\nmulti-layer perceptron section is enlarged and copied into multiple distinct\nexperts. Our MoE variants perform well over $N$ scientific domains with $N$\ndedicated experts, whereas standard BERT models excel in only one domain at a\ntime. Notably, extending just a single transformer block to MoE captures 85% of\nthe benefit seen from full MoE extension at every layer. This holds promise for\nversatile and efficient One-Size-Fits-All transformer networks for numerically\nrepresenting diverse inputs. Our methodology marks advancements in\nrepresentation learning and holds promise for enhancing vector database search\nand compilation.\n","authors":["Logan Hallee","Rohan Kapur","Arjun Patel","Jason P. Gleghorn","Bohdan Khomtchouk"],"pdf_url":"https://arxiv.org/pdf/2401.15713v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13328v1","updated":"2024-12-17T20:55:42Z","published":"2024-12-17T20:55:42Z","title":"Expansion Span: Combining Fading Memory and Retrieval in Hybrid State\n  Space Models","summary":"  The \"state\" of State Space Models (SSMs) represents their memory, which fades\nexponentially over an unbounded span. By contrast, Attention-based models have\n\"eidetic\" (i.e., verbatim, or photographic) memory over a finite span (context\nsize). Hybrid architectures combine State Space layers with Attention, but\nstill cannot recall the distant past and can access only the most recent tokens\neidetically. Unlike current methods of combining SSM and Attention layers, we\nallow the state to be allocated based on relevancy rather than recency. In this\nway, for every new set of query tokens, our models can \"eidetically\" access\ntokens from beyond the Attention span of current Hybrid SSMs without requiring\nextra hardware resources. We describe a method to expand the memory span of the\nhybrid state by \"reserving\" a fraction of the Attention context for tokens\nretrieved from arbitrarily distant in the past, thus expanding the eidetic\nmemory span of the overall state. We call this reserved fraction of tokens the\n\"expansion span,\" and the mechanism to retrieve and aggregate it \"Span-Expanded\nAttention\" (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose a\nnovel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows\nefficient adaptation on long spans of tokens. We show that SE-Attn enables us\nto efficiently adapt pre-trained Hybrid models on sequences of tokens up to 8\ntimes longer than the ones used for pre-training. We show that HyLoRA with\nSE-Attn is cheaper and more performant than alternatives like LongLoRA when\napplied to Hybrid models on natural language benchmarks with long-range\ndependencies, such as PG-19, RULER, and other common natural language\ndownstream tasks.\n","authors":["Elvis Nunez","Luca Zancato","Benjamin Bowman","Aditya Golatkar","Wei Xia","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2412.13328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15231v2","updated":"2024-12-17T20:50:40Z","published":"2024-06-21T15:19:21Z","title":"Synthetic Lyrics Detection Across Languages and Genres","summary":"  In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the modality of lyrics in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We conducted a comprehensive\nevaluation of existing synthetic text detection features on this novel data\ntype. Additionally, we explored strategies to adjust the best feature for\nlyrics using unsupervised adaptation. Adhering to constraints of our\napplication domain, we investigated cross-lingual generalization, data\nscalability, robustness to language combinations, and the impact of genre\nnovelty in a few-shot detection scenario. Our findings show promising results\nwithin language families and similar genres, yet challenges persist with lyrics\nin languages that exhibit distinct semantic structures.\n","authors":["Yanis Labrak","Markus Frohmann","Gabriel Meseguer-Brocal","Elena V. Epure"],"pdf_url":"https://arxiv.org/pdf/2406.15231v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2412.12094v2","updated":"2024-12-17T20:41:59Z","published":"2024-12-16T18:58:57Z","title":"SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator","summary":"  Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.\n","authors":["Guoxuan Chen","Han Shi","Jiawei Li","Yihang Gao","Xiaozhe Ren","Yimeng Chen","Xin Jiang","Zhenguo Li","Weiyang Liu","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12094v2.pdf","comment":"We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!"},{"id":"http://arxiv.org/abs/2412.07030v2","updated":"2024-12-17T20:38:21Z","published":"2024-12-09T22:35:44Z","title":"FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge\n  Distillation for Question Answering","summary":"  Multimodal multihop question answering is a complex task that requires\nreasoning over multiple sources of information, such as images and text, to\nanswer questions. While there has been significant progress in visual question\nanswering, the multihop setting remains unexplored due to the lack of\nhigh-quality datasets. Current methods focus on single-hop question answering\nor a single modality, which makes them unsuitable for real-world scenarios such\nas analyzing multimodal educational materials, summarizing lengthy academic\narticles, or interpreting scientific studies that combine charts, images, and\ntext. To address this gap, we propose a novel methodology, introducing the\nfirst framework for creating a high-quality dataset that enables training\nmodels for multimodal multihop question answering. Our approach consists of a\n5-stage pipeline that involves acquiring relevant multimodal documents from\nWikipedia, synthetically generating high-level questions and answers, and\nvalidating them through rigorous criteria to ensure quality data. We evaluate\nour methodology by training models on our synthesized dataset and testing on\ntwo benchmarks, our results demonstrate that, with an equal sample size, models\ntrained on our synthesized data outperform those trained on human-collected\ndata by 1.9 in exact match (EM) on average. We believe our data synthesis\nmethod will serve as a strong foundation for training and evaluating multimodal\nmultihop question answering models.\n","authors":["Amirhossein Abaskohi","Spandana Gella","Giuseppe Carenini","Issam H. Laradji"],"pdf_url":"https://arxiv.org/pdf/2412.07030v2.pdf","comment":"20 pages, 11 figures, 10 tables, Submitted to CVPR 2025"},{"id":"http://arxiv.org/abs/2412.13292v1","updated":"2024-12-17T19:45:53Z","published":"2024-12-17T19:45:53Z","title":"Hint Marginalization for Improved Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have exhibited an impressive capability to\nperform reasoning tasks, especially if they are encouraged to generate a\nsequence of intermediate steps. Reasoning performance can be improved by\nsuitably combining multiple LLM responses, generated either in parallel in a\nsingle query, or via sequential interactions with LLMs throughout the reasoning\nprocess. Existing strategies for combination, such as self-consistency and\nprogressive-hint-prompting, make inefficient usage of the LLM responses. We\npresent Hint Marginalization, a novel and principled algorithmic framework to\nenhance the reasoning capabilities of LLMs. Our approach can be viewed as an\niterative sampling strategy for forming a Monte Carlo approximation of an\nunderlying distribution of answers, with the goal of identifying the mode the\nmost likely answer. Empirical evaluation on several benchmark datasets for\narithmetic reasoning demonstrates the superiority of the proposed approach.\n","authors":["Soumyasundar Pal","Didier Chételat","Yingxue Zhang","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2412.13292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13283v1","updated":"2024-12-17T19:27:24Z","published":"2024-12-17T19:27:24Z","title":"Enhancing Persona Classification in Dialogue Systems: A Graph Neural\n  Network Approach","summary":"  In recent years, Large Language Models (LLMs) gain considerable attention for\ntheir potential to enhance personalized experiences in virtual assistants and\nchatbots. A key area of interest is the integration of personas into LLMs to\nimprove dialogue naturalness and user engagement. This study addresses the\nchallenge of persona classification, a crucial component in dialogue\nunderstanding, by proposing a framework that combines text embeddings with\nGraph Neural Networks (GNNs) for effective persona classification. Given the\nabsence of dedicated persona classification datasets, we create a manually\nannotated dataset to facilitate model training and evaluation. Our method\ninvolves extracting semantic features from persona statements using text\nembeddings and constructing a graph where nodes represent personas and edges\ncapture their similarities. The GNN component uses this graph structure to\npropagate relevant information, thereby improving classification performance.\nExperimental results show that our approach, in particular the integration of\nGNNs, significantly improves classification performance, especially with\nlimited data. Our contributions include the development of a persona\nclassification framework and the creation of a dataset.\n","authors":["Konstantin Zaitsev"],"pdf_url":"https://arxiv.org/pdf/2412.13283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13243v1","updated":"2024-12-17T18:49:21Z","published":"2024-12-17T18:49:21Z","title":"In-Context Learning Distillation for Efficient Few-Shot Fine-Tuning","summary":"  We applied few-shot in-context learning on the OPT-1.3B model for the natural\nlanguage inference task and employed knowledge distillation to internalize the\ncontext information, reducing model parameter from 1.3B to 125M and achieving a\nsize reduction from 2.5GB to 0.25GB. Compared to using in-context learning\nalone on similarly sized models, this context distillation approach achieved a\nnearly 50% improvement in out-of-domain accuracy, demonstrating superior\nknowledge transfer capabilities over prompt-based methods. Furthermore, this\napproach reduced memory consumption by up to 60% while delivering a 20%\nimprovement in out-of-domain accuracy compared to conventional pattern-based\nfine-tuning.\n","authors":["Yifei Duan","Liu Li","Zirui Zhai","Jinxia Yao"],"pdf_url":"https://arxiv.org/pdf/2412.13243v1.pdf","comment":"7 pages, 6 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.13194v1","updated":"2024-12-17T18:59:50Z","published":"2024-12-17T18:59:50Z","title":"Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation\n  Model Internet Agents","summary":"  The vision of a broadly capable and goal-directed agent, such as an\nInternet-browsing agent in the digital world and a household humanoid in the\nphysical world, has rapidly advanced, thanks to the generalization capability\nof foundation models. Such a generalist agent needs to have a large and diverse\nskill repertoire, such as finding directions between two travel locations and\nbuying specific items from the Internet. If each skill needs to be specified\nmanually through a fixed set of human-annotated instructions, the agent's skill\nrepertoire will necessarily be limited due to the quantity and diversity of\nhuman-annotated instructions. In this work, we address this challenge by\nproposing Proposer-Agent-Evaluator, an effective learning system that enables\nfoundation model agents to autonomously discover and practice skills in the\nwild. At the heart of PAE is a context-aware task proposer that autonomously\nproposes tasks for the agent to practice with context information of the\nenvironment such as user demos or even just the name of the website itself for\nInternet-browsing agents. Then, the agent policy attempts those tasks with\nthoughts and actual grounded operations in the real world with resulting\ntrajectories evaluated by an autonomous VLM-based success evaluator. The\nsuccess evaluation serves as the reward signal for the agent to refine its\npolicies through RL. We validate PAE on challenging vision-based web\nnavigation, using both real-world and self-hosted websites from WebVoyager and\nWebArena.To the best of our knowledge, this work represents the first effective\nlearning system to apply autonomous task proposal with RL for agents that\ngeneralizes real-world human-annotated benchmarks with SOTA performances. Our\nopen-source checkpoints and code can be found in https://yanqval.github.io/PAE/\n","authors":["Yifei Zhou","Qianlan Yang","Kaixiang Lin","Min Bai","Xiong Zhou","Yu-Xiong Wang","Sergey Levine","Erran Li"],"pdf_url":"https://arxiv.org/pdf/2412.13194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13195v1","updated":"2024-12-17T18:59:50Z","published":"2024-12-17T18:59:50Z","title":"CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion\n  Models","summary":"  Text-to-image diffusion models excel at generating photorealistic images, but\ncommonly struggle to render accurate spatial relationships described in text\nprompts. We identify two core issues underlying this common failure: 1) the\nambiguous nature of spatial-related data in existing datasets, and 2) the\ninability of current text encoders to accurately interpret the spatial\nsemantics of input descriptions. We address these issues with CoMPaSS, a\nversatile training framework that enhances spatial understanding of any T2I\ndiffusion model. CoMPaSS solves the ambiguity of spatial-related data with the\nSpatial Constraints-Oriented Pairing (SCOP) data engine, which curates\nspatially-accurate training data through a set of principled spatial\nconstraints. To better exploit the curated high-quality spatial priors, CoMPaSS\nfurther introduces a Token ENcoding ORdering (TENOR) module to allow better\nexploitation of high-quality spatial priors, effectively compensating for the\nshortcoming of text encoders. Extensive experiments on four popular open-weight\nT2I diffusion models covering both UNet- and MMDiT-based architectures\ndemonstrate the effectiveness of CoMPaSS by setting new state-of-the-arts with\nsubstantial relative gains across well-known benchmarks on spatial\nrelationships generation, including VISOR (+98%), T2I-CompBench Spatial (+67%),\nand GenEval Position (+131%). Code will be available at\nhttps://github.com/blurgyy/CoMPaSS.\n","authors":["Gaoyang Zhang","Bingtao Fu","Qingnan Fan","Qi Zhang","Runxing Liu","Hong Gu","Huaqi Zhang","Xinguo Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13195v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.13193v1","updated":"2024-12-17T18:59:46Z","published":"2024-12-17T18:59:46Z","title":"GaussTR: Foundation Model-Aligned Gaussian Transformer for\n  Self-Supervised 3D Spatial Understanding","summary":"  3D Semantic Occupancy Prediction is fundamental for spatial understanding as\nit provides a comprehensive semantic cognition of surrounding environments.\nHowever, prevalent approaches primarily rely on extensive labeled data and\ncomputationally intensive voxel-based modeling, restricting the scalability and\ngeneralizability of 3D representation learning. In this paper, we introduce\nGaussTR, a novel Gaussian Transformer that leverages alignment with foundation\nmodels to advance self-supervised 3D spatial understanding. GaussTR adopts a\nTransformer architecture to predict sparse sets of 3D Gaussians that represent\nscenes in a feed-forward manner. Through aligning rendered Gaussian features\nwith diverse knowledge from pre-trained foundation models, GaussTR facilitates\nthe learning of versatile 3D representations and enables open-vocabulary\noccupancy prediction without explicit annotations. Empirical evaluations on the\nOcc3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shot\nperformance, achieving 11.70 mIoU while reducing training duration by\napproximately 50%. These experimental results highlight the significant\npotential of GaussTR for scalable and holistic 3D spatial understanding, with\npromising implications for autonomous driving and embodied agents. Code is\navailable at https://github.com/hustvl/GaussTR.\n","authors":["Haoyi Jiang","Liu Liu","Tianheng Cheng","Xinjie Wang","Tianwei Lin","Zhizhong Su","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13190v1","updated":"2024-12-17T18:59:33Z","published":"2024-12-17T18:59:33Z","title":"MotionBridge: Dynamic Video Inbetweening with Flexible Controls","summary":"  By generating plausible and smooth transitions between two image frames,\nvideo inbetweening is an essential tool for video editing and long video\nsynthesis. Traditional works lack the capability to generate complex large\nmotions. While recent video generation techniques are powerful in creating\nhigh-quality results, they often lack fine control over the details of\nintermediate frames, which can lead to results that do not align with the\ncreative mind. We introduce MotionBridge, a unified video inbetweening\nframework that allows flexible controls, including trajectory strokes,\nkeyframes, masks, guide pixels, and text. However, learning such multi-modal\ncontrols in a unified framework is a challenging task. We thus design two\ngenerators to extract the control signal faithfully and encode feature through\ndual-branch embedders to resolve ambiguities. We further introduce a curriculum\ntraining strategy to smoothly learn various controls. Extensive qualitative and\nquantitative experiments have demonstrated that such multi-modal controls\nenable a more dynamic, customizable, and contextually accurate visual\nnarrative.\n","authors":["Maham Tanveer","Yang Zhou","Simon Niklaus","Ali Mahdavi Amiri","Hao Zhang","Krishna Kumar Singh","Nanxuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.13190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20309v4","updated":"2024-12-17T18:59:07Z","published":"2024-03-29T17:29:58Z","title":"InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds","summary":"  While neural 3D reconstruction has advanced substantially, it typically\nrequires densely captured multi-view data with carefully initialized poses\n(e.g., using COLMAP). However, this requirement limits its broader\napplicability, as Structure-from-Motion (SfM) is often unreliable in\nsparse-view scenarios where feature matches are limited, resulting in\ncumulative errors. In this paper, we introduce InstantSplat, a novel and\nlightning-fast neural reconstruction system that builds accurate 3D\nrepresentations from as few as 2-3 images. InstantSplat adopts a\nself-supervised framework that bridges the gap between 2D images and 3D\nrepresentations using Gaussian Bundle Adjustment (GauBA) and can be optimized\nin an end-to-end manner. InstantSplat integrates dense stereo priors and\nco-visibility relationships between frames to initialize pixel-aligned geometry\nby progressively expanding the scene avoiding redundancy. Gaussian Bundle\nAdjustment is used to adapt both the scene representation and camera parameters\nquickly by minimizing gradient-based photometric error. Overall, InstantSplat\nachieves large-scale 3D reconstruction in mere seconds by reducing the required\nnumber of input views. It achieves an acceleration of over 20 times in\nreconstruction, improves visual quality (SSIM) from 0.3755 to 0.7624 than\nCOLMAP with 3D-GS, and is compatible with multiple 3D representations (3D-GS,\n2D-GS, and Mip-Splatting).\n","authors":["Zhiwen Fan","Kairun Wen","Wenyan Cong","Kevin Wang","Jian Zhang","Xinghao Ding","Danfei Xu","Boris Ivanovic","Marco Pavone","Georgios Pavlakos","Zhangyang Wang","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20309v4.pdf","comment":"Project Page: https://instantsplat.github.io/"},{"id":"http://arxiv.org/abs/2412.13188v1","updated":"2024-12-17T18:58:55Z","published":"2024-12-17T18:58:55Z","title":"StreetCrafter: Street View Synthesis with Controllable Video Diffusion\n  Models","summary":"  This paper aims to tackle the problem of photorealistic view synthesis from\nvehicle sensor data. Recent advancements in neural scene representation have\nachieved notable success in rendering high-quality autonomous driving scenes,\nbut the performance significantly degrades as the viewpoint deviates from the\ntraining trajectory. To mitigate this problem, we introduce StreetCrafter, a\nnovel controllable video diffusion model that utilizes LiDAR point cloud\nrenderings as pixel-level conditions, which fully exploits the generative prior\nfor novel view synthesis, while preserving precise camera control. Moreover,\nthe utilization of pixel-level LiDAR conditions allows us to make accurate\npixel-level edits to target scenes. In addition, the generative prior of\nStreetCrafter can be effectively incorporated into dynamic scene\nrepresentations to achieve real-time rendering. Experiments on Waymo Open\nDataset and PandaSet demonstrate that our model enables flexible control over\nviewpoint changes, enlarging the view synthesis regions for satisfying\nrendering, which outperforms existing methods.\n","authors":["Yunzhi Yan","Zhen Xu","Haotong Lin","Haian Jin","Haoyu Guo","Yida Wang","Kun Zhan","Xianpeng Lang","Hujun Bao","Xiaowei Zhou","Sida Peng"],"pdf_url":"https://arxiv.org/pdf/2412.13188v1.pdf","comment":"Project page: https://zju3dv.github.io/street_crafter"},{"id":"http://arxiv.org/abs/2412.13187v1","updated":"2024-12-17T18:58:33Z","published":"2024-12-17T18:58:33Z","title":"HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction","summary":"  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results \\url{https://www.chenbao.tech/handsonvlm/}\n","authors":["Chen Bao","Jiarui Xu","Xiaolong Wang","Abhinav Gupta","Homanga Bharadhwaj"],"pdf_url":"https://arxiv.org/pdf/2412.13187v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2412.13185v1","updated":"2024-12-17T18:58:07Z","published":"2024-12-17T18:58:07Z","title":"Move-in-2D: 2D-Conditioned Human Motion Generation","summary":"  Generating realistic human videos remains a challenging task, with the most\neffective methods currently relying on a human motion sequence as a control\nsignal. Existing approaches often use existing motion extracted from other\nvideos, which restricts applications to specific motion types and global scene\nmatching. We propose Move-in-2D, a novel approach to generate human motion\nsequences conditioned on a scene image, allowing for diverse motion that adapts\nto different scenes. Our approach utilizes a diffusion model that accepts both\na scene image and text prompt as inputs, producing a motion sequence tailored\nto the scene. To train this model, we collect a large-scale video dataset\nfeaturing single-human activities, annotating each video with the corresponding\nhuman motion as the target output. Experiments demonstrate that our method\neffectively predicts human motion that aligns with the scene image after\nprojection. Furthermore, we show that the generated motion sequence improves\nhuman motion quality in video synthesis tasks.\n","authors":["Hsin-Ping Huang","Yang Zhou","Jui-Hsien Wang","Difan Liu","Feng Liu","Ming-Hsuan Yang","Zhan Xu"],"pdf_url":"https://arxiv.org/pdf/2412.13185v1.pdf","comment":"Project page: https://hhsinping.github.io/Move-in-2D/"},{"id":"http://arxiv.org/abs/2412.13183v1","updated":"2024-12-17T18:57:38Z","published":"2024-12-17T18:57:38Z","title":"Real-time Free-view Human Rendering from Sparse-view RGB Videos using\n  Double Unprojected Textures","summary":"  Real-time free-view human rendering from sparse-view RGB inputs is a\nchallenging task due to the sensor scarcity and the tight time budget. To\nensure efficiency, recent methods leverage 2D CNNs operating in texture space\nto learn rendering primitives. However, they either jointly learn geometry and\nappearance, or completely ignore sparse image information for geometry\nestimation, significantly harming visual quality and robustness to unseen body\nposes. To address these issues, we present Double Unprojected Textures, which\nat the core disentangles coarse geometric deformation estimation from\nappearance synthesis, enabling robust and photorealistic 4K rendering in\nreal-time. Specifically, we first introduce a novel image-conditioned template\ndeformation network, which estimates the coarse deformation of the human\ntemplate from a first unprojected texture. This updated geometry is then used\nto apply a second and more accurate texture unprojection. The resulting texture\nmap has fewer artifacts and better alignment with input views, which benefits\nour learning of finer-level geometry and appearance represented by Gaussian\nsplats. We validate the effectiveness and efficiency of the proposed method in\nquantitative and qualitative experiments, which significantly surpasses other\nstate-of-the-art methods.\n","authors":["Guoxing Sun","Rishabh Dabral","Heming Zhu","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2412.13183v1.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/DUT/"},{"id":"http://arxiv.org/abs/2412.13180v1","updated":"2024-12-17T18:56:50Z","published":"2024-12-17T18:56:50Z","title":"Feather the Throttle: Revisiting Visual Token Pruning for\n  Vision-Language Model Acceleration","summary":"  Recent works on accelerating Vision-Language Models show that strong\nperformance can be maintained across a variety of vision-language tasks despite\nhighly compressing visual information. In this work, we examine the popular\nacceleration approach of early pruning of visual tokens inside the language\nmodel and find that its strong performance across many tasks is not due to an\nexceptional ability to compress visual information, but rather the benchmarks'\nlimited ability to assess fine-grained visual capabilities. Namely, we\ndemonstrate a core issue with the acceleration approach where most tokens\ntowards the top of the image are pruned away. Yet, this issue is only reflected\nin performance for a small subset of tasks such as localization. For the other\nevaluated tasks, strong performance is maintained with the flawed pruning\nstrategy. Noting the limited visual capabilities of the studied acceleration\ntechnique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble\ncRiteria), a straightforward approach that (1) resolves the identified issue\nwith early-layer pruning, (2) incorporates uniform sampling to ensure coverage\nacross all image regions, and (3) applies pruning in two stages to allow the\ncriteria to become more effective at a later layer while still achieving\nsignificant speedup through early-layer pruning. With comparable computational\nsavings, we find that FEATHER has more than $5\\times$ performance improvement\non the vision-centric localization benchmarks compared to the original\nacceleration approach.\n","authors":["Mark Endo","Xiaohan Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2412.13180v1.pdf","comment":"Project page:\n  https://web.stanford.edu/~markendo/projects/feather.html"},{"id":"http://arxiv.org/abs/2412.13176v1","updated":"2024-12-17T18:54:28Z","published":"2024-12-17T18:54:28Z","title":"NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle\n  Adjustment","summary":"  Simultaneous Localization And Mapping (SLAM) from a monocular endoscopy video\ncan enable autonomous navigation, guidance to unsurveyed regions, and 3D\nvisualizations, which can significantly improve endoscopy experience for\nsurgeons and patient outcomes. Existing dense SLAM algorithms often assume\ndistant and static lighting and textured surfaces, and alternate between\noptimizing scene geometry and camera parameters by minimizing a photometric\nrendering loss, often called Photometric Bundle Adjustment. However, endoscopic\nenvironments exhibit dynamic near-field lighting due to the co-located light\nand camera moving extremely close to the surface, textureless surfaces, and\nstrong specular reflections due to mucus layers. When not considered, these\nnear-field lighting effects can cause significant performance reductions for\nexisting SLAM algorithms from indoor/outdoor scenes when applied to endoscopy\nvideos. To mitigate this problem, we introduce a new Near-Field Lighting Bundle\nAdjustment Loss $(L_{NFL-BA})$ that can also be alternatingly optimized, along\nwith the Photometric Bundle Adjustment loss, such that the captured images'\nintensity variations match the relative distance and orientation between the\nsurface and the co-located light and camera. We derive a general NFL-BA loss\nfunction for 3D Gaussian surface representations and demonstrate that adding\n$L_{NFL-BA}$ can significantly improve the tracking and mapping performance of\ntwo state-of-the-art 3DGS-SLAM systems, MonoGS (35% improvement in tracking,\n48% improvement in mapping with predicted depth maps) and EndoGSLAM (22%\nimprovement in tracking, marginal improvement in mapping with predicted\ndepths), on the C3VD endoscopy dataset for colons. The project page is\navailable at https://asdunnbe.github.io/NFL-BA/\n","authors":["Andrea Dunn Beltran","Daniel Rho","Marc Niethammer","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2412.13176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13174v1","updated":"2024-12-17T18:53:43Z","published":"2024-12-17T18:53:43Z","title":"ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark\n  Detection","summary":"  Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.\n","authors":["Jui-Che Chiang","Hou-Ning Hu","Bo-Syuan Hou","Chia-Yu Tseng","Yu-Lun Liu","Min-Hung Chen","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13174v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2412.13173v1","updated":"2024-12-17T18:52:30Z","published":"2024-12-17T18:52:30Z","title":"Locate n' Rotate: Two-stage Openable Part Detection with Foundation\n  Model Priors","summary":"  Detecting the openable parts of articulated objects is crucial for downstream\napplications in intelligent robotics, such as pulling a drawer. This task poses\na multitasking challenge due to the necessity of understanding object\ncategories and motion. Most existing methods are either category-specific or\ntrained on specific datasets, lacking generalization to unseen environments and\nobjects. In this paper, we propose a Transformer-based Openable Part Detection\n(OPD) framework named Multi-feature Openable Part Detection (MOPD) that\nincorporates perceptual grouping and geometric priors, outperforming previous\nmethods in performance. In the first stage of the framework, we introduce a\nperceptual grouping feature model that provides perceptual grouping feature\npriors for openable part detection, enhancing detection results through a\ncross-attention mechanism. In the second stage, a geometric understanding\nfeature model offers geometric feature priors for predicting motion parameters.\nCompared to existing methods, our proposed approach shows better performance in\nboth detection and motion parameter prediction. Codes and models are publicly\navailable at https://github.com/lisiqi-zju/MOPD\n","authors":["Siqi Li","Xiaoxue Chen","Haoyu Cheng","Guyue Zhou","Hao Zhao","Guanzhong Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13173v1.pdf","comment":"ACCV 2024 Oral, Project: https://github.com/lisiqi-zju/MOPD"},{"id":"http://arxiv.org/abs/2412.12095v2","updated":"2024-12-17T18:45:55Z","published":"2024-12-16T18:59:29Z","title":"Causal Diffusion Transformers for Generative Modeling","summary":"  We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.\n","authors":["Chaorui Deng","Deyao Zhu","Kunchang Li","Shi Guang","Haoqi Fan"],"pdf_url":"https://arxiv.org/pdf/2412.12095v2.pdf","comment":"22 figures, 21 pages"},{"id":"http://arxiv.org/abs/2412.13168v1","updated":"2024-12-17T18:45:53Z","published":"2024-12-17T18:45:53Z","title":"Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial\n  Dynamics in the Wild","summary":"  In-the-wild Dynamic facial expression recognition (DFER) encounters a\nsignificant challenge in recognizing emotion-related expressions, which are\noften temporally and spatially diluted by emotion-irrelevant expressions and\nglobal context respectively. Most of the prior DFER methods model tightly\ncoupled spatiotemporal representations which may incorporate weakly relevant\nfeatures, leading to information redundancy and emotion-irrelevant context\nbias. Several DFER methods have highlighted the significance of dynamic\ninformation, but utilize explicit manners to extract dynamic features with\noverly strong prior knowledge. In this paper, we propose a novel Implicit\nFacial Dynamics Disentanglement framework (IFDD). Through expanding wavelet\nlifting scheme to fully learnable framework, IFDD disentangles emotion-related\ndynamic information from emotion-irrelevant global context in an implicit\nmanner, i.e., without exploit operations and external guidance. The\ndisentanglement process of IFDD contains two stages, i.e., Inter-frame\nStatic-dynamic Splitting Module (ISSM) for rough disentanglement estimation and\nLifting-based Aggregation-Disentanglement Module (LADM) for further refinement.\nSpecifically, ISSM explores inter-frame correlation to generate content-aware\nsplitting indexes on-the-fly. We preliminarily utilize these indexes to split\nframe features into two groups, one with greater global similarity, and the\nother with more unique dynamic features. Subsequently, LADM first aggregates\nthese two groups of features to obtain fine-grained global context features by\nan updater, and then disentangles emotion-related facial dynamic features from\nthe global context by a predictor. Extensive experiments on in-the-wild\ndatasets have demonstrated that IFDD outperforms prior supervised DFER methods\nwith higher recognition accuracy and comparable efficiency.\n","authors":["Xingjian Wang","Li Chai"],"pdf_url":"https://arxiv.org/pdf/2412.13168v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13161v1","updated":"2024-12-17T18:39:10Z","published":"2024-12-17T18:39:10Z","title":"BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce","summary":"  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n","authors":["Mohammad Nazmush Shamael","Sabila Nawshin","Swakkhar Shatabda","Salekul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.13161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03665v3","updated":"2024-12-17T18:39:00Z","published":"2024-10-04T17:59:57Z","title":"Estimating Body and Hand Motion in an Ego-sensed World","summary":"  We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture a device wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve hand estimation: the resulting kinematic and temporal\nconstraints can reduce world-frame errors in single-frame estimates by 40%.\nProject page: https://egoallo.github.io/\n","authors":["Brent Yi","Vickie Ye","Maya Zheng","Yunqi Li","Lea Müller","Georgios Pavlakos","Yi Ma","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2410.03665v3.pdf","comment":"Project page: https://egoallo.github.io/"},{"id":"http://arxiv.org/abs/2412.13156v1","updated":"2024-12-17T18:30:22Z","published":"2024-12-17T18:30:22Z","title":"S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical\n  Imaging","summary":"  Robustness and generalizability in medical image segmentation are often\nhindered by scarcity and limited diversity of training data, which stands in\ncontrast to the variability encountered during inference. While conventional\nstrategies -- such as domain-specific augmentation, specialized architectures,\nand tailored training procedures -- can alleviate these issues, they depend on\nthe availability and reliability of domain knowledge. When such knowledge is\nunavailable, misleading, or improperly applied, performance may deteriorate. In\nresponse, we introduce a novel, domain-agnostic, add-on, and data-driven\nstrategy inspired by image stacking in image denoising. Termed ``semantic\nstacking,'' our method estimates a denoised semantic representation that\ncomplements the conventional segmentation loss during training. This method\ndoes not depend on domain-specific assumptions, making it broadly applicable\nacross diverse image modalities, model architectures, and augmentation\ntechniques. Through extensive experiments, we validate the superiority of our\napproach in improving segmentation performance under diverse conditions. Code\nis available at https://github.com/ymp5078/Semantic-Stacking.\n","authors":["Yimu Pan","Sitao Zhang","Alison D. Gernand","Jeffery A. Goldstein","James Z. Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13156v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.13155v1","updated":"2024-12-17T18:28:48Z","published":"2024-12-17T18:28:48Z","title":"F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking\n  Face Generation, Customization, and Restoration","summary":"  Artificial intelligence generative models exhibit remarkable capabilities in\ncontent creation, particularly in face image generation, customization, and\nrestoration. However, current AI-generated faces (AIGFs) often fall short of\nhuman preferences due to unique distortions, unrealistic details, and\nunexpected identity shifts, underscoring the need for a comprehensive quality\nevaluation framework for AIGFs. To address this need, we introduce FaceQ, a\nlarge-scale, comprehensive database of AI-generated Face images with\nfine-grained Quality annotations reflecting human preferences. The FaceQ\ndatabase comprises 12,255 images generated by 29 models across three tasks: (1)\nface generation, (2) face customization, and (3) face restoration. It includes\n32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multiple\ndimensions: quality, authenticity, identity (ID) fidelity, and text-image\ncorrespondence. Using the FaceQ database, we establish F-Bench, a benchmark for\ncomparing and evaluating face generation, customization, and restoration\nmodels, highlighting strengths and weaknesses across various prompts and\nevaluation dimensions. Additionally, we assess the performance of existing\nimage quality assessment (IQA), face quality assessment (FQA), AI-generated\ncontent image quality assessment (AIGCIQA), and preference evaluation metrics,\nmanifesting that these standard metrics are relatively ineffective in\nevaluating authenticity, ID fidelity, and text-image correspondence. The FaceQ\ndatabase will be publicly available upon publication.\n","authors":["Lu Liu","Huiyu Duan","Qiang Hu","Liu Yang","Chunlei Cai","Tianxiao Ye","Huayu Liu","Xiaoyun Zhang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2412.13155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13152v1","updated":"2024-12-17T18:23:33Z","published":"2024-12-17T18:23:33Z","title":"Continuous Patient Monitoring with AI: Real-Time Analysis of Video in\n  Hospital Care Settings","summary":"  This study introduces an AI-driven platform for continuous and passive\npatient monitoring in hospital settings, developed by LookDeep Health.\nLeveraging advanced computer vision, the platform provides real-time insights\ninto patient behavior and interactions through video analysis, securely storing\ninference results in the cloud for retrospective evaluation. The dataset,\ncompiled in collaboration with 11 hospital partners, encompasses over 300\nhigh-risk fall patients and over 1,000 days of inference, enabling applications\nsuch as fall detection and safety monitoring for vulnerable patient\npopulations. To foster innovation and reproducibility, an anonymized subset of\nthis dataset is publicly available. The AI system detects key components in\nhospital rooms, including individual presence and role, furniture location,\nmotion magnitude, and boundary crossings. Performance evaluation demonstrates\nstrong accuracy in object detection (macro F1-score = 0.92) and patient-role\nclassification (F1-score = 0.98), as well as reliable trend analysis for the\n\"patient alone\" metric (mean logistic regression accuracy = 0.82 \\pm 0.15).\nThese capabilities enable automated detection of patient isolation, wandering,\nor unsupervised movement-key indicators for fall risk and other adverse events.\nThis work establishes benchmarks for validating AI-driven patient monitoring\nsystems, highlighting the platform's potential to enhance patient safety and\ncare by providing continuous, data-driven insights into patient behavior and\ninteractions.\n","authors":["Paolo Gabriel","Peter Rehani","Tyler Troy","Tiffany Wyatt","Michael Choma","Narinder Singh"],"pdf_url":"https://arxiv.org/pdf/2412.13152v1.pdf","comment":"21 pages, 9 figures, 3 tables, submitted to Frontiers in Imaging >\n  Imaging Applications > (Research Topic) Deep Learning for Medical Imaging\n  Applications for publication"},{"id":"http://arxiv.org/abs/2412.13140v1","updated":"2024-12-17T18:06:28Z","published":"2024-12-17T18:06:28Z","title":"Label Errors in the Tobacco3482 Dataset","summary":"  Tobacco3482 is a widely used document classification benchmark dataset.\nHowever, our manual inspection of the entire dataset uncovers widespread\nontological issues, especially large amounts of annotation label problems in\nthe dataset. We establish data label guidelines and find that 11.7% of the\ndataset is improperly annotated and should either have an unknown label or a\ncorrected label, and 16.7% of samples in the dataset have multiple valid\nlabels. We then analyze the mistakes of a top-performing model and find that\n35% of the model's mistakes can be directly attributed to these label issues,\nhighlighting the inherent problems with using a noisily labeled dataset as a\nbenchmark. Supplementary material, including dataset annotations and code, is\navailable at https://github.com/gordon-lim/tobacco3482-mistakes/.\n","authors":["Gordon Lim","Stefan Larson","Kevin Leach"],"pdf_url":"https://arxiv.org/pdf/2412.13140v1.pdf","comment":"WACV VisionDocs Workshop 2025"},{"id":"http://arxiv.org/abs/2412.13137v1","updated":"2024-12-17T18:04:33Z","published":"2024-12-17T18:04:33Z","title":"Unlocking the Potential of Digital Pathology: Novel Baselines for\n  Compression","summary":"  Digital pathology offers a groundbreaking opportunity to transform clinical\npractice in histopathological image analysis, yet faces a significant hurdle:\nthe substantial file sizes of pathological Whole Slide Images (WSI). While\ncurrent digital pathology solutions rely on lossy JPEG compression to address\nthis issue, lossy compression can introduce color and texture disparities,\npotentially impacting clinical decision-making. While prior research addresses\nperceptual image quality and downstream performance independently of each\nother, we jointly evaluate compression schemes for perceptual and downstream\ntask quality on four different datasets. In addition, we collect an initially\nuncompressed dataset for an unbiased perceptual evaluation of compression\nschemes. Our results show that deep learning models fine-tuned for perceptual\nquality outperform conventional compression schemes like JPEG-XL or WebP for\nfurther compression of WSI. However, they exhibit a significant bias towards\nthe compression artifacts present in the training data and struggle to\ngeneralize across various compression schemes. We introduce a novel evaluation\nmetric based on feature similarity between original files and compressed files\nthat aligns very well with the actual downstream performance on the compressed\nWSI. Our metric allows for a general and standardized evaluation of lossy\ncompression schemes and mitigates the requirement to independently assess\ndifferent downstream tasks. Our study provides novel insights for the\nassessment of lossy compression schemes for WSI and encourages a unified\nevaluation of lossy compression schemes to accelerate the clinical uptake of\ndigital pathology.\n","authors":["Maximilian Fischer","Peter Neher","Peter Schüffler","Sebastian Ziegler","Shuhan Xiao","Robin Peretzke","David Clunie","Constantin Ulrich","Michael Baumgartner","Alexander Muckenhuber","Silvia Dias Almeida","Michael Götz","Jens Kleesiek","Marco Nolden","Rickmer Braren","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2412.13137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13126v1","updated":"2024-12-17T17:45:21Z","published":"2024-12-17T17:45:21Z","title":"A Knowledge-enhanced Pathology Vision-language Foundation Model for\n  Cancer Diagnosis","summary":"  Deep learning has enabled the development of highly robust foundation models\nfor various pathological tasks across diverse diseases and patient cohorts.\nAmong these models, vision-language pre-training, which leverages large-scale\npaired data to align pathology image and text embedding spaces, and provides a\nnovel zero-shot paradigm for downstream tasks. However, existing models have\nbeen primarily data-driven and lack the incorporation of domain-specific\nknowledge, which limits their performance in cancer diagnosis, especially for\nrare tumor subtypes. To address this limitation, we establish a\nKnowledge-enhanced Pathology (KEEP) foundation model that harnesses disease\nknowledge to facilitate vision-language pre-training. Specifically, we first\nconstruct a disease knowledge graph (KG) that covers 11,454 human diseases with\n139,143 disease attributes, including synonyms, definitions, and hypernym\nrelations. We then systematically reorganize the millions of publicly available\nnoisy pathology image-text pairs, into 143K well-structured semantic groups\nlinked through the hierarchical relations of the disease KG. To derive more\nnuanced image and text representations, we propose a novel knowledge-enhanced\nvision-language pre-training approach that integrates disease knowledge into\nthe alignment within hierarchical semantic groups instead of unstructured\nimage-text pairs. Validated on 18 diverse benchmarks with more than 14,000\nwhole slide images (WSIs), KEEP achieves state-of-the-art performance in\nzero-shot cancer diagnostic tasks. Notably, for cancer detection, KEEP\ndemonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7\ncancer types. For cancer subtyping, KEEP achieves a median balanced accuracy of\n0.456 in subtyping 30 rare brain cancers, indicating strong generalizability\nfor diagnosing rare tumors.\n","authors":["Xiao Zhou","Luoyi Sun","Dexuan He","Wenbin Guan","Ruifen Wang","Lifeng Wang","Xin Sun","Kun Sun","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2412.13126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05271v3","updated":"2024-12-17T17:44:38Z","published":"2024-12-06T18:57:08Z","title":"Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling","summary":"  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n","authors":["Zhe Chen","Weiyun Wang","Yue Cao","Yangzhou Liu","Zhangwei Gao","Erfei Cui","Jinguo Zhu","Shenglong Ye","Hao Tian","Zhaoyang Liu","Lixin Gu","Xuehui Wang","Qingyun Li","Yimin Ren","Zixuan Chen","Jiapeng Luo","Jiahao Wang","Tan Jiang","Bo Wang","Conghui He","Botian Shi","Xingcheng Zhang","Han Lv","Yi Wang","Wenqi Shao","Pei Chu","Zhongying Tu","Tong He","Zhiyong Wu","Huipeng Deng","Jiaye Ge","Kai Chen","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.05271v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.13111v1","updated":"2024-12-17T17:34:52Z","published":"2024-12-17T17:34:52Z","title":"Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation","summary":"  Text-driven human motion synthesis is capturing significant attention for its\nability to effortlessly generate intricate movements from abstract text cues,\nshowcasing its potential for revolutionizing motion design not only in film\nnarratives but also in virtual reality experiences and computer game\ndevelopment. Existing methods often rely on 3D motion capture data, which\nrequire special setups resulting in higher costs for data acquisition,\nultimately limiting the diversity and scope of human motion. In contrast, 2D\nhuman videos offer a vast and accessible source of motion data, covering a\nwider range of styles and activities. In this paper, we explore leveraging 2D\nhuman motion extracted from videos as an alternative data source to improve\ntext-driven 3D motion generation. Our approach introduces a novel framework\nthat disentangles local joint motion from global movements, enabling efficient\nlearning of local motion priors from 2D data. We first train a single-view 2D\nlocal motion generator on a large dataset of text-motion pairs. To enhance this\nmodel to synthesize 3D motion, we fine-tune the generator with 3D data,\ntransforming it into a multi-view generator that predicts view-consistent local\njoint motion and root dynamics. Experiments on the HumanML3D dataset and novel\ntext prompts demonstrate that our method efficiently utilizes 2D data,\nsupporting realistic 3D human motion generation and broadening the range of\nmotion types it supports. Our code will be made publicly available at\nhttps://zju3dv.github.io/Motion-2-to-3/.\n","authors":["Huaijin Pi","Ruoxi Guo","Zehong Shen","Qing Shuai","Zechen Hu","Zhumei Wang","Yajiao Dong","Ruizhen Hu","Taku Komura","Sida Peng","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13111v1.pdf","comment":"Project page: https://zju3dv.github.io/Motion-2-to-3/"},{"id":"http://arxiv.org/abs/2412.13099v1","updated":"2024-12-17T17:10:02Z","published":"2024-12-17T17:10:02Z","title":"Accuracy Limits as a Barrier to Biometric System Security","summary":"  Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.\n","authors":["Axel Durbet","Paul-Marie Grollemund","Pascal Lafourcade","Kevin Thiry-Atighehchi"],"pdf_url":"https://arxiv.org/pdf/2412.13099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13096v1","updated":"2024-12-17T17:06:33Z","published":"2024-12-17T17:06:33Z","title":"Incremental Online Learning of Randomized Neural Network with Forward\n  Regularization","summary":"  Online learning of deep neural networks suffers from challenges such as\nhysteretic non-incremental updating, increasing memory usage, past\nretrospective retraining, and catastrophic forgetting. To alleviate these\ndrawbacks and achieve progressive immediate decision-making, we propose a novel\nIncremental Online Learning (IOL) process of Randomized Neural Networks\n(Randomized NN), a framework facilitating continuous improvements to Randomized\nNN performance in restrictive online scenarios. Within the framework, we\nfurther introduce IOL with ridge regularization (-R) and IOL with forward\nregularization (-F). -R generates stepwise incremental updates without\nretrospective retraining and avoids catastrophic forgetting. Moreover, we\nsubstituted -R with -F as it enhanced precognition learning ability using\nsemi-supervision and realized better online regrets to offline global experts\ncompared to -R during IOL. The algorithms of IOL for Randomized NN with -R/-F\non non-stationary batch stream were derived respectively, featuring recursive\nweight updates and variable learning rates. Additionally, we conducted a\ndetailed analysis and theoretically derived relative cumulative regret bounds\nof the Randomized NN learners with -R/-F in IOL under adversarial assumptions\nusing a novel methodology and presented several corollaries, from which we\nobserved the superiority on online learning acceleration and regret bounds of\nemploying -F in IOL. Finally, our proposed methods were rigorously examined\nacross regression and classification tasks on diverse datasets, which\ndistinctly validated the efficacy of IOL frameworks of Randomized NN and the\nadvantages of forward regularization.\n","authors":["Junda Wang","Minghui Hu","Ning Li","Abdulaziz Al-Ali","Ponnuthurai Nagaratnam Suganthan"],"pdf_url":"https://arxiv.org/pdf/2412.13096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13081v1","updated":"2024-12-17T16:54:05Z","published":"2024-12-17T16:54:05Z","title":"Prompt Augmentation for Self-supervised Text-guided Image Manipulation","summary":"  Text-guided image editing finds applications in various creative and\npractical fields. While recent studies in image generation have advanced the\nfield, they often struggle with the dual challenges of coherent image\ntransformation and context preservation. In response, our work introduces\nprompt augmentation, a method amplifying a single input prompt into several\ntarget prompts, strengthening textual context and enabling localised image\nediting. Specifically, we use the augmented prompts to delineate the intended\nmanipulation area. We propose a Contrastive Loss tailored to driving effective\nimage editing by displacing edited areas and drawing preserved regions closer.\nAcknowledging the continuous nature of image manipulations, we further refine\nour approach by incorporating the similarity concept, creating a Soft\nContrastive Loss. The new losses are incorporated to the diffusion model,\ndemonstrating improved or competitive image editing results on public datasets\nand generated images over state-of-the-art approaches.\n","authors":["Rumeysa Bodur","Binod Bhattarai","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.13081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13079v1","updated":"2024-12-17T16:51:44Z","published":"2024-12-17T16:51:44Z","title":"Identifying Bias in Deep Neural Networks Using Image Transforms","summary":"  CNNs have become one of the most commonly used computational tool in the past\ntwo decades. One of the primary downsides of CNNs is that they work as a\n``black box\", where the user cannot necessarily know how the image data are\nanalyzed, and therefore needs to rely on empirical evaluation to test the\nefficacy of a trained CNN. This can lead to hidden biases that affect the\nperformance evaluation of neural networks, but are difficult to identify. Here\nwe discuss examples of such hidden biases in common and widely used benchmark\ndatasets, and propose techniques for identifying dataset biases that can affect\nthe standard performance evaluation metrics. One effective approach to identify\ndataset bias is to perform image classification by using merely blank\nbackground parts of the original images. However, in some situations a blank\nbackground in the images is not available, making it more difficult to separate\nforeground or contextual information from the bias. To overcome this, we\npropose a method to identify dataset bias without the need to crop background\ninformation from the images. That method is based on applying several image\ntransforms to the original images, including Fourier transform, wavelet\ntransforms, median filter, and their combinations. These transforms were\napplied to recover background bias information that CNNs use to classify\nimages. This transformations affect the contextual visual information in a\ndifferent manner than it affects the systemic background bias. Therefore, the\nmethod can distinguish between contextual information and the bias, and alert\non the presence of background bias even without the need to separate sub-images\nparts from the blank background of the original images. Code used in the\nexperiments is publicly available.\n","authors":["Sai Teja Erukude","Akhil Joshi","Lior Shamir"],"pdf_url":"https://arxiv.org/pdf/2412.13079v1.pdf","comment":"Computers, published"},{"id":"http://arxiv.org/abs/2410.14672v2","updated":"2024-12-17T16:47:41Z","published":"2024-10-18T17:59:04Z","title":"BiGR: Harnessing Binary Latent Codes for Image Generation and Improved\n  Visual Representation Capabilities","summary":"  We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field. We further enable BiGR to perform text-to-image generation,\nshowcasing its potential for broader applications.\n","authors":["Shaozhe Hao","Xuantong Liu","Xianbiao Qi","Shihao Zhao","Bojia Zi","Rong Xiao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2410.14672v2.pdf","comment":"Updated with additional T2I results; Project page:\n  https://haoosz.github.io/BiGR"},{"id":"http://arxiv.org/abs/2412.13070v1","updated":"2024-12-17T16:34:32Z","published":"2024-12-17T16:34:32Z","title":"Learning of Patch-Based Smooth-Plus-Sparse Models for Image\n  Reconstruction","summary":"  We aim at the solution of inverse problems in imaging, by combining a\npenalized sparse representation of image patches with an unconstrained smooth\none. This allows for a straightforward interpretation of the reconstruction. We\nformulate the optimization as a bilevel problem. The inner problem deploys\nclassical algorithms while the outer problem optimizes the dictionary and the\nregularizer parameters through supervised learning. The process is carried out\nvia implicit differentiation and gradient-based optimization. We evaluate our\nmethod for denoising, super-resolution, and compressed-sensing\nmagnetic-resonance imaging. We compare it to other classical models as well as\ndeep-learning-based methods and show that it always outperforms the former and\nalso the latter in some instances.\n","authors":["Stanislas Ducotterd","Sebastian Neumayer","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2412.13070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13063v1","updated":"2024-12-17T16:28:08Z","published":"2024-12-17T16:28:08Z","title":"Smartphone-based Iris Recognition through High-Quality Visible Spectrum\n  Iris Capture","summary":"  Iris recognition is widely acknowledged for its exceptional accuracy in\nbiometric authentication, traditionally relying on near-infrared (NIR) imaging.\nRecently, visible spectrum (VIS) imaging via accessible smartphone cameras has\nbeen explored for biometric capture. However, a thorough study of iris\nrecognition using smartphone-captured 'High-Quality' VIS images and\ncross-spectral matching with previously enrolled NIR images has not been\nconducted. The primary challenge lies in capturing high-quality biometrics, a\nknown limitation of smartphone cameras. This study introduces a novel Android\napplication designed to consistently capture high-quality VIS iris images\nthrough automated focus and zoom adjustments. The application integrates a\nYOLOv3-tiny model for precise eye and iris detection and a lightweight\nGhost-Attention U-Net (G-ATTU-Net) for segmentation, while adhering to ISO/IEC\n29794-6 standards for image quality. The approach was validated using\nsmartphone-captured VIS and NIR iris images from 47 subjects, achieving a True\nAcceptance Rate (TAR) of 96.57% for VIS images and 97.95% for NIR images, with\nconsistent performance across various capture distances and iris colors. This\nrobust solution is expected to significantly advance the field of iris\nbiometrics, with important implications for enhancing smartphone security.\n","authors":["Naveenkumar G Venkataswamy","Yu Liu","Surendra Singh","Soumyabrata Dey","Stephanie Schuckers","Masudul H Imtiaz"],"pdf_url":"https://arxiv.org/pdf/2412.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11017v2","updated":"2024-12-17T16:27:21Z","published":"2024-12-15T02:10:18Z","title":"On Distilling the Displacement Knowledge for Few-Shot Class-Incremental\n  Learning","summary":"  Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of\nevolving data distributions and the difficulty of data acquisition in\nreal-world scenarios. To counteract the catastrophic forgetting typically\nencountered in FSCIL, knowledge distillation is employed as a way to maintain\nthe knowledge from learned data distribution. Recognizing the limitations of\ngenerating discriminative feature representations in a few-shot context, our\napproach incorporates structural information between samples into knowledge\ndistillation. This structural information serves as a remedy for the low\nquality of features. Diverging from traditional structured distillation methods\nthat compute sample similarity, we introduce the Displacement Knowledge\nDistillation (DKD) method. DKD utilizes displacement rather than similarity\nbetween samples, incorporating both distance and angular information to\nsignificantly enhance the information density retained through knowledge\ndistillation. Observing performance disparities in feature distribution between\nbase and novel classes, we propose the Dual Distillation Network (DDNet). This\nnetwork applies traditional knowledge distillation to base classes and DKD to\nnovel classes, challenging the conventional integration of novel classes with\nbase classes. Additionally, we implement an instance-aware sample selector\nduring inference to dynamically adjust dual branch weights, thereby leveraging\nthe complementary strengths of each approach. Extensive testing on three\nbenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,\nthrough rigorous experimentation and comparison, we establish the robustness\nand general applicability of our proposed DKD method.\n","authors":["Pengfei Fang","Yongchun Qin","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2412.11017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13061v1","updated":"2024-12-17T16:27:11Z","published":"2024-12-17T16:27:11Z","title":"VidTok: A Versatile and Open-Source Video Tokenizer","summary":"  Encoding video content into compact latent tokens has become a fundamental\nstep in video generation and understanding, driven by the need to address the\ninherent redundancy in pixel-level representations. Consequently, there is a\ngrowing demand for high-performance, open-source video tokenizers as\nvideo-centric research gains prominence. We introduce VidTok, a versatile video\ntokenizer that delivers state-of-the-art performance in both continuous and\ndiscrete tokenizations. VidTok incorporates several key advancements over\nexisting approaches: 1) model architecture such as convolutional layers and\nup/downsampling modules; 2) to address the training instability and codebook\ncollapse commonly associated with conventional Vector Quantization (VQ), we\nintegrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)\nimproved training strategies, including a two-stage training process and the\nuse of reduced frame rates. By integrating these advancements, VidTok achieves\nsubstantial improvements over existing methods, demonstrating superior\nperformance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,\nunder standardized evaluation settings.\n","authors":["Anni Tang","Tianyu He","Junliang Guo","Xinle Cheng","Li Song","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2412.13061v1.pdf","comment":"Code & Models: https://github.com/microsoft/VidTok"},{"id":"http://arxiv.org/abs/2412.13059v1","updated":"2024-12-17T16:25:40Z","published":"2024-12-17T16:25:40Z","title":"3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and\n  High-quality Medical Image Generation","summary":"  The generation of medical images presents significant challenges due to their\nhigh-resolution and three-dimensional nature. Existing methods often yield\nsuboptimal performance in generating high-quality 3D medical images, and there\nis currently no universal generative framework for medical imaging. In this\npaper, we introduce the 3D Medical Diffusion (3D MedDiffusion) model for\ncontrollable, high-quality 3D medical image generation. 3D MedDiffusion\nincorporates a novel, highly efficient Patch-Volume Autoencoder that compresses\nmedical images into latent space through patch-wise encoding and recovers back\ninto image space through volume-wise decoding. Additionally, we design a new\nnoise estimator to capture both local details and global structure information\nduring diffusion denoising process. 3D MedDiffusion can generate fine-detailed,\nhigh-resolution images (up to 512x512x512) and effectively adapt to various\ndownstream tasks as it is trained on large-scale datasets covering CT and MRI\nmodalities and different anatomical regions (from head to leg). Experimental\nresults demonstrate that 3D MedDiffusion surpasses state-of-the-art methods in\ngenerative quality and exhibits strong generalizability across tasks such as\nsparse-view CT reconstruction, fast MRI reconstruction, and data augmentation.\n","authors":["Haoshen Wang","Zhentao Liu","Kaicong Sun","Xiaodong Wang","Dinggang Shen","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2412.13059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13058v1","updated":"2024-12-17T16:22:56Z","published":"2024-12-17T16:22:56Z","title":"CondiMen: Conditional Multi-Person Mesh Recovery","summary":"  Multi-person human mesh recovery (HMR) consists in detecting all individuals\nin a given input image, and predicting the body shape, pose, and 3D location\nfor each detected person. The dominant approaches to this task rely on neural\nnetworks trained to output a single prediction for each detected individual. In\ncontrast, we propose CondiMen, a method that outputs a joint parametric\ndistribution over likely poses, body shapes, intrinsics and distances to the\ncamera, using a Bayesian network. This approach offers several advantages.\nFirst, a probability distribution can handle some inherent ambiguities of this\ntask -- such as the uncertainty between a person's size and their distance to\nthe camera, or simply the loss of information when projecting 3D data onto the\n2D image plane. Second, the output distribution can be combined with additional\ninformation to produce better predictions, by using e.g. known camera or body\nshape parameters, or by exploiting multi-view observations. Third, one can\nefficiently extract the most likely predictions from the output distribution,\nmaking our proposed approach suitable for real-time applications. Empirically\nwe find that our model i) achieves performance on par with or better than the\nstate-of-the-art, ii) captures uncertainties and correlations inherent in pose\nestimation and iii) can exploit additional information at test time, such as\nmulti-view consistency or body shape priors. CondiMen spices up the modeling of\nambiguity, using just the right ingredients on hand.\n","authors":["Brégier Romain","Baradel Fabien","Lucas Thomas","Galaaoui Salma","Armando Matthieu","Weinzaepfel Philippe","Rogez Grégory"],"pdf_url":"https://arxiv.org/pdf/2412.13058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16268v2","updated":"2024-12-17T16:22:55Z","published":"2024-10-21T17:59:19Z","title":"SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a\n  Training-Free Memory Tree","summary":"  The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation\nmodel for object segmentation in both images and videos, paving the way for\nvarious downstream video applications. The crucial design of SAM 2 for video\nsegmentation is its memory module, which prompts object-aware memories from\nprevious frames for current frame prediction. However, its greedy-selection\nmemory design suffers from the \"error accumulation\" problem, where an errored\nor missed mask will cascade and influence the segmentation of the subsequent\nframes, which limits the performance of SAM 2 toward complex long-term videos.\nTo this end, we introduce SAM2Long, an improved training-free video object\nsegmentation strategy, which considers the segmentation uncertainty within each\nframe and chooses the video-level optimal results from multiple segmentation\npathways in a constrained tree search manner. In practice, we maintain a fixed\nnumber of segmentation pathways throughout the video. For each frame, multiple\nmasks are proposed based on the existing pathways, creating various candidate\nbranches. We then select the same fixed number of branches with higher\ncumulative scores as the new pathways for the next frame. After processing the\nfinal frame, the pathway with the highest cumulative score is chosen as the\nfinal segmentation result. Benefiting from its heuristic search design,\nSAM2Long is robust toward occlusions and object reappearances, and can\neffectively segment and track objects for complex long-term videos. Notably,\nSAM2Long achieves an average improvement of 3.0 points across all 24\nhead-to-head comparisons, with gains of up to 5.3 points in J&F on long-term\nvideo object segmentation benchmarks such as SA-V and LVOS. The code is\nreleased at https://github.com/Mark12Ding/SAM2Long.\n","authors":["Shuangrui Ding","Rui Qian","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Yuwei Guo","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16268v2.pdf","comment":"update results including single VOT, Project page:\n  https://mark12ding.github.io/project/SAM2Long/"},{"id":"http://arxiv.org/abs/2408.08495v2","updated":"2024-12-17T16:21:31Z","published":"2024-08-16T02:33:55Z","title":"FunEditor: Achieving Complex Image Edits via Function Aggregation with\n  Diffusion Models","summary":"  Diffusion models have demonstrated outstanding performance in generative\ntasks, making them ideal candidates for image editing. Recent studies highlight\ntheir ability to apply desired edits effectively by following textual\ninstructions, yet with two key challenges remaining. First, these models\nstruggle to apply multiple edits simultaneously, resulting in computational\ninefficiencies due to their reliance on sequential processing. Second, relying\non textual prompts to determine the editing region can lead to unintended\nalterations to the image. We introduce FunEditor, an efficient diffusion model\ndesigned to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. Our experiments demonstrate that FunEditor\nsignificantly outperforms recent inference-time optimization methods and\nfine-tuned models, either quantitatively across various metrics or through\nvisual comparisons or both, on complex tasks like object movement and object\npasting. In the meantime, with only 4 steps of inference, FunEditor achieves\n5-24x inference speedups over existing popular methods. The code is available\nat: mhmdsmdi.github.io/funeditor/.\n","authors":["Mohammadreza Samadi","Fred X. Han","Mohammad Salameh","Hao Wu","Fengyu Sun","Chunhua Zhou","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2408.08495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13050v1","updated":"2024-12-17T16:13:56Z","published":"2024-12-17T16:13:56Z","title":"Modality-Inconsistent Continual Learning of Multimodal Large Language\n  Models","summary":"  In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),\na new continual learning scenario for Multimodal Large Language Models (MLLMs)\nthat involves tasks with inconsistent modalities (image, audio, or video) and\nvarying task types (captioning or question-answering). Unlike existing\nvision-only or modality-incremental settings, MICL combines modality and task\ntype shifts, both of which drive catastrophic forgetting. To address these\nchallenges, we propose MoInCL, which employs a Pseudo Targets Generation Module\nto mitigate forgetting caused by task type shifts in previously seen\nmodalities. It also incorporates Instruction-based Knowledge Distillation to\npreserve the model's ability to handle previously learned modalities when new\nones are introduced. We benchmark MICL using a total of six tasks and conduct\nexperiments to validate the effectiveness of our proposed MoInCL. The\nexperimental results highlight the superiority of MoInCL, showing significant\nimprovements over representative and state-of-the-art continual learning\nbaselines.\n","authors":["Weiguo Pian","Shijian Deng","Shentong Mo","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10488v2","updated":"2024-12-17T16:13:15Z","published":"2024-12-13T15:24:11Z","title":"SVGBuilder: Component-Based Colored SVG Generation with Text-Guided\n  Autoregressive Transformers","summary":"  Scalable Vector Graphics (SVG) are essential XML-based formats for versatile\ngraphics, offering resolution independence and scalability. Unlike raster\nimages, SVGs use geometric shapes and support interactivity, animation, and\nmanipulation via CSS and JavaScript. Current SVG generation methods face\nchallenges related to high computational costs and complexity. In contrast,\nhuman designers use component-based tools for efficient SVG creation. Inspired\nby this, SVGBuilder introduces a component-based, autoregressive model for\ngenerating high-quality colored SVGs from textual input. It significantly\nreduces computational overhead and improves efficiency compared to traditional\nmethods. Our model generates SVGs up to 604 times faster than\noptimization-based approaches. To address the limitations of existing SVG\ndatasets and support our research, we introduce ColorSVG-100K, the first\nlarge-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset\nfills the gap in color information for SVG generation models and enhances\ndiversity in model training. Evaluation against state-of-the-art models\ndemonstrates SVGBuilder's superior performance in practical applications,\nhighlighting its efficiency and quality in generating complex SVG graphics.\n","authors":["Zehao Chen","Rong Pan"],"pdf_url":"https://arxiv.org/pdf/2412.10488v2.pdf","comment":"Project: https://svgbuilder.github.io"},{"id":"http://arxiv.org/abs/2412.13047v1","updated":"2024-12-17T16:11:14Z","published":"2024-12-17T16:11:14Z","title":"EOGS: Gaussian Splatting for Earth Observation","summary":"  Recently, Gaussian splatting has emerged as a strong alternative to NeRF,\ndemonstrating impressive 3D modeling capabilities while requiring only a\nfraction of the training and rendering time. In this paper, we show how the\nstandard Gaussian splatting framework can be adapted for remote sensing,\nretaining its high efficiency. This enables us to achieve state-of-the-art\nperformance in just a few minutes, compared to the day-long optimization\nrequired by the best-performing NeRF-based Earth observation methods. The\nproposed framework incorporates remote-sensing improvements from EO-NeRF, such\nas radiometric correction and shadow modeling, while introducing novel\ncomponents, including sparsity, view consistency, and opacity regularizations.\n","authors":["Luca Savant Aira","Gabriele Facciolo","Thibaud Ehret"],"pdf_url":"https://arxiv.org/pdf/2412.13047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07841v2","updated":"2024-12-17T15:54:07Z","published":"2024-07-10T17:00:57Z","title":"Benchmarking Embedding Aggregation Methods in Computational Pathology: A\n  Clinical Data Perspective","summary":"  Recent advances in artificial intelligence (AI), in particular\nself-supervised learning of foundation models (FMs), are revolutionizing\nmedical imaging and computational pathology (CPath). A constant challenge in\nthe analysis of digital Whole Slide Images (WSIs) is the problem of aggregating\ntens of thousands of tile-level image embeddings to a slide-level\nrepresentation. Due to the prevalent use of datasets created for genomic\nresearch, such as TCGA, for method development, the performance of these\ntechniques on diagnostic slides from clinical practice has been inadequately\nexplored. This study conducts a thorough benchmarking analysis of ten\nslide-level aggregation techniques across nine clinically relevant tasks,\nincluding diagnostic assessment, biomarker classification, and outcome\nprediction. The results yield following key insights: (1) Embeddings derived\nfrom domain-specific (histological images) FMs outperform those from generic\nImageNet-based models across aggregation methods. (2) Spatial-aware aggregators\nenhance the performance significantly when using ImageNet pre-trained models\nbut not when using FMs. (3) No single model excels in all tasks and\nspatially-aware models do not show general superiority as it would be expected.\nThese findings underscore the need for more adaptable and universally\napplicable aggregation techniques, guiding future research towards tools that\nbetter meet the evolving needs of clinical-AI in pathology. The code used in\nthis work is available at\n\\url{https://github.com/fuchs-lab-public/CPath_SABenchmark}.\n","authors":["Shengjia Chen","Gabriele Campanella","Abdulkadir Elmas","Aryeh Stock","Jennifer Zeng","Alexandros D. Polydorides","Adam J. Schoenfeld","Kuan-lin Huang","Jane Houldsworth","Chad Vanderbilt","Thomas J. Fuchs"],"pdf_url":"https://arxiv.org/pdf/2407.07841v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13026v1","updated":"2024-12-17T15:48:25Z","published":"2024-12-17T15:48:25Z","title":"NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation","summary":"  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n","authors":["Karan Wanchoo","Xiaoye Zuo","Hannah Gonzalez","Soham Dan","Georgios Georgakis","Dan Roth","Kostas Daniilidis","Eleni Miltsakaki"],"pdf_url":"https://arxiv.org/pdf/2412.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13017v1","updated":"2024-12-17T15:36:55Z","published":"2024-12-17T15:36:55Z","title":"A New Adversarial Perspective for LiDAR-based 3D Object Detection","summary":"  Autonomous vehicles (AVs) rely on LiDAR sensors for environmental perception\nand decision-making in driving scenarios. However, ensuring the safety and\nreliability of AVs in complex environments remains a pressing challenge. To\naddress this issue, we introduce a real-world dataset (ROLiD) comprising\nLiDAR-scanned point clouds of two random objects: water mist and smoke. In this\npaper, we introduce a novel adversarial perspective by proposing an attack\nframework that utilizes water mist and smoke to simulate environmental\ninterference. Specifically, we propose a point cloud sequence generation method\nusing a motion and content decomposition generative adversarial network named\nPCS-GAN to simulate the distribution of random objects. Furthermore, leveraging\nthe simulated LiDAR scanning characteristics implemented with Range Image, we\nexamine the effects of introducing random object perturbations at various\npositions on the target vehicle. Extensive experiments demonstrate that\nadversarial perturbations based on random objects effectively deceive vehicle\ndetection and reduce the recognition rate of 3D object detection models.\n","authors":["Shijun Zheng","Weiquan Liu","Yu Guo","Yu Zang","Siqi Shen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13017v1.pdf","comment":"11 pages, 7 figures, AAAI2025"},{"id":"http://arxiv.org/abs/2412.13010v1","updated":"2024-12-17T15:32:12Z","published":"2024-12-17T15:32:12Z","title":"Measurement of Medial Elbow Joint Space using Landmark Detection","summary":"  Ultrasound imaging of the medial elbow is crucial for the early\nidentification of Ulnar Collateral Ligament (UCL) injuries. Specifically,\nmeasuring the elbow joint space in ultrasound images is used to assess the\nvalgus instability of elbow. To automate this measurement, a precisely\nannotated dataset is necessary; however, no publicly available dataset has been\nproposed thus far. This study introduces a novel ultrasound medial elbow\ndataset for measuring joint space to diagnose Ulnar Collateral Ligament (UCL)\ninjuries. The dataset comprises 4,201 medial elbow ultrasound images from 22\nsubjects, with landmark annotations on the humerus and ulna. The annotations\nare made precisely by the authors under the supervision of three orthopedic\nsurgeons. We evaluated joint space measurement methods using our proposed\ndataset with several landmark detection approaches, including ViTPose, HRNet,\nPCT, YOLOv8, and U-Net. In addition, we propose using Shape Subspace (SS) for\nlandmark refinement in heatmap-based landmark detection. The results show that\nthe mean Euclidean distance error of joint space is 0.116 mm when using HRNet.\nFurthermore, the SS landmark refinement improves the mean absolute error of\nlandmark positions by 0.010 mm with HRNet and by 0.103 mm with ViTPose on\naverage. These highlight the potential for high-precision, real-time diagnosis\nof UCL injuries and associated risks, which could be leveraged in large-scale\nscreening. Lastly, we demonstrate point-based segmentation of the humerus and\nulna using the detected landmarks as input. The dataset will be made publicly\navailable upon acceptance of this paper at:\nhttps://github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset.\n","authors":["Shizuka Akahori","Shotaro Teruya","Pragyan Shrestha","Yuichi Yoshii","Ryuhei Michinobu","Satoshi Iizuka","Itaru Kitahara"],"pdf_url":"https://arxiv.org/pdf/2412.13010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05966v3","updated":"2024-12-17T15:31:49Z","published":"2024-03-09T17:17:07Z","title":"Can Generative Models Improve Self-Supervised Representation Learning?","summary":"  The rapid advancement in self-supervised representation learning has\nhighlighted its potential to leverage unlabeled data for learning rich visual\nrepresentations. However, the existing techniques, particularly those employing\ndifferent augmentations of the same image, often rely on a limited set of\nsimple transformations that cannot fully capture variations in the real world.\nThis constrains the diversity and quality of samples, which leads to\nsub-optimal representations. In this paper, we introduce a framework that\nenriches the self-supervised learning (SSL) paradigm by utilizing generative\nmodels to produce semantically consistent image augmentations. By directly\nconditioning generative models on a source image, our method enables the\ngeneration of diverse augmentations while maintaining the semantics of the\nsource image, thus offering a richer set of data for SSL. Our extensive\nexperimental results on various joint-embedding SSL techniques demonstrate that\nour framework significantly enhances the quality of learned visual\nrepresentations by up to 10\\% Top-1 accuracy in downstream tasks. This research\ndemonstrates that incorporating generative models into the joint-embedding SSL\nworkflow opens new avenues for exploring the potential of synthetic data. This\ndevelopment paves the way for more robust and versatile representation learning\ntechniques.\n","authors":["Sana Ayromlou","Vahid Reza Khazaie","Fereshteh Forghani","Arash Afkanpour"],"pdf_url":"https://arxiv.org/pdf/2403.05966v3.pdf","comment":"To be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13006v1","updated":"2024-12-17T15:26:15Z","published":"2024-12-17T15:26:15Z","title":"What is YOLOv6? A Deep Insight into the Object Detection Model","summary":"  This work explores the YOLOv6 object detection model in depth, concentrating\non its design framework, optimization techniques, and detection capabilities.\nYOLOv6's core elements consist of the EfficientRep Backbone for robust feature\nextraction and the Rep-PAN Neck for seamless feature aggregation, ensuring\nhigh-performance object detection. Evaluated on the COCO dataset, YOLOv6-N\nachieves 37.5\\% AP at 1187 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S reaches\n45.0\\% AP at 484 FPS, outperforming models like PPYOLOE-S, YOLOv5-S, YOLOX-S,\nand YOLOv8-S in the same class. Moreover, YOLOv6-M and YOLOv6-L also show\nbetter accuracy (50.0\\% and 52.8\\%) while maintaining comparable inference\nspeeds to other detectors. With an upgraded backbone and neck structure,\nYOLOv6-L6 delivers cutting-edge accuracy in real-time.\n","authors":["Athulya Sundaresan Geetha"],"pdf_url":"https://arxiv.org/pdf/2412.13006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12990v1","updated":"2024-12-17T15:07:50Z","published":"2024-12-17T15:07:50Z","title":"Future Aspects in Human Action Recognition: Exploring Emerging\n  Techniques and Ethical Influences","summary":"  Visual-based human action recognition can be found in various application\nfields, e.g., surveillance systems, sports analytics, medical assistive\ntechnologies, or human-robot interaction frameworks, and it concerns the\nidentification and classification of individuals' activities within a video.\nSince actions typically occur over a sequence of consecutive images, it is\nparticularly challenging due to the inclusion of temporal analysis, which\nintroduces an extra layer of complexity. However, although multiple approaches\ntry to handle temporal analysis, there are still difficulties because of their\ncomputational cost and lack of adaptability. Therefore, different types of\nvision data, containing transition information between consecutive images,\nprovided by next-generation hardware sensors will guide the robotics community\nin tackling the problem of human action recognition. On the other hand, while\nthere is a plethora of still-image datasets, that researchers can adopt to\ntrain new artificial intelligence models, videos representing human activities\nare of limited capabilities, e.g., small and unbalanced datasets or selected\nwithout control from multiple sources. To this end, generating new and\nrealistic synthetic videos is possible since labeling is performed throughout\nthe data creation process, while reinforcement learning techniques can permit\nthe avoidance of considerable dataset dependence. At the same time, human\nfactors' involvement raises ethical issues for the research community, as\ndoubts and concerns about new technologies already exist.\n","authors":["Antonios Gasteratos","Stavros N. Moutsis","Konstantinos A. Tsintotas","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2412.12990v1.pdf","comment":"2 pages, 1 figure, 40th Anniversary of the IEEE Conference on\n  Robotics and Automation (ICRA@40), Rotterdam, Netherlands | September 23-26,\n  2024"},{"id":"http://arxiv.org/abs/2205.10691v2","updated":"2024-12-17T15:04:46Z","published":"2022-05-21T23:04:20Z","title":"Producing Histopathology Phantom Images using Generative Adversarial\n  Networks to improve Tumor Detection","summary":"  Advance in medical imaging is an important part in deep learning research.\nOne of the goals of computer vision is development of a holistic, comprehensive\nmodel which can identify tumors from histology slides obtained via biopsies. A\nmajor problem that stands in the way is lack of data for a few cancer-types. In\nthis paper, we ascertain that data augmentation using GANs can be a viable\nsolution to reduce the unevenness in the distribution of different cancer types\nin our dataset. Our demonstration showed that a dataset augmented to a 50%\nincrease causes an increase in tumor detection from 80% to 87.5%\n","authors":["Vidit Gautam"],"pdf_url":"https://arxiv.org/pdf/2205.10691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12982v1","updated":"2024-12-17T15:01:35Z","published":"2024-12-17T15:01:35Z","title":"Stable Diffusion is a Natural Cross-Modal Decoder for Layered\n  AI-generated Image Compression","summary":"  Recent advances in Artificial Intelligence Generated Content (AIGC) have\ngarnered significant interest, accompanied by an increasing need to transmit\nand compress the vast number of AI-generated images (AIGIs). However, there is\na noticeable deficiency in research focused on compression methods for AIGIs.\nTo address this critical gap, we introduce a scalable cross-modal compression\nframework that incorporates multiple human-comprehensible modalities, designed\nto efficiently capture and relay essential visual information for AIGIs. In\nparticular, our framework encodes images into a layered bitstream consisting of\na semantic layer that delivers high-level semantic information through text\nprompts; a structural layer that captures spatial details using edge or\nskeleton maps; and a texture layer that preserves local textures via a\ncolormap. Utilizing Stable Diffusion as the backend, the framework effectively\nleverages these multimodal priors for image generation, effectively functioning\nas a decoder when these priors are encoded. Qualitative and quantitative\nresults show that our method proficiently restores both semantic and visual\ndetails, competing against baseline approaches at extremely low bitrates (\n<0.02 bpp). Additionally, our framework facilitates downstream editing\napplications without requiring full decoding, thereby paving a new direction\nfor future research in AIGI compression.\n","authors":["Ruijie Chen","Qi Mao","Zhengxue Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.12982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v1","updated":"2024-12-17T14:56:59Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Jingqun Tang","Xue-Mei Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12974v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12966v1","updated":"2024-12-17T14:51:13Z","published":"2024-12-17T14:51:13Z","title":"Fruit Deformity Classification through Single-Input and Multi-Input\n  Architectures based on CNN Models using Real and Synthetic Images","summary":"  The present study focuses on detecting the degree of deformity in fruits such\nas apples, mangoes, and strawberries during the process of inspecting their\nexternal quality, employing Single-Input and Multi-Input architectures based on\nconvolutional neural network (CNN) models using sets of real and synthetic\nimages. The datasets are segmented using the Segment Anything Model (SAM),\nwhich provides the silhouette of the fruits. Regarding the single-input\narchitecture, the evaluation of the CNN models is performed only with real\nimages, but a methodology is proposed to improve these results using a\npre-trained model with synthetic images. In the Multi-Input architecture,\nbranches with RGB images and fruit silhouettes are implemented as inputs for\nevaluating CNN models such as VGG16, MobileNetV2, and CIDIS. However, the\nresults revealed that the Multi-Input architecture with the MobileNetV2 model\nwas the most effective in identifying deformities in the fruits, achieving\naccuracies of 90\\%, 94\\%, and 92\\% for apples, mangoes, and strawberries,\nrespectively. In conclusion, the Multi-Input architecture with the MobileNetV2\nmodel is the most accurate for classifying levels of deformity in fruits.\n","authors":["Tommy D. Beltran","Raul J. Villao","Luis E. Chuquimarca","Boris X. Vintimilla","Sergio A. Velastin"],"pdf_url":"https://arxiv.org/pdf/2412.12966v1.pdf","comment":"15 pages, 9 figures, CIARP 2024"},{"id":"http://arxiv.org/abs/2412.00876v3","updated":"2024-12-17T14:45:12Z","published":"2024-12-01T16:32:31Z","title":"Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification","summary":"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n","authors":["Wenxuan Huang","Zijie Zhai","Yunhang Shen","Shaosheng Cao","Fei Zhao","Xiangfeng Xu","Zheyu Ye","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.00876v3.pdf","comment":"Code is available at https://github.com/Osilly/dynamic_llava"},{"id":"http://arxiv.org/abs/2403.15698v3","updated":"2024-12-17T14:39:07Z","published":"2024-03-23T03:23:29Z","title":"SceneX: Procedural Controllable Large-scale Scene Generation","summary":"  Developing comprehensive explicit world models is crucial for understanding\nand simulating real-world scenarios. Recently, Procedural Controllable\nGeneration (PCG) has gained significant attention in large-scale scene\ngeneration by enabling the creation of scalable, high-quality assets. However,\nPCG faces challenges such as limited modular diversity, high expertise\nrequirements, and challenges in managing the diverse elements and structures in\ncomplex scenes. In this paper, we introduce a large-scale scene generation\nframework, SceneX, which can automatically produce high-quality procedural\nmodels according to designers' textual descriptions. Specifically, the proposed\nmethod comprises two components, PCGHub and PCGPlanner. The former encompasses\nan extensive collection of accessible procedural assets and thousands of\nhand-craft API documents to perform as a standard protocol for PCG controller.\nThe latter aims to generate executable actions for Blender to produce\ncontrollable and precise 3D assets guided by the user's instructions. Extensive\nexperiments demonstrated the capability of our method in controllable\nlarge-scale scene generation, including nature scenes and unbounded cities, as\nwell as scene editing such as asset placement and season translation.\n","authors":["Mengqi Zhou","Yuxi Wang","Jun Hou","Shougao Zhang","Yiwei Li","Chuanchen Luo","Junran Peng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15698v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10702v2","updated":"2024-12-17T14:37:34Z","published":"2024-12-14T06:21:24Z","title":"Memory Efficient Matting with Adaptive Token Routing","summary":"  Transformer-based models have recently achieved outstanding performance in\nimage matting. However, their application to high-resolution images remains\nchallenging due to the quadratic complexity of global self-attention. To\naddress this issue, we propose MEMatte, a \\textbf{m}emory-\\textbf{e}fficient\n\\textbf{m}atting framework for processing high-resolution images. MEMatte\nincorporates a router before each global attention block, directing informative\ntokens to the global attention while routing other tokens to a Lightweight\nToken Refinement Module (LTRM). Specifically, the router employs a local-global\nstrategy to predict the routing probability of each token, and the LTRM\nutilizes efficient modules to simulate global attention. Additionally, we\nintroduce a Batch-constrained Adaptive Token Routing (BATR) mechanism, which\nallows each router to dynamically route tokens based on image content and the\nstages of attention block in the network. Furthermore, we construct an ultra\nhigh-resolution image matting dataset, UHR-395, comprising 35,500 training\nimages and 1,000 test images, with an average resolution of $4872\\times6017$.\nThis dataset is created by compositing 395 different alpha mattes across 11\ncategories onto various backgrounds, all with high-quality manual annotation.\nExtensive experiments demonstrate that MEMatte outperforms existing methods on\nboth high-resolution and real-world datasets, significantly reducing memory\nusage by approximately 88% and latency by 50% on the Composition-1K benchmark.\nOur code is available at https://github.com/linyiheng123/MEMatte.\n","authors":["Yiheng Lin","Yihan Hu","Chenyi Zhang","Ting Liu","Xiaochao Qu","Luoqi Liu","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2412.10702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12949v1","updated":"2024-12-17T14:29:12Z","published":"2024-12-17T14:29:12Z","title":"Synthetic Data Generation for Anomaly Detection on Table Grapes","summary":"  Early detection of illnesses and pest infestations in fruit cultivation is\ncritical for maintaining yield quality and plant health. Computer vision and\nrobotics are increasingly employed for the automatic detection of such issues,\nparticularly using data-driven solutions. However, the rarity of these problems\nmakes acquiring and processing the necessary data to train such algorithms a\nsignificant obstacle. One solution to this scarcity is the generation of\nsynthetic high-quality anomalous samples. While numerous methods exist for this\ntask, most require highly trained individuals for setup.\n  This work addresses the challenge of generating synthetic anomalies in an\nautomatic fashion that requires only an initial collection of normal and\nanomalous samples from the user - a task that is straightforward for farmers.\nWe demonstrate the approach in the context of table grape cultivation.\nSpecifically, based on the observation that normal berries present relatively\nsmooth surfaces, while defects result in more complex textures, we introduce a\nDual-Canny Edge Detection (DCED) filter. This filter emphasizes the additional\ntexture indicative of diseases, pest infestations, or other defects. Using\nsegmentation masks provided by the Segment Anything Model, we then select and\nseamlessly blend anomalous berries onto normal ones. We show that the proposed\ndataset augmentation technique improves the accuracy of an anomaly classifier\nfor table grapes and that the approach can be generalized to other fruit types.\n","authors":["Ionut Marian Motoi","Valerio Belli","Alberto Carpineto","Daniele Nardi","Thomas Alessandro Ciarfuglia"],"pdf_url":"https://arxiv.org/pdf/2412.12949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12944v1","updated":"2024-12-17T14:22:44Z","published":"2024-12-17T14:22:44Z","title":"Online optimisation for dynamic electrical impedance tomography","summary":"  Online optimisation studies the convergence of optimisation methods as the\ndata embedded in the problem changes. Based on this idea, we propose a primal\ndual online method for nonlinear time-discrete inverse problems. We analyse the\nmethod through regret theory and demonstrate its performance in real-time\nmonitoring of moving bodies in a fluid with Electrical Impedance Tomography\n(EIT). To do so, we also prove the second-order differentiability of the\nComplete Electrode Model (CEM) solution operator on $L^\\infty$.\n","authors":["Neil Dizon","Jyrki Jauhiainen","Tuomo Valkonen"],"pdf_url":"https://arxiv.org/pdf/2412.12944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16424v2","updated":"2024-12-17T14:17:51Z","published":"2024-07-23T12:21:23Z","title":"ESOD: Efficient Small Object Detection on High-Resolution Images","summary":"  Enlarging input images is a straightforward and effective approach to promote\nsmall object detection. However, simple image enlargement is significantly\nexpensive on both computations and GPU memory. In fact, small objects are\nusually sparsely distributed and locally clustered. Therefore, massive feature\nextraction computations are wasted on the non-target background area of images.\nRecent works have tried to pick out target-containing regions using an extra\nnetwork and perform conventional object detection, but the newly introduced\ncomputation limits their final performance. In this paper, we propose to reuse\nthe detector's backbone to conduct feature-level object-seeking and\npatch-slicing, which can avoid redundant feature extraction and reduce the\ncomputation cost. Incorporating a sparse detection head, we are able to detect\nsmall objects on high-resolution inputs (e.g., 1080P or larger) for superior\nperformance. The resulting Efficient Small Object Detection (ESOD) approach is\na generic framework, which can be applied to both CNN- and ViT-based detectors\nto save the computation and GPU memory costs. Extensive experiments demonstrate\nthe efficacy and efficiency of our method. In particular, our method\nconsistently surpasses the SOTA detectors by a large margin (e.g., 8% gains on\nAP) on the representative VisDrone, UAVDT, and TinyPerson datasets. Code is\navailable at https://github.com/alibaba/esod.\n","authors":["Kai Liu","Zhihang Fu","Sheng Jin","Ze Chen","Fan Zhou","Rongxin Jiang","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16424v2.pdf","comment":"This paper has been recerived by IEEE TIP 2024. Code is available at\n  https://github.com/alibaba/esod"},{"id":"http://arxiv.org/abs/2412.11974v2","updated":"2024-12-17T14:12:56Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v2.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.12932v1","updated":"2024-12-17T14:10:16Z","published":"2024-12-17T14:10:16Z","title":"CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have recently demonstrated amazing\nsuccess in multi-modal tasks, including advancements in Multi-modal\nChain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks\nstill follow a traditional paradigm with multi-modal input and text-modal\noutput, which leads to significant drawbacks such as missing visual operations\nand vague expressions. Motivated by this, we introduce a novel Chain of\nMulti-modal Thought (CoMT) benchmark to address these limitations. Different\nfrom the traditional MCoT benchmark, CoMT requires both multi-modal input and\nmulti-modal reasoning output, aiming to mimic human-like reasoning that\ninherently integrates visual operation. Specifically, CoMT consists of four\ncategories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and\n(4) Visual Selection to comprehensively explore complex visual operations and\nconcise expression in real scenarios. We evaluate various LVLMs and strategies\non CoMT, revealing some key insights into the capabilities and limitations of\nthe current approaches. We hope that CoMT can inspire more research on\nintroducing multi-modal generation into the reasoning process.\n","authors":["Zihui Cheng","Qiguang Chen","Jin Zhang","Hao Fei","Xiaocheng Feng","Wanxiang Che","Min Li","Libo Qin"],"pdf_url":"https://arxiv.org/pdf/2412.12932v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10908v2","updated":"2024-12-17T14:06:48Z","published":"2024-12-14T17:35:27Z","title":"Do large language vision models understand 3D shapes?","summary":"  Large vision language models (LVLM) are the leading A.I approach for\nachieving a general visual understanding of the world. Models such as GPT,\nClaude, Gemini, and LLama can use images to understand and analyze complex\nvisual scenes. 3D objects and shapes are the basic building blocks of the\nworld, recognizing them is a fundamental part of human perception. The goal of\nthis work is to test whether LVLMs truly understand 3D shapes by testing the\nmodels ability to identify and match objects of the exact same 3D shapes but\nwith different orientations and materials/textures. Test images were created\nusing CGI with a huge number of highly diverse objects, materials, and scenes.\nThe results of this test show that the ability of such models to match 3D\nshapes is significantly below humans but much higher than random guesses.\nSuggesting that the models have gained some abstract understanding of 3D shapes\nbut still trail far beyond humans in this task. Mainly it seems that the models\ncan easily identify the same object with a different orientation as well as\nmatching identical 3D shapes of the same orientation but with different\nmaterial textures. However, when both the object material and orientation are\nchanged, all models perform poorly relative to humans.\n","authors":["Sagi Eppel"],"pdf_url":"https://arxiv.org/pdf/2412.10908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12919v1","updated":"2024-12-17T13:51:56Z","published":"2024-12-17T13:51:56Z","title":"4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel\n  Reconstruction from Sparse-View Dynamic DSA Images","summary":"  Reconstructing 3D vessel structures from sparse-view dynamic digital\nsubtraction angiography (DSA) images enables accurate medical assessment while\nreducing radiation exposure. Existing methods often produce suboptimal results\nor require excessive computation time. In this work, we propose 4D radiative\nGaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently.\nIn detail, we represent the vessels with 4D radiative Gaussian kernels. Each\nkernel has time-invariant geometry parameters, including position, rotation,\nand scale, to model static vessel structures. The time-dependent central\nattenuation of each kernel is predicted from a compact neural network to\ncapture the temporal varying response of contrast agent flow. We splat these\nGaussian kernels to synthesize DSA images via X-ray rasterization and optimize\nthe model with real captured ones. The final 3D vessel volume is voxelized from\nthe well-trained kernels. Moreover, we introduce accumulated attenuation\npruning and bounded scaling activation to improve reconstruction quality.\nExtensive experiments on real-world patient data demonstrate that 4DRGS\nachieves impressive results in 5 minutes training, which is 32x faster than the\nstate-of-the-art method. This underscores the potential of 4DRGS for real-world\nclinics.\n","authors":["Zhentao Liu","Ruyi Zha","Huangxuan Zhao","Hongdong Li","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2412.12919v1.pdf","comment":"Zhentao Liu and Ruyi Zha made equal contributions"},{"id":"http://arxiv.org/abs/2412.06418v2","updated":"2024-12-17T13:49:59Z","published":"2024-12-09T11:51:28Z","title":"Continual Learning for Segment Anything Model Adaptation","summary":"  Although the current different types of SAM adaptation methods have achieved\npromising performance for various downstream tasks, such as prompt-based ones\nand adapter-based ones, most of them belong to the one-step adaptation\nparadigm. In real-world scenarios, we are generally confronted with the dynamic\nscenario where the data comes in a streaming manner. Driven by the practical\nneed, in this paper, we first propose a novel Continual SAM adaptation (CoSAM)\nbenchmark with 8 different task domains and carefully analyze the limitations\nof the existing SAM one-step adaptation methods in the continual segmentation\nscenario. Then we propose a novel simple-yet-effective Mixture of Domain\nAdapters (MoDA) algorithm which utilizes the Global Feature Tokens (GFT) and\nGlobal Assistant Tokens (GAT) modules to help the SAM encoder extract\nwell-separated features for different task domains, and then provide the\naccurate task-specific information for continual learning. Extensive\nexperiments demonstrate that our proposed MoDA obviously surpasses the existing\nclassic continual learning methods, as well as prompt-based and adapter-based\napproaches for continual segmentation. Moreover, after sequential learning on\nthe CoSAM benchmark with diverse data distributions, our MoDA maintains highly\ncompetitive results in the natural image domain, approaching the zero-shot\nperformance of the original SAM, demonstrating its superior capability in\nknowledge preservation. Notably, the proposed MoDA can be seamlessly integrated\ninto various one-step adaptation methods of SAM, which can consistently bring\nobvious performance gains. Code is available at\n\\url{https://github.com/yangjl1215/CoSAM}\n","authors":["Jinglong Yang","Yichen Wu","Jun Cen","Wenjian Huang","Hong Wang","Jianguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.06418v2.pdf","comment":"Code is available at \\url{https://github.com/yangjl1215/CoSAM}"},{"id":"http://arxiv.org/abs/2412.12912v1","updated":"2024-12-17T13:46:12Z","published":"2024-12-17T13:46:12Z","title":"Unsupervised Region-Based Image Editing of Denoising Diffusion Models","summary":"  Although diffusion models have achieved remarkable success in the field of\nimage generation, their latent space remains under-explored. Current methods\nfor identifying semantics within latent space often rely on external\nsupervision, such as textual information and segmentation masks. In this paper,\nwe propose a method to identify semantic attributes in the latent space of\npre-trained diffusion models without any further training. By projecting the\nJacobian of the targeted semantic region into a low-dimensional subspace which\nis orthogonal to the non-masked regions, our approach facilitates precise\nsemantic discovery and control over local masked areas, eliminating the need\nfor annotations. We conducted extensive experiments across multiple datasets\nand various architectures of diffusion models, achieving state-of-the-art\nperformance. In particular, for some specific face attributes, the performance\nof our proposed method even surpasses that of supervised approaches,\ndemonstrating its superior ability in editing local image properties.\n","authors":["Zixiang Li","Yue Song","Renshuai Tao","Xiaohong Jia","Yao Zhao","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10710v2","updated":"2024-12-17T13:41:32Z","published":"2024-12-14T06:50:10Z","title":"Virtual Trial Room with Computer Vision and Machine Learning","summary":"  Online shopping has revolutionized the retail industry, providing customers\nwith convenience and accessibility. However, customers often hesitate to\npurchase wearable products such as watches, jewelry, glasses, shoes, and\nclothes due to the lack of certainty regarding fit and suitability. This leads\nto significant return rates, causing problems for both customers and vendors.\nTo address this issue, a platform called the Virtual Trial Room with Computer\nVision and Machine Learning is designed which enables customers to easily check\nwhether a product will fit and suit them or not. To achieve this, an\nAI-generated 3D model of the human head was created from a single 2D image\nusing the DECA model. This 3D model was then superimposed with a custom-made 3D\nmodel of glass which is based on real-world measurements and fitted over the\nhuman head. To replicate the real-world look and feel, the model was retouched\nwith textures, lightness, and smoothness. Furthermore, a full-stack application\nwas developed utilizing various fornt-end and back-end technologies. This\napplication enables users to view 3D-generated results on the website,\nproviding an immersive and interactive experience.\n","authors":["Tulashi Prasad Joshi","Amrendra Kumar Yadav","Arjun Chhetri","Suraj Agrahari","Umesh Kanta Ghimire"],"pdf_url":"https://arxiv.org/pdf/2412.10710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12906v1","updated":"2024-12-17T13:32:04Z","published":"2024-12-17T13:32:04Z","title":"CATSplat: Context-Aware Transformer with Spatial Guidance for\n  Generalizable 3D Gaussian Splatting from A Single-View Image","summary":"  Recently, generalizable feed-forward methods based on 3D Gaussian Splatting\nhave gained significant attention for their potential to reconstruct 3D scenes\nusing finite resources. These approaches create a 3D radiance field,\nparameterized by per-pixel 3D Gaussian primitives, from just a few images in a\nsingle forward pass. However, unlike multi-view methods that benefit from\ncross-view correspondences, 3D scene reconstruction with a single-view image\nremains an underexplored area. In this work, we introduce CATSplat, a novel\ngeneralizable transformer-based framework designed to break through the\ninherent constraints in monocular settings. First, we propose leveraging\ntextual guidance from a visual-language model to complement insufficient\ninformation from a single image. By incorporating scene-specific contextual\ndetails from text embeddings through cross-attention, we pave the way for\ncontext-aware 3D scene reconstruction beyond relying solely on visual cues.\nMoreover, we advocate utilizing spatial guidance from 3D point features toward\ncomprehensive geometric understanding under single-view settings. With 3D\npriors, image features can capture rich structural insights for predicting 3D\nGaussians without multi-view techniques. Extensive experiments on large-scale\ndatasets demonstrate the state-of-the-art performance of CATSplat in\nsingle-view 3D scene reconstruction with high-quality novel view synthesis.\n","authors":["Wonseok Roh","Hwanhee Jung","Jong Wook Kim","Seunggwan Lee","Innfarn Yoo","Andreas Lugmayr","Seunggeun Chi","Karthik Ramani","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12902v1","updated":"2024-12-17T13:26:31Z","published":"2024-12-17T13:26:31Z","title":"DoPTA: Improving Document Layout Analysis using Patch-Text Alignment","summary":"  The advent of multimodal learning has brought a significant improvement in\ndocument AI. Documents are now treated as multimodal entities, incorporating\nboth textual and visual information for downstream analysis. However, works in\nthis space are often focused on the textual aspect, using the visual space as\nauxiliary information. While some works have explored pure vision based\ntechniques for document image understanding, they require OCR identified text\nas input during inference, or do not align with text in their learning\nprocedure. Therefore, we present a novel image-text alignment technique\nspecially designed for leveraging the textual information in document images to\nimprove performance on visual tasks. Our document encoder model DoPTA - trained\nwith this technique demonstrates strong performance on a wide range of document\nimage understanding tasks, without requiring OCR during inference. Combined\nwith an auxiliary reconstruction objective, DoPTA consistently outperforms\nlarger models, while using significantly lesser pre-training compute. DoPTA\nalso sets new state-of-the art results on D4LA, and FUNSD, two challenging\ndocument visual analysis benchmarks.\n","authors":["Nikitha SR","Tarun Ram Menta","Mausoom Sarkar"],"pdf_url":"https://arxiv.org/pdf/2412.12902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12892v1","updated":"2024-12-17T13:18:41Z","published":"2024-12-17T13:18:41Z","title":"SAUGE: Taming SAM for Uncertainty-Aligned Multi-Granularity Edge\n  Detection","summary":"  Edge labels are typically at various granularity levels owing to the varying\npreferences of annotators, thus handling the subjectivity of per-pixel labels\nhas been a focal point for edge detection. Previous methods often employ a\nsimple voting strategy to diminish such label uncertainty or impose a strong\nassumption of labels with a pre-defined distribution, e.g., Gaussian. In this\nwork, we unveil that the segment anything model (SAM) provides strong prior\nknowledge to model the uncertainty in edge labels. Our key insight is that the\nintermediate SAM features inherently correspond to object edges at various\ngranularities, which reflects different edge options due to uncertainty.\nTherefore, we attempt to align uncertainty with granularity by regressing\nintermediate SAM features from different layers to object edges at\nmulti-granularity levels. In doing so, the model can fully and explicitly\nexplore diverse ``uncertainties'' in a data-driven fashion. Specifically, we\ninject a lightweight module (~ 1.5% additional parameters) into the frozen SAM\nto progressively fuse and adapt its intermediate features to estimate edges\nfrom coarse to fine. It is crucial to normalize the granularity level of human\nedge labels to match their innate uncertainty. For this, we simply perform\nlinear blending to the real edge labels at hand to create pseudo labels with\nvarying granularities. Consequently, our uncertainty-aligned edge detector can\nflexibly produce edges at any desired granularity (including an optimal one).\nThanks to SAM, our model uniquely demonstrates strong generalizability for\ncross-dataset edge detection. Extensive experimental results on BSDS500,\nMuticue and NYUDv2 validate our model's superiority.\n","authors":["Xing Liufu","Chaolei Tan","Xiaotong Lin","Yonggang Qi","Jinxuan Li","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2412.12892v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12890v1","updated":"2024-12-17T13:17:19Z","published":"2024-12-17T13:17:19Z","title":"Suppressing Uncertainty in Gaze Estimation","summary":"  Uncertainty in gaze estimation manifests in two aspects: 1) low-quality\nimages caused by occlusion, blurriness, inconsistent eye movements, or even\nnon-face images; 2) incorrect labels resulting from the misalignment between\nthe labeled and actual gaze points during the annotation process. Allowing\nthese uncertainties to participate in training hinders the improvement of gaze\nestimation. To tackle these challenges, in this paper, we propose an effective\nsolution, named Suppressing Uncertainty in Gaze Estimation (SUGE), which\nintroduces a novel triplet-label consistency measurement to estimate and reduce\nthe uncertainties. Specifically, for each training sample, we propose to\nestimate a novel ``neighboring label'' calculated by a linearly weighted\nprojection from the neighbors to capture the similarity relationship between\nimage features and their corresponding labels, which can be incorporated with\nthe predicted pseudo label and ground-truth label for uncertainty estimation.\nBy modeling such triplet-label consistency, we can measure the qualities of\nboth images and labels, and further largely reduce the negative effects of\nunqualified images and wrong labels through our designed sample weighting and\nlabel correction strategies. Experimental results on the gaze estimation\nbenchmarks indicate that our proposed SUGE achieves state-of-the-art\nperformance.\n","authors":["Shijing Wang","Yaping Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12890v1.pdf","comment":"This paper has been accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2412.12888v1","updated":"2024-12-17T13:12:31Z","published":"2024-12-17T13:12:31Z","title":"ArtAug: Enhancing Text-to-Image Generation through\n  Synthesis-Understanding Interaction","summary":"  The emergence of diffusion models has significantly advanced image synthesis.\nThe recent studies of model interaction and self-corrective reasoning approach\nin large language models offer new insights for enhancing text-to-image models.\nInspired by these studies, we propose a novel method called ArtAug for\nenhancing text-to-image models in this paper. To the best of our knowledge,\nArtAug is the first one that improves image synthesis models via model\ninteractions with understanding models. In the interactions, we leverage human\npreferences implicitly learned by image understanding models to provide\nfine-grained suggestions for image synthesis models. The interactions can\nmodify the image content to make it aesthetically pleasing, such as adjusting\nexposure, changing shooting angles, and adding atmospheric effects. The\nenhancements brought by the interaction are iteratively fused into the\nsynthesis model itself through an additional enhancement module. This enables\nthe synthesis model to directly produce aesthetically pleasing images without\nany extra computational cost. In the experiments, we train the ArtAug\nenhancement module on existing text-to-image models. Various evaluation metrics\nconsistently demonstrate that ArtAug enhances the generative capabilities of\ntext-to-image models without incurring additional computational costs. The\nsource code and models will be released publicly.\n","authors":["Zhongjie Duan","Qianyi Zhao","Cen Chen","Daoyuan Chen","Wenmeng Zhou","Yaliang Li","Yingda Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12888v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.12887v1","updated":"2024-12-17T13:11:48Z","published":"2024-12-17T13:11:48Z","title":"Learning Coarse-to-Fine Pruning of Graph Convolutional Networks for\n  Skeleton-based Recognition","summary":"  Magnitude Pruning is a staple lightweight network design method which seeks\nto remove connections with the smallest magnitude. This process is either\nachieved in a structured or unstructured manner. While structured pruning\nallows reaching high efficiency, unstructured one is more flexible and leads to\nbetter accuracy, but this is achieved at the expense of low computational\nperformance. In this paper, we devise a novel coarse-to-fine (CTF) method that\ngathers the advantages of structured and unstructured pruning while discarding\ntheir inconveniences to some extent. Our method relies on a novel CTF\nparametrization that models the mask of each connection as the Hadamard product\ninvolving four parametrizations which capture channel-wise, column-wise,\nrow-wise and entry-wise pruning respectively. Hence, fine-grained pruning is\nenabled only when the coarse-grained one is disabled, and this leads to highly\nefficient networks while being effective. Extensive experiments conducted on\nthe challenging task of skeleton-based recognition, using the standard SBU and\nFPHA datasets, show the clear advantage of our CTF approach against different\nbaselines as well as the related work.\n","authors":["Hichem Sahbi"],"pdf_url":"https://arxiv.org/pdf/2412.12887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14322v2","updated":"2024-12-17T13:02:10Z","published":"2024-11-21T17:12:47Z","title":"SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting\n  and Dense Feature Matching","summary":"  Experience Goal Visual Rearrangement task stands as a foundational challenge\nwithin Embodied AI, requiring an agent to construct a robust world model that\naccurately captures the goal state. The agent uses this world model to restore\na shuffled scene to its original configuration, making an accurate\nrepresentation of the world essential for successfully completing the task. In\nthis work, we present a novel framework that leverages on 3D Gaussian Splatting\nas a 3D scene representation for experience goal visual rearrangement task.\nRecent advances in volumetric scene representation like 3D Gaussian Splatting,\noffer fast rendering of high quality and photo-realistic novel views. Our\napproach enables the agent to have consistent views of the current and the goal\nsetting of the rearrangement task, which enables the agent to directly compare\nthe goal state and the shuffled state of the world in image space. To compare\nthese views, we propose to use a dense feature matching method with visual\nfeatures extracted from a foundation model, leveraging its advantages of a more\nuniversal feature representation, which facilitates robustness, and\ngeneralization. We validate our approach on the AI2-THOR rearrangement\nchallenge benchmark and demonstrate improvements over the current state of the\nart methods\n","authors":["Arjun P S","Andrew Melnik","Gora Chand Nandi"],"pdf_url":"https://arxiv.org/pdf/2411.14322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12877v1","updated":"2024-12-17T13:00:04Z","published":"2024-12-17T13:00:04Z","title":"MIVE: New Design and Benchmark for Multi-Instance Video Editing","summary":"  Recent AI-based video editing has enabled users to edit videos through simple\ntext prompts, significantly simplifying the editing process. However, recent\nzero-shot video editing techniques primarily focus on global or single-object\nedits, which can lead to unintended changes in other parts of the video. When\nmultiple objects require localized edits, existing methods face challenges,\nsuch as unfaithful editing, editing leakage, and lack of suitable evaluation\ndatasets and metrics. To overcome these limitations, we propose a zero-shot\n$\\textbf{M}$ulti-$\\textbf{I}$nstance $\\textbf{V}$ideo $\\textbf{E}$diting\nframework, called MIVE. MIVE is a general-purpose mask-based framework, not\ndedicated to specific objects (e.g., people). MIVE introduces two key modules:\n(i) Disentangled Multi-instance Sampling (DMS) to prevent editing leakage and\n(ii) Instance-centric Probability Redistribution (IPR) to ensure precise\nlocalization and faithful editing. Additionally, we present our new MIVE\nDataset featuring diverse video scenarios and introduce the Cross-Instance\nAccuracy (CIA) Score to evaluate editing leakage in multi-instance video\nediting tasks. Our extensive qualitative, quantitative, and user study\nevaluations demonstrate that MIVE significantly outperforms recent\nstate-of-the-art methods in terms of editing faithfulness, accuracy, and\nleakage prevention, setting a new benchmark for multi-instance video editing.\nThe project page is available at https://kaist-viclab.github.io/mive-site/\n","authors":["Samuel Teodoro","Agus Gunawan","Soo Ye Kim","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12877v1.pdf","comment":"The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors. Please visit our project page at\n  https://kaist-viclab.github.io/mive-site/"},{"id":"http://arxiv.org/abs/2312.16476v6","updated":"2024-12-17T12:55:57Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduces\nattention-based primitive control and an attention-mask loss function for\neffective control and manipulation of individual elements. Additionally, we\npropose a Vectorized Particle-based Score Distillation (VPSD) approach to\naddress issues of shape over-smoothing, color over-saturation, limited\ndiversity, and slow convergence of the existing text-to-SVG generation methods\nby modeling SVGs as distributions of control points and colors. Furthermore,\nVPSD leverages a reward model to re-weight vector particles, which improves\naesthetic appeal and accelerates convergence. Extensive experiments are\nconducted to validate the effectiveness of SVGDreamer, demonstrating its\nsuperiority over baseline methods in terms of editability, visual quality, and\ndiversity. Project page: https://ximinng.github.io/SVGDreamer-project/\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v6.pdf","comment":"Accepted by CVPR 2024. project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2412.12861v1","updated":"2024-12-17T12:43:10Z","published":"2024-12-17T12:43:10Z","title":"Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera","summary":"  We propose Dyn-HaMR, to the best of our knowledge, the first approach to\nreconstruct 4D global hand motion from monocular videos recorded by dynamic\ncameras in the wild. Reconstructing accurate 3D hand meshes from monocular\nvideos is a crucial task for understanding human behaviour, with significant\napplications in augmented and virtual reality (AR/VR). However, existing\nmethods for monocular hand reconstruction typically rely on a weak perspective\ncamera model, which simulates hand motion within a limited camera frustum. As a\nresult, these approaches struggle to recover the full 3D global trajectory and\noften produce noisy or incorrect depth estimations, particularly when the video\nis captured by dynamic or moving cameras, which is common in egocentric\nscenarios. Our Dyn-HaMR consists of a multi-stage, multi-objective optimization\npipeline, that factors in (i) simultaneous localization and mapping (SLAM) to\nrobustly estimate relative camera motion, (ii) an interacting-hand prior for\ngenerative infilling and to refine the interaction dynamics, ensuring plausible\nrecovery under (self-)occlusions, and (iii) hierarchical initialization through\na combination of state-of-the-art hand tracking methods. Through extensive\nevaluations on both in-the-wild and indoor datasets, we show that our approach\nsignificantly outperforms state-of-the-art methods in terms of 4D global mesh\nrecovery. This establishes a new benchmark for hand motion reconstruction from\nmonocular video with moving cameras. Our project page is at\nhttps://dyn-hamr.github.io/.\n","authors":["Zhengdi Yu","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2412.12861v1.pdf","comment":"Project page is available at https://dyn-hamr.github.io/"},{"id":"http://arxiv.org/abs/2412.12853v1","updated":"2024-12-17T12:29:32Z","published":"2024-12-17T12:29:32Z","title":"Automatic Left Ventricular Cavity Segmentation via Deep Spatial\n  Sequential Network in 4D Computed Tomography Studies","summary":"  Automated segmentation of left ventricular cavity (LVC) in temporal cardiac\nimage sequences (multiple time points) is a fundamental requirement for\nquantitative analysis of its structural and functional changes. Deep learning\nbased methods for the segmentation of LVC are the state of the art; however,\nthese methods are generally formulated to work on single time points, and fails\nto exploit the complementary information from the temporal image sequences that\ncan aid in segmentation accuracy and consistency among the images across the\ntime points. Furthermore, these segmentation methods perform poorly in\nsegmenting the end-systole (ES) phase images, where the left ventricle deforms\nto the smallest irregular shape, and the boundary between the blood chamber and\nmyocardium becomes inconspicuous. To overcome these limitations, we propose a\nnew method to automatically segment temporal cardiac images where we introduce\na spatial sequential (SS) network to learn the deformation and motion\ncharacteristics of the LVC in an unsupervised manner; these characteristics\nwere then integrated with sequential context information derived from\nbi-directional learning (BL) where both chronological and reverse-chronological\ndirections of the image sequence were used. Our experimental results on a\ncardiac computed tomography (CT) dataset demonstrated that our\nspatial-sequential network with bi-directional learning (SS-BL) method\noutperformed existing methods for LVC segmentation. Our method was also applied\nto MRI cardiac dataset and the results demonstrated the generalizability of our\nmethod.\n","authors":["Yuyu Guo","Lei Bi","Zhengbin Zhu","David Dagan Feng","Ruiyan Zhang","Qian Wang","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12853v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2412.12850v1","updated":"2024-12-17T12:24:08Z","published":"2024-12-17T12:24:08Z","title":"Boosting Fine-Grained Visual Anomaly Detection with\n  Coarse-Knowledge-Aware Adversarial Learning","summary":"  Many unsupervised visual anomaly detection methods train an auto-encoder to\nreconstruct normal samples and then leverage the reconstruction error map to\ndetect and localize the anomalies. However, due to the powerful modeling and\ngeneralization ability of neural networks, some anomalies can also be well\nreconstructed, resulting in unsatisfactory detection and localization accuracy.\nIn this paper, a small coarsely-labeled anomaly dataset is first collected.\nThen, a coarse-knowledge-aware adversarial learning method is developed to\nalign the distribution of reconstructed features with that of normal features.\nThe alignment can effectively suppress the auto-encoder's reconstruction\nability on anomalies and thus improve the detection accuracy. Considering that\nanomalies often only occupy very small areas in anomalous images, a patch-level\nadversarial learning strategy is further developed. Although no patch-level\nanomalous information is available, we rigorously prove that by simply viewing\nany patch features from anomalous images as anomalies, the proposed\nknowledge-aware method can also align the distribution of reconstructed patch\nfeatures with the normal ones. Experimental results on four medical datasets\nand two industrial datasets demonstrate the effectiveness of our method in\nimproving the detection and localization performance.\n","authors":["Qingqing Fang","Qinliang Su","Wenxi Lv","Wenchao Xu","Jianxing Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12850v1.pdf","comment":"The paper is accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12849v1","updated":"2024-12-17T12:23:07Z","published":"2024-12-17T12:23:07Z","title":"HyperGS: Hyperspectral 3D Gaussian Splatting","summary":"  We introduce HyperGS, a novel framework for Hyperspectral Novel View\nSynthesis (HNVS), based on a new latent 3D Gaussian Splatting (3DGS) technique.\nOur approach enables simultaneous spatial and spectral renderings by encoding\nmaterial properties from multi-view 3D hyperspectral datasets. HyperGS\nreconstructs high-fidelity views from arbitrary perspectives with improved\naccuracy and speed, outperforming currently existing methods. To address the\nchallenges of high-dimensional data, we perform view synthesis in a learned\nlatent space, incorporating a pixel-wise adaptive density function and a\npruning technique for increased training stability and efficiency.\nAdditionally, we introduce the first HNVS benchmark, implementing a number of\nnew baselines based on recent SOTA RGB-NVS techniques, alongside the small\nnumber of prior works on HNVS. We demonstrate HyperGS's robustness through\nextensive evaluation of real and simulated hyperspectral scenes with a 14db\naccuracy improvement upon previously published models.\n","authors":["Christopher Thirgood","Oscar Mendez","Erin Chao Ling","Jon Storey","Simon Hadfield"],"pdf_url":"https://arxiv.org/pdf/2412.12849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12843v1","updated":"2024-12-17T12:11:04Z","published":"2024-12-17T12:11:04Z","title":"Efficient Event-based Semantic Segmentation with Spike-driven\n  Lightweight Transformer-based Networks","summary":"  Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at least 7.30% and 3.30% mIoU,\nrespectively, with extremely 5.48x lower energy consumption and 1.14x faster\ninference speed.\n","authors":["Xiaxin Zhu","Fangming Guo","Xianlei Long","Qingyi Gu","Chao Chen","Fuqiang Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12843v1.pdf","comment":"Submitted to IEEE ICRA 2025"},{"id":"http://arxiv.org/abs/2409.03487v3","updated":"2024-12-17T11:57:52Z","published":"2024-09-05T12:52:24Z","title":"ScreenMark: Watermarking Arbitrary Visual Content on Screen","summary":"  Digital watermarking has shown its effectiveness in protecting multimedia\ncontent. However, existing watermarking is predominantly tailored for specific\nmedia types, rendering them less effective for the protection of content\ndisplayed on computer screens, which is often multi-modal and dynamic. Visual\nScreen Content (VSC), is particularly susceptible to theft and leakage through\nscreenshots, a vulnerability that current watermarking methods fail to\nadequately address.To address these challenges, we propose ScreenMark, a robust\nand practical watermarking method designed specifically for arbitrary VSC\nprotection. ScreenMark utilizes a three-stage progressive watermarking\nframework. Initially, inspired by diffusion principles, we initialize the\nmutual transformation between regular watermark information and irregular\nwatermark patterns. Subsequently, these patterns are integrated with screen\ncontent using a pre-multiplication alpha blending technique, supported by a\npre-trained screen decoder for accurate watermark retrieval. The progressively\ncomplex distorter enhances the robustness of the watermark in real-world\nscreenshot scenarios. Finally, the model undergoes fine-tuning guided by a\njoint-level distorter to ensure optimal performance. To validate the\neffectiveness of ScreenMark, we compiled a dataset comprising 100,000\nscreenshots from various devices and resolutions. Extensive experiments on\ndifferent datasets confirm the superior robustness, imperceptibility, and\npractical applicability of the method.\n","authors":["Xiujian Liang","Gaozhi Liu","Yichao Si","Xiaoxiao Hu","Zhenxing Qian"],"pdf_url":"https://arxiv.org/pdf/2409.03487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12833v1","updated":"2024-12-17T11:54:47Z","published":"2024-12-17T11:54:47Z","title":"FocusChat: Text-guided Long Video Understanding via Spatiotemporal\n  Information Filtering","summary":"  Recently, multi-modal large language models have made significant progress.\nHowever, visual information lacking of guidance from the user's intention may\nlead to redundant computation and involve unnecessary visual noise, especially\nin long, untrimmed videos. To address this issue, we propose FocusChat, a\ntext-guided multi-modal large language model (LLM) that emphasizes visual\ninformation correlated to the user's prompt. In detail, Our model first\nundergoes the semantic extraction module, which comprises a visual semantic\nbranch and a text semantic branch to extract image and text semantics,\nrespectively. The two branches are combined using the Spatial-Temporal\nFiltering Module (STFM). STFM enables explicit spatial-level information\nfiltering and implicit temporal-level feature filtering, ensuring that the\nvisual tokens are closely aligned with the user's query. It lowers the\nessential number of visual tokens inputted into the LLM. FocusChat\nsignificantly outperforms Video-LLaMA in zero-shot experiments, using an order\nof magnitude less training data with only 16 visual tokens occupied. It\nachieves results comparable to the state-of-the-art in few-shot experiments,\nwith only 0.72M pre-training data.\n","authors":["Zheng Cheng","Rendong Wang","Zhicheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12833v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.12830v1","updated":"2024-12-17T11:52:10Z","published":"2024-12-17T11:52:10Z","title":"Differential Alignment for Domain Adaptive Object Detection","summary":"  Domain adaptive object detection (DAOD) aims to generalize an object detector\ntrained on labeled source-domain data to a target domain without annotations,\nthe core principle of which is \\emph{source-target feature alignment}.\nTypically, existing approaches employ adversarial learning to align the\ndistributions of the source and target domains as a whole, barely considering\nthe varying significance of distinct regions, say instances under different\ncircumstances and foreground \\emph{vs} background areas, during feature\nalignment. To overcome the shortcoming, we investigates a differential feature\nalignment strategy. Specifically, a prediction-discrepancy feedback instance\nalignment module (dubbed PDFA) is designed to adaptively assign higher weights\nto instances of higher teacher-student detection discrepancy, effectively\nhandling heavier domain-specific information. Additionally, an\nuncertainty-based foreground-oriented image alignment module (UFOA) is proposed\nto explicitly guide the model to focus more on regions of interest. Extensive\nexperiments on widely-used DAOD datasets together with ablation studies are\nconducted to demonstrate the efficacy of our proposed method and reveal its\nsuperiority over other SOTA alternatives. Our code is available at\nhttps://github.com/EstrellaXyu/Differential-Alignment-for-DAOD.\n","authors":["Xinyu He","Xinhui Li","Xiaojie Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12830v1.pdf","comment":"11 pages, 8 figures, accepted by aaai25"},{"id":"http://arxiv.org/abs/2412.12829v1","updated":"2024-12-17T11:49:36Z","published":"2024-12-17T11:49:36Z","title":"2by2: Weakly-Supervised Learning for Global Action Segmentation","summary":"  This paper presents a simple yet effective approach for the poorly\ninvestigated task of global action segmentation, aiming at grouping frames\ncapturing the same action across videos of different activities. Unlike the\ncase of videos depicting all the same activity, the temporal order of actions\nis not roughly shared among all videos, making the task even more challenging.\nWe propose to use activity labels to learn, in a weakly-supervised fashion,\naction representations suitable for global action segmentation. For this\npurpose, we introduce a triadic learning approach for video pairs, to ensure\nintra-video action discrimination, as well as inter-video and inter-activity\naction association. For the backbone architecture, we use a Siamese network\nbased on sparse transformers that takes as input video pairs and determine\nwhether they belong to the same activity. The proposed approach is validated on\ntwo challenging benchmark datasets: Breakfast and YouTube Instructions,\noutperforming state-of-the-art methods.\n","authors":["Elena Bueno-Benito","Mariella Dimiccoli"],"pdf_url":"https://arxiv.org/pdf/2412.12829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12827v1","updated":"2024-12-17T11:47:59Z","published":"2024-12-17T11:47:59Z","title":"TabSniper: Towards Accurate Table Detection & Structure Recognition for\n  Bank Statements","summary":"  Extraction of transaction information from bank statements is required to\nassess one's financial well-being for credit rating and underwriting decisions.\nUnlike other financial documents such as tax forms or financial statements,\nextracting the transaction descriptions from bank statements can provide a\ncomprehensive and recent view into the cash flows and spending patterns. With\nmultiple variations in layout and templates across several banks, extracting\ntransactional level information from different table categories is an arduous\ntask. Existing table structure recognition approaches produce sub optimal\nresults for long, complex tables and are unable to capture all transactions\naccurately. This paper proposes TabSniper, a novel approach for efficient table\ndetection, categorization and structure recognition from bank statements. The\npipeline starts with detecting and categorizing tables of interest from the\nbank statements. The extracted table regions are then processed by the table\nstructure recognition model followed by a post-processing module to transform\nthe transactional data into a structured and standardised format. The detection\nand structure recognition architectures are based on DETR, fine-tuned with\ndiverse bank statements along with additional feature enhancements. Results on\nchallenging datasets demonstrate that TabSniper outperforms strong baselines\nand produces high-quality extraction of transaction information from bank and\nother financial documents across multiple layouts and templates.\n","authors":["Abhishek Trivedi","Sourajit Mukherjee","Rajat Kumar Singh","Vani Agarwal","Sriranjani Ramakrishnan","Himanshu S. Bhatt"],"pdf_url":"https://arxiv.org/pdf/2412.12827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12821v1","updated":"2024-12-17T11:41:49Z","published":"2024-12-17T11:41:49Z","title":"ComprehendEdit: A Comprehensive Dataset and Evaluation Framework for\n  Multimodal Knowledge Editing","summary":"  Large multimodal language models (MLLMs) have revolutionized natural language\nprocessing and visual understanding, but often contain outdated or inaccurate\ninformation. Current multimodal knowledge editing evaluations are limited in\nscope and potentially biased, focusing on narrow tasks and failing to assess\nthe impact on in-domain samples. To address these issues, we introduce\nComprehendEdit, a comprehensive benchmark comprising eight diverse tasks from\nmultiple datasets. We propose two novel metrics: Knowledge Generalization Index\n(KGI) and Knowledge Preservation Index (KPI), which evaluate editing effects on\nin-domain samples without relying on AI-synthetic samples. Based on insights\nfrom our framework, we establish Hierarchical In-Context Editing (HICE), a\nbaseline method employing a two-stage approach that balances performance across\nall metrics. This study provides a more comprehensive evaluation framework for\nmultimodal knowledge editing, reveals unique challenges in this field, and\noffers a baseline method demonstrating improved performance. Our work opens new\nperspectives for future research and provides a foundation for developing more\nrobust and effective editing techniques for MLLMs. The ComprehendEdit benchmark\nand implementation code are available at\nhttps://github.com/yaohui120/ComprehendEdit.\n","authors":["Yaohui Ma","Xiaopeng Hong","Shizhou Zhang","Huiyun Li","Zhilin Zhu","Wei Luo","Zhiheng Ma"],"pdf_url":"https://arxiv.org/pdf/2412.12821v1.pdf","comment":"Extended version for paper accepted to AAAI 2025. Project Page:\n  https://github.com/yaohui120/ComprehendEdit"},{"id":"http://arxiv.org/abs/2412.12801v1","updated":"2024-12-17T11:10:46Z","published":"2024-12-17T11:10:46Z","title":"Multi-View Incremental Learning with Structured Hebbian Plasticity for\n  Enhanced Fusion Efficiency","summary":"  The rapid evolution of multimedia technology has revolutionized human\nperception, paving the way for multi-view learning. However, traditional\nmulti-view learning approaches are tailored for scenarios with fixed data\nviews, falling short of emulating the intricate cognitive procedures of the\nhuman brain processing signals sequentially. Our cerebral architecture\nseamlessly integrates sequential data through intricate feed-forward and\nfeedback mechanisms. In stark contrast, traditional methods struggle to\ngeneralize effectively when confronted with data spanning diverse domains,\nhighlighting the need for innovative strategies that can mimic the brain's\nadaptability and dynamic integration capabilities. In this paper, we propose a\nbio-neurologically inspired multi-view incremental framework named MVIL aimed\nat emulating the brain's fine-grained fusion of sequentially arriving views.\nMVIL lies two fundamental modules: structured Hebbian plasticity and synaptic\npartition learning. The structured Hebbian plasticity reshapes the structure of\nweights to express the high correlation between view representations,\nfacilitating a fine-grained fusion of view representations. Moreover, synaptic\npartition learning is efficient in alleviating drastic changes in weights and\nalso retaining old knowledge by inhibiting partial synapses. These modules\nbionically play a central role in reinforcing crucial associations between\nnewly acquired information and existing knowledge repositories, thereby\nenhancing the network's capacity for generalization. Experimental results on\nsix benchmark datasets show MVIL's effectiveness over state-of-the-art methods.\n","authors":["Yuhong Chen","Ailin Song","Huifeng Yin","Shuai Zhong","Fuhai Chen","Qi Xu","Shiping Wang","Mingkun Xu"],"pdf_url":"https://arxiv.org/pdf/2412.12801v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.12799v1","updated":"2024-12-17T11:02:36Z","published":"2024-12-17T11:02:36Z","title":"RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential\n  Decoder for 3D Object Detection","summary":"  In radar-camera 3D object detection, the radar point clouds are sparse and\nnoisy, which causes difficulties in fusing camera and radar modalities. To\nsolve this, we introduce a novel query-based detection method named\nRadar-Camera Transformer (RCTrans). Specifically, we first design a Radar Dense\nEncoder to enrich the sparse valid radar tokens, and then concatenate them with\nthe image tokens. By doing this, we can fully explore the 3D information of\neach interest region and reduce the interference of empty tokens during the\nfusing stage. We then design a Pruning Sequential Decoder to predict 3D boxes\nbased on the obtained tokens and random initialized queries. To alleviate the\neffect of elevation ambiguity in radar point clouds, we gradually locate the\nposition of the object via a sequential fusion structure. It helps to get more\nprecise and flexible correspondences between tokens and queries. A pruning\ntraining strategy is adopted in the decoder, which can save much time during\ninference and inhibit queries from losing their distinctiveness. Extensive\nexperiments on the large-scale nuScenes dataset prove the superiority of our\nmethod, and we also achieve new state-of-the-art radar-camera 3D detection\nresults. Our implementation is available at https://github.com/liyih/RCTrans.\n","authors":["Yiheng Li","Yang Yang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2412.12799v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12798v1","updated":"2024-12-17T11:00:56Z","published":"2024-12-17T11:00:56Z","title":"ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation","summary":"  Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.\n","authors":["Shiqi Huang","Shuting He","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2412.12798v1.pdf","comment":"AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI"},{"id":"http://arxiv.org/abs/2412.12793v1","updated":"2024-12-17T10:56:18Z","published":"2024-12-17T10:56:18Z","title":"CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels","summary":"  Noisy labels threaten the robustness of few-shot learning (FSL) due to the\ninexact features in a new domain. CLIP, a large-scale vision-language model,\nperforms well in FSL on image-text embedding similarities, but it is\nsusceptible to misclassification caused by noisy labels. How to enhance domain\ngeneralization of CLIP on noisy data within FSL tasks is a critical challenge.\nIn this paper, we provide a novel view to mitigate the influence of noisy\nlabels, CLIP-based Robust Few-shot learning (CRoF). CRoF is a general plug-in\nmodule for CLIP-based models. To avoid misclassification and confused label\nembedding, we design the few-shot task-oriented prompt generator to give more\ndiscriminative descriptions of each category. The proposed prompt achieves\nlarger distances of inter-class textual embedding. Furthermore, rather than\nfully trusting zero-shot classification by CLIP, we fine-tune CLIP on noisy\nfew-shot data in a new domain with a weighting strategy like label-smooth. The\nweights for multiple potentially correct labels consider the relationship\nbetween CLIP's prior knowledge and original label information to ensure\nreliability. Our multiple label loss function further supports robust training\nunder this paradigm. Comprehensive experiments show that CRoF, as a plug-in,\noutperforms fine-tuned and vanilla CLIP models on different noise types and\nnoise ratios.\n","authors":["Shizhuo Deng","Bowen Han","Jiaqi Chen","Hao Wang","Dongyue Chen","Tong Jia"],"pdf_url":"https://arxiv.org/pdf/2412.12793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12791v1","updated":"2024-12-17T10:52:50Z","published":"2024-12-17T10:52:50Z","title":"Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning","summary":"  Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.\n","authors":["Shiping Ge","Qiang Chen","Zhiwei Jiang","Yafeng Yin","Liu Qin","Ziyao Chen","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12791v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12788v1","updated":"2024-12-17T10:47:13Z","published":"2024-12-17T10:47:13Z","title":"RA-SGG: Retrieval-Augmented Scene Graph Generation Framework via\n  Multi-Prototype Learning","summary":"  Scene Graph Generation (SGG) research has suffered from two fundamental\nchallenges: the long-tailed predicate distribution and semantic ambiguity\nbetween predicates. These challenges lead to a bias towards head predicates in\nSGG models, favoring dominant general predicates while overlooking fine-grained\npredicates. In this paper, we address the challenges of SGG by framing it as\nmulti-label classification problem with partial annotation, where relevant\nlabels of fine-grained predicates are missing. Under the new frame, we propose\nRetrieval-Augmented Scene Graph Generation (RA-SGG), which identifies potential\ninstances to be multi-labeled and enriches the single-label with multi-labels\nthat are semantically similar to the original label by retrieving relevant\nsamples from our established memory bank. Based on augmented relations (i.e.,\ndiscovered multi-labels), we apply multi-prototype learning to train our SGG\nmodel. Several comprehensive experiments have demonstrated that RA-SGG\noutperforms state-of-the-art baselines by up to 3.6% on VG and 5.9% on GQA,\nparticularly in terms of F@K, showing that RA-SGG effectively alleviates the\nissue of biased prediction caused by the long-tailed distribution and semantic\nambiguity of predicates.\n","authors":["Kanghoon Yoon","Kibum Kim","Jaehyung Jeon","Yeonjun In","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2412.12788v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2310.12848v2","updated":"2024-12-17T10:44:49Z","published":"2023-10-19T15:59:24Z","title":"Neural Degradation Representation Learning for All-In-One Image\n  Restoration","summary":"  Existing methods have demonstrated effective performance on a single\ndegradation type. In practical applications, however, the degradation is often\nunknown, and the mismatch between the model and the degradation will result in\na severe performance drop. In this paper, we propose an all-in-one image\nrestoration network that tackles multiple degradations. Due to the\nheterogeneous nature of different types of degradations, it is difficult to\nprocess multiple degradations in a single network. To this end, we propose to\nlearn a neural degradation representation (NDR) that captures the underlying\ncharacteristics of various degradations. The learned NDR decomposes different\ntypes of degradations adaptively, similar to a neural dictionary that\nrepresents basic degradation components. Subsequently, we develop a degradation\nquery module and a degradation injection module to effectively recognize and\nutilize the specific degradation based on NDR, enabling the all-in-one\nrestoration ability for multiple degradations. Moreover, we propose a\nbidirectional optimization strategy to effectively drive NDR to learn the\ndegradation representation by optimizing the degradation and restoration\nprocesses alternately. Comprehensive experiments on representative types of\ndegradations (including noise, haze, rain, and downsampling) demonstrate the\neffectiveness and generalization capability of our method.\n","authors":["Mingde Yao","Ruikang Xu","Yuanshen Guan","Jie Huang","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.12848v2.pdf","comment":"Code: https://github.com/mdyao/NDR-Restore"},{"id":"http://arxiv.org/abs/2412.12785v1","updated":"2024-12-17T10:44:47Z","published":"2024-12-17T10:44:47Z","title":"Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference","summary":"  Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Drawing inspiration from the concept of visual region in the human\nbrain, we investigate the existence of an analogous \\textit{visual region}\nwithin LLMs that functions as a cognitive core, and explore the possibility of\nefficient training of LVLMs via selective layers tuning. We use\nBunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B\nfor validation across a range of visual and textual tasks. Our findings reveal\nthat selectively updating 25\\% of LLMs layers, when sparsely and uniformly\ndistributed, can preserve nearly 99\\% of visual performance while maintaining\nor enhancing textual task results, and also effectively reducing training time.\nBased on this targeted training approach, we further propose a novel visual\nregion-based pruning paradigm, removing non-critical layers outside the visual\nregion, which can achieve minimal performance loss. This study offers an\neffective and efficient strategy for LVLM training and inference by activating\na layer-wise visual region within LLMs, which is consistently effective across\ndifferent models and parameter scales.\n","authors":["Siyuan Wang","Dianyi Wang","Chengxing Zhou","Zejun Li","Zhihao Fan","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.12785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12782v1","updated":"2024-12-17T10:42:19Z","published":"2024-12-17T10:42:19Z","title":"Bidirectional Logits Tree: Pursuing Granularity Reconcilement in\n  Fine-Grained Classification","summary":"  This paper addresses the challenge of Granularity Competition in fine-grained\nclassification tasks, which arises due to the semantic gap between\nmulti-granularity labels. Existing approaches typically develop independent\nhierarchy-aware models based on shared features extracted from a common base\nencoder. However, because coarse-grained levels are inherently easier to learn\nthan finer ones, the base encoder tends to prioritize coarse feature\nabstractions, which impedes the learning of fine-grained features. To overcome\nthis challenge, we propose a novel framework called the Bidirectional Logits\nTree (BiLT) for Granularity Reconcilement. The key idea is to develop\nclassifiers sequentially from the finest to the coarsest granularities, rather\nthan parallelly constructing a set of classifiers based on the same input\nfeatures. In this setup, the outputs of finer-grained classifiers serve as\ninputs for coarser-grained ones, facilitating the flow of hierarchical semantic\ninformation across different granularities. On top of this, we further\nintroduce an Adaptive Intra-Granularity Difference Learning (AIGDL) approach to\nuncover subtle semantic differences between classes within the same\ngranularity. Extensive experiments demonstrate the effectiveness of our\nproposed method.\n","authors":["Zhiguang Lu","Qianqian Xu","Shilong Bao","Zhiyong Yang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12778v1","updated":"2024-12-17T10:37:46Z","published":"2024-12-17T10:37:46Z","title":"Rethinking Diffusion-Based Image Generators for Fundus Fluorescein\n  Angiography Synthesis on Limited Data","summary":"  Fundus imaging is a critical tool in ophthalmology, with different imaging\nmodalities offering unique advantages. For instance, fundus fluorescein\nangiography (FFA) can accurately identify eye diseases. However, traditional\ninvasive FFA involves the injection of sodium fluorescein, which can cause\ndiscomfort and risks. Generating corresponding FFA images from non-invasive\nfundus images holds significant practical value but also presents challenges.\nFirst, limited datasets constrain the performance and effectiveness of models.\nSecond, previous studies have primarily focused on generating FFA for single\ndiseases or single modalities, often resulting in poor performance for patients\nwith various ophthalmic conditions. To address these issues, we propose a novel\nlatent diffusion model-based framework, Diffusion, which introduces a\nfine-tuning protocol to overcome the challenge of limited medical data and\nunleash the generative capabilities of diffusion models. Furthermore, we\ndesigned a new approach to tackle the challenges of generating across different\nmodalities and disease types. On limited datasets, our framework achieves\nstate-of-the-art results compared to existing methods, offering significant\npotential to enhance ophthalmic diagnostics and patient care. Our code will be\nreleased soon to support further research in this field.\n","authors":["Chengzhou Yu","Huihui Fang","Hongqiu Wang","Ting Deng","Qing Du","Yanwu Xu","Weihua Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12778v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.12774v1","updated":"2024-12-17T10:35:27Z","published":"2024-12-17T10:35:27Z","title":"A Framework for Critical Evaluation of Text-to-Image Models: Integrating\n  Art Historical Analysis, Artistic Exploration, and Critical Prompt\n  Engineering","summary":"  This paper proposes a novel interdisciplinary framework for the critical\nevaluation of text-to-image models, addressing the limitations of current\ntechnical metrics and bias studies. By integrating art historical analysis,\nartistic exploration, and critical prompt engineering, the framework offers a\nmore nuanced understanding of these models' capabilities and societal\nimplications. Art historical analysis provides a structured approach to examine\nvisual and symbolic elements, revealing potential biases and\nmisrepresentations. Artistic exploration, through creative experimentation,\nuncovers hidden potentials and limitations, prompting critical reflection on\nthe algorithms' assumptions. Critical prompt engineering actively challenges\nthe model's assumptions, exposing embedded biases. Case studies demonstrate the\nframework's practical application, showcasing how it can reveal biases related\nto gender, race, and cultural representation. This comprehensive approach not\nonly enhances the evaluation of text-to-image models but also contributes to\nthe development of more equitable, responsible, and culturally aware AI\nsystems.\n","authors":["Amalia Foka"],"pdf_url":"https://arxiv.org/pdf/2412.12774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12772v1","updated":"2024-12-17T10:33:36Z","published":"2024-12-17T10:33:36Z","title":"Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior","summary":"  Neural Radiance Fields (NeRF) have advanced photorealistic novel view\nsynthesis, but their reliance on photometric reconstruction introduces\nartifacts, commonly known as \"floaters\". These artifacts degrade novel view\nquality, especially in areas unseen by the training cameras. We present a fast,\npost-hoc NeRF cleanup method that eliminates such artifacts by enforcing our\nFree Space Prior, effectively minimizing floaters without disrupting the NeRF's\nrepresentation of observed regions. Unlike existing approaches that rely on\neither Maximum Likelihood (ML) estimation to fit the data or a complex, local\ndata-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,\nselecting the optimal model parameters under a simple global prior assumption\nthat unseen regions should remain empty. This enables our method to clean\nartifacts in both seen and unseen areas, enhancing novel view quality even in\nchallenging scene regions. Our method is comparable with existing NeRF cleanup\nmodels while being 2.5x faster in inference time, requires no additional memory\nbeyond the original NeRF, and achieves cleanup training in less than 30\nseconds. Our code will be made publically available.\n","authors":["Leo Segre","Shai Avidan"],"pdf_url":"https://arxiv.org/pdf/2412.12772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12771v1","updated":"2024-12-17T10:33:34Z","published":"2024-12-17T10:33:34Z","title":"Guided and Variance-Corrected Fusion with One-shot Style Alignment for\n  Large-Content Image Generation","summary":"  Producing large images using small diffusion models is gaining increasing\npopularity, as the cost of training large models could be prohibitive. A common\napproach involves jointly generating a series of overlapped image patches and\nobtaining large images by merging adjacent patches. However, results from\nexisting methods often exhibit obvious artifacts, e.g., seams and inconsistent\nobjects and styles. To address the issues, we proposed Guided Fusion (GF),\nwhich mitigates the negative impact from distant image regions by applying a\nweighted average to the overlapping regions. Moreover, we proposed\nVariance-Corrected Fusion (VCF), which corrects data variance at\npost-averaging, generating more accurate fusion for the Denoising Diffusion\nProbabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA),\nwhich generates a coherent style for large images by adjusting the initial\ninput noise without adding extra computational burden. Extensive experiments\ndemonstrated that the proposed fusion methods improved the quality of the\ngenerated image significantly. As a plug-and-play module, the proposed method\ncan be widely applied to enhance other fusion-based methods for large image\ngeneration.\n","authors":["Shoukun Sun","Min Xian","Tiankai Yao","Fei Xu","Luca Capriotti"],"pdf_url":"https://arxiv.org/pdf/2412.12771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12766v1","updated":"2024-12-17T10:31:03Z","published":"2024-12-17T10:31:03Z","title":"Towards a Training Free Approach for 3D Scene Editing","summary":"  Text driven diffusion models have shown remarkable capabilities in editing\nimages. However, when editing 3D scenes, existing works mostly rely on training\na NeRF for 3D editing. Recent NeRF editing methods leverages edit operations by\ndeploying 2D diffusion models and project these edits into 3D space. They\nrequire strong positional priors alongside text prompt to identify the edit\nlocation. These methods are operational on small 3D scenes and are more\ngeneralized to particular scene. They require training for each specific edit\nand cannot be exploited in real-time edits. To address these limitations, we\npropose a novel method, FreeEdit, to make edits in training free manner using\nmesh representations as a substitute for NeRF. Training-free methods are now a\npossibility because of the advances in foundation model's space. We leverage\nthese models to bring a training-free alternative and introduce solutions for\ninsertion, replacement and deletion. We consider insertion, replacement and\ndeletion as basic blocks for performing intricate edits with certain\ncombinations of these operations. Given a text prompt and a 3D scene, our model\nis capable of identifying what object should be inserted/replaced or deleted\nand location where edit should be performed. We also introduce a novel\nalgorithm as part of FreeEdit to find the optimal location on grounding object\nfor placement. We evaluate our model by comparing it with baseline models on a\nwide range of scenes using quantitative and qualitative metrics and showcase\nthe merits of our method with respect to others.\n","authors":["Vivek Madhavaram","Shivangana Rawat","Chaitanya Devaguptapu","Charu Sharma","Manohar Kaul"],"pdf_url":"https://arxiv.org/pdf/2412.12766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12765v1","updated":"2024-12-17T10:30:56Z","published":"2024-12-17T10:30:56Z","title":"Monocular Facial Appearance Capture in the Wild","summary":"  We present a new method for reconstructing the appearance properties of human\nfaces from a lightweight capture procedure in an unconstrained environment. Our\nmethod recovers the surface geometry, diffuse albedo, specular intensity and\nspecular roughness from a monocular video containing a simple head rotation\nin-the-wild. Notably, we make no simplifying assumptions on the environment\nlighting, and we explicitly take visibility and occlusions into account. As a\nresult, our method can produce facial appearance maps that approach the\nfidelity of studio-based multi-view captures, but with a far easier and cheaper\nprocedure.\n","authors":["Yingyan Xu","Kate Gadola","Prashanth Chandran","Sebastian Weiss","Markus Gross","Gaspard Zoss","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2412.12765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12755v1","updated":"2024-12-17T10:20:29Z","published":"2024-12-17T10:20:29Z","title":"Progressive Monitoring of Generative Model Training Evolution","summary":"  While deep generative models (DGMs) have gained popularity, their\nsusceptibility to biases and other inefficiencies that lead to undesirable\noutcomes remains an issue. With their growing complexity, there is a critical\nneed for early detection of issues to achieve desired results and optimize\nresources. Hence, we introduce a progressive analysis framework to monitor the\ntraining process of DGMs. Our method utilizes dimensionality reduction\ntechniques to facilitate the inspection of latent representations, the\ngenerated and real distributions, and their evolution across training\niterations. This monitoring allows us to pause and fix the training method if\nthe representations or distributions progress undesirably. This approach allows\nfor the analysis of a models' training dynamics and the timely identification\nof biases and failures, minimizing computational loads. We demonstrate how our\nmethod supports identifying and mitigating biases early in training a\nGenerative Adversarial Network (GAN) and improving the quality of the generated\ndata distribution.\n","authors":["Vidya Prasad","Anna Vilanova","Nicola Pezzotti"],"pdf_url":"https://arxiv.org/pdf/2412.12755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12743v1","updated":"2024-12-17T10:06:42Z","published":"2024-12-17T10:06:42Z","title":"Training a Distributed Acoustic Sensing Traffic Monitoring Network With\n  Video Inputs","summary":"  Distributed Acoustic Sensing (DAS) has emerged as a promising tool for\nreal-time traffic monitoring in densely populated areas. In this paper, we\npresent a novel concept that integrates DAS data with co-located visual\ninformation. We use YOLO-derived vehicle location and classification from\ncamera inputs as labeled data to train a detection and classification neural\nnetwork utilizing DAS data only. Our model achieves a performance exceeding 94%\nfor detection and classification, and about 1.2% false alarm rate. We\nillustrate the model's application in monitoring traffic over a week, yielding\nstatistical insights that could benefit future smart city developments. Our\napproach highlights the potential of combining fiber-optic sensors with visual\ninformation, focusing on practicality and scalability, protecting privacy, and\nminimizing infrastructure costs. To encourage future research, we share our\ndataset.\n","authors":["Khen Cohen","Liav Hen","Ariel Lellouch"],"pdf_url":"https://arxiv.org/pdf/2412.12743v1.pdf","comment":"12 pages, 11 figures, 5 appendices. Shared dataset in:\n  https://zenodo.org/records/14502092"},{"id":"http://arxiv.org/abs/2412.12740v1","updated":"2024-12-17T10:03:39Z","published":"2024-12-17T10:03:39Z","title":"Open-World Panoptic Segmentation","summary":"  Perception is a key building block of autonomously acting vision systems such\nas autonomous vehicles. It is crucial that these systems are able to understand\ntheir surroundings in order to operate safely and robustly. Additionally,\nautonomous systems deployed in unconstrained real-world scenarios must be able\nof dealing with novel situations and object that have never been seen before.\nIn this article, we tackle the problem of open-world panoptic segmentation,\ni.e., the task of discovering new semantic categories and new object instances\nat test time, while enforcing consistency among the categories that we\nincrementally discover. We propose Con2MAV, an approach for open-world panoptic\nsegmentation that extends our previous work, ContMAV, which was developed for\nopen-world semantic segmentation. Through extensive experiments across multiple\ndatasets, we show that our model achieves state-of-the-art results on\nopen-world segmentation tasks, while still performing competitively on the\nknown categories. We will open-source our implementation upon acceptance.\nAdditionally, we propose PANIC (Panoptic ANomalies In Context), a benchmark for\nevaluating open-world panoptic segmentation in autonomous driving scenarios.\nThis dataset, recorded with a multi-modal sensor suite mounted on a car,\nprovides high-quality, pixel-wise annotations of anomalous objects at both\nsemantic and instance level. Our dataset contains 800 images, with more than 50\nunknown classes, i.e., classes that do not appear in the training set, and 4000\nobject instances, making it an extremely challenging dataset for open-world\nsegmentation tasks in the autonomous driving scenario. We provide competitions\nfor multiple open-world tasks on a hidden test set. Our dataset and\ncompetitions are available at https://www.ipb.uni-bonn.de/data/panic.\n","authors":["Matteo Sodano","Federico Magistri","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2412.12740v1.pdf","comment":"Submitted to PAMI"},{"id":"http://arxiv.org/abs/2410.15628v2","updated":"2024-12-17T10:01:59Z","published":"2024-10-21T04:24:10Z","title":"Towards Kriging-informed Conditional Diffusion for Regional Sea-Level\n  Data Downscaling","summary":"  Given coarser-resolution projections from global climate models or satellite\ndata, the downscaling problem aims to estimate finer-resolution regional\nclimate data, capturing fine-scale spatial patterns and variability.\nDownscaling is any method to derive high-resolution data from low-resolution\nvariables, often to provide more detailed and local predictions and analyses.\nThis problem is societally crucial for effective adaptation, mitigation, and\nresilience against significant risks from climate change. The challenge arises\nfrom spatial heterogeneity and the need to recover finer-scale features while\nensuring model generalization. Most downscaling methods \\cite{Li2020} fail to\ncapture the spatial dependencies at finer scales and underperform on real-world\nclimate datasets, such as sea-level rise. We propose a novel Kriging-informed\nConditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial\nvariability while preserving fine-scale features. Experimental results on\nclimate data show that our proposed method is more accurate than\nstate-of-the-art downscaling techniques.\n","authors":["Subhankar Ghosh","Arun Sharma","Jayant Gupta","Aneesh Subramanian","Shashi Shekhar"],"pdf_url":"https://arxiv.org/pdf/2410.15628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12737v1","updated":"2024-12-17T09:59:53Z","published":"2024-12-17T09:59:53Z","title":"PolSAM: Polarimetric Scattering Mechanism Informed Segment Anything\n  Model","summary":"  PolSAR data presents unique challenges due to its rich and complex\ncharacteristics. Existing data representations, such as complex-valued data,\npolarimetric features, and amplitude images, are widely used. However, these\nformats often face issues related to usability, interpretability, and data\nintegrity. Most feature extraction networks for PolSAR are small, limiting\ntheir ability to capture features effectively. To address these issues, We\npropose the Polarimetric Scattering Mechanism-Informed SAM (PolSAM), an\nenhanced Segment Anything Model (SAM) that integrates domain-specific\nscattering characteristics and a novel prompt generation strategy. PolSAM\nintroduces Microwave Vision Data (MVD), a lightweight and interpretable data\nrepresentation derived from polarimetric decomposition and semantic\ncorrelations. We propose two key components: the Feature-Level Fusion Prompt\n(FFP), which fuses visual tokens from pseudo-colored SAR images and MVD to\naddress modality incompatibility in the frozen SAM encoder, and the\nSemantic-Level Fusion Prompt (SFP), which refines sparse and dense segmentation\nprompts using semantic information. Experimental results on the PhySAR-Seg\ndatasets demonstrate that PolSAM significantly outperforms existing SAM-based\nand multimodal fusion models, improving segmentation accuracy, reducing data\nstorage, and accelerating inference time. The source code and datasets will be\nmade publicly available at \\url{https://github.com/XAI4SAR/PolSAM}.\n","authors":["Yuqing Wang","Zhongling Huang","Shuxin Yang","Hao Tang","Xiaolan Qiu","Junwei Han","Dingwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12737v1.pdf","comment":"The manuscript is 15 pages long, includes 14 figures and 5 tables"},{"id":"http://arxiv.org/abs/2412.12735v1","updated":"2024-12-17T09:57:21Z","published":"2024-12-17T09:57:21Z","title":"GIRAFFE: Design Choices for Extending the Context Length of Visual\n  Language Models","summary":"  Visual Language Models (VLMs) demonstrate impressive capabilities in\nprocessing multimodal inputs, yet applications such as visual agents, which\nrequire handling multiple images and high-resolution videos, demand enhanced\nlong-range modeling. Moreover, existing open-source VLMs lack systematic\nexploration into extending their context length, and commercial models often\nprovide limited details. To tackle this, we aim to establish an effective\nsolution that enhances long context performance of VLMs while preserving their\ncapacities in short context scenarios. Towards this goal, we make the best\ndesign choice through extensive experiment settings from data curation to\ncontext window extending and utilizing: (1) we analyze data sources and length\ndistributions to construct ETVLM - a data recipe to balance the performance\nacross scenarios; (2) we examine existing position extending methods, identify\ntheir limitations and propose M-RoPE++ as an enhanced approach; we also choose\nto solely instruction-tune the backbone with mixed-source data; (3) we discuss\nhow to better utilize extended context windows and propose hybrid-resolution\ntraining. Built on the Qwen-VL series model, we propose Giraffe, which is\neffectively extended to 128K lengths. Evaluated on extensive long context VLM\nbenchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves\nstate-of-the-art performance among similarly sized open-source long VLMs and is\ncompetitive with commercial model GPT-4V. We will open-source the code, data,\nand models.\n","authors":["Mukai Li","Lei Li","Shansan Gong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12735v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.12734v1","updated":"2024-12-17T09:57:04Z","published":"2024-12-17T09:57:04Z","title":"Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures","summary":"  Gaussian Splatting has recently emerged as the go-to representation for\nreconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian\nprimitives has further improved multi-view consistency and surface\nreconstruction accuracy. In this work we highlight the similarity between 2D\nGaussian Splatting (2DGS) and billboards from traditional computer graphics.\nBoth use flat semi-transparent 2D geometry that is positioned, oriented and\nscaled in 3D space. However 2DGS uses a solid color per splat and an opacity\nmodulated by a Gaussian distribution, where billboards are more expressive,\nmodulating the color with a uv-parameterized texture. We propose to unify these\nconcepts by presenting Gaussian Billboards, a modification of 2DGS to add\nspatially-varying color achieved using per-splat texture interpolation. The\nresult is a mixture of the two representations, which benefits from both the\nrobust scene optimization power of 2DGS and the expressiveness of texture\nmapping. We show that our method can improve the sharpness and quality of the\nscene representation in a wide range of qualitative and quantitative\nevaluations compared to the original 2DGS implementation.\n","authors":["Sebastian Weiss","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2412.12734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12725v1","updated":"2024-12-17T09:47:48Z","published":"2024-12-17T09:47:48Z","title":"RaCFormer: Towards High-Quality 3D Object Detection via Query-based\n  Radar-Camera Fusion","summary":"  We propose Radar-Camera fusion transformer (RaCFormer) to boost the accuracy\nof 3D object detection by the following insight. The Radar-Camera fusion in\noutdoor 3D scene perception is capped by the image-to-BEV transformation--if\nthe depth of pixels is not accurately estimated, the naive combination of BEV\nfeatures actually integrates unaligned visual content. To avoid this problem,\nwe propose a query-based framework that enables adaptively sample\ninstance-relevant features from both the BEV and the original image view.\nFurthermore, we enhance system performance by two key designs: optimizing query\ninitialization and strengthening the representational capacity of BEV. For the\nformer, we introduce an adaptive circular distribution in polar coordinates to\nrefine the initialization of object queries, allowing for a distance-based\nadjustment of query density. For the latter, we initially incorporate a\nradar-guided depth head to refine the transformation from image view to BEV.\nSubsequently, we focus on leveraging the Doppler effect of radar and introduce\nan implicit dynamic catcher to capture the temporal elements within the BEV.\nExtensive experiments on nuScenes and View-of-Delft (VoD) datasets validate the\nmerits of our design. Remarkably, our method achieves superior results of 64.9%\nmAP and 70.2% NDS on nuScenes, even outperforming several LiDAR-based\ndetectors. RaCFormer also secures the 1st ranking on the VoD dataset. The code\nwill be released.\n","authors":["Xiaomeng Chu","Jiajun Deng","Guoliang You","Yifan Duan","Houqiang Li","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13362v2","updated":"2024-12-17T09:46:19Z","published":"2024-06-19T09:07:31Z","title":"VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models","summary":"  Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.\n","authors":["Haowen Hou","Peigen Zeng","Fei Ma","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2406.13362v2.pdf","comment":"Accepted at COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2412.12722v1","updated":"2024-12-17T09:38:58Z","published":"2024-12-17T09:38:58Z","title":"Defending LVLMs Against Vision Attacks through Partial-Perception\n  Supervision","summary":"  Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.\n","authors":["Qi Zhou","Tianlin Li","Qing Guo","Dongxia Wang","Yun Lin","Yang Liu","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18263v2","updated":"2024-12-17T09:34:49Z","published":"2024-11-27T12:01:08Z","title":"TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World\n  Image Super-Resolution","summary":"  Pre-trained text-to-image diffusion models are increasingly applied to\nreal-world image super-resolution (Real-ISR) task. Given the iterative\nrefinement nature of diffusion models, most existing approaches are\ncomputationally expensive. While methods such as SinSR and OSEDiff have emerged\nto condense inference steps via distillation, their performance in image\nrestoration or details recovery is not satisfied. To address this, we propose\nTSD-SR, a novel distillation framework specifically designed for real-world\nimage super-resolution, aiming to construct an efficient and effective one-step\nmodel. We first introduce the Target Score Distillation, which leverages the\npriors of diffusion models and real image references to achieve more realistic\nimage restoration. Secondly, we propose a Distribution-Aware Sampling Module to\nmake detail-oriented gradients more readily accessible, addressing the\nchallenge of recovering fine details. Extensive experiments demonstrate that\nour TSD-SR has superior restoration results (most of the metrics perform the\nbest) and the fastest inference speed (e.g. 40 times faster than SeeSR)\ncompared to the past Real-ISR approaches based on pre-trained diffusion priors.\n","authors":["Linwei Dong","Qingnan Fan","Yihong Guo","Zhonghao Wang","Qi Zhang","Jinwei Chen","Yawei Luo","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2411.18263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12718v1","updated":"2024-12-17T09:33:06Z","published":"2024-12-17T09:33:06Z","title":"ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding","summary":"  We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin.\n","authors":["Zhenxing Zhang","Yaxiong Wang","Lechao Cheng","Zhun Zhong","Dan Guo","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12718v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.12716v1","updated":"2024-12-17T09:30:31Z","published":"2024-12-17T09:30:31Z","title":"Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds","summary":"  Compact UAV systems, while advancing delivery and surveillance, pose\nsignificant security challenges due to their small size, which hinders\ndetection by traditional methods. This paper presents a cost-effective,\nunsupervised UAV detection method using spatial-temporal sequence processing to\nfuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios.\nOur approach segments point clouds into foreground and background, analyzes\nspatial-temporal data, and employs a scoring mechanism to enhance detection\naccuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024\nUG2+ Challenge, demonstrating its practical effectiveness. We plan to\nopen-source all designs, code, and sample data for the research community\ngithub.com/lianghanfang/UnLiDAR-UAV-Est.\n","authors":["Hanfang Liang","Yizhuo Yang","Jinming Hu","Jianfei Yang","Fen Liu","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09951v2","updated":"2024-12-17T09:27:33Z","published":"2024-12-13T08:14:24Z","title":"WiseAD: Knowledge Augmented End-to-End Autonomous Driving with\n  Vision-Language Model","summary":"  The emergence of general human knowledge and impressive logical reasoning\ncapacity in rapidly progressed vision-language models (VLMs) have driven\nincreasing interest in applying VLMs to high-level autonomous driving tasks,\nsuch as scene understanding and decision-making. However, an in-depth study on\nthe relationship between knowledge proficiency, especially essential driving\nexpertise, and closed-loop autonomous driving performance requires further\nexploration. In this paper, we investigate the effects of the depth and breadth\nof fundamental driving knowledge on closed-loop trajectory planning and\nintroduce WiseAD, a specialized VLM tailored for end-to-end autonomous driving\ncapable of driving reasoning, action justification, object recognition, risk\nanalysis, driving suggestions, and trajectory planning across diverse\nscenarios. We employ joint training on driving knowledge and planning datasets,\nenabling the model to perform knowledge-aligned trajectory planning\naccordingly. Extensive experiments indicate that as the diversity of driving\nknowledge extends, critical accidents are notably reduced, contributing 11.9%\nand 12.4% improvements in the driving score and route completion on the Carla\nclosed-loop evaluations, achieving state-of-the-art performance. Moreover,\nWiseAD also demonstrates remarkable performance in knowledge evaluations on\nboth in-domain and out-of-domain datasets.\n","authors":["Songyan Zhang","Wenhui Huang","Zihui Gao","Hao Chen","Chen Lv"],"pdf_url":"https://arxiv.org/pdf/2412.09951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12709v1","updated":"2024-12-17T09:23:46Z","published":"2024-12-17T09:23:46Z","title":"Accelerating lensed quasars discovery and modeling with physics-informed\n  variational autoencoders","summary":"  Strongly lensed quasars provide valuable insights into the rate of cosmic\nexpansion, the distribution of dark matter in foreground deflectors, and the\ncharacteristics of quasar hosts. However, detecting them in astronomical images\nis difficult due to the prevalence of non-lensing objects. To address this\nchallenge, we developed a generative deep learning model called VariLens, built\nupon a physics-informed variational autoencoder. This model seamlessly\nintegrates three essential modules: image reconstruction, object\nclassification, and lens modeling, offering a fast and comprehensive approach\nto strong lens analysis. VariLens is capable of rapidly determining both (1)\nthe probability that an object is a lens system and (2) key parameters of a\nsingular isothermal ellipsoid (SIE) mass model -- including the Einstein radius\n($\\theta_\\mathrm{E}$), lens center, and ellipticity -- in just milliseconds\nusing a single CPU. A direct comparison of VariLens estimates with traditional\nlens modeling for 20 known lensed quasars within the Subaru Hyper Suprime-Cam\n(HSC) footprint shows good agreement, with both results consistent within\n$2\\sigma$ for systems with $\\theta_\\mathrm{E}<3$ arcsecs. To identify new\nlensed quasar candidates, we begin with an initial sample of approximately 80\nmillion sources, combining HSC data with multiwavelength information from\nvarious surveys. After applying a photometric preselection aimed at locating\n$z>1.5$ sources, the number of candidates is reduced to 710,966. Subsequently,\nVariLens highlights 13,831 sources, each showing a high likelihood of being a\nlens. A visual assessment of these objects results in 42 promising candidates\nthat await spectroscopic confirmation. These results underscore the potential\nof automated deep learning pipelines to efficiently detect and model strong\nlenses in large datasets.\n","authors":["Irham T. Andika","Stefan Schuldt","Sherry H. Suyu","Satadru Bag","Raoul Cañameras","Alejandra Melo","Claudio Grillo","James H. H. Chan"],"pdf_url":"https://arxiv.org/pdf/2412.12709v1.pdf","comment":"Submitted to the Astronomy & Astrophysics journal. The paper consists\n  of 17 main pages, 14 figures, and 5 tables. We welcome feedback and comments\n  from readers!"},{"id":"http://arxiv.org/abs/2408.09429v2","updated":"2024-12-17T09:19:46Z","published":"2024-08-18T10:07:02Z","title":"Reefknot: A Comprehensive Benchmark for Relation Hallucination\n  Evaluation, Analysis and Mitigation in Multimodal Large Language Models","summary":"  Hallucination issues continue to affect multimodal large language models\n(MLLMs), with existing research mainly addressing object-level or\nattribute-level hallucinations, neglecting the more complex relation\nhallucinations that require advanced reasoning. Current benchmarks for relation\nhallucinations lack detailed evaluation and effective mitigation, and their\ndatasets often suffer from biases due to systematic annotation processes. To\naddress these challenges, we introduce Reefknot, a comprehensive benchmark\ntargeting relation hallucinations, comprising over 20,000 real-world samples.\nWe provide a systematic definition of relation hallucinations, integrating\nperceptive and cognitive perspectives, and construct a relation-based corpus\nusing the Visual Genome scene graph dataset. Our comparative evaluation reveals\nsignificant limitations in current MLLMs' ability to handle relation\nhallucinations. Additionally, we propose a novel confidence-based mitigation\nstrategy, which reduces the hallucination rate by an average of 9.75% across\nthree datasets, including Reefknot. Our work offers valuable insights for\nachieving trustworthy multimodal intelligence.\n","authors":["Kening Zheng","Junkai Chen","Yibo Yan","Xin Zou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2408.09429v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12704v1","updated":"2024-12-17T09:19:44Z","published":"2024-12-17T09:19:44Z","title":"MapExpert: Online HD Map Construction with Simple and Efficient Sparse\n  Map Element Expert","summary":"  Constructing online High-Definition (HD) maps is crucial for the static\nenvironment perception of autonomous driving systems (ADS). Existing solutions\ntypically attempt to detect vectorized HD map elements with unified models;\nhowever, these methods often overlook the distinct characteristics of different\nnon-cubic map elements, making accurate distinction challenging. To address\nthese issues, we introduce an expert-based online HD map method, termed\nMapExpert. MapExpert utilizes sparse experts, distributed by our routers, to\ndescribe various non-cubic map elements accurately. Additionally, we propose an\nauxiliary balance loss function to distribute the load evenly across experts.\nFurthermore, we theoretically analyze the limitations of prevalent bird's-eye\nview (BEV) feature temporal fusion methods and introduce an efficient temporal\nfusion module called Learnable Weighted Moving Descentage. This module\neffectively integrates relevant historical information into the final BEV\nfeatures. Combined with an enhanced slice head branch, the proposed MapExpert\nachieves state-of-the-art performance and maintains good efficiency on both\nnuScenes and Argoverse2 datasets.\n","authors":["Dapeng Zhang","Dayu Chen","Peng Zhi","Yinda Chen","Zhenlong Yuan","Chenyang Li"," Sunjing","Rui Zhou","Qingguo Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12696v1","updated":"2024-12-17T09:13:22Z","published":"2024-12-17T09:13:22Z","title":"ALADE-SNN: Adaptive Logit Alignment in Dynamically Expandable Spiking\n  Neural Networks for Class Incremental Learning","summary":"  Inspired by the human brain's ability to adapt to new tasks without erasing\nprior knowledge, we develop spiking neural networks (SNNs) with dynamic\nstructures for Class Incremental Learning (CIL). Our comparative experiments\nreveal that limited datasets introduce biases in logits distributions among\ntasks. Fixed features from frozen past-task extractors can cause overfitting\nand hinder the learning of new tasks. To address these challenges, we propose\nthe ALADE-SNN framework, which includes adaptive logit alignment for balanced\nfeature representation and OtoN suppression to manage weights mapping frozen\nold features to new classes during training, releasing them during fine-tuning.\nThis approach dynamically adjusts the network architecture based on analytical\nobservations, improving feature extraction and balancing performance between\nnew and old tasks. Experiment results show that ALADE-SNN achieves an average\nincremental accuracy of 75.42 on the CIFAR100-B0 benchmark over 10 incremental\nsteps. ALADE-SNN not only matches the performance of DNN-based methods but also\nsurpasses state-of-the-art SNN-based continual learning algorithms. This\nadvancement enhances continual learning in neuromorphic computing, offering a\nbrain-inspired, energy-efficient solution for real-time data processing.\n","authors":["Wenyao Ni","Jiangrong Shen","Qi Xu","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12693v1","updated":"2024-12-17T09:10:55Z","published":"2024-12-17T09:10:55Z","title":"SPHERE: A Hierarchical Evaluation on Spatial Perception and Reasoning\n  for Vision-Language Models","summary":"  Current vision-language models may incorporate single-dimensional spatial\ncues, such as depth, object boundary, and basic spatial directions (e.g. left,\nright, front, back), yet often lack the multi-dimensional spatial reasoning\nnecessary for human-like understanding and real-world applications. To address\nthis gap, we develop SPHERE (Spatial Perception and Hierarchical Evaluation of\nREasoning), a hierarchical evaluation framework with a new human-annotated\ndataset to pinpoint model strengths and weaknesses, advancing from single-skill\ntasks to multi-skill tasks, and ultimately to complex reasoning tasks that\nrequire the integration of multiple spatial and visual cues with logical\nreasoning. Benchmark evaluation of state-of-the-art open-source models reveal\nsignificant shortcomings, especially in the abilities to understand distance\nand proximity, to reason from both allocentric and egocentric viewpoints, and\nto perform complex reasoning in a physical context. This work underscores the\nneed for more advanced approaches to spatial understanding and reasoning,\npaving the way for improvements in vision-language models and their alignment\nwith human-like spatial capabilities. The dataset will be open-sourced upon\npublication.\n","authors":["Wenyu Zhang","Wei En Ng","Lixin Ma","Yuwen Wang","Jungqi Zhao","Boyang Li","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12685v1","updated":"2024-12-17T09:02:55Z","published":"2024-12-17T09:02:55Z","title":"SemStereo: Semantic-Constrained Stereo Matching Network for Remote\n  Sensing","summary":"  Semantic segmentation and 3D reconstruction are two fundamental tasks in\nremote sensing, typically treated as separate or loosely coupled tasks. Despite\nattempts to integrate them into a unified network, the constraints between the\ntwo heterogeneous tasks are not explicitly modeled, since the pioneering\nstudies either utilize a loosely coupled parallel structure or engage in only\nimplicit interactions, failing to capture the inherent connections. In this\nwork, we explore the connections between the two tasks and propose a new\nnetwork that imposes semantic constraints on the stereo matching task, both\nimplicitly and explicitly. Implicitly, we transform the traditional parallel\nstructure to a new cascade structure termed Semantic-Guided Cascade structure,\nwhere the deep features enriched with semantic information are utilized for the\ncomputation of initial disparity maps, enhancing semantic guidance. Explicitly,\nwe propose a Semantic Selective Refinement (SSR) module and a Left-Right\nSemantic Consistency (LRSC) module. The SSR refines the initial disparity map\nunder the guidance of the semantic map. The LRSC ensures semantic consistency\nbetween two views via reducing the semantic divergence after transforming the\nsemantic map from one view to the other using the disparity map. Experiments on\nthe US3D and WHU datasets demonstrate that our method achieves state-of-the-art\nperformance for both semantic segmentation and stereo matching.\n","authors":["Chen Chen","Liangjin Zhao","Yuanchun He","Yingxuan Long","Kaiqiang Chen","Zhirui Wang","Yanfeng Hu","Xian Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12685v1.pdf","comment":"9 pages, 6 figures, AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12683v1","updated":"2024-12-17T08:56:59Z","published":"2024-12-17T08:56:59Z","title":"ShiftedBronzes: Benchmarking and Analysis of Domain Fine-Grained\n  Classification in Open-World Settings","summary":"  In real-world applications across specialized domains, addressing complex\nout-of-distribution (OOD) challenges is a common and significant concern. In\nthis study, we concentrate on the task of fine-grained bronze ware dating, a\ncritical aspect in the study of ancient Chinese history, and developed a\nbenchmark dataset named ShiftedBronzes. By extensively expanding the bronze\nDing dataset, ShiftedBronzes incorporates two types of bronze ware data and\nseven types of OOD data, which exhibit distribution shifts commonly encountered\nin bronze ware dating scenarios. We conduct benchmarking experiments on\nShiftedBronzes and five commonly used general OOD datasets, employing a variety\nof widely adopted post-hoc, pre-trained Vision Large Model (VLM)-based and\ngeneration-based OOD detection methods. Through analysis of the experimental\nresults, we validate previous conclusions regarding post-hoc, VLM-based, and\ngeneration-based methods, while also highlighting their distinct behaviors on\nspecialized datasets. These findings underscore the unique challenges of\napplying general OOD detection methods to domain-specific tasks such as bronze\nware dating. We hope that the ShiftedBronzes benchmark provides valuable\ninsights into both the field of bronze ware dating and the and the development\nof OOD detection methods. The dataset and associated code will be available\nlater.\n","authors":["Rixin Zhou","Honglin Pang","Qian Zhang","Ruihua Qi","Xi Yang","Chuntao Li"],"pdf_url":"https://arxiv.org/pdf/2412.12683v1.pdf","comment":"9pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.11594v2","updated":"2024-12-17T08:52:51Z","published":"2024-12-16T09:32:23Z","title":"VersaGen: Unleashing Versatile Visual Control for Text-to-Image\n  Synthesis","summary":"  Despite the rapid advancements in text-to-image (T2I) synthesis, enabling\nprecise visual control remains a significant challenge. Existing works\nattempted to incorporate multi-facet controls (text and sketch), aiming to\nenhance the creative control over generated images. However, our pilot study\nreveals that the expressive power of humans far surpasses the capabilities of\ncurrent methods. Users desire a more versatile approach that can accommodate\ntheir diverse creative intents, ranging from controlling individual subjects to\nmanipulating the entire scene composition. We present VersaGen, a generative AI\nagent that enables versatile visual control in T2I synthesis. VersaGen admits\nfour types of visual controls: i) single visual subject; ii) multiple visual\nsubjects; iii) scene background; iv) any combination of the three above or\nmerely no control at all. We train an adaptor upon a frozen T2I model to\naccommodate the visual information into the text-dominated diffusion process.\nWe introduce three optimization strategies during the inference phase of\nVersaGen to improve generation results and enhance user experience.\nComprehensive experiments on COCO and Sketchy validate the effectiveness and\nflexibility of VersaGen, as evidenced by both qualitative and quantitative\nresults.\n","authors":["Zhipeng Chen","Lan Yang","Yonggang Qi","Honggang Zhang","Kaiyue Pang","Ke Li","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2412.11594v2.pdf","comment":"The paper has been accepted by AAAI 2025. Paper code:\n  https://github.com/FelixChan9527/VersaGen_official"},{"id":"http://arxiv.org/abs/2412.12675v1","updated":"2024-12-17T08:44:29Z","published":"2024-12-17T08:44:29Z","title":"ShotVL: Human-Centric Highlight Frame Retrieval via Language Queries","summary":"  Existing works on human-centric video understanding typically focus on\nanalyzing specific moment or entire videos. However, many applications require\nhigher precision at the frame level. In this work, we propose a novel task,\nBestShot, which aims to locate highlight frames within human-centric videos via\nlanguage queries. This task demands not only a deep semantic comprehension of\nhuman actions but also precise temporal localization. To support this task, we\nintroduce the BestShot Benchmark. %The benchmark is meticulously constructed by\ncombining human detection and tracking, potential frame selection based on\nhuman judgment, and detailed textual descriptions crafted by human input to\nensure precision. The benchmark is meticulously constructed by combining\nhuman-annotated highlight frames, detailed textual descriptions and duration\nlabeling. These descriptions encompass three critical elements: (1) Visual\ncontent; (2) Fine-grained action; and (3) Human Pose Description. Together,\nthese elements provide the necessary precision to identify the exact highlight\nframes in videos.\n  To tackle this problem, we have collected two distinct datasets: (i)\nShotGPT4o Dataset, which is algorithmically generated by GPT-4o and (ii)\nImage-SMPLText Dataset, a dataset with large-scale and accurate per-frame pose\ndescription leveraging PoseScript and existing pose estimation datasets. Based\non these datasets, we present a strong baseline model, ShotVL, fine-tuned from\nInternVL, specifically for BestShot. We highlight the impressive zero-shot\ncapabilities of our model and offer comparative analyses with existing SOTA\nmodels. ShotVL demonstrates a significant 52% improvement over InternVL on the\nBestShot Benchmark and a notable 57% improvement on the THUMOS14 Benchmark, all\nwhile maintaining the SOTA performance in general image classification and\nretrieval.\n","authors":["Wangyu Xue","Chen Qian","Jiayi Wu","Yang Zhou","Wentao Liu","Ju Ren","Siming Fan","Yaoxue Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12672v1","updated":"2024-12-17T08:41:50Z","published":"2024-12-17T08:41:50Z","title":"Structural Pruning via Spatial-aware Information Redundancy for Semantic\n  Segmentation","summary":"  In recent years, semantic segmentation has flourished in various\napplications. However, the high computational cost remains a significant\nchallenge that hinders its further adoption. The filter pruning method for\nstructured network slimming offers a direct and effective solution for the\nreduction of segmentation networks. Nevertheless, we argue that most existing\npruning methods, originally designed for image classification, overlook the\nfact that segmentation is a location-sensitive task, which consequently leads\nto their suboptimal performance when applied to segmentation networks. To\naddress this issue, this paper proposes a novel approach, denoted as\nSpatial-aware Information Redundancy Filter Pruning~(SIRFP), which aims to\nreduce feature redundancy between channels. First, we formulate the pruning\nprocess as a maximum edge weight clique problem~(MEWCP) in graph theory,\nthereby minimizing the redundancy among the remaining features after pruning.\nWithin this framework, we introduce a spatial-aware redundancy metric based on\nfeature maps, thus endowing the pruning process with location sensitivity to\nbetter adapt to pruning segmentation networks. Additionally, based on the\nMEWCP, we propose a low computational complexity greedy strategy to solve this\nNP-hard problem, making it feasible and efficient for structured pruning. To\nvalidate the effectiveness of our method, we conducted extensive comparative\nexperiments on various challenging datasets. The results demonstrate the\nsuperior performance of SIRFP for semantic segmentation tasks.\n","authors":["Dongyue Wu","Zilin Guo","Li Yu","Nong Sang","Changxin Gao"],"pdf_url":"https://arxiv.org/pdf/2412.12672v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12669v1","updated":"2024-12-17T08:40:23Z","published":"2024-12-17T08:40:23Z","title":"Adaptive Prototype Replay for Class Incremental Semantic Segmentation","summary":"  Class incremental semantic segmentation (CISS) aims to segment new classes\nduring continual steps while preventing the forgetting of old knowledge.\nExisting methods alleviate catastrophic forgetting by replaying distributions\nof previously learned classes using stored prototypes or features. However,\nthey overlook a critical issue: in CISS, the representation of class knowledge\nis updated continuously through incremental learning, whereas prototype replay\nmethods maintain fixed prototypes. This mismatch between updated representation\nand fixed prototypes limits the effectiveness of the prototype replay strategy.\nTo address this issue, we propose the Adaptive prototype replay (Adapter) for\nCISS in this paper. Adapter comprises an adaptive deviation compen sation (ADC)\nstrategy and an uncertainty-aware constraint (UAC) loss. Specifically, the ADC\nstrategy dynamically updates the stored prototypes based on the estimated\nrepresentation shift distance to match the updated representation of old class.\nThe UAC loss reduces prediction uncertainty, aggregating discriminative\nfeatures to aid in generating compact prototypes. Additionally, we introduce a\ncompensation-based prototype similarity discriminative (CPD) loss to ensure\nadequate differentiation between similar prototypes, thereby enhancing the\nefficiency of the adaptive prototype replay strategy. Extensive experiments on\nPascal VOC and ADE20K datasets demonstrate that Adapter achieves\nstate-of-the-art results and proves effective across various CISS tasks,\nparticularly in challenging multi-step scenarios. The code and model is\navailable at https://github.com/zhu-gl-ux/Adapter.\n","authors":["Guilin Zhu","Dongyue Wu","Changxin Gao","Runmin Wang","Weidong Yang","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2412.12669v1.pdf","comment":"Accepted by the Main Technical Track of the 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2412.12667v1","updated":"2024-12-17T08:36:47Z","published":"2024-12-17T08:36:47Z","title":"A Two-Fold Patch Selection Approach for Improved 360-Degree Image\n  Quality Assessment","summary":"  This article presents a novel approach to improving the accuracy of\n360-degree perceptual image quality assessment (IQA) through a two-fold patch\nselection process. Our methodology combines visual patch selection with\nembedding similarity-based refinement. The first stage focuses on selecting\npatches from 360-degree images using three distinct sampling methods to ensure\ncomprehensive coverage of visual content for IQA. The second stage, which is\nthe core of our approach, employs an embedding similarity-based selection\nprocess to filter and prioritize the most informative patches based on their\nembeddings similarity distances. This dual selection mechanism ensures that the\ntraining data is both relevant and informative, enhancing the model's learning\nefficiency. Extensive experiments and statistical analyses using three distance\nmetrics across three benchmark datasets validate the effectiveness of our\nselection algorithm. The results highlight its potential to deliver robust and\naccurate 360-degree IQA, with performance gains of up to 4.5% in accuracy and\nmonotonicity of quality score prediction, while using only 40% to 50% of the\ntraining patches. These improvements are consistent across various\nconfigurations and evaluation metrics, demonstrating the strength of the\nproposed method. The code for the selection process is available at:\nhttps://github.com/sendjasni/patch-selection-360-image-quality.\n","authors":["Abderrezzaq Sendjasni","Seif-Eddine Benkabou","Mohamed-Chaker Larabi"],"pdf_url":"https://arxiv.org/pdf/2412.12667v1.pdf","comment":"Submitted to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2412.12661v1","updated":"2024-12-17T08:30:00Z","published":"2024-12-17T08:30:00Z","title":"MedMax: Mixed-Modal Instruction Tuning for Training Biomedical\n  Assistants","summary":"  Recent advancements in mixed-modal generative models have enabled flexible\nintegration of information across image-text content. These models have opened\nnew avenues for developing unified biomedical assistants capable of analyzing\nbiomedical images, answering complex questions about them, and predicting the\nimpact of medical procedures on a patient's health. However, existing resources\nface challenges such as limited data availability, narrow domain coverage, and\nrestricted sources (e.g., medical papers). To address these gaps, we present\nMedMax, the first large-scale multimodal biomedical instruction-tuning dataset\nfor mixed-modal foundation models. With 1.47 million instances, MedMax\nencompasses a diverse range of tasks, including multimodal content generation\n(interleaved image-text data), biomedical image captioning and generation,\nvisual chatting, and report understanding. These tasks span diverse medical\ndomains such as radiology and histopathology. Subsequently, we fine-tune a\nmixed-modal foundation model on the MedMax dataset, achieving significant\nperformance improvements: a 26% gain over the Chameleon model and an 18.3%\nimprovement over GPT-4o across 12 downstream biomedical visual\nquestion-answering tasks. Additionally, we introduce a unified evaluation suite\nfor biomedical tasks, providing a robust framework to guide the development of\nnext-generation mixed-modal biomedical AI assistants.\n","authors":["Hritik Bansal","Daniel Israel","Siyan Zhao","Shufan Li","Tung Nguyen","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2412.12661v1.pdf","comment":"12 figures, 15 tables"},{"id":"http://arxiv.org/abs/2412.12660v1","updated":"2024-12-17T08:29:13Z","published":"2024-12-17T08:29:13Z","title":"SEG-SAM: Semantic-Guided SAM for Unified Medical Image Segmentation","summary":"  Recently, developing unified medical image segmentation models gains\nincreasing attention, especially with the advent of the Segment Anything Model\n(SAM). SAM has shown promising binary segmentation performance in natural\ndomains, however, transferring it to the medical domain remains challenging, as\nmedical images often possess substantial inter-category overlaps. To address\nthis, we propose the SEmantic-Guided SAM (SEG-SAM), a unified medical\nsegmentation model that incorporates semantic medical knowledge to enhance\nmedical segmentation performance. First, to avoid the potential conflict\nbetween binary and semantic predictions, we introduce a semantic-aware decoder\nindependent of SAM's original decoder, specialized for both semantic\nsegmentation on the prompted object and classification on unprompted objects in\nimages. To further enhance the model's semantic understanding, we solicit key\ncharacteristics of medical categories from large language models and\nincorporate them into SEG-SAM through a text-to-vision semantic module,\nadaptively transferring the language information into the visual segmentation\ntask. In the end, we introduce the cross-mask spatial alignment strategy to\nencourage greater overlap between the predicted masks from SEG-SAM's two\ndecoders, thereby benefiting both predictions. Extensive experiments\ndemonstrate that SEG-SAM outperforms state-of-the-art SAM-based methods in\nunified binary medical segmentation and task-specific methods in semantic\nmedical segmentation, showcasing promising results and potential for broader\nmedical applications.\n","authors":["Shuangping Huang","Hao Liang","Qingfeng Wang","Chulong Zhong","Zijian Zhou","Miaojing Shi"],"pdf_url":"https://arxiv.org/pdf/2412.12660v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.12654v1","updated":"2024-12-17T08:21:46Z","published":"2024-12-17T08:21:46Z","title":"CALA: A Class-Aware Logit Adapter for Few-Shot Class-Incremental\n  Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) defines a practical but\nchallenging task where models are required to continuously learn novel concepts\nwith only a few training samples. Due to data scarcity, existing FSCIL methods\nresort to training a backbone with abundant base data and then keeping it\nfrozen afterward. However, the above operation often causes the backbone to\noverfit to base classes while overlooking the novel ones, leading to severe\nconfusion between them. To address this issue, we propose Class-Aware Logit\nAdapter (CALA). Our method involves a lightweight adapter that learns to\nrectify biased predictions through a pseudo-incremental learning paradigm. In\nthe real FSCIL process, we use the learned adapter to dynamically generate\nrobust balancing factors. These factors can adjust confused novel instances\nback to their true label space based on their similarity to base classes.\nSpecifically, when confusion is more likely to occur in novel instances that\nclosely resemble base classes, greater rectification is required. Notably, CALA\noperates on the classifier level, preserving the original feature space, thus\nit can be flexibly plugged into most of the existing FSCIL works for improved\nperformance. Experiments on three benchmark datasets consistently validate the\neffectiveness and flexibility of CALA. Codes will be available upon acceptance.\n","authors":["Chengyan Liu","Linglan Zhao","Fan Lyu","Kaile Du","Fuyuan Hu","Tao Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12654v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2405.05095v4","updated":"2024-12-17T08:12:37Z","published":"2024-05-08T14:44:34Z","title":"Approximation properties relative to continuous scale space for hybrid\n  discretizations of Gaussian derivative operators","summary":"  This paper presents an analysis of properties of two hybrid discretization\nmethods for Gaussian derivatives, based on convolutions with either the\nnormalized sampled Gaussian kernel or the integrated Gaussian kernel followed\nby central differences. The motivation for studying these discretization\nmethods is that in situations when multiple spatial derivatives of different\norder are needed at the same scale level, they can be computed significantly\nmore efficiently compared to more direct derivative approximations based on\nexplicit convolutions with either sampled Gaussian kernels or integrated\nGaussian kernels.\n  While these computational benefits do also hold for the genuinely discrete\napproach for computing discrete analogues of Gaussian derivatives, based on\nconvolution with the discrete analogue of the Gaussian kernel followed by\ncentral differences, the underlying mathematical primitives for the discrete\nanalogue of the Gaussian kernel, in terms of modified Bessel functions of\ninteger order, may not be available in certain frameworks for image processing,\nsuch as when performing deep learning based on scale-parameterized filters in\nterms of Gaussian derivatives, with learning of the scale levels.\n  In this paper, we present a characterization of the properties of these\nhybrid discretization methods, in terms of quantitative performance measures\nconcerning the amount of spatial smoothing that they imply, as well as the\nrelative consistency of scale estimates obtained from scale-invariant feature\ndetectors with automatic scale selection, with an emphasis on the behaviour for\nvery small values of the scale parameter, which may differ significantly from\ncorresponding results obtained from the fully continuous scale-space theory, as\nwell as between different types of discretization methods.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2405.05095v4.pdf","comment":"23 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2311.11317"},{"id":"http://arxiv.org/abs/2408.13854v2","updated":"2024-12-17T08:12:25Z","published":"2024-08-25T14:47:25Z","title":"Tangram: Benchmark for Evaluating Geometric Element Recognition in Large\n  Multimodal Models","summary":"  Significant advancements in Large Multimodal Models (LMMs) have enabled them\nto tackle complex problems involving visual-mathematical reasoning. However,\ntheir ability to identify geometric elements remains underexplored. To address\nthis gap, we introduce Tangram, a novel benchmark designed to evaluate the\nperformance of LMMs on geometric element recognition. Tangram comprises 1,080\ndiverse geometric diagrams sourced from primary and secondary school exams,\ncompetitions, and textbooks, ranging from simple geometric shapes to complex\ncombinations. Each diagram is paired with four questions, resulting in 4,320\nvisual-question-answer pairs. Unlike existing benchmarks that emphasize\nhigher-level cognition and reasoning, Tangram focuses on understanding\ngeometric elements, requiring models to perform a ``simple yet challenging\"\ncounting task. Systematic evaluation of 13 prominent LMMs, such as GPT-4o and\nClaude 3.5 Sonnet, reveals that these models face significant challenges even\nin seemingly straightforward tasks. The top-performing model achieves an\naccuracy of only 53.0%, highlighting a substantial gap compared to human\nperformance. These findings underscore the limitations of current multimodal AI\nsystems in handling basic perception tasks and serve to inspire the development\nof the next generation of expert-level multimodal foundational models. The data\nand code will be released soon.\n","authors":["Chao Zhang","Jiamin Tang","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.13854v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11024v2","updated":"2024-12-17T07:45:29Z","published":"2024-12-15T02:35:31Z","title":"Exploring Diffusion and Flow Matching Under Generator Matching","summary":"  In this paper, we present a comprehensive theoretical comparison of diffusion\nand flow matching under the Generator Matching framework. Despite their\napparent differences, both diffusion and flow matching can be viewed under the\nunified framework of Generator Matching. By recasting both diffusion and flow\nmatching under the same generative Markov framework, we provide theoretical\ninsights into why flow matching models can be more robust empirically and how\nnovel model classes can be constructed by mixing deterministic and stochastic\ncomponents. Our analysis offers a fresh perspective on the relationships\nbetween state-of-the-art generative modeling paradigms.\n","authors":["Zeeshan Patel","James DeLoye","Lance Mathias"],"pdf_url":"https://arxiv.org/pdf/2412.11024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12629v1","updated":"2024-12-17T07:44:25Z","published":"2024-12-17T07:44:25Z","title":"a2z-1 for Multi-Disease Detection in Abdomen-Pelvis CT: External\n  Validation and Performance Analysis Across 21 Conditions","summary":"  We present a comprehensive evaluation of a2z-1, an artificial intelligence\n(AI) model designed to analyze abdomen-pelvis CT scans for 21 time-sensitive\nand actionable findings. Our study focuses on rigorous assessment of the\nmodel's performance and generalizability. Large-scale retrospective analysis\ndemonstrates an average AUC of 0.931 across 21 conditions. External validation\nacross two distinct health systems confirms consistent performance (AUC 0.923),\nestablishing generalizability to different evaluation scenarios, with notable\nperformance in critical findings such as small bowel obstruction (AUC 0.958)\nand acute pancreatitis (AUC 0.961). Subgroup analysis shows consistent accuracy\nacross patient sex, age groups, and varied imaging protocols, including\ndifferent slice thicknesses and contrast administration types. Comparison of\nhigh-confidence model outputs to radiologist reports reveals instances where\na2z-1 identified overlooked findings, suggesting potential for quality\nassurance applications.\n","authors":["Pranav Rajpurkar","Julian N. Acosta","Siddhant Dogra","Jaehwan Jeong","Deepanshu Jindal","Michael Moritz","Samir Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2412.12629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12628v1","updated":"2024-12-17T07:43:36Z","published":"2024-12-17T07:43:36Z","title":"Dense Audio-Visual Event Localization under Cross-Modal Consistency and\n  Multi-Temporal Granularity Collaboration","summary":"  In the field of audio-visual learning, most research tasks focus exclusively\non short videos. This paper focuses on the more practical Dense Audio-Visual\nEvent Localization (DAVEL) task, advancing audio-visual scene understanding for\nlonger, {untrimmed} videos. This task seeks to identify and temporally pinpoint\nall events simultaneously occurring in both audio and visual streams.\nTypically, each video encompasses dense events of multiple classes, which may\noverlap on the timeline, each exhibiting varied durations. Given these\nchallenges, effectively exploiting the audio-visual relations and the temporal\nfeatures encoded at various granularities becomes crucial. To address these\nchallenges, we introduce a novel \\ul{CC}Net, comprising two core modules: the\nCross-Modal Consistency \\ul{C}ollaboration (CMCC) and the Multi-Temporal\nGranularity \\ul{C}ollaboration (MTGC). Specifically, the CMCC module contains\ntwo branches: a cross-modal interaction branch and a temporal consistency-gated\nbranch. The former branch facilitates the aggregation of consistent event\nsemantics across modalities through the encoding of audio-visual relations,\nwhile the latter branch guides one modality's focus to pivotal event-relevant\ntemporal areas as discerned in the other modality. The MTGC module includes a\ncoarse-to-fine collaboration block and a fine-to-coarse collaboration block,\nproviding bidirectional support among coarse- and fine-grained temporal\nfeatures. Extensive experiments on the UnAV-100 dataset validate our module\ndesign, resulting in a new state-of-the-art performance in dense audio-visual\nevent localization. The code is available at\n\\url{https://github.com/zzhhfut/CCNet-AAAI2025}.\n","authors":["Ziheng Zhou","Jinxing Zhou","Wei Qian","Shengeng Tang","Xiaojun Chang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12628v1.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://github.com/zzhhfut/CCNet-AAAI2025. Jinxing Zhou and Dan Guo are the\n  corresponding authors"},{"id":"http://arxiv.org/abs/2412.12626v1","updated":"2024-12-17T07:41:06Z","published":"2024-12-17T07:41:06Z","title":"Improving the Transferability of 3D Point Cloud Attack via\n  Spectral-aware Admix and Optimization Designs","summary":"  Deep learning models for point clouds have shown to be vulnerable to\nadversarial attacks, which have received increasing attention in various\nsafety-critical applications such as autonomous driving, robotics, and\nsurveillance. Existing 3D attackers generally design various attack strategies\nin the white-box setting, requiring the prior knowledge of 3D model details.\nHowever, real-world 3D applications are in the black-box setting, where we can\nonly acquire the outputs of the target classifier. Although few recent works\ntry to explore the black-box attack, they still achieve limited attack success\nrates (ASR). To alleviate this issue, this paper focuses on attacking the 3D\nmodels in a transfer-based black-box setting, where we first carefully design\nadversarial examples in a white-box surrogate model and then transfer them to\nattack other black-box victim models. Specifically, we propose a novel\nSpectral-aware Admix with Augmented Optimization method (SAAO) to improve the\nadversarial transferability. In particular, since traditional Admix strategy\nare deployed in the 2D domain that adds pixel-wise images for perturbing, we\ncan not directly follow it to merge point clouds in coordinate domain as it\nwill destroy the geometric shapes. Therefore, we design spectral-aware fusion\nthat performs Graph Fourier Transform (GFT) to get spectral features of the\npoint clouds and add them in the spectral domain. Afterward, we run a few steps\nwith spectral-aware weighted Admix to select better optimization paths as well\nas to adjust corresponding learning weights. At last, we run more steps to\ngenerate adversarial spectral feature along the optimization path and perform\nInverse-GFT on the adversarial spectral feature to obtain the adversarial\nexample in the data domain. Experiments show that our SAAO achieves better\ntransferability compared to existing 3D attack methods.\n","authors":["Shiyu Hu","Daizong Liu","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2412.12626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12620v1","updated":"2024-12-17T07:33:07Z","published":"2024-12-17T07:33:07Z","title":"Multi-Domain Features Guided Supervised Contrastive Learning for Radar\n  Target Detection","summary":"  Detecting small targets in sea clutter is challenging due to dynamic maritime\nconditions. Existing solutions either model sea clutter for detection or\nextract target features based on clutter-target echo differences, including\nstatistical and deep features. While more common, the latter often excels in\ncontrolled scenarios but struggles with robust detection and generalization in\ndiverse environments, limiting practical use. In this letter, we propose a\nmulti-domain features guided supervised contrastive learning (MDFG_SCL) method,\nwhich integrates statistical features derived from multi-domain differences\nwith deep features obtained through supervised contrastive learning, thereby\ncapturing both low-level domain-specific variations and high-level semantic\ninformation. This comprehensive feature integration enables the model to\neffectively distinguish between small targets and sea clutter, even under\nchallenging conditions. Experiments conducted on real-world datasets\ndemonstrate that the proposed shallow-to-deep detector not only achieves\neffective identification of small maritime targets but also maintains superior\ndetection performance across varying sea conditions, outperforming the\nmainstream unsupervised contrastive learning and supervised contrastive\nlearning methods.\n","authors":["Junjie Wang","Yuze Gao","Dongying Li","Wenxian Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11248v2","updated":"2024-12-17T07:31:27Z","published":"2024-12-15T16:54:53Z","title":"Multimodal Class-aware Semantic Enhancement Network for Audio-Visual\n  Video Parsing","summary":"  The Audio-Visual Video Parsing task aims to recognize and temporally localize\nall events occurring in either the audio or visual stream, or both. Capturing\naccurate event semantics for each audio/visual segment is vital. Prior works\ndirectly utilize the extracted holistic audio and visual features for intra-\nand cross-modal temporal interactions. However, each segment may contain\nmultiple events, resulting in semantically mixed holistic features that can\nlead to semantic interference during intra- or cross-modal interactions: the\nevent semantics of one segment may incorporate semantics of unrelated events\nfrom other segments. To address this issue, our method begins with a\nClass-Aware Feature Decoupling (CAFD) module, which explicitly decouples the\nsemantically mixed features into distinct class-wise features, including\nmultiple event-specific features and a dedicated background feature. The\ndecoupled class-wise features enable our model to selectively aggregate useful\nsemantics for each segment from clearly matched classes contained in other\nsegments, preventing semantic interference from irrelevant classes.\nSpecifically, we further design a Fine-Grained Semantic Enhancement module for\nencoding intra- and cross-modal relations. It comprises a Segment-wise Event\nCo-occurrence Modeling (SECM) block and a Local-Global Semantic Fusion (LGSF)\nblock. The SECM exploits inter-class dependencies of concurrent events within\nthe same timestamp with the aid of a new event co-occurrence loss. The LGSF\nfurther enhances the event semantics of each segment by incorporating relevant\nsemantics from more informative global video features. Extensive experiments\nvalidate the effectiveness of the proposed modules and loss functions,\nresulting in a new state-of-the-art parsing performance.\n","authors":["Pengcheng Zhao","Jinxing Zhou","Yang Zhao","Dan Guo","Yanxiang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11248v2.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2412.12617v1","updated":"2024-12-17T07:30:09Z","published":"2024-12-17T07:30:09Z","title":"PO3AD: Predicting Point Offsets toward Better 3D Point Cloud Anomaly\n  Detection","summary":"  Point cloud anomaly detection under the anomaly-free setting poses\nsignificant challenges as it requires accurately capturing the features of 3D\nnormal data to identify deviations indicative of anomalies. Current efforts\nfocus on devising reconstruction tasks, such as acquiring normal data\nrepresentations by restoring normal samples from altered, pseudo-anomalous\ncounterparts. Our findings reveal that distributing attention equally across\nnormal and pseudo-anomalous data tends to dilute the model's focus on anomalous\ndeviations. The challenge is further compounded by the inherently disordered\nand sparse nature of 3D point cloud data. In response to those predicaments, we\nintroduce an innovative approach that emphasizes learning point offsets,\ntargeting more informative pseudo-abnormal points, thus fostering more\neffective distillation of normal data representations. We also have crafted an\naugmentation technique that is steered by normal vectors, facilitating the\ncreation of credible pseudo anomalies that enhance the efficiency of the\ntraining process. Our comprehensive experimental evaluation on the\nAnomaly-ShapeNet and Real3D-AD datasets evidences that our proposed method\noutperforms existing state-of-the-art approaches, achieving an average\nenhancement of 9.0% and 1.4% in the AUC-ROC detection metric across these\ndatasets, respectively.\n","authors":["Jianan Ye","Weiguang Zhao","Xi Yang","Guangliang Cheng","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20962v3","updated":"2024-12-17T07:13:38Z","published":"2024-07-30T16:43:24Z","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","summary":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","authors":["Xiaowei Chi","Yatian Wang","Aosong Cheng","Pengjun Fang","Zeyue Tian","Yingqing He","Zhaoyang Liu","Xingqun Qi","Jiahao Pan","Rongyu Zhang","Mengfei Li","Ruibin Yuan","Yanbing Jiang","Wei Xue","Wenhan Luo","Qifeng Chen","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2407.20962v3.pdf","comment":"15 Pages. Dataset report"},{"id":"http://arxiv.org/abs/2410.05993v3","updated":"2024-12-17T07:13:18Z","published":"2024-10-08T12:44:57Z","title":"Aria: An Open Multimodal Native Mixture-of-Experts Model","summary":"  Information comes in diverse modalities. Multimodal native AI models are\nessential to integrate real-world information and deliver comprehensive\nunderstanding. While proprietary multimodal native models exist, their lack of\nopenness imposes obstacles for adoptions, let alone adaptations. To fill this\ngap, we introduce Aria, an open multimodal native model with best-in-class\nperformance across a wide range of multimodal, language, and coding tasks. Aria\nis a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual\ntoken and text token, respectively. It outperforms Pixtral-12B and\nLlama3.2-11B, and is competitive against the best proprietary models on various\nmultimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline,\nwhich progressively equips the model with strong capabilities in language\nunderstanding, multimodal understanding, long context window, and instruction\nfollowing. We open-source the model weights along with a codebase that\nfacilitates easy adoptions and adaptations of Aria in real-world applications.\n","authors":["Dongxu Li","Yudong Liu","Haoning Wu","Yue Wang","Zhiqi Shen","Bowen Qu","Xinyao Niu","Fan Zhou","Chengen Huang","Yanpeng Li","Chongyan Zhu","Xiaoyi Ren","Chao Li","Yifan Ye","Lihuan Zhang","Hanshu Yan","Guoyin Wang","Bei Chen","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2410.05993v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12606v1","updated":"2024-12-17T07:06:10Z","published":"2024-12-17T07:06:10Z","title":"Multi-Dimensional Insights: Benchmarking Real-World Personalization in\n  Large Multimodal Models","summary":"  The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to\nevaluate the model's ability to analyze and reason beyond basic content. (2)\nRecognizing that people of different age groups have varying needs and\nperspectives when faced with the same scenario, our benchmark stratifies\nquestions into three age categories: young people, middle-aged people, and\nolder people. This design allows for a detailed assessment of LMMs'\ncapabilities in meeting the preferences and needs of different age groups. With\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\ntasks, indicating that existing LMMs still have considerable room for\nimprovement in addressing real-world applications. Looking ahead, we anticipate\nthat the MDI-Benchmark will open new pathways for aligning real-world\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\navailable at https://mdi-benchmark.github.io/\n","authors":["YiFan Zhang","Shanglin Lei","Runqi Qiao","Zhuoma GongQue","Xiaoshuai Song","Guanting Dong","Qiuna Tan","Zhe Wei","Peiqing Yang","Ye Tian","Yadong Xue","Xiaofei Wang","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12606v1.pdf","comment":"33 pages, 33 figures, Work in progress"},{"id":"http://arxiv.org/abs/2412.12603v1","updated":"2024-12-17T07:00:07Z","published":"2024-12-17T07:00:07Z","title":"RemoteTrimmer: Adaptive Structural Pruning for Remote Sensing Image\n  Classification","summary":"  Since high resolution remote sensing image classification often requires a\nrelatively high computation complexity, lightweight models tend to be practical\nand efficient. Model pruning is an effective method for model compression.\nHowever, existing methods rarely take into account the specificity of remote\nsensing images, resulting in significant accuracy loss after pruning. To this\nend, we propose an effective structural pruning approach for remote sensing\nimage classification. Specifically, a pruning strategy that amplifies the\ndifferences in channel importance of the model is introduced. Then an adaptive\nmining loss function is designed for the fine-tuning process of the pruned\nmodel. Finally, we conducted experiments on two remote sensing classification\ndatasets. The experimental results demonstrate that our method achieves minimal\naccuracy loss after compressing remote sensing classification models, achieving\nstate-of-the-art (SoTA) performance.\n","authors":["Guanwenjie Zou","Liang Yao","Fan Liu","Chuanyi Zhang","Xin Li","Ning Chen","Shengxiang Xu","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12596v1","updated":"2024-12-17T06:54:54Z","published":"2024-12-17T06:54:54Z","title":"OpenViewer: Openness-Aware Multi-View Learning","summary":"  Multi-view learning methods leverage multiple data sources to enhance\nperception by mining correlations across views, typically relying on predefined\ncategories. However, deploying these models in real-world scenarios presents\ntwo primary openness challenges. 1) Lack of Interpretability: The integration\nmechanisms of multi-view data in existing black-box models remain poorly\nexplained; 2) Insufficient Generalization: Most models are not adapted to\nmulti-view scenarios involving unknown categories. To address these challenges,\nwe propose OpenViewer, an openness-aware multi-view learning framework with\ntheoretical support. This framework begins with a Pseudo-Unknown Sample\nGeneration Mechanism to efficiently simulate open multi-view environments and\npreviously adapt to potential unknown samples. Subsequently, we introduce an\nExpression-Enhanced Deep Unfolding Network to intuitively promote\ninterpretability by systematically constructing functional prior-mapping\nmodules and effectively providing a more transparent integration mechanism for\nmulti-view data. Additionally, we establish a Perception-Augmented Open-Set\nTraining Regime to significantly enhance generalization by precisely boosting\nconfidences for known categories and carefully suppressing inappropriate\nconfidences for unknown ones. Experimental results demonstrate that OpenViewer\neffectively addresses openness challenges while ensuring recognition\nperformance for both known and unknown samples. The code is released at\nhttps://github.com/dushide/OpenViewer.\n","authors":["Shide Du","Zihan Fang","Yanchao Tan","Changwei Wang","Shiping Wang","Wenzhong Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12596v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2411.16657v2","updated":"2024-12-17T06:52:46Z","published":"2024-11-25T18:41:56Z","title":"DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation","summary":"  Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.\n","authors":["Zun Wang","Jialu Li","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.16657v2.pdf","comment":"Project website: https://zunwang1.github.io/DreamRunner"},{"id":"http://arxiv.org/abs/2412.12594v1","updated":"2024-12-17T06:50:23Z","published":"2024-12-17T06:50:23Z","title":"A Simple and Efficient Baseline for Zero-Shot Generative Classification","summary":"  Large diffusion models have become mainstream generative models in both\nacademic studies and industrial AIGC applications. Recently, a number of works\nfurther explored how to employ the power of large diffusion models as zero-shot\nclassifiers. While recent zero-shot diffusion-based classifiers have made\nperformance advancement on benchmark datasets, they still suffered badly from\nextremely slow classification speed (e.g., ~1000 seconds per classifying single\nimage on ImageNet). The extremely slow classification speed strongly prohibits\nexisting zero-shot diffusion-based classifiers from practical applications. In\nthis paper, we propose an embarrassingly simple and efficient zero-shot\nGaussian Diffusion Classifiers (GDC) via pretrained text-to-image diffusion\nmodels and DINOv2. The proposed GDC can not only significantly surpass previous\nzero-shot diffusion-based classifiers by over 10 points (61.40% - 71.44%) on\nImageNet, but also accelerate more than 30000 times (1000 - 0.03 seconds)\nclassifying a single image on ImageNet. Additionally, it provides probability\ninterpretation of the results. Our extensive experiments further demonstrate\nthat GDC can achieve highly competitive zero-shot classification performance\nover various datasets and can promisingly self-improve with stronger diffusion\nmodels. To the best of our knowledge, the proposed GDC is the first zero-shot\ndiffusionbased classifier that exhibits both competitive accuracy and practical\nefficiency.\n","authors":["Zipeng Qi","Buhua Liu","Shiyan Zhang","Bao Li","Zhiqiang Xu","Haoyi Xiong","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2412.12594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10638v2","updated":"2024-12-17T06:48:10Z","published":"2024-06-15T13:58:26Z","title":"Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly","summary":"  Multimodal Large Language Models (MLLMs) have displayed remarkable\nperformance in multi-modal tasks, particularly in visual comprehension.\nHowever, we reveal that MLLMs often generate incorrect answers even when they\nunderstand the visual content. To this end, we manually construct a benchmark\nwith 12 categories and design evaluation metrics that assess the degree of\nerror in MLLM responses even when the visual content is seemingly understood.\nBased on this benchmark, we test 15 leading MLLMs and analyze the distribution\nof attention maps and logits of some MLLMs. Our investigation identifies two\nprimary issues: 1) most instruction tuning datasets predominantly feature\nquestions that 'directly' relate to the visual content, leading to a bias in\nMLLMs' responses to other indirect questions, and 2) MLLMs' attention to visual\ntokens is notably lower than to system and question tokens. We further observe\nthat attention scores between questions and visual tokens as well as the\nmodel's confidence in the answers are lower in response to misleading questions\nthan to straightforward ones. To address the first challenge, we introduce a\npaired positive and negative data construction pipeline to diversify the\ndataset. For the second challenge, we propose to enhance the model's focus on\nvisual content during decoding by refining the text and visual prompt. For the\ntext prompt, we propose a content guided refinement strategy that performs\npreliminary visual content analysis to generate structured information before\nanswering the question. Additionally, we employ a visual attention refinement\nstrategy that highlights question-relevant visual tokens to increase the\nmodel's attention to visual content that aligns with the question. Extensive\nexperiments demonstrate that these challenges can be significantly mitigated\nwith our proposed dataset and techniques.\n","authors":["Yexin Liu","Zhengyang Liang","Yueze Wang","Xianfeng Wu","Feilong Tang","Muyang He","Jian Li","Zheng Liu","Harry Yang","Sernam Lim","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.10638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14358v2","updated":"2024-12-17T06:40:05Z","published":"2024-11-21T17:58:07Z","title":"InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating SLAM in\n  Indoor Pedestrian-Rich Spaces for Human Navigation","summary":"  Simultaneous localization and mapping (SLAM) techniques can be used to\nnavigate the visually impaired, but the development of robust SLAM solutions\nfor crowded spaces is limited by the lack of realistic datasets. To address\nthis, we introduce InCrowd-VI, a novel visual-inertial dataset specifically\ndesigned for human navigation in indoor pedestrian-rich environments. Recorded\nusing Meta Aria Project glasses, it captures realistic scenarios without\nenvironmental control. InCrowd-VI features 58 sequences totaling a 5 km\ntrajectory length and 1.5 hours of recording time, including RGB, stereo\nimages, and IMU measurements. The dataset captures important challenges such as\npedestrian occlusions, varying crowd densities, complex layouts, and lighting\nchanges. Ground-truth trajectories, accurate to approximately 2 cm, are\nprovided in the dataset, originating from the Meta Aria project machine\nperception SLAM service. In addition, a semi-dense 3D point cloud of scenes is\nprovided for each sequence. The evaluation of state-of-the-art visual odometry\n(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitations\nin these realistic scenarios. Under challenging conditions, systems exceeded\nthe required localization accuracy of 0.5 meters and the 1\\% drift threshold,\nwith classical methods showing drift up to 5-10\\%. While deep learning-based\napproaches maintained high pose estimation coverage (>90\\%), they failed to\nachieve real-time processing speeds necessary for walking pace navigation.\nThese results demonstrate the need and value of a new dataset to advance SLAM\nresearch for visually impaired navigation in complex indoor environments. The\ndataset and associated tools are publicly available at\nhttps://incrowd-vi.cloudlab.zhaw.ch/.\n","authors":["Marziyeh Bamdad","Hans-Peter Hutter","Alireza Darvishy"],"pdf_url":"https://arxiv.org/pdf/2411.14358v2.pdf","comment":"24 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.08939v2","updated":"2024-12-17T06:30:00Z","published":"2024-12-12T05:01:17Z","title":"Dynamic Contrastive Knowledge Distillation for Efficient Image\n  Restoration","summary":"  Knowledge distillation (KD) is a valuable yet challenging approach that\nenhances a compact student network by learning from a high-performance but\ncumbersome teacher model. However, previous KD methods for image restoration\noverlook the state of the student during the distillation, adopting a fixed\nsolution space that limits the capability of KD. Additionally, relying solely\non L1-type loss struggles to leverage the distribution information of images.\nIn this work, we propose a novel dynamic contrastive knowledge distillation\n(DCKD) framework for image restoration. Specifically, we introduce dynamic\ncontrastive regularization to perceive the student's learning state and\ndynamically adjust the distilled solution space using contrastive learning.\nAdditionally, we also propose a distribution mapping module to extract and\nalign the pixel-level category distribution of the teacher and student models.\nNote that the proposed DCKD is a structure-agnostic distillation framework,\nwhich can adapt to different backbones and can be combined with methods that\noptimize upper-bound constraints to further enhance model performance.\nExtensive experiments demonstrate that DCKD significantly outperforms the\nstate-of-the-art KD methods across various image restoration tasks and\nbackbones.\n","authors":["Yunshuai Zhou","Junbo Qiao","Jincheng Liao","Wei Li","Simiao Li","Jiao Xie","Yunhang Shen","Jie Hu","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.08939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09224v3","updated":"2024-12-17T06:21:52Z","published":"2024-12-12T12:26:08Z","title":"DASK: Distribution Rehearsing via Adaptive Style Kernel Learning for\n  Exemplar-Free Lifelong Person Re-Identification","summary":"  Lifelong person re-identification (LReID) is an important but challenging\ntask that suffers from catastrophic forgetting due to significant domain gaps\nbetween training steps. Existing LReID approaches typically rely on data replay\nand knowledge distillation to mitigate this issue. However, data replay methods\ncompromise data privacy by storing historical exemplars, while knowledge\ndistillation methods suffer from limited performance due to the cumulative\nforgetting of undistilled knowledge. To overcome these challenges, we propose a\nnovel paradigm that models and rehearses the distribution of the old domains to\nenhance knowledge consolidation during the new data learning, possessing a\nstrong anti-forgetting capacity without storing any exemplars. Specifically, we\nintroduce an exemplar-free LReID method called Distribution Rehearsing via\nAdaptive Style Kernel Learning (DASK). DASK includes a Distribution Rehearser\nLearning (DRL) mechanism that learns to transform arbitrary distribution data\ninto the current data style at each learning step. To enhance the style\ntransfer capacity of DRL, an Adaptive Kernel Prediction Network (AKPNet) is\nexplored to achieve an instance-specific distribution adjustment. Additionally,\nwe design a Distribution Rehearsing-driven LReID Training (DRRT) module, which\nrehearses old distribution based on the new data via the old AKPNet model,\nachieving effective new-old knowledge accumulation under a joint knowledge\nconsolidation scheme. Experimental results show our DASK outperforms the\nexisting methods by 3.6%-6.8% and 4.5%-6.5% on anti-forgetting and\ngeneralization capacity, respectively. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-LReID-DASK\n","authors":["Kunlun Xu","Chenghao Jiang","Peixi Xiong","Yuxin Peng","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.09224v3.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.00473v4","updated":"2024-12-17T06:09:08Z","published":"2024-11-30T13:21:15Z","title":"Jailbreak Large Vision-Language Models Through Multi-Modal Linkage","summary":"  With the significant advancement of Large Vision-Language Models (VLMs),\nconcerns about their potential misuse and abuse have grown rapidly. Previous\nstudies have highlighted VLMs' vulnerability to jailbreak attacks, where\ncarefully crafted inputs can lead the model to produce content that violates\nethical and legal standards. However, existing methods struggle against\nstate-of-the-art VLMs like GPT-4o, due to the over-exposure of harmful content\nand lack of stealthy malicious guidance. In this work, we propose a novel\njailbreak attack framework: Multi-Modal Linkage (MML) Attack. Drawing\ninspiration from cryptography, MML utilizes an encryption-decryption process\nacross text and image modalities to mitigate over-exposure of malicious\ninformation. To align the model's output with malicious intent covertly, MML\nemploys a technique called \"evil alignment\", framing the attack within a video\ngame production scenario. Comprehensive experiments demonstrate MML's\neffectiveness. Specifically, MML jailbreaks GPT-4o with attack success rates of\n97.80% on SafeBench, 98.81% on MM-SafeBench and 99.07% on HADES-Dataset. Our\ncode is available at https://github.com/wangyu-ovo/MML\n","authors":["Yu Wang","Xiaofei Zhou","Yichen Wang","Geyuan Zhang","Tianxing He"],"pdf_url":"https://arxiv.org/pdf/2412.00473v4.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.13170v1","updated":"2024-12-17T18:47:57Z","published":"2024-12-17T18:47:57Z","title":"Re-calibrating methodologies in social media research: Challenge the\n  visual, work with Speech","summary":"  This article methodologically reflects on how social media scholars can\neffectively engage with speech-based data in their analyses. While contemporary\nmedia studies have embraced textual, visual, and relational data, the aural\ndimension remained comparatively under-explored. Building on the notion of\nsecondary orality and rejection towards purely visual culture, the paper argues\nthat considering voice and speech at scale enriches our understanding of\nmultimodal digital content. The paper presents the TikTok Subtitles Toolkit\nthat offers accessible speech processing readily compatible with existing\nworkflows. In doing so, it opens new avenues for large-scale inquiries that\nblend quantitative insights with qualitative precision. Two illustrative cases\nhighlight both opportunities and limitations of speech research: while genres\nlike #storytime on TikTok benefit from the exploration of spoken narratives,\nnonverbal or music-driven content may not yield significant insights using\nspeech data. The article encourages researchers to integrate aural exploration\nthoughtfully to complement existing methods, rather than replacing them. I\nconclude that the expansion of our methodological repertoire enables richer\ninterpretations of platformised content, and our capacity to unpack digital\ncultures as they become increasingly multimodal.\n","authors":["Hongrui Jin"],"pdf_url":"https://arxiv.org/pdf/2412.13170v1.pdf","comment":"11 pages (excluding references), 3 figures"},{"id":"http://arxiv.org/abs/2412.13163v1","updated":"2024-12-17T18:42:21Z","published":"2024-12-17T18:42:21Z","title":"C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System","summary":"  Organizations seeking to utilize Large Language Models (LLMs) for knowledge\nquerying and analysis often encounter challenges in maintaining an LLM\nfine-tuned on targeted, up-to-date information that keeps answers relevant and\ngrounded. Retrieval Augmented Generation (RAG) has quickly become a feasible\nsolution for organizations looking to overcome the challenges of maintaining\nproprietary models and to help reduce LLM hallucinations in their query\nresponses. However, RAG comes with its own issues regarding scaling data\npipelines across tiered-access and disparate data sources. In many scenarios,\nit is necessary to query beyond a single data silo to provide richer and more\nrelevant context for an LLM. Analyzing data sources within and across\norganizational trust boundaries is often limited by complex data-sharing\npolicies that prohibit centralized data storage, therefore, inhibit the fast\nand effective setup and scaling of RAG solutions. In this paper, we introduce\nConfidential Computing (CC) techniques as a solution for secure Federated\nRetrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG\nsystem (C-FedRAG) enables secure connection and scaling of a RAG workflows\nacross a decentralized network of data providers by ensuring context\nconfidentiality. We also demonstrate how to implement a C-FedRAG system using\nthe NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and\nMIRAGE benchmarking dataset.\n","authors":["Parker Addison","Minh-Tuan H. Nguyen","Tomislav Medan","Mohammad T. Manzari","Brendan McElrone","Laksh Lalwani","Aboli More","Smita Sharma","Holger R. Roth","Isaac Yang","Chester Chen","Daguang Xu","Yan Cheng","Andrew Feng","Ziyue Xu"],"pdf_url":"https://arxiv.org/pdf/2412.13163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13102v1","updated":"2024-12-17T17:15:21Z","published":"2024-12-17T17:15:21Z","title":"AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark","summary":"  Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.\n","authors":["Jianlyu Chen","Nan Wang","Chaofan Li","Bo Wang","Shitao Xiao","Han Xiao","Hao Liao","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13102v1.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13071v1","updated":"2024-12-17T16:38:10Z","published":"2024-12-17T16:38:10Z","title":"CLASP: Contrastive Language-Speech Pretraining for Multilingual\n  Multimodal Information Retrieval","summary":"  This study introduces CLASP (Contrastive Language-Speech Pretraining), a\nmultilingual, multimodal representation tailored for audio-text information\nretrieval. CLASP leverages the synergy between spoken content and textual data.\nDuring training, we utilize our newly introduced speech-text dataset, which\nencompasses 15 diverse categories ranging from fiction to religion. CLASP's\naudio component integrates audio spectrograms with a pre-trained\nself-supervised speech model, while its language encoding counterpart employs a\nsentence encoder pre-trained on over 100 languages. This unified lightweight\nmodel bridges the gap between various modalities and languages, enhancing its\neffectiveness in handling and retrieving multilingual and multimodal data. Our\nevaluations across multiple languages demonstrate that CLASP establishes new\nbenchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional\nASR-based retrieval approaches in specific scenarios.\n","authors":["Mohammad Mahdi Abootorabi","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2412.13071v1.pdf","comment":"accepted at ECIR 2025"},{"id":"http://arxiv.org/abs/2412.12997v1","updated":"2024-12-17T15:21:28Z","published":"2024-12-17T15:21:28Z","title":"Enabling Low-Resource Language Retrieval: Establishing Baselines for\n  Urdu MS MARCO","summary":"  As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. This paper introduces the first large-scale\nUrdu IR dataset, created by translating the MS MARCO dataset through machine\ntranslation. We establish baseline results through zero-shot learning for IR in\nUrdu and subsequently apply the mMARCO multilingual IR methodology to this\nnewly translated dataset. Our findings demonstrate that the fine-tuned model\n(Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a\nRecall@10 of 0.439, representing significant improvements over zero-shot\nresults and showing the potential for expanding IR access for Urdu speakers. By\nbridging access gaps for speakers of low-resource languages, this work not only\nadvances multilingual IR research but also emphasizes the ethical and societal\nimportance of inclusive IR technologies. This work provides valuable insights\ninto the challenges and solutions for improving language representation and\nlays the groundwork for future research, especially in South Asian languages,\nwhich can benefit from the adaptable methods used in this study.\n","authors":["Umer Butt","Stalin Veranasi","Günter Neumann"],"pdf_url":"https://arxiv.org/pdf/2412.12997v1.pdf","comment":"6 pages, ECIR 2025, conference submission version"},{"id":"http://arxiv.org/abs/2412.12984v1","updated":"2024-12-17T15:04:54Z","published":"2024-12-17T15:04:54Z","title":"Cluster-guided Contrastive Class-imbalanced Graph Classification","summary":"  This paper studies the problem of class-imbalanced graph classification,\nwhich aims at effectively classifying the categories of graphs in scenarios\nwith imbalanced class distribution. Despite the tremendous success of graph\nneural networks (GNNs), their modeling ability for imbalanced graph-structured\ndata is inadequate, which typically leads to predictions biased towards the\nmajority classes. Besides, existing class-imbalanced learning methods in\nvisions may overlook the rich graph semantic substructures of the majority\nclasses and excessively emphasize learning from the minority classes. To tackle\nthis issue, this paper proposes a simple yet powerful approach called C$^3$GNN\nthat incorporates the idea of clustering into contrastive learning to enhance\nclass-imbalanced graph classification. Technically, C$^3$GNN clusters graphs\nfrom each majority class into multiple subclasses, ensuring they have similar\nsizes to the minority class, thus alleviating class imbalance. Additionally, it\nutilizes the Mixup technique to synthesize new samples and enrich the semantic\ninformation of each subclass, and leverages supervised contrastive learning to\nhierarchically learn effective graph representations. In this way, we can not\nonly sufficiently explore the semantic substructures within the majority class\nbut also effectively alleviate excessive focus on the minority class. Extensive\nexperiments on real-world graph benchmark datasets verify the superior\nperformance of our proposed method.\n","authors":["Wei Ju","Zhengyang Mao","Siyu Yi","Yifang Qin","Yiyang Gu","Zhiping Xiao","Jianhao Shen","Ziyue Qiao","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12984v1.pdf","comment":"Accepted by Proceedings of the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2406.08270v2","updated":"2024-12-17T12:56:42Z","published":"2024-06-12T14:35:43Z","title":"It is Never Too Late to Mend: Separate Learning for Multimedia\n  Recommendation","summary":"  Multimedia recommendation, which incorporates various modalities (e.g.,\nimages, texts, etc.) into user or item representation to improve recommendation\nquality, and self-supervised learning carries multimedia recommendation to a\nplateau of performance, because of its superior performance in aligning\ndifferent modalities. However, more and more research finds that aligning all\nmodal representations is suboptimal because it damages the unique attributes of\neach modal. These studies use subtraction and orthogonal constraints in\ngeometric space to learn unique parts. However, our rigorous analysis reveals\nthe flaws in this approach, such as that subtraction does not necessarily yield\nthe desired modal-unique and that orthogonal constraints are ineffective in\nuser and item high-dimensional representation spaces. To make up for the\nprevious weaknesses, we propose Separate Learning (SEA) for multimedia\nrecommendation, which mainly includes mutual information view of modal-unique\nand -generic learning. Specifically, we first use GNN to learn the\nrepresentations of users and items in different modalities and split each modal\nrepresentation into generic and unique parts. We employ contrastive log-ratio\nupper bound to minimize the mutual information between the general and unique\nparts within the same modality, to distance their representations, thus\nlearning modal-unique features. Then, we design Solosimloss to maximize the\nlower bound of mutual information, to align the general parts of different\nmodalities, thus learning more high-quality modal-generic features. Finally,\nextensive experiments on three datasets demonstrate the effectiveness and\ngeneralization of our proposed framework. The code is available at SEA and the\nfull training record of the main experiment.\n","authors":["Zhuangzhuang He","Zihan Wang","Yonghui Yang","Haoyue Bai","Le Wu"],"pdf_url":"https://arxiv.org/pdf/2406.08270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12852v1","updated":"2024-12-17T12:26:14Z","published":"2024-12-17T12:26:14Z","title":"Selective Shot Learning for Code Explanation","summary":"  Code explanation plays a crucial role in the software engineering domain,\naiding developers in grasping code functionality efficiently. Recent work shows\nthat the performance of LLMs for code explanation improves in a few-shot\nsetting, especially when the few-shot examples are selected intelligently.\nState-of-the-art approaches for such Selective Shot Learning (SSL) include\ntoken-based and embedding-based methods. However, these SSL approaches have\nbeen evaluated on proprietary LLMs, without much exploration on open-source\nCode-LLMs. Additionally, these methods lack consideration for programming\nlanguage syntax. To bridge these gaps, we present a comparative study and\npropose a novel SSL method (SSL_ner) that utilizes entity information for\nfew-shot example selection. We present several insights and show the\neffectiveness of SSL_ner approach over state-of-the-art methods across two\ndatasets. To the best of our knowledge, this is the first systematic\nbenchmarking of open-source Code-LLMs while assessing the performances of the\nvarious few-shot examples selection approaches for the code explanation task.\n","authors":["Paheli Bhattacharya","Rishabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2412.12852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12836v1","updated":"2024-12-17T11:58:55Z","published":"2024-12-17T11:58:55Z","title":"A Survey on Recommendation Unlearning: Fundamentals, Taxonomy,\n  Evaluation, and Open Questions","summary":"  Recommender systems have become increasingly influential in shaping user\nbehavior and decision-making, highlighting their growing impact in various\ndomains. Meanwhile, the widespread adoption of machine learning models in\nrecommender systems has raised significant concerns regarding user privacy and\nsecurity. As compliance with privacy regulations becomes more critical, there\nis a pressing need to address the issue of recommendation unlearning, i.e.,\neliminating the memory of specific training data from the learned\nrecommendation models. Despite its importance, traditional machine unlearning\nmethods are ill-suited for recommendation unlearning due to the unique\nchallenges posed by collaborative interactions and model parameters. This\nsurvey offers a comprehensive review of the latest advancements in\nrecommendation unlearning, exploring the design principles, challenges, and\nmethodologies associated with this emerging field. We provide a unified\ntaxonomy that categorizes different recommendation unlearning approaches,\nfollowed by a summary of widely used benchmarks and metrics for evaluation. By\nreviewing the current state of research, this survey aims to guide the\ndevelopment of more efficient, scalable, and robust recommendation unlearning\ntechniques. Furthermore, we identify open research questions in this field,\nwhich could pave the way for future innovations not only in recommendation\nunlearning but also in a broader range of unlearning tasks across different\nmachine learning applications.\n","authors":["Yuyuan Li","Xiaohua Feng","Chaochao Chen","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12806v1","updated":"2024-12-17T11:21:09Z","published":"2024-12-17T11:21:09Z","title":"Cross-Dialect Information Retrieval: Information Access in Low-Resource\n  and High-Variance Languages","summary":"  A large amount of local and culture-specific knowledge (e.g., people,\ntraditions, food) can only be found in documents written in dialects. While\nthere has been extensive research conducted on cross-lingual information\nretrieval (CLIR), the field of cross-dialect retrieval (CDIR) has received\nlimited attention. Dialect retrieval poses unique challenges due to the limited\navailability of resources to train retrieval models and the high variability in\nnon-standardized languages. We study these challenges on the example of German\ndialects and introduce the first German dialect retrieval dataset, dubbed\nWikiDIR, which consists of seven German dialects extracted from Wikipedia.\nUsing WikiDIR, we demonstrate the weakness of lexical methods in dealing with\nhigh lexical variation in dialects. We further show that commonly used\nzero-shot cross-lingual transfer approach with multilingual encoders do not\ntransfer well to extremely low-resource setups, motivating the need for\nresource-lean and dialect-specific retrieval models. We finally demonstrate\nthat (document) translation is an effective way to reduce the dialect gap in\nCDIR.\n","authors":["Robert Litschko","Oliver Kraus","Verena Blaschke","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2412.12806v1.pdf","comment":"Accepted at COLING 2025"},{"id":"http://arxiv.org/abs/2412.12775v1","updated":"2024-12-17T10:36:52Z","published":"2024-12-17T10:36:52Z","title":"RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service","summary":"  Retrieval-augmented generation (RAG) improves the service quality of large\nlanguage models by retrieving relevant documents from credible literature and\nintegrating them into the context of the user query. Recently, the rise of the\ncloud RAG service has made it possible for users to query relevant documents\nconveniently. However, directly sending queries to the cloud brings potential\nprivacy leakage. In this paper, we are the first to formally define the\nprivacy-preserving cloud RAG service to protect the user query and propose\nRemoteRAG as a solution regarding privacy, efficiency, and accuracy. For\nprivacy, we introduce $(n,\\epsilon)$-DistanceDP to characterize privacy leakage\nof the user query and the leakage inferred from relevant documents. For\nefficiency, we limit the search range from the total documents to a small\nnumber of selected documents related to a perturbed embedding generated from\n$(n,\\epsilon)$-DistanceDP, so that computation and communication costs required\nfor privacy protection significantly decrease. For accuracy, we ensure that the\nsmall range includes target documents related to the user query with detailed\ntheoretical analysis. Experimental results also demonstrate that RemoteRAG can\nresist existing embedding inversion attack methods while achieving no loss in\nretrieval under various settings. Moreover, RemoteRAG is efficient, incurring\nonly $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$\nGB with the non-optimized privacy-preserving scheme) when retrieving from a\ntotal of $10^6$ documents.\n","authors":["Yihang Cheng","Lan Zhang","Junyang Wang","Mu Yuan","Yunhao Yao"],"pdf_url":"https://arxiv.org/pdf/2412.12775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12770v1","updated":"2024-12-17T10:33:13Z","published":"2024-12-17T10:33:13Z","title":"A Survey on Sequential Recommendation","summary":"  Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield.\n","authors":["Liwei Pan","Weike Pan","Meiyan Wei","Hongzhi Yin","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2412.12770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12754v1","updated":"2024-12-17T10:19:44Z","published":"2024-12-17T10:19:44Z","title":"Token-Level Graphs for Short Text Classification","summary":"  The classification of short texts is a common subtask in Information\nRetrieval (IR). Recent advances in graph machine learning have led to interest\nin graph-based approaches for low resource scenarios, showing promise in such\nsettings. However, existing methods face limitations such as not accounting for\ndifferent meanings of the same words or constraints from transductive\napproaches. We propose an approach which constructs text graphs entirely based\non tokens obtained through pre-trained language models (PLMs). By applying a\nPLM to tokenize and embed the texts when creating the graph(-nodes), our method\ncaptures contextual and semantic information, overcomes vocabulary constraints,\nand allows for context-dependent word meanings. Our approach also makes\nclassification more efficient with reduced parameters compared to classical PLM\nfine-tuning, resulting in more robust training with few samples. Experimental\nresults demonstrate how our method consistently achieves higher scores or\non-par performance with existing methods, presenting an advancement in\ngraph-based text classification techniques. To support reproducibility of our\nwork we make all implementations publicly available to the\ncommunity\\footnote{\\url{https://github.com/doGregor/TokenGraph}}.\n","authors":["Gregor Donabauer","Udo Kruschwitz"],"pdf_url":"https://arxiv.org/pdf/2412.12754v1.pdf","comment":"Preprint accepted at the 47th European Conference on Information\n  Retrieval (ECIR 2025)"},{"id":"http://arxiv.org/abs/2412.12612v1","updated":"2024-12-17T07:21:25Z","published":"2024-12-17T07:21:25Z","title":"SynthCypher: A Fully Synthetic Data Generation Framework for\n  Text-to-Cypher Querying in Knowledge Graphs","summary":"  Cypher, the query language for Neo4j graph databases, plays a critical role\nin enabling graph-based analytics and data exploration. While substantial\nresearch has been dedicated to natural language to SQL query generation\n(Text2SQL), the analogous problem for graph databases referred to as\nText2Cypher remains underexplored. In this work, we introduce SynthCypher, a\nfully synthetic and automated data generation pipeline designed to address this\ngap. SynthCypher employs a novel LLMSupervised Generation-Verification\nframework, ensuring syntactically and semantically correct Cypher queries\nacross diverse domains and query complexities. Using this pipeline, we create\nSynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher\ninstances. Fine-tuning open-source large language models (LLMs), including\nLLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant\nperformance improvements of up to 40% on the Text2Cypher test set and 30% on\nthe SPIDER benchmark adapted for graph databases. This work demonstrates that\nhigh-quality synthetic data can effectively advance the state-of-the-art in\nText2Cypher tasks.\n","authors":["Aman Tiwari","Shiva Krishna Reddy Malay","Vikas Yadav","Masoud Hashemi","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2412.12612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12559v1","updated":"2024-12-17T05:38:27Z","published":"2024-12-17T05:38:27Z","title":"EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation","summary":"  We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT\n","authors":["Taeho Hwang","Sukmin Cho","Soyeong Jeong","Hoyun Song","SeungYoon Han","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2412.12559v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2405.15328v2","updated":"2024-12-17T05:35:15Z","published":"2024-05-24T08:11:59Z","title":"Multi-Modal Recommendation Unlearning for Legal, Licensing, and Modality\n  Constraints","summary":"  User data spread across multiple modalities has popularized multi-modal\nrecommender systems (MMRS). They recommend diverse content such as products,\nsocial media posts, TikTok reels, etc., based on a user-item interaction graph.\nWith rising data privacy demands, recent methods propose unlearning private\nuser data from uni-modal recommender systems (RS). However, methods for\nunlearning item data related to outdated user preferences, revoked licenses,\nand legally requested removals are still largely unexplored.\n  Previous RS unlearning methods are unsuitable for MMRS due to the\nincompatibility of their matrix-based representation with the multi-modal\nuser-item interaction graph. Moreover, their data partitioning step degrades\nperformance on each shard due to poor data heterogeneity and requires costly\nperformance aggregation across shards.\n  This paper introduces MMRecUn, the first approach known to us for unlearning\nin MMRS and unlearning item data. Given a trained RS model, MMRecUn employs a\nnovel Reverse Bayesian Personalized Ranking (BPR) objective to enable the model\nto forget marked data. The reverse BPR attenuates the impact of user-item\ninteractions within the forget set, while the forward BPR reinforces the\nsignificance of user-item interactions within the retain set. Our experiments\ndemonstrate that MMRecUn outperforms baseline methods across various unlearning\nrequests when evaluated on benchmark MMRS datasets. MMRecUn achieves recall\nperformance improvements of up to 49.85% compared to baseline methods and is up\nto $\\mathbf{1.3}\\times$ faster than the Gold model, which is trained on retain\nset from scratch. MMRecUn offers significant advantages, including superiority\nin removing target interactions, preserving retained interactions, and zero\noverhead costs compared to previous methods. The code will be released after\nreview.\n","authors":["Yash Sinha","Murari Mandal","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2405.15328v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2402.06871v5","updated":"2024-12-17T05:19:27Z","published":"2024-02-10T03:21:13Z","title":"Non-autoregressive Generative Models for Reranking Recommendation","summary":"  Contemporary recommendation systems are designed to meet users' needs by\ndelivering tailored lists of items that align with their specific demands or\ninterests. In a multi-stage recommendation system, reranking plays a crucial\nrole by modeling the intra-list correlations among items. The key challenge of\nreranking lies in the exploration of optimal sequences within the combinatorial\nspace of permutations. Recent research proposes a generator-evaluator learning\nparadigm, where the generator generates multiple feasible sequences and the\nevaluator picks out the best sequence based on the estimated listwise score.\nThe generator is of vital importance, and generative models are well-suited for\nthe generator function. Current generative models employ an autoregressive\nstrategy for sequence generation. However, deploying autoregressive models in\nreal-time industrial systems is challenging. To address these issues, we\npropose a Non-AutoRegressive generative model for reranking Recommendation\n(NAR4Rec) designed to enhance efficiency and effectiveness. To tackle\nchallenges such as sparse training samples and dynamic candidates, we introduce\na matching model. Considering the diverse nature of user feedback, we employ a\nsequence-level unlikelihood training objective to differentiate feasible\nsequences from unfeasible ones. Additionally, to overcome the lack of\ndependency modeling in non-autoregressive models regarding target items, we\nintroduce contrastive decoding to capture correlations among these items.\nExtensive offline experiments validate the superior performance of NAR4Rec over\nstate-of-the-art reranking methods. Online A/B tests reveal that NAR4Rec\nsignificantly enhances the user experience. Furthermore, NAR4Rec has been fully\ndeployed in a popular video app Kuaishou with over 300 million daily active\nusers.\n","authors":["Yuxin Ren","Qiya Yang","Yichun Wu","Wei Xu","Yalong Wang","Zhiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.06871v5.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2412.12504v1","updated":"2024-12-17T03:10:47Z","published":"2024-12-17T03:10:47Z","title":"Boosting LLM-based Relevance Modeling with Distribution-Aware Robust\n  Learning","summary":"  With the rapid advancement of pre-trained large language models (LLMs),\nrecent endeavors have leveraged the capabilities of LLMs in relevance modeling,\nresulting in enhanced performance. This is usually done through the process of\nfine-tuning LLMs on specifically annotated datasets to determine the relevance\nbetween queries and items. However, there are two limitations when LLMs are\nnaively employed for relevance modeling through fine-tuning and inference.\nFirst, it is not inherently efficient for performing nuanced tasks beyond\nsimple yes or no answers, such as assessing search relevance. It may therefore\ntend to be overconfident and struggle to distinguish fine-grained degrees of\nrelevance (e.g., strong relevance, weak relevance, irrelevance) used in search\nengines. Second, it exhibits significant performance degradation when\nconfronted with data distribution shift in real-world scenarios. In this paper,\nwe propose a novel Distribution-Aware Robust Learning framework (DaRL) for\nrelevance modeling in Alipay Search. Specifically, we design an effective loss\nfunction to enhance the discriminability of LLM-based relevance modeling across\nvarious fine-grained degrees of query-item relevance. To improve the\ngeneralizability of LLM-based relevance modeling, we first propose the\nDistribution-Aware Sample Augmentation (DASA) module. This module utilizes\nout-of-distribution (OOD) detection techniques to actively select appropriate\nsamples that are not well covered by the original training set for model\nfine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to\nsimultaneously improve in-distribution (ID) and OOD performance, bridging the\nperformance gap between them. DaRL has been deployed online to serve the\nAlipay's insurance product search...\n","authors":["Hong Liu","Saisai Gong","Yixin Ji","Kaixin Wu","Jia Xu","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12504v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.12486v1","updated":"2024-12-17T02:43:54Z","published":"2024-12-17T02:43:54Z","title":"Boosting Long-Context Information Seeking via Query-Guided Activation\n  Refilling","summary":"  Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.\n","authors":["Hongjin Qian","Zheng Liu","Peitian Zhang","Zhicheng Dou","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2412.12486v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.12464v1","updated":"2024-12-17T01:52:15Z","published":"2024-12-17T01:52:15Z","title":"LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph\n  Reasoning for Cold-start Sequential Recommendation","summary":"  Knowledge Graphs (KGs) represent relationships between entities in a graph\nstructure and have been widely studied as promising tools for realizing\nrecommendations that consider the accurate content information of items.\nHowever, traditional KG-based recommendation methods face fundamental\nchallenges: insufficient consideration of temporal information and poor\nperformance in cold-start scenarios. On the other hand, Large Language Models\n(LLMs) can be considered databases with a wealth of knowledge learned from the\nweb data, and they have recently gained attention due to their potential\napplication as recommendation systems. Although approaches that treat LLMs as\nrecommendation systems can leverage LLMs' high recommendation literacy, their\ninput token limitations make it impractical to consider the entire\nrecommendation domain dataset and result in scalability issues. To address\nthese challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning\nmodel (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive\nexploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we\ntrained a recommendation agent through reinforcement learning using a reward\nfunction that integrates different recommendation strategies, including LLM's\nintuition and KG embeddings. By incorporating temporal awareness through prompt\nengineering and generating textual representations of user preferences from\nlimited interactions, LIKR can improve recommendation performance in cold-start\nscenarios. Furthermore, LIKR can avoid scalability issues by using KGs to\nrepresent recommendation domain datasets and limiting the LLM's output to KG\nexploration strategies. Experiments on real-world datasets demonstrate that our\nmodel outperforms state-of-the-art recommendation methods in cold-start\nsequential recommendation scenarios.\n","authors":["Keigo Sakurai","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2412.12464v1.pdf","comment":"Accepted to the 47th European Conference on Information Retrieval\n  (ECIR2025)"},{"id":"http://arxiv.org/abs/2412.12459v1","updated":"2024-12-17T01:43:44Z","published":"2024-12-17T01:43:44Z","title":"LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework","summary":"  Topic modeling is widely used for uncovering thematic structures within text\ncorpora, yet traditional models often struggle with specificity and coherence\nin domain-focused applications. Guided approaches, such as SeededLDA and CorEx,\nincorporate user-provided seed words to improve relevance but remain\nlabor-intensive and static. Large language models (LLMs) offer potential for\ndynamic topic refinement and discovery, yet their application often incurs high\nAPI costs. To address these challenges, we propose the LLM-assisted Iterative\nTopic Augmentation framework (LITA), an LLM-assisted approach that integrates\nuser-provided seeds with embedding-based clustering and iterative refinement.\nLITA identifies a small number of ambiguous documents and employs an LLM to\nreassign them to existing or new topics, minimizing API costs while enhancing\ntopic quality. Experiments on two datasets across topic quality and clustering\nperformance metrics demonstrate that LITA outperforms five baseline models,\nincluding LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an\nefficient and adaptable framework for advancing topic modeling and text\nclustering.\n","authors":["Chia-Hsuan Chang","Jui-Tse Tsai","Yi-Hang Tsai","San-Yih Hwang"],"pdf_url":"https://arxiv.org/pdf/2412.12459v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.11156v3","updated":"2024-12-17T01:34:37Z","published":"2024-06-17T02:47:09Z","title":"DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential\n  Recommendation","summary":"  Sequential recommendation (SR) tasks aim to predict users' next interaction\nby learning their behavior sequence and capturing the connection between users'\npast interactions and their changing preferences. Conventional SR models often\nfocus solely on capturing sequential patterns within the training data,\nneglecting the broader context and semantic information embedded in item titles\nfrom external sources. This limits their predictive power and adaptability.\nLarge language models (LLMs) have recently shown promise in SR tasks due to\ntheir advanced understanding capabilities and strong generalization abilities.\nResearchers have attempted to enhance LLMs-based recommendation performance by\nincorporating information from conventional SR models. However, previous\napproaches have encountered problems such as 1) limited textual information\nleading to poor recommendation performance, 2) incomplete understanding and\nutilization of conventional SR model information by LLMs, and 3) excessive\ncomplexity and low interpretability of LLMs-based methods. To improve the\nperformance of LLMs-based SR, we propose a novel framework, Distilling\nSequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),\nwhich aims to extract knowledge from conventional SR models and enable LLMs to\neasily comprehend and utilize the extracted knowledge for more effective SRs.\nDELRec consists of two main stages: 1) Distill Pattern from Conventional SR\nModels, focusing on extracting behavioral patterns exhibited by conventional SR\nmodels using soft prompts through two well-designed strategies; 2) LLMs-based\nSequential Recommendation, aiming to fine-tune LLMs to effectively use the\ndistilled auxiliary information to perform SR tasks. Extensive experimental\nresults conducted on four real datasets validate the effectiveness of the\nDELRec framework.\n","authors":["Haoyi Zhang","Guohao Sun","Jinhu Lu","Guanfeng Liu","Xiu Susie Fang"],"pdf_url":"https://arxiv.org/pdf/2406.11156v3.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.12433v1","updated":"2024-12-17T00:50:23Z","published":"2024-12-17T00:50:23Z","title":"Refining Dimensions for Improving Clustering-based Cross-lingual Topic\n  Models","summary":"  Recent works in clustering-based topic models perform well in monolingual\ntopic identification by introducing a pipeline to cluster the contextualized\nrepresentations. However, the pipeline is suboptimal in identifying topics\nacross languages due to the presence of language-dependent dimensions (LDDs)\ngenerated by multilingual language models. To address this issue, we introduce\na novel, SVD-based dimension refinement component into the pipeline of the\nclustering-based topic model. This component effectively neutralizes the\nnegative impact of LDDs, enabling the model to accurately identify topics\nacross languages. Our experiments on three datasets demonstrate that the\nupdated pipeline with the dimension refinement component generally outperforms\nother state-of-the-art cross-lingual topic models.\n","authors":["Chia-Hsuan Chang","Tien-Yuan Huang","Yi-Hang Tsai","Chia-Ming Chang","San-Yih Hwang"],"pdf_url":"https://arxiv.org/pdf/2412.12433v1.pdf","comment":"Accepted to 18th BUCC Workshop at COLING 2025"},{"id":"http://arxiv.org/abs/2409.12468v2","updated":"2024-12-17T00:25:46Z","published":"2024-09-19T05:14:55Z","title":"Familiarity-Aware Evidence Compression for Retrieval-Augmented\n  Generation","summary":"  Retrieval-augmented generation (RAG) improves large language models (LMs) by\nincorporating non-parametric knowledge through evidence retrieved from external\nsources. However, it often struggles to cope with inconsistent and irrelevant\ninformation that can distract the LM from its tasks, especially when multiple\nevidence pieces are required. While compressing the retrieved evidence with a\ncompression model aims to address this issue, the compressed evidence may still\nbe unfamiliar to the target model used for downstream tasks, potentially\nfailing to utilize the evidence effectively. We propose FaviComp\n(Familarity-Aware Evidence Compression), a novel training-free evidence\ncompression technique that makes retrieved evidence more familiar to the target\nmodel, while seamlessly integrating parametric knowledge from the model.\nExperimental results show that FaviComp consistently outperforms most recent\nevidence compression baselines across multiple open-domain QA datasets,\nimproving accuracy by up to 28.1% while achieving high compression rates.\nAdditionally, we demonstrate the effective integration of both parametric and\nnon-parametric knowledge during evidence compression.\n","authors":["Dongwon Jung","Qin Liu","Tenghao Huang","Ben Zhou","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2409.12468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07857v2","updated":"2024-12-17T00:14:25Z","published":"2023-08-15T16:16:02Z","title":"Impression-Aware Recommender Systems","summary":"  Novel data sources bring new opportunities to improve the quality of\nrecommender systems and serve as a catalyst for the creation of new paradigms\non personalized recommendations. Impressions are a novel data source containing\nthe items shown to users on their screens. Past research focused on providing\npersonalized recommendations using interactions, and occasionally using\nimpressions when such a data source was available. Interest in impressions has\nincreased due to their potential to provide more accurate recommendations.\nDespite this increased interest, research in recommender systems using\nimpressions is still dispersed. Many works have distinct interpretations of\nimpressions and use impressions in recommender systems in numerous different\nmanners. To unify those interpretations into a single framework, we present a\nsystematic literature review on recommender systems using impressions, focusing\non three fundamental perspectives: recommendation models, datasets, and\nevaluation methodologies. We define a theoretical framework to delimit\nrecommender systems using impressions and a novel paradigm for personalized\nrecommendations, called impression-aware recommender systems. We propose a\nclassification system for recommenders in this paradigm, which we use to\ncategorize the recommendation models, datasets, and evaluation methodologies\nused in past research. Lastly, we identify open questions and future\ndirections, highlighting missing aspects in the reviewed literature.\n","authors":["Fernando B. Pérez Maurera","Maurizio Ferrari Dacrema","Pablo Castells","Paolo Cremonesi"],"pdf_url":"https://arxiv.org/pdf/2308.07857v2.pdf","comment":"44 pages, 127 references, 6 tables, 5 figures, ACM TORS ACCEPTED"},{"id":"http://arxiv.org/abs/2412.07030v2","updated":"2024-12-17T20:38:21Z","published":"2024-12-09T22:35:44Z","title":"FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge\n  Distillation for Question Answering","summary":"  Multimodal multihop question answering is a complex task that requires\nreasoning over multiple sources of information, such as images and text, to\nanswer questions. While there has been significant progress in visual question\nanswering, the multihop setting remains unexplored due to the lack of\nhigh-quality datasets. Current methods focus on single-hop question answering\nor a single modality, which makes them unsuitable for real-world scenarios such\nas analyzing multimodal educational materials, summarizing lengthy academic\narticles, or interpreting scientific studies that combine charts, images, and\ntext. To address this gap, we propose a novel methodology, introducing the\nfirst framework for creating a high-quality dataset that enables training\nmodels for multimodal multihop question answering. Our approach consists of a\n5-stage pipeline that involves acquiring relevant multimodal documents from\nWikipedia, synthetically generating high-level questions and answers, and\nvalidating them through rigorous criteria to ensure quality data. We evaluate\nour methodology by training models on our synthesized dataset and testing on\ntwo benchmarks, our results demonstrate that, with an equal sample size, models\ntrained on our synthesized data outperform those trained on human-collected\ndata by 1.9 in exact match (EM) on average. We believe our data synthesis\nmethod will serve as a strong foundation for training and evaluating multimodal\nmultihop question answering models.\n","authors":["Amirhossein Abaskohi","Spandana Gella","Giuseppe Carenini","Issam H. Laradji"],"pdf_url":"https://arxiv.org/pdf/2412.07030v2.pdf","comment":"20 pages, 11 figures, 10 tables, Submitted to CVPR 2025"},{"id":"http://arxiv.org/abs/2412.13268v1","updated":"2024-12-17T19:04:15Z","published":"2024-12-17T19:04:15Z","title":"JudgeBlender: Ensembling Judgments for Automatic Relevance Assessment","summary":"  The effective training and evaluation of retrieval systems require a\nsubstantial amount of relevance judgments, which are traditionally collected\nfrom human assessors -- a process that is both costly and time-consuming. Large\nLanguage Models (LLMs) have shown promise in generating relevance labels for\nsearch tasks, offering a potential alternative to manual assessments. Current\napproaches often rely on a single LLM, such as GPT-4, which, despite being\neffective, are expensive and prone to intra-model biases that can favour\nsystems leveraging similar models. In this work, we introduce JudgeBlender, a\nframework that employs smaller, open-source models to provide relevance\njudgments by combining evaluations across multiple LLMs (LLMBlender) or\nmultiple prompts (PromptBlender). By leveraging the LLMJudge benchmark [18], we\ncompare JudgeBlender with state-of-the-art methods and the top performers in\nthe LLMJudge challenge. Our results show that JudgeBlender achieves competitive\nperformance, demonstrating that very large models are often unnecessary for\nreliable relevance assessments.\n","authors":["Hossein A. Rahmani","Emine Yilmaz","Nick Craswell","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.13268v1.pdf","comment":"14 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2412.13196v1","updated":"2024-12-17T18:59:51Z","published":"2024-12-17T18:59:51Z","title":"ExBody2: Advanced Expressive Humanoid Whole-Body Control","summary":"  This paper enables real-world humanoid robots to maintain stability while\nperforming expressive motions like humans do. We propose ExBody2, a generalized\nwhole-body tracking framework that can take any reference motion inputs and\ncontrol the humanoid to mimic the motion. The model is trained in simulation\nwith Reinforcement Learning and then transferred to the real world. It\ndecouples keypoint tracking with velocity control, and effectively leverages a\nprivileged teacher policy to distill precise mimic skills into the target\nstudent policy, which enables high-fidelity replication of dynamic movements\nsuch as running, crouching, dancing, and other challenging motions. We present\na comprehensive qualitative and quantitative analysis of crucial design factors\nin the paper. We conduct our experiments on two humanoid platforms and\ndemonstrate the superiority of our approach against state-of-the-arts,\nproviding practical guidelines to pursue the extreme of whole-body control for\nhumanoid robots.\n","authors":["Mazeyu Ji","Xuanbin Peng","Fangchen Liu","Jialong Li","Ge Yang","Xuxin Cheng","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13196v1.pdf","comment":"website: https://exbody2.github.io"},{"id":"http://arxiv.org/abs/2412.13194v1","updated":"2024-12-17T18:59:50Z","published":"2024-12-17T18:59:50Z","title":"Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation\n  Model Internet Agents","summary":"  The vision of a broadly capable and goal-directed agent, such as an\nInternet-browsing agent in the digital world and a household humanoid in the\nphysical world, has rapidly advanced, thanks to the generalization capability\nof foundation models. Such a generalist agent needs to have a large and diverse\nskill repertoire, such as finding directions between two travel locations and\nbuying specific items from the Internet. If each skill needs to be specified\nmanually through a fixed set of human-annotated instructions, the agent's skill\nrepertoire will necessarily be limited due to the quantity and diversity of\nhuman-annotated instructions. In this work, we address this challenge by\nproposing Proposer-Agent-Evaluator, an effective learning system that enables\nfoundation model agents to autonomously discover and practice skills in the\nwild. At the heart of PAE is a context-aware task proposer that autonomously\nproposes tasks for the agent to practice with context information of the\nenvironment such as user demos or even just the name of the website itself for\nInternet-browsing agents. Then, the agent policy attempts those tasks with\nthoughts and actual grounded operations in the real world with resulting\ntrajectories evaluated by an autonomous VLM-based success evaluator. The\nsuccess evaluation serves as the reward signal for the agent to refine its\npolicies through RL. We validate PAE on challenging vision-based web\nnavigation, using both real-world and self-hosted websites from WebVoyager and\nWebArena.To the best of our knowledge, this work represents the first effective\nlearning system to apply autonomous task proposal with RL for agents that\ngeneralizes real-world human-annotated benchmarks with SOTA performances. Our\nopen-source checkpoints and code can be found in https://yanqval.github.io/PAE/\n","authors":["Yifei Zhou","Qianlan Yang","Kaixiang Lin","Min Bai","Xiong Zhou","Yu-Xiong Wang","Sergey Levine","Erran Li"],"pdf_url":"https://arxiv.org/pdf/2412.13194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13187v1","updated":"2024-12-17T18:58:33Z","published":"2024-12-17T18:58:33Z","title":"HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction","summary":"  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results \\url{https://www.chenbao.tech/handsonvlm/}\n","authors":["Chen Bao","Jiarui Xu","Xiaolong Wang","Abhinav Gupta","Homanga Bharadhwaj"],"pdf_url":"https://arxiv.org/pdf/2412.13187v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2412.13184v1","updated":"2024-12-17T18:58:00Z","published":"2024-12-17T18:58:00Z","title":"Tilted Quantile Gradient Updates for Quantile-Constrained Reinforcement\n  Learning","summary":"  Safe reinforcement learning (RL) is a popular and versatile paradigm to learn\nreward-maximizing policies with safety guarantees. Previous works tend to\nexpress the safety constraints in an expectation form due to the ease of\nimplementation, but this turns out to be ineffective in maintaining safety\nconstraints with high probability. To this end, we move to the\nquantile-constrained RL that enables a higher level of safety without any\nexpectation-form approximations. We directly estimate the quantile gradients\nthrough sampling and provide the theoretical proofs of convergence. Then a\ntilted update strategy for quantile gradients is implemented to compensate the\nasymmetric distributional density, with a direct benefit of return performance.\nExperiments demonstrate that the proposed model fully meets safety requirements\n(quantile constraints) while outperforming the state-of-the-art benchmarks with\nhigher return.\n","authors":["Chenglin Li","Guangchun Ruan","Hua Geng"],"pdf_url":"https://arxiv.org/pdf/2412.13184v1.pdf","comment":"Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2410.06215v2","updated":"2024-12-17T18:54:45Z","published":"2024-10-08T17:20:37Z","title":"DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback","summary":"  The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.\n","authors":["Zaid Khan","Elias Stengel-Eskin","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.06215v2.pdf","comment":"Project Page: https://DataEnvGym.github.io"},{"id":"http://arxiv.org/abs/2412.13174v1","updated":"2024-12-17T18:53:43Z","published":"2024-12-17T18:53:43Z","title":"ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark\n  Detection","summary":"  Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.\n","authors":["Jui-Che Chiang","Hou-Ning Hu","Bo-Syuan Hou","Chia-Yu Tseng","Yu-Lun Liu","Min-Hung Chen","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13174v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2412.04619v2","updated":"2024-12-17T18:42:57Z","published":"2024-12-05T21:12:37Z","title":"Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization","summary":"  Language models (LMs), like other neural networks, often favor shortcut\nheuristics based on surface-level patterns. Although LMs behave like n-gram\nmodels early in training, they must eventually learn hierarchical syntactic\nrepresentations to correctly apply grammatical rules out-of-distribution (OOD).\nIn this work, we use case studies of English grammar to explore how complex,\ndiverse training data drives models to generalize OOD. We construct a framework\nthat unifies our understanding of random variation with training dynamics, rule\nselection with memorization, and data diversity with complexity. We show that\nthese factors are nuanced, and that intermediate levels of diversity and\ncomplexity lead to inconsistent behavior across random seeds and to unstable\ntraining dynamics. Our findings emphasize the critical role of training data in\nshaping generalization patterns and illuminate how competing model strategies\nlead to inconsistent generalization outcomes across random seeds. Code is\navailable at https://github.com/sunnytqin/concept_comp.git.\n","authors":["Tian Qin","Naomi Saphra","David Alvarez-Melis"],"pdf_url":"https://arxiv.org/pdf/2412.04619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13161v1","updated":"2024-12-17T18:39:10Z","published":"2024-12-17T18:39:10Z","title":"BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce","summary":"  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n","authors":["Mohammad Nazmush Shamael","Sabila Nawshin","Swakkhar Shatabda","Salekul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.13161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09237v7","updated":"2024-12-17T18:37:07Z","published":"2024-08-17T16:06:14Z","title":"QEDCartographer: Automating Formal Verification Using Reward-Free\n  Reinforcement Learning","summary":"  Formal verification is a promising method for producing reliable software,\nbut the difficulty of manually writing verification proofs severely limits its\nutility in practice. Recent methods have automated some proof synthesis by\nguiding a search through the proof space using a theorem prover. Unfortunately,\nthe theorem prover provides only the crudest estimate of progress, resulting in\neffectively undirected search. To address this problem, we create\nQEDCartographer, an automated proof-synthesis tool that combines supervised and\nreinforcement learning to more effectively explore the proof space.\nQEDCartographer incorporates the proofs' branching structure, enabling\nreward-free search and overcoming the sparse reward problem inherent to formal\nverification. We evaluate QEDCartographer using the CoqGym benchmark of 68.5K\ntheorems from 124 open-source Coq projects. QEDCartographer fully automatically\nproves 21.4% of the test-set theorems. Previous search-based proof-synthesis\ntools Tok, Tac, ASTactic, Passport, and Proverbot9001, which rely only on\nsupervised learning, prove 9.6%, 9.8%, 10.9%, 12.5%, and 19.8%, respectively.\nDiva, which combines 62 tools, proves 19.2%. Comparing to the most effective\nprior tool, Proverbot9001, QEDCartographer produces 34% shorter proofs 29%\nfaster, on average over the theorems both tools prove. Together,\nQEDCartographer and non-learning-based CoqHammer prove 30.3% of the theorems,\nwhile CoqHammer alone proves 26.6%. Our work demonstrates that reinforcement\nlearning is a fruitful research direction for improving proof-synthesis tools'\nsearch mechanisms.\n","authors":["Alex Sanchez-Stern","Abhishek Varghese","Zhanna Kaufman","Dylan Zhang","Talia Ringer","Yuriy Brun"],"pdf_url":"https://arxiv.org/pdf/2408.09237v7.pdf","comment":"Authors could not agree on final revision. Please see author websites\n  for individual versions of paper"},{"id":"http://arxiv.org/abs/2412.13159v1","updated":"2024-12-17T18:34:43Z","published":"2024-12-17T18:34:43Z","title":"A Conformal Approach to Feature-based Newsvendor under Model\n  Misspecification","summary":"  In many data-driven decision-making problems, performance guarantees often\ndepend heavily on the correctness of model assumptions, which may frequently\nfail in practice. We address this issue in the context of a feature-based\nnewsvendor problem, where demand is influenced by observed features such as\ndemographics and seasonality. To mitigate the impact of model misspecification,\nwe propose a model-free and distribution-free framework inspired by conformal\nprediction. Our approach consists of two phases: a training phase, which can\nutilize any type of prediction method, and a calibration phase that\nconformalizes the model bias. To enhance predictive performance, we explore the\nbalance between data quality and quantity, recognizing the inherent trade-off:\nmore selective training data improves quality but reduces quantity.\nImportantly, we provide statistical guarantees for the conformalized critical\nquantile, independent of the correctness of the underlying model. Moreover, we\nquantify the confidence interval of the critical quantile, with its width\ndecreasing as data quality and quantity improve. We validate our framework\nusing both simulated data and a real-world dataset from the Capital Bikeshare\nprogram in Washington, D.C. Across these experiments, our proposed method\nconsistently outperforms benchmark algorithms, reducing newsvendor loss by up\nto 40% on the simulated data and 25% on the real-world dataset.\n","authors":["Junyu Cao"],"pdf_url":"https://arxiv.org/pdf/2412.13159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13158v1","updated":"2024-12-17T18:33:14Z","published":"2024-12-17T18:33:14Z","title":"On Model Extrapolation in Marginal Shapley Values","summary":"  As the use of complex machine learning models continues to grow, so does the\nneed for reliable explainability methods. One of the most popular methods for\nmodel explainability is based on Shapley values. There are two most commonly\nused approaches to calculating Shapley values which produce different results\nwhen features are correlated, conditional and marginal. In our previous work,\nit was demonstrated that the conditional approach is fundamentally flawed due\nto implicit assumptions of causality. However, it is a well-known fact that\nmarginal approach to calculating Shapley values leads to model extrapolation\nwhere it might not be well defined. In this paper we explore the impacts of\nmodel extrapolation on Shapley values in the case of a simple linear spline\nmodel. Furthermore, we propose an approach which while using marginal averaging\navoids model extrapolation and with addition of causal information replicates\ncausal Shapley values. Finally, we demonstrate our method on the real data\nexample.\n","authors":["Ilya Rozenfeld"],"pdf_url":"https://arxiv.org/pdf/2412.13158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13157v1","updated":"2024-12-17T18:33:05Z","published":"2024-12-17T18:33:05Z","title":"Learning Visuotactile Estimation and Control for Non-prehensile\n  Manipulation under Occlusions","summary":"  Manipulation without grasping, known as non-prehensile manipulation, is\nessential for dexterous robots in contact-rich environments, but presents many\nchallenges relating with underactuation, hybrid-dynamics, and frictional\nuncertainty. Additionally, object occlusions in a scenario of contact\nuncertainty and where the motion of the object evolves independently from the\nrobot becomes a critical problem, which previous literature fails to address.\nWe present a method for learning visuotactile state estimators and\nuncertainty-aware control policies for non-prehensile manipulation under\nocclusions, by leveraging diverse interaction data from privileged policies\ntrained in simulation. We formulate the estimator within a Bayesian deep\nlearning framework, to model its uncertainty, and then train uncertainty-aware\ncontrol policies by incorporating the pre-learned estimator into the\nreinforcement learning (RL) loop, both of which lead to significantly improved\nestimator and policy performance. Therefore, unlike prior non-prehensile\nresearch that relies on complex external perception set-ups, our method\nsuccessfully handles occlusions after sim-to-real transfer to robotic hardware\nwith a simple onboard camera. See our video: https://youtu.be/hW-C8i_HWgs.\n","authors":["Juan Del Aguila Ferrandis","João Moura","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2412.13157v1.pdf","comment":"Conference on Robot Learning (CoRL 2024)"},{"id":"http://arxiv.org/abs/2412.13148v1","updated":"2024-12-17T18:13:18Z","published":"2024-12-17T18:13:18Z","title":"SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training\n  With Significant Memory Reduction","summary":"  Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they maintain additional moving\naverage states throughout training, which results in memory requirements\nseveral times greater than the model. This overhead imposes constraints on\nscalability and computational efficiency. On the other hand, while stochastic\ngradient descent (SGD) is optimal in terms of memory efficiency, their\ncapability in LLM training is limited (Zhao et al., 2024b).\n  To address this dilemma, we show that pre-processing SGD is sufficient to\nreach Adam-level performance on LLMs. Specifically, we propose to preprocess\nthe instantaneous stochastic gradients with two simple operators:\n$\\mathtt{GradNorm}$ and $\\mathtt{GradWhitening}$. $\\mathtt{GradNorm}$\nstabilizes gradient distributions, and $\\mathtt{GradWhitening}$ counteracts the\nlocal curvature of the loss landscape, respectively. This results in SWAN (SGD\nwith Whitening And Normalization), a stochastic optimizer that eliminates the\nneed to store any accumulative state variables. Empirically, SWAN has the same\nmemory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end\nmemory compared to Adam. In language modeling tasks, SWAN demonstrates the same\nor even a substantial improvement over Adam. Specifically, when pre-training\nthe LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by\nreaching the same evaluation perplexity in less than half tokens seen.\n","authors":["Chao Ma","Wenbo Gong","Meyer Scetbon","Edward Meeds"],"pdf_url":"https://arxiv.org/pdf/2412.13148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10400v2","updated":"2024-12-17T18:05:11Z","published":"2024-12-05T16:10:42Z","title":"Reinforcement Learning Enhanced LLMs: A Survey","summary":"  This paper surveys research in the rapidly growing field of enhancing large\nlanguage models (LLMs) with reinforcement learning (RL), a technique that\nenables LLMs to improve their performance by receiving feedback in the form of\nrewards based on the quality of their outputs, allowing them to generate more\naccurate, coherent, and contextually appropriate responses. In this work, we\nmake a systematic review of the most up-to-date state of knowledge on\nRL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing\nresearch in this field, helping researchers understand the current challenges\nand advancements. Specifically, we (1) detail the basics of RL; (2) introduce\npopular RL-enhanced LLMs; (3) review researches on two widely-used reward\nmodel-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)\nand Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct\nPreference Optimization (DPO), a set of methods that bypass the reward model to\ndirectly use human preference data for aligning LLM outputs with human\nexpectations. We will also point out current challenges and deficiencies of\nexisting methods and suggest some avenues for further improvements. Project\npage of this work can be found at:\n\\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}.\n","authors":["Shuhe Wang","Shengyu Zhang","Jie Zhang","Runyi Hu","Xiaoya Li","Tianwei Zhang","Jiwei Li","Fei Wu","Guoyin Wang","Eduard Hovy"],"pdf_url":"https://arxiv.org/pdf/2412.10400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13134v1","updated":"2024-12-17T17:53:32Z","published":"2024-12-17T17:53:32Z","title":"Practicable Black-box Evasion Attacks on Link Prediction in Dynamic\n  Graphs -- A Graph Sequential Embedding Method","summary":"  Link prediction in dynamic graphs (LPDG) has been widely applied to\nreal-world applications such as website recommendation, traffic flow\nprediction, organizational studies, etc. These models are usually kept local\nand secure, with only the interactive interface restrictively available to the\npublic. Thus, the problem of the black-box evasion attack on the LPDG model,\nwhere model interactions and data perturbations are restricted, seems to be\nessential and meaningful in practice. In this paper, we propose the first\npracticable black-box evasion attack method that achieves effective attacks\nagainst the target LPDG model, within a limited amount of interactions and\nperturbations. To perform effective attacks under limited perturbations, we\ndevelop a graph sequential embedding model to find the desired state embedding\nof the dynamic graph sequences, under a deep reinforcement learning framework.\nTo overcome the scarcity of interactions, we design a multi-environment\ntraining pipeline and train our agent for multiple instances, by sharing an\naggregate interaction buffer. Finally, we evaluate our attack against three\nadvanced LPDG models on three real-world graph datasets of different scales and\ncompare its performance with related methods under the interaction and\nperturbation constraints. Experimental results show that our attack is both\neffective and practicable.\n","authors":["Jiate Li","Meng Pang","Binghui Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13474v3","updated":"2024-12-17T17:45:07Z","published":"2024-09-20T13:05:07Z","title":"Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models","summary":"  Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/Alternate-Preference-Optimization.\n","authors":["Anmol Mekala","Vineeth Dorna","Shreya Dubey","Abhishek Lalwani","David Koleczek","Mukund Rungta","Sadid Hasan","Elita Lobo"],"pdf_url":"https://arxiv.org/pdf/2409.13474v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06829v2","updated":"2024-12-17T17:28:59Z","published":"2024-12-06T22:15:22Z","title":"Stably unactivated neurons in ReLU neural networks","summary":"  The choice of architecture of a neural network influences which functions\nwill be realizable by that neural network and, as a result, studying the\nexpressiveness of a chosen architecture has received much attention. In ReLU\nneural networks, the presence of stably unactivated neurons can reduce the\nnetwork's expressiveness. In this work, we investigate the probability of a\nneuron in the second hidden layer of such neural networks being stably\nunactivated when the weights and biases are initialized from symmetric\nprobability distributions. For networks with input dimension $n_0$, we prove\nthat if the first hidden layer has $n_0+1$ neurons then this probability is\nexactly $\\frac{2^{n_0}+1}{4^{n_0+1}}$, and if the first hidden layer has $n_1$\nneurons, $n_1 \\le n_0$, then the probability is $\\frac{1}{2^{n_1+1}}$. Finally,\nfor the case when the first hidden layer has more neurons than $n_0+1$, a\nconjecture is proposed along with the rationale. Computational evidence is\npresented to support the conjecture.\n","authors":["Natalie Brownlowe","Christopher R. Cornwell","Ethan Montes","Gabriel Quijano","Grace Stulman","Na Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.06829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13106v1","updated":"2024-12-17T17:22:52Z","published":"2024-12-17T17:22:52Z","title":"Active Reinforcement Learning Strategies for Offline Policy Improvement","summary":"  Learning agents that excel at sequential decision-making tasks must\ncontinuously resolve the problem of exploration and exploitation for optimal\nlearning. However, such interactions with the environment online might be\nprohibitively expensive and may involve some constraints, such as a limited\nbudget for agent-environment interactions and restricted exploration in certain\nregions of the state space. Examples include selecting candidates for medical\ntrials and training agents in complex navigation environments. This problem\nnecessitates the study of active reinforcement learning strategies that collect\nminimal additional experience trajectories by reusing existing offline data\npreviously collected by some unknown behavior policy. In this work, we propose\na representation-aware uncertainty-based active trajectory collection method\nthat intelligently decides interaction strategies that consider the\ndistribution of the existing offline data. With extensive experimentation, we\ndemonstrate that our proposed method reduces additional online interaction with\nthe environment by up to 75% over competitive baselines across various\ncontinuous control environments.\n","authors":["Ambedkar Dukkipati","Ranga Shaarad Ayyagari","Bodhisattwa Dasgupta","Parag Dutta","Prabhas Reddy Onteru"],"pdf_url":"https://arxiv.org/pdf/2412.13106v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2405.07344v3","updated":"2024-12-17T17:13:03Z","published":"2024-05-12T17:40:48Z","title":"TKAN: Temporal Kolmogorov-Arnold Networks","summary":"  Recurrent Neural Networks (RNNs) have revolutionized many areas of machine\nlearning, particularly in natural language and data sequence processing. Long\nShort-Term Memory (LSTM) has demonstrated its ability to capture long-term\ndependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks\n(KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed\na new neural networks architecture inspired by KAN and the LSTM, the Temporal\nKolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both\nnetworks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers\nembedding memory management. This innovation enables us to perform multi-step\ntime series forecasting with enhanced accuracy and efficiency. By addressing\nthe limitations of traditional models in handling complex sequential patterns,\nthe TKAN architecture offers significant potential for advancements in fields\nrequiring more than one step ahead forecasting.\n","authors":["Remi Genet","Hugo Inzirillo"],"pdf_url":"https://arxiv.org/pdf/2405.07344v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13096v1","updated":"2024-12-17T17:06:33Z","published":"2024-12-17T17:06:33Z","title":"Incremental Online Learning of Randomized Neural Network with Forward\n  Regularization","summary":"  Online learning of deep neural networks suffers from challenges such as\nhysteretic non-incremental updating, increasing memory usage, past\nretrospective retraining, and catastrophic forgetting. To alleviate these\ndrawbacks and achieve progressive immediate decision-making, we propose a novel\nIncremental Online Learning (IOL) process of Randomized Neural Networks\n(Randomized NN), a framework facilitating continuous improvements to Randomized\nNN performance in restrictive online scenarios. Within the framework, we\nfurther introduce IOL with ridge regularization (-R) and IOL with forward\nregularization (-F). -R generates stepwise incremental updates without\nretrospective retraining and avoids catastrophic forgetting. Moreover, we\nsubstituted -R with -F as it enhanced precognition learning ability using\nsemi-supervision and realized better online regrets to offline global experts\ncompared to -R during IOL. The algorithms of IOL for Randomized NN with -R/-F\non non-stationary batch stream were derived respectively, featuring recursive\nweight updates and variable learning rates. Additionally, we conducted a\ndetailed analysis and theoretically derived relative cumulative regret bounds\nof the Randomized NN learners with -R/-F in IOL under adversarial assumptions\nusing a novel methodology and presented several corollaries, from which we\nobserved the superiority on online learning acceleration and regret bounds of\nemploying -F in IOL. Finally, our proposed methods were rigorously examined\nacross regression and classification tasks on diverse datasets, which\ndistinctly validated the efficacy of IOL frameworks of Randomized NN and the\nadvantages of forward regularization.\n","authors":["Junda Wang","Minghui Hu","Ning Li","Abdulaziz Al-Ali","Ponnuthurai Nagaratnam Suganthan"],"pdf_url":"https://arxiv.org/pdf/2412.13096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13093v1","updated":"2024-12-17T17:02:06Z","published":"2024-12-17T17:02:06Z","title":"Reservoir Computing for Fast, Simplified Reinforcement Learning on\n  Memory Tasks","summary":"  Tasks in which rewards depend upon past information not available in the\ncurrent observation set can only be solved by agents that are equipped with\nshort-term memory. Usual choices for memory modules include trainable recurrent\nhidden layers, often with gated memory. Reservoir computing presents an\nalternative, in which a recurrent layer is not trained, but rather has a set of\nfixed, sparse recurrent weights. The weights are scaled to produce stable\ndynamical behavior such that the reservoir state contains a high-dimensional,\nnonlinear impulse response function of the inputs. An output decoder network\ncan then be used to map the compressive history represented by the reservoir's\nstate to any outputs, including agent actions or predictions. In this study, we\nfind that reservoir computing greatly simplifies and speeds up reinforcement\nlearning on memory tasks by (1) eliminating the need for backpropagation of\ngradients through time, (2) presenting all recent history simultaneously to the\ndownstream network, and (3) performing many useful and generic nonlinear\ncomputations upstream from the trained modules. In particular, these findings\noffer significant benefit to meta-learning that depends primarily on efficient\nand highly general memory systems.\n","authors":["Kevin McKee"],"pdf_url":"https://arxiv.org/pdf/2412.13093v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13079v1","updated":"2024-12-17T16:51:44Z","published":"2024-12-17T16:51:44Z","title":"Identifying Bias in Deep Neural Networks Using Image Transforms","summary":"  CNNs have become one of the most commonly used computational tool in the past\ntwo decades. One of the primary downsides of CNNs is that they work as a\n``black box\", where the user cannot necessarily know how the image data are\nanalyzed, and therefore needs to rely on empirical evaluation to test the\nefficacy of a trained CNN. This can lead to hidden biases that affect the\nperformance evaluation of neural networks, but are difficult to identify. Here\nwe discuss examples of such hidden biases in common and widely used benchmark\ndatasets, and propose techniques for identifying dataset biases that can affect\nthe standard performance evaluation metrics. One effective approach to identify\ndataset bias is to perform image classification by using merely blank\nbackground parts of the original images. However, in some situations a blank\nbackground in the images is not available, making it more difficult to separate\nforeground or contextual information from the bias. To overcome this, we\npropose a method to identify dataset bias without the need to crop background\ninformation from the images. That method is based on applying several image\ntransforms to the original images, including Fourier transform, wavelet\ntransforms, median filter, and their combinations. These transforms were\napplied to recover background bias information that CNNs use to classify\nimages. This transformations affect the contextual visual information in a\ndifferent manner than it affects the systemic background bias. Therefore, the\nmethod can distinguish between contextual information and the bias, and alert\non the presence of background bias even without the need to separate sub-images\nparts from the blank background of the original images. Code used in the\nexperiments is publicly available.\n","authors":["Sai Teja Erukude","Akhil Joshi","Lior Shamir"],"pdf_url":"https://arxiv.org/pdf/2412.13079v1.pdf","comment":"Computers, published"},{"id":"http://arxiv.org/abs/2408.01880v3","updated":"2024-12-17T16:47:41Z","published":"2024-08-03T23:15:57Z","title":"Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via\n  Efficient Guidance-Exploration","summary":"  Recent years, multi-hop reasoning has been widely studied for knowledge graph\n(KG) reasoning due to its efficacy and interpretability. However, previous\nmulti-hop reasoning approaches are subject to two primary shortcomings. First,\nagents struggle to learn effective and robust policies at the early phase due\nto sparse rewards. Second, these approaches often falter on specific datasets\nlike sparse knowledge graphs, where agents are required to traverse lengthy\nreasoning paths. To address these problems, we propose a multi-hop reasoning\nmodel with dual agents based on hierarchical reinforcement learning (HRL),\nwhich is named FULORA. FULORA tackles the above reasoning challenges by\neFficient GUidance-ExpLORAtion between dual agents. The high-level agent walks\non the simplified knowledge graph to provide stage-wise hints for the low-level\nagent walking on the original knowledge graph. In this framework, the low-level\nagent optimizes a value function that balances two objectives: (1) maximizing\nreturn, and (2) integrating efficient guidance from the high-level agent.\nExperiments conducted on three real-word knowledge graph datasets demonstrate\nthat FULORA outperforms RL-based baselines, especially in the case of\nlong-distance reasoning.\n","authors":["Zijian Wang","Bin Wang","Haifeng Jing","Huayu Li","Hongbo Dou"],"pdf_url":"https://arxiv.org/pdf/2408.01880v3.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2412.13076v1","updated":"2024-12-17T16:44:39Z","published":"2024-12-17T16:44:39Z","title":"Dual Interpretation of Machine Learning Forecasts","summary":"  Machine learning predictions are typically interpreted as the sum of\ncontributions of predictors. Yet, each out-of-sample prediction can also be\nexpressed as a linear combination of in-sample values of the predicted\nvariable, with weights corresponding to pairwise proximity scores between\ncurrent and past economic events. While this dual route leads nowhere in some\ncontexts (e.g., large cross-sectional datasets), it provides sparser\ninterpretations in settings with many regressors and little training data-like\nmacroeconomic forecasting. In this case, the sequence of contributions can be\nvisualized as a time series, allowing analysts to explain predictions as\nquantifiable combinations of historical analogies. Moreover, the weights can be\nviewed as those of a data portfolio, inspiring new diagnostic measures such as\nforecast concentration, short position, and turnover. We show how weights can\nbe retrieved seamlessly for (kernel) ridge regression, random forest, boosted\ntrees, and neural networks. Then, we apply these tools to analyze post-pandemic\nforecasts of inflation, GDP growth, and recession probabilities. In all cases,\nthe approach opens the black box from a new angle and demonstrates how machine\nlearning models leverage history partly repeating itself.\n","authors":["Philippe Goulet Coulombe","Maximilian Goebel","Karin Klieber"],"pdf_url":"https://arxiv.org/pdf/2412.13076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13074v1","updated":"2024-12-17T16:41:53Z","published":"2024-12-17T16:41:53Z","title":"Predicting Change, Not States: An Alternate Framework for Neural PDE\n  Surrogates","summary":"  Neural surrogates for partial differential equations (PDEs) have become\npopular due to their potential to quickly simulate physics. With a few\nexceptions, neural surrogates generally treat the forward evolution of\ntime-dependent PDEs as a black box by directly predicting the next state. While\nthis is a natural and easy framework for applying neural surrogates, it can be\nan over-simplified and rigid framework for predicting physics. In this work, we\npropose an alternative framework in which neural solvers predict the temporal\nderivative and an ODE integrator forwards the solution in time, which has\nlittle overhead and is broadly applicable across model architectures and PDEs.\nWe find that by simply changing the training target and introducing numerical\nintegration during inference, neural surrogates can gain accuracy and\nstability. Predicting temporal derivatives also allows models to not be\nconstrained to a specific temporal discretization, allowing for flexible\ntime-stepping during inference or training on higher-resolution PDE data.\nLastly, we investigate why this new framework can be beneficial and in what\nsituations does it work well.\n","authors":["Anthony Zhou","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2412.13074v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.04158v2","updated":"2024-12-17T16:40:37Z","published":"2024-12-05T13:46:55Z","title":"LossVal: Efficient Data Valuation for Neural Networks","summary":"  Assessing the importance of individual training samples is a key challenge in\nmachine learning. Traditional approaches retrain models with and without\nspecific samples, which is computationally expensive and ignores dependencies\nbetween data points. We introduce LossVal, an efficient data valuation method\nthat computes importance scores during neural network training by embedding a\nself-weighting mechanism into loss functions like cross-entropy and mean\nsquared error. LossVal reduces computational costs, making it suitable for\nlarge datasets and practical applications. Experiments on classification and\nregression tasks across multiple datasets show that LossVal effectively\nidentifies noisy samples and is able to distinguish helpful from harmful\nsamples. We examine the gradient calculation of LossVal to highlight its\nadvantages. The source code is available at:\nhttps://github.com/twibiral/LossVal\n","authors":["Tim Wibiral","Mohamed Karim Belaid","Maximilian Rabus","Ansgar Scherp"],"pdf_url":"https://arxiv.org/pdf/2412.04158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10304v2","updated":"2024-12-17T16:34:49Z","published":"2024-01-18T12:11:27Z","title":"On the Readiness of Scientific Data for a Fair and Transparent Use in\n  Machine Learning","summary":"  To ensure the fairness and trustworthiness of machine learning (ML) systems,\nrecent legislative initiatives and relevant research in the ML community have\npointed out the need to document the data used to train ML models. Besides,\ndata-sharing practices in many scientific domains have evolved in recent years\nfor reproducibility purposes. In this sense, academic institutions' adoption of\nthese practices has encouraged researchers to publish their data and technical\ndocumentation in peer-reviewed publications such as data papers. In this study,\nwe analyze how this broader scientific data documentation meets the needs of\nthe ML community and regulatory bodies for its use in ML technologies. We\nexamine a sample of 4041 data papers of different domains, assessing their\ncompleteness, coverage of the requested dimensions, and trends in recent years.\nWe focus on the most and least documented dimensions and compare the results\nwith those of an ML-focused venue (NeurIPS D&B track) publishing papers\ndescribing datasets. As a result, we propose a set of recommendation guidelines\nfor data creators and scientific data publishers to increase their data's\npreparedness for its transparent and fairer use in ML technologies.\n","authors":["Joan Giner-Miguelez","Abel Gómez","Jordi Cabot"],"pdf_url":"https://arxiv.org/pdf/2401.10304v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13070v1","updated":"2024-12-17T16:34:32Z","published":"2024-12-17T16:34:32Z","title":"Learning of Patch-Based Smooth-Plus-Sparse Models for Image\n  Reconstruction","summary":"  We aim at the solution of inverse problems in imaging, by combining a\npenalized sparse representation of image patches with an unconstrained smooth\none. This allows for a straightforward interpretation of the reconstruction. We\nformulate the optimization as a bilevel problem. The inner problem deploys\nclassical algorithms while the outer problem optimizes the dictionary and the\nregularizer parameters through supervised learning. The process is carried out\nvia implicit differentiation and gradient-based optimization. We evaluate our\nmethod for denoising, super-resolution, and compressed-sensing\nmagnetic-resonance imaging. We compare it to other classical models as well as\ndeep-learning-based methods and show that it always outperforms the former and\nalso the latter in some instances.\n","authors":["Stanislas Ducotterd","Sebastian Neumayer","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2412.13070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12059v3","updated":"2024-12-17T16:30:39Z","published":"2024-09-18T15:32:48Z","title":"MeTHanol: Modularized Thinking Language Models with Intermediate Layer\n  Thinking, Decoding and Bootstrapping Reasoning","summary":"  Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.\n","authors":["Ningyuan Xi","Xiaoyu Wang","Yetao Wu","Teng Chen","Qingqing Gu","Yue Zhao","Jinxian Qu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.12059v3.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.11017v2","updated":"2024-12-17T16:27:21Z","published":"2024-12-15T02:10:18Z","title":"On Distilling the Displacement Knowledge for Few-Shot Class-Incremental\n  Learning","summary":"  Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of\nevolving data distributions and the difficulty of data acquisition in\nreal-world scenarios. To counteract the catastrophic forgetting typically\nencountered in FSCIL, knowledge distillation is employed as a way to maintain\nthe knowledge from learned data distribution. Recognizing the limitations of\ngenerating discriminative feature representations in a few-shot context, our\napproach incorporates structural information between samples into knowledge\ndistillation. This structural information serves as a remedy for the low\nquality of features. Diverging from traditional structured distillation methods\nthat compute sample similarity, we introduce the Displacement Knowledge\nDistillation (DKD) method. DKD utilizes displacement rather than similarity\nbetween samples, incorporating both distance and angular information to\nsignificantly enhance the information density retained through knowledge\ndistillation. Observing performance disparities in feature distribution between\nbase and novel classes, we propose the Dual Distillation Network (DDNet). This\nnetwork applies traditional knowledge distillation to base classes and DKD to\nnovel classes, challenging the conventional integration of novel classes with\nbase classes. Additionally, we implement an instance-aware sample selector\nduring inference to dynamically adjust dual branch weights, thereby leveraging\nthe complementary strengths of each approach. Extensive testing on three\nbenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,\nthrough rigorous experimentation and comparison, we establish the robustness\nand general applicability of our proposed DKD method.\n","authors":["Pengfei Fang","Yongchun Qin","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2412.11017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13061v1","updated":"2024-12-17T16:27:11Z","published":"2024-12-17T16:27:11Z","title":"VidTok: A Versatile and Open-Source Video Tokenizer","summary":"  Encoding video content into compact latent tokens has become a fundamental\nstep in video generation and understanding, driven by the need to address the\ninherent redundancy in pixel-level representations. Consequently, there is a\ngrowing demand for high-performance, open-source video tokenizers as\nvideo-centric research gains prominence. We introduce VidTok, a versatile video\ntokenizer that delivers state-of-the-art performance in both continuous and\ndiscrete tokenizations. VidTok incorporates several key advancements over\nexisting approaches: 1) model architecture such as convolutional layers and\nup/downsampling modules; 2) to address the training instability and codebook\ncollapse commonly associated with conventional Vector Quantization (VQ), we\nintegrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)\nimproved training strategies, including a two-stage training process and the\nuse of reduced frame rates. By integrating these advancements, VidTok achieves\nsubstantial improvements over existing methods, demonstrating superior\nperformance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,\nunder standardized evaluation settings.\n","authors":["Anni Tang","Tianyu He","Junliang Guo","Xinle Cheng","Li Song","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2412.13061v1.pdf","comment":"Code & Models: https://github.com/microsoft/VidTok"},{"id":"http://arxiv.org/abs/2407.15906v2","updated":"2024-12-17T16:25:06Z","published":"2024-07-22T14:43:10Z","title":"An Ad-hoc graph node vector embedding algorithm for general knowledge\n  graphs using Kinetica-Graph","summary":"  This paper discusses how to generate general graph node embeddings from\nknowledge graph representations. The embedded space is composed of a number of\nsub-features to mimic both local affinity and remote structural relevance.\nThese sub-feature dimensions are defined by several indicators that we\nspeculate to catch nodal similarities, such as hop-based topological patterns,\nthe number of overlapping labels, the transitional probabilities (markov-chain\nprobabilities), and the cluster indices computed by our recursive spectral\nbisection (RSB) algorithm. These measures are flattened over the one\ndimensional vector space into their respective sub-component ranges such that\nthe entire set of vector similarity functions could be used for finding similar\nnodes. The error is defined by the sum of pairwise square differences across a\nrandomly selected sample of graph nodes between the assumed embeddings and the\nground truth estimates as our novel loss function. The ground truth is\nestimated to be a combination of pairwise Jaccard similarity and the number of\noverlapping labels. Finally, we demonstrate a multi-variate stochastic gradient\ndescent (SGD) algorithm to compute the weighing factors among sub-vector spaces\nto minimize the average error using a random sampling logic.\n","authors":["B. Kaan Karamete","Eli Glaser"],"pdf_url":"https://arxiv.org/pdf/2407.15906v2.pdf","comment":"11 pages, 16 figures, 16 references"},{"id":"http://arxiv.org/abs/2412.13057v1","updated":"2024-12-17T16:20:29Z","published":"2024-12-17T16:20:29Z","title":"On the Hardness of Training Deep Neural Networks Discretely","summary":"  We study neural network training (NNT): optimizing a neural network's\nparameters to minimize the training loss over a given dataset. NNT has been\nstudied extensively under theoretic lenses, mainly on two-layer networks with\nlinear or ReLU activation functions where the parameters can take any real\nvalue (here referred to as continuous NNT (C-NNT)). However, less is known\nabout deeper neural networks, which exhibit substantially stronger capabilities\nin practice. In addition, the complexity of the discrete variant of the problem\n(D-NNT in short), in which the parameters are taken from a given finite set of\noptions, has remained less explored despite its theoretical and practical\nsignificance.\n  In this work, we show that the hardness of NNT is dramatically affected by\nthe network depth. Specifically, we show that, under standard complexity\nassumptions, D-NNT is not in the complexity class NP even for instances with\nfixed dimensions and dataset size, having a deep architecture. This separates\nD-NNT from any NP-complete problem. Furthermore, using a polynomial reduction\nwe show that the above result also holds for C-NNT, albeit with more structured\ninstances. We complement these results with a comprehensive list of NP-hardness\nlower bounds for D-NNT on two-layer networks, showing that fixing the number of\ndimensions, the dataset size, or the number of neurons in the hidden layer\nleaves the problem challenging. Finally, we obtain a pseudo-polynomial\nalgorithm for D-NNT on a two-layer network with a fixed dataset size.\n","authors":["Ilan Doron-Arad"],"pdf_url":"https://arxiv.org/pdf/2412.13057v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13053v1","updated":"2024-12-17T16:15:04Z","published":"2024-12-17T16:15:04Z","title":"SMOSE: Sparse Mixture of Shallow Experts for Interpretable Reinforcement\n  Learning in Continuous Control Tasks","summary":"  Continuous control tasks often involve high-dimensional, dynamic, and\nnon-linear environments. State-of-the-art performance in these tasks is\nachieved through complex closed-box policies that are effective, but suffer\nfrom an inherent opacity. Interpretable policies, while generally\nunderperforming compared to their closed-box counterparts, advantageously\nfacilitate transparent decision-making within automated systems. Hence, their\nusage is often essential for diagnosing and mitigating errors, supporting\nethical and legal accountability, and fostering trust among stakeholders. In\nthis paper, we propose SMOSE, a novel method to train sparsely activated\ninterpretable controllers, based on a top-1 Mixture-of-Experts architecture.\nSMOSE combines a set of interpretable decisionmakers, trained to be experts in\ndifferent basic skills, and an interpretable router that assigns tasks among\nthe experts. The training is carried out via state-of-the-art Reinforcement\nLearning algorithms, exploiting load-balancing techniques to ensure fair expert\nusage. We then distill decision trees from the weights of the router,\nsignificantly improving the ease of interpretation. We evaluate SMOSE on six\nbenchmark environments from MuJoCo: our method outperforms recent interpretable\nbaselines and narrows the gap with noninterpretable state-of-the-art algorithms\n","authors":["Mátyás Vincze","Laura Ferrarotti","Leonardo Lucio Custode","Bruno Lepri","Giovanni Iacca"],"pdf_url":"https://arxiv.org/pdf/2412.13053v1.pdf","comment":"To be published in the Proceedings of the 39th AAAI Conference on\n  Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.13050v1","updated":"2024-12-17T16:13:56Z","published":"2024-12-17T16:13:56Z","title":"Modality-Inconsistent Continual Learning of Multimodal Large Language\n  Models","summary":"  In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),\na new continual learning scenario for Multimodal Large Language Models (MLLMs)\nthat involves tasks with inconsistent modalities (image, audio, or video) and\nvarying task types (captioning or question-answering). Unlike existing\nvision-only or modality-incremental settings, MICL combines modality and task\ntype shifts, both of which drive catastrophic forgetting. To address these\nchallenges, we propose MoInCL, which employs a Pseudo Targets Generation Module\nto mitigate forgetting caused by task type shifts in previously seen\nmodalities. It also incorporates Instruction-based Knowledge Distillation to\npreserve the model's ability to handle previously learned modalities when new\nones are introduced. We benchmark MICL using a total of six tasks and conduct\nexperiments to validate the effectiveness of our proposed MoInCL. The\nexperimental results highlight the superiority of MoInCL, showing significant\nimprovements over representative and state-of-the-art continual learning\nbaselines.\n","authors":["Weiguo Pian","Shijian Deng","Shentong Mo","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13049v1","updated":"2024-12-17T16:13:37Z","published":"2024-12-17T16:13:37Z","title":"TIMESAFE: Timing Interruption Monitoring and Security Assessment for\n  Fronthaul Environments","summary":"  5G and beyond cellular systems embrace the disaggregation of Radio Access\nNetwork (RAN) components, exemplified by the evolution of the fronthual (FH)\nconnection between cellular baseband and radio unit equipment. Crucially,\nsynchronization over the FH is pivotal for reliable 5G services. In recent\nyears, there has been a push to move these links to an Ethernet-based packet\nnetwork topology, leveraging existing standards and ongoing research for\nTime-Sensitive Networking (TSN). However, TSN standards, such as Precision Time\nProtocol (PTP), focus on performance with little to no concern for security.\nThis increases the exposure of the open FH to security risks. Attacks targeting\nsynchronization mechanisms pose significant threats, potentially disrupting 5G\nnetworks and impairing connectivity.\n  In this paper, we demonstrate the impact of successful spoofing and replay\nattacks against PTP synchronization. We show how a spoofing attack is able to\ncause a production-ready O-RAN and 5G-compliant private cellular base station\nto catastrophically fail within 2 seconds of the attack, necessitating manual\nintervention to restore full network operations. To counter this, we design a\nMachine Learning (ML)-based monitoring solution capable of detecting various\nmalicious attacks with over 97.5% accuracy.\n","authors":["Joshua Groen","Simone Di Valerio","Imtiaz Karim","Davide Villa","Yiewi Zhang","Leonardo Bonati","Michele Polese","Salvatore D'Oro","Tommaso Melodia","Elisa Bertino","Francesca Cuomo","Kaushik Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2412.13049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13041v1","updated":"2024-12-17T16:05:30Z","published":"2024-12-17T16:05:30Z","title":"Harnessing Event Sensory Data for Error Pattern Prediction in Vehicles:\n  A Language Model Approach","summary":"  In this paper, we draw an analogy between processing natural languages and\nprocessing multivariate event streams from vehicles in order to predict\n$\\textit{when}$ and $\\textit{what}$ error pattern is most likely to occur in\nthe future for a given car. Our approach leverages the temporal dynamics and\ncontextual relationships of our event data from a fleet of cars. Event data is\ncomposed of discrete values of error codes as well as continuous values such as\ntime and mileage. Modelled by two causal Transformers, we can anticipate\nvehicle failures and malfunctions before they happen. Thus, we introduce\n$\\textit{CarFormer}$, a Transformer model trained via a new self-supervised\nlearning strategy, and $\\textit{EPredictor}$, an autoregressive Transformer\ndecoder model capable of predicting $\\textit{when}$ and $\\textit{what}$ error\npattern will most likely occur after some error code apparition. Despite the\nchallenges of high cardinality of event types, their unbalanced frequency of\nappearance and limited labelled data, our experimental results demonstrate the\nexcellent predictive ability of our novel model. Specifically, with sequences\nof $160$ error codes on average, our model is able with only half of the error\ncodes to achieve $80\\%$ F1 score for predicting $\\textit{what}$ error pattern\nwill occur and achieves an average absolute error of $58.4 \\pm 13.2$h\n$\\textit{when}$ forecasting the time of occurrence, thus enabling confident\npredictive maintenance and enhancing vehicle safety.\n","authors":["Hugo Math","Rainer Lienhart","Robin Schön"],"pdf_url":"https://arxiv.org/pdf/2412.13041v1.pdf","comment":"10 pages, 8 figures, accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13036v1","updated":"2024-12-17T15:59:52Z","published":"2024-12-17T15:59:52Z","title":"Open-Set Heterogeneous Domain Adaptation: Theoretical Analysis and\n  Algorithm","summary":"  Domain adaptation (DA) tackles the issue of distribution shift by learning a\nmodel from a source domain that generalizes to a target domain. However, most\nexisting DA methods are designed for scenarios where the source and target\ndomain data lie within the same feature space, which limits their applicability\nin real-world situations. Recently, heterogeneous DA (HeDA) methods have been\nintroduced to address the challenges posed by heterogeneous feature space\nbetween source and target domains. Despite their successes, current HeDA\ntechniques fall short when there is a mismatch in both feature and label\nspaces. To address this, this paper explores a new DA scenario called open-set\nHeDA (OSHeDA). In OSHeDA, the model must not only handle heterogeneity in\nfeature space but also identify samples belonging to novel classes. To tackle\nthis challenge, we first develop a novel theoretical framework that constructs\nlearning bounds for prediction error on target domain. Guided by this\nframework, we propose a new DA method called Representation Learning for OSHeDA\n(RL-OSHeDA). This method is designed to simultaneously transfer knowledge\nbetween heterogeneous data sources and identify novel classes. Experiments\nacross text, image, and clinical data demonstrate the effectiveness of our\nalgorithm. Model implementation is available at\n\\url{https://github.com/pth1993/OSHeDA}.\n","authors":["Thai-Hoang Pham","Yuanlong Wang","Changchang Yin","Xueru Zhang","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13036v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2310.20587v5","updated":"2024-12-17T15:59:44Z","published":"2023-10-31T16:24:17Z","title":"Unleashing the Power of Pre-trained Language Models for Offline\n  Reinforcement Learning","summary":"  Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks and\ncloses the gap between value-based offline RL methods and decision transformers\nin dense-reward tasks. In particular, our method demonstrates superior\nperformance in scenarios with limited data samples.\n","authors":["Ruizhe Shi","Yuyao Liu","Yanjie Ze","Simon S. Du","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2310.20587v5.pdf","comment":"Format adjustment"},{"id":"http://arxiv.org/abs/2412.11850v2","updated":"2024-12-17T15:44:30Z","published":"2024-12-16T15:11:02Z","title":"Causal Invariance Learning via Efficient Optimization of a Nonconvex\n  Objective","summary":"  Data from multiple environments offer valuable opportunities to uncover\ncausal relationships among variables. Leveraging the assumption that the causal\noutcome model remains invariant across heterogeneous environments,\nstate-of-the-art methods attempt to identify causal outcome models by learning\ninvariant prediction models and rely on exhaustive searches over all\n(exponentially many) covariate subsets. These approaches present two major\nchallenges: 1) determining the conditions under which the invariant prediction\nmodel aligns with the causal outcome model, and 2) devising computationally\nefficient causal discovery algorithms that scale polynomially, instead of\nexponentially, with the number of covariates. To address both challenges, we\nfocus on the additive intervention regime and propose nearly necessary and\nsufficient conditions for ensuring that the invariant prediction model matches\nthe causal outcome model. Exploiting the essentially necessary identifiability\nconditions, we introduce Negative Weight Distributionally Robust Optimization\n(NegDRO), a nonconvex continuous minimax optimization whose global optimizer\nrecovers the causal outcome model. Unlike standard group DRO problems that\nmaximize over the simplex, NegDRO allows negative weights on environment\nlosses, which break the convexity. Despite its nonconvexity, we demonstrate\nthat a standard gradient method converges to the causal outcome model, and we\nestablish the convergence rate with respect to the sample size and the number\nof iterations. Our algorithm avoids exhaustive search, making it scalable\nespecially when the number of covariates is large. The numerical results\nfurther validate the efficiency of the proposed method.\n","authors":["Zhenyu Wang","Yifan Hu","Peter Bühlmann","Zijian Guo"],"pdf_url":"https://arxiv.org/pdf/2412.11850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13023v1","updated":"2024-12-17T15:41:51Z","published":"2024-12-17T15:41:51Z","title":"Relational Neurosymbolic Markov Models","summary":"  Sequential problems are ubiquitous in AI, such as in reinforcement learning\nor natural language processing. State-of-the-art deep sequential models, like\ntransformers, excel in these settings but fail to guarantee the satisfaction of\nconstraints necessary for trustworthy deployment. In contrast, neurosymbolic AI\n(NeSy) provides a sound formalism to enforce constraints in deep probabilistic\nmodels but scales exponentially on sequential problems. To overcome these\nlimitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a\nnew class of end-to-end differentiable sequential models that integrate and\nprovably satisfy relational logical constraints. We propose a strategy for\ninference and learning that scales on sequential settings, and that combines\napproximate Bayesian inference, automated reasoning, and gradient estimation.\nOur experiments show that NeSy-MMs can solve problems beyond the current\nstate-of-the-art in neurosymbolic AI and still provide strong guarantees with\nrespect to desired properties. Moreover, we show that our models are more\ninterpretable and that constraints can be adapted at test time to\nout-of-distribution scenarios.\n","authors":["Lennert De Smet","Gabriele Venturato","Luc De Raedt","Giuseppe Marra"],"pdf_url":"https://arxiv.org/pdf/2412.13023v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13021v1","updated":"2024-12-17T15:41:36Z","published":"2024-12-17T15:41:36Z","title":"Queries, Representation & Detection: The Next 100 Model Fingerprinting\n  Schemes","summary":"  The deployment of machine learning models in operational contexts represents\na significant investment for any organisation. Consequently, the risk of these\nmodels being misappropriated by competitors needs to be addressed. In recent\nyears, numerous proposals have been put forth to detect instances of model\nstealing. However, these proposals operate under implicit and disparate data\nand model access assumptions; as a consequence, it remains unclear how they can\nbe effectively compared to one another. Our evaluation shows that a simple\nbaseline that we introduce performs on par with existing state-of-the-art\nfingerprints, which, on the other hand, are much more complex. To uncover the\nreasons behind this intriguing result, this paper introduces a systematic\napproach to both the creation of model fingerprinting schemes and their\nevaluation benchmarks. By dividing model fingerprinting into three core\ncomponents -- Query, Representation and Detection (QuRD) -- we are able to\nidentify $\\sim100$ previously unexplored QuRD combinations and gain insights\ninto their performance. Finally, we introduce a set of metrics to compare and\nguide the creation of more representative model stealing detection benchmarks.\nOur approach reveals the need for more challenging benchmarks and a sound\ncomparison with baselines. To foster the creation of new fingerprinting schemes\nand benchmarks, we open-source our fingerprinting toolbox.\n","authors":["Augustin Godinot","Erwan Le Merrer","Camilla Penzo","François Taïani","Gilles Trédan"],"pdf_url":"https://arxiv.org/pdf/2412.13021v1.pdf","comment":"Accepted to AAAI2025 Main Technical Track"},{"id":"http://arxiv.org/abs/2404.12957v2","updated":"2024-12-17T15:38:23Z","published":"2024-04-19T15:40:39Z","title":"Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt\n  Many-Shot Based Factual Knowledge Extraction","summary":"  In this paper, we focus on the challenging task of reliably estimating\nfactual knowledge that is embedded inside large language models (LLMs). To\navoid reliability concerns with prior approaches, we propose to eliminate\nprompt engineering when probing LLMs for factual knowledge. Our approach,\ncalled Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the\nin-context learning ability of LLMs to communicate both the factual knowledge\nquestion as well as the expected answer format. Our knowledge estimator is both\nconceptually simpler (i.e., doesn't depend on meta-linguistic judgments of\nLLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that\nit can surface more of the latent knowledge embedded in LLMs. We also\ninvestigate how different design choices affect the performance of ZP-LKE.\nUsing the proposed estimator, we perform a large-scale evaluation of the\nfactual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2),\nMistral, Gemma, etc. over a large set of relations and facts from the Wikidata\nknowledge base. We observe differences in the factual knowledge between\ndifferent model families and models of different sizes, that some relations are\nconsistently better known than others but that models differ in the precise\nfacts they know, and differences in the knowledge of base models and their\nfinetuned counterparts. Code available at:\nhttps://github.com/QinyuanWu0710/ZeroPrompt_LKE\n","authors":["Qinyuan Wu","Mohammad Aflah Khan","Soumi Das","Vedant Nanda","Bishwamittra Ghosh","Camila Kolling","Till Speicher","Laurent Bindschaedler","Krishna P. Gummadi","Evimaria Terzi"],"pdf_url":"https://arxiv.org/pdf/2404.12957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13012v1","updated":"2024-12-17T15:33:48Z","published":"2024-12-17T15:33:48Z","title":"Deep Learning Based Superconductivity: Prediction and Experimental Tests","summary":"  The discovery of novel superconducting materials is a longstanding challenge\nin materials science, with a wealth of potential for applications in energy,\ntransportation, and computing. Recent advances in artificial intelligence (AI)\nhave enabled expediting the search for new materials by efficiently utilizing\nvast materials databases. In this study, we developed an approach based on deep\nlearning (DL) to predict new superconducting materials. We have synthesized a\ncompound derived from our DL network and confirmed its superconducting\nproperties in agreement with our prediction. Our approach is also compared to\nprevious work based on random forests (RFs). In particular, RFs require\nknowledge of the chem-ical properties of the compound, while our neural net\ninputs depend solely on the chemical composition. With the help of hints from\nour network, we discover a new ternary compound\n$\\textrm{Mo}_{20}\\textrm{Re}_{6}\\textrm{Si}_{4}$, which becomes superconducting\nbelow 5.4 K. We further discuss the existing limitations and challenges\nassociated with using AI to predict and, along with potential future research\ndirections.\n","authors":["Daniel Kaplan","Adam Zhang","Joanna Blawat","Rongying Jin","Robert J. Cava","Viktor Oudovenko","Gabriel Kotliar","Anirvan M. Sengupta","Weiwei Xie"],"pdf_url":"https://arxiv.org/pdf/2412.13012v1.pdf","comment":"14 pages + 2 appendices + references. EPJ submission"},{"id":"http://arxiv.org/abs/2410.05346v2","updated":"2024-12-17T15:32:04Z","published":"2024-10-07T09:45:18Z","title":"AnyAttack: Targeted Adversarial Attacks on Vision-Language Models toward\n  Any Images","summary":"  Due to their multimodal capabilities, Vision-Language Models (VLMs) have\nfound numerous impactful applications in real-world scenarios. However, recent\nstudies have revealed that VLMs are vulnerable to image-based adversarial\nattacks, particularly targeted adversarial images that manipulate the model to\ngenerate harmful content specified by the adversary. Current attack methods\nrely on predefined target labels to create targeted adversarial attacks, which\nlimits their scalability and applicability for large-scale robustness\nevaluations. In this paper, we propose AnyAttack, a self-supervised framework\nthat generates targeted adversarial images for VLMs without label supervision,\nallowing any image to serve as a target for the attack. Our framework employs\nthe pre-training and fine-tuning paradigm, with the adversarial noise generator\npre-trained on the large-scale LAION-400M dataset. This large-scale\npre-training endows our method with powerful transferability across a wide\nrange of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP,\nBLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks\n(image-text retrieval, multimodal classification, and image captioning)\ndemonstrate the effectiveness of our attack. Additionally, we successfully\ntransfer AnyAttack to multiple commercial VLMs, including Google Gemini, Claude\nSonnet, Microsoft Copilot and OpenAI GPT. These results reveal an unprecedented\nrisk to VLMs, highlighting the need for effective countermeasures.\n","authors":["Jiaming Zhang","Junhong Ye","Xingjun Ma","Yige Li","Yunfan Yang","Jitao Sang","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.05346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05966v3","updated":"2024-12-17T15:31:49Z","published":"2024-03-09T17:17:07Z","title":"Can Generative Models Improve Self-Supervised Representation Learning?","summary":"  The rapid advancement in self-supervised representation learning has\nhighlighted its potential to leverage unlabeled data for learning rich visual\nrepresentations. However, the existing techniques, particularly those employing\ndifferent augmentations of the same image, often rely on a limited set of\nsimple transformations that cannot fully capture variations in the real world.\nThis constrains the diversity and quality of samples, which leads to\nsub-optimal representations. In this paper, we introduce a framework that\nenriches the self-supervised learning (SSL) paradigm by utilizing generative\nmodels to produce semantically consistent image augmentations. By directly\nconditioning generative models on a source image, our method enables the\ngeneration of diverse augmentations while maintaining the semantics of the\nsource image, thus offering a richer set of data for SSL. Our extensive\nexperimental results on various joint-embedding SSL techniques demonstrate that\nour framework significantly enhances the quality of learned visual\nrepresentations by up to 10\\% Top-1 accuracy in downstream tasks. This research\ndemonstrates that incorporating generative models into the joint-embedding SSL\nworkflow opens new avenues for exploring the potential of synthetic data. This\ndevelopment paves the way for more robust and versatile representation learning\ntechniques.\n","authors":["Sana Ayromlou","Vahid Reza Khazaie","Fereshteh Forghani","Arash Afkanpour"],"pdf_url":"https://arxiv.org/pdf/2403.05966v3.pdf","comment":"To be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13003v1","updated":"2024-12-17T15:25:24Z","published":"2024-12-17T15:25:24Z","title":"Boosting Test Performance with Importance Sampling--a Subpopulation\n  Perspective","summary":"  Despite empirical risk minimization (ERM) is widely applied in the machine\nlearning community, its performance is limited on data with spurious\ncorrelation or subpopulation that is introduced by hidden attributes. Existing\nliterature proposed techniques to maximize group-balanced or worst-group\naccuracy when such correlation presents, yet, at the cost of lower average\naccuracy. In addition, many existing works conduct surveys on different\nsubpopulation methods without revealing the inherent connection between these\nmethods, which could hinder the technology advancement in this area. In this\npaper, we identify important sampling as a simple yet powerful tool for solving\nthe subpopulation problem. On the theory side, we provide a new systematic\nformulation of the subpopulation problem and explicitly identify the\nassumptions that are not clearly stated in the existing works. This helps to\nuncover the cause of the dropped average accuracy. We provide the first\ntheoretical discussion on the connections of existing methods, revealing the\ncore components that make them different. On the application side, we\ndemonstrate a single estimator is enough to solve the subpopulation problem. In\nparticular, we introduce the estimator in both attribute-known and -unknown\nscenarios in the subpopulation setup, offering flexibility in practical use\ncases. And empirically, we achieve state-of-the-art performance on commonly\nused benchmark datasets.\n","authors":["Hongyu Shen","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.13003v1.pdf","comment":"16 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2412.12996v1","updated":"2024-12-17T15:15:30Z","published":"2024-12-17T15:15:30Z","title":"Neural Control and Certificate Repair via Runtime Monitoring","summary":"  Learning-based methods provide a promising approach to solving highly\nnon-linear control tasks that are often challenging for classical control\nmethods. To ensure the satisfaction of a safety property, learning-based\nmethods jointly learn a control policy together with a certificate function for\nthe property. Popular examples include barrier functions for safety and\nLyapunov functions for asymptotic stability. While there has been significant\nprogress on learning-based control with certificate functions in the white-box\nsetting, where the correctness of the certificate function can be formally\nverified, there has been little work on ensuring their reliability in the\nblack-box setting where the system dynamics are unknown. In this work, we\nconsider the problems of certifying and repairing neural network control\npolicies and certificate functions in the black-box setting. We propose a novel\nframework that utilizes runtime monitoring to detect system behaviors that\nviolate the property of interest under some initially trained neural network\npolicy and certificate. These violating behaviors are used to extract new\ntraining data, that is used to re-train the neural network policy and the\ncertificate function and to ultimately repair them. We demonstrate the\neffectiveness of our approach empirically by using it to repair and to boost\nthe safety rate of neural network policies learned by a state-of-the-art method\nfor learning-based control on two autonomous system control tasks.\n","authors":["Emily Yu","Đorđe Žikelić","Thomas A. Henzinger"],"pdf_url":"https://arxiv.org/pdf/2412.12996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13391v3","updated":"2024-12-17T15:12:02Z","published":"2024-01-24T11:41:30Z","title":"Reranking individuals: The effect of fair classification within-groups","summary":"  Artificial Intelligence (AI) finds widespread application across various\ndomains, but it sparks concerns about fairness in its deployment. The\nprevailing discourse in classification often emphasizes outcome-based metrics\ncomparing sensitive subgroups without a nuanced consideration of the\ndifferential impacts within subgroups. Bias mitigation techniques not only\naffect the ranking of pairs of instances across sensitive groups, but often\nalso significantly affect the ranking of instances within these groups. Such\nchanges are hard to explain and raise concerns regarding the validity of the\nintervention. Unfortunately, these effects remain under the radar in the\naccuracy-fairness evaluation framework that is usually applied. Additionally,\nwe illustrate the effect of several popular bias mitigation methods, and how\ntheir output often does not reflect real-world scenarios.\n","authors":["Sofie Goethals","Marco Favier","Toon Calders"],"pdf_url":"https://arxiv.org/pdf/2401.13391v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12987v1","updated":"2024-12-17T15:06:44Z","published":"2024-12-17T15:06:44Z","title":"Stochastic interior-point methods for smooth conic optimization with\n  applications","summary":"  Conic optimization plays a crucial role in many machine learning (ML)\nproblems. However, practical algorithms for conic constrained ML problems with\nlarge datasets are often limited to specific use cases, as stochastic\nalgorithms for general conic optimization remain underdeveloped. To fill this\ngap, we introduce a stochastic interior-point method (SIPM) framework for\ngeneral conic optimization, along with four novel SIPM variants leveraging\ndistinct stochastic gradient estimators. Under mild assumptions, we establish\nthe global convergence rates of our proposed SIPMs, which, up to a logarithmic\nfactor, match the best-known rates in stochastic unconstrained optimization.\nFinally, our numerical experiments on robust linear regression, multi-task\nrelationship learning, and clustering data streams demonstrate the\neffectiveness and efficiency of our approach.\n","authors":["Chuan He","Zhanwang Deng"],"pdf_url":"https://arxiv.org/pdf/2412.12987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12984v1","updated":"2024-12-17T15:04:54Z","published":"2024-12-17T15:04:54Z","title":"Cluster-guided Contrastive Class-imbalanced Graph Classification","summary":"  This paper studies the problem of class-imbalanced graph classification,\nwhich aims at effectively classifying the categories of graphs in scenarios\nwith imbalanced class distribution. Despite the tremendous success of graph\nneural networks (GNNs), their modeling ability for imbalanced graph-structured\ndata is inadequate, which typically leads to predictions biased towards the\nmajority classes. Besides, existing class-imbalanced learning methods in\nvisions may overlook the rich graph semantic substructures of the majority\nclasses and excessively emphasize learning from the minority classes. To tackle\nthis issue, this paper proposes a simple yet powerful approach called C$^3$GNN\nthat incorporates the idea of clustering into contrastive learning to enhance\nclass-imbalanced graph classification. Technically, C$^3$GNN clusters graphs\nfrom each majority class into multiple subclasses, ensuring they have similar\nsizes to the minority class, thus alleviating class imbalance. Additionally, it\nutilizes the Mixup technique to synthesize new samples and enrich the semantic\ninformation of each subclass, and leverages supervised contrastive learning to\nhierarchically learn effective graph representations. In this way, we can not\nonly sufficiently explore the semantic substructures within the majority class\nbut also effectively alleviate excessive focus on the minority class. Extensive\nexperiments on real-world graph benchmark datasets verify the superior\nperformance of our proposed method.\n","authors":["Wei Ju","Zhengyang Mao","Siyu Yi","Yifang Qin","Yiyang Gu","Zhiping Xiao","Jianhao Shen","Ziyue Qiao","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12984v1.pdf","comment":"Accepted by Proceedings of the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2205.10691v2","updated":"2024-12-17T15:04:46Z","published":"2022-05-21T23:04:20Z","title":"Producing Histopathology Phantom Images using Generative Adversarial\n  Networks to improve Tumor Detection","summary":"  Advance in medical imaging is an important part in deep learning research.\nOne of the goals of computer vision is development of a holistic, comprehensive\nmodel which can identify tumors from histology slides obtained via biopsies. A\nmajor problem that stands in the way is lack of data for a few cancer-types. In\nthis paper, we ascertain that data augmentation using GANs can be a viable\nsolution to reduce the unevenness in the distribution of different cancer types\nin our dataset. Our demonstration showed that a dataset augmented to a 50%\nincrease causes an increase in tumor detection from 80% to 87.5%\n","authors":["Vidit Gautam"],"pdf_url":"https://arxiv.org/pdf/2205.10691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10337v2","updated":"2024-12-17T14:57:57Z","published":"2024-12-13T18:32:21Z","title":"Generative AI in Medicine","summary":"  The increased capabilities of generative AI have dramatically expanded its\npossible use cases in medicine. We provide a comprehensive overview of\ngenerative AI use cases for clinicians, patients, clinical trial organizers,\nresearchers, and trainees. We then discuss the many challenges -- including\nmaintaining privacy and security, improving transparency and interpretability,\nupholding equity, and rigorously evaluating models -- which must be overcome to\nrealize this potential, and the open research directions they give rise to.\n","authors":["Divya Shanmugam","Monica Agrawal","Rajiv Movva","Irene Y. Chen","Marzyeh Ghassemi","Maia Jacobs","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2412.10337v2.pdf","comment":"To appear in the Annual Review of Biomedical Data Science, August\n  2025"},{"id":"http://arxiv.org/abs/2412.12971v1","updated":"2024-12-17T14:54:30Z","published":"2024-12-17T14:54:30Z","title":"ArchesWeather & ArchesWeatherGen: a deterministic and generative model\n  for efficient ML weather forecasting","summary":"  Weather forecasting plays a vital role in today's society, from agriculture\nand logistics to predicting the output of renewable energies, and preparing for\nextreme weather events. Deep learning weather forecasting models trained with\nthe next state prediction objective on ERA5 have shown great success compared\nto numerical global circulation models. However, for a wide range of\napplications, being able to provide representative samples from the\ndistribution of possible future weather states is critical. In this paper, we\npropose a methodology to leverage deterministic weather models in the design of\nprobabilistic weather models, leading to improved performance and reduced\ncomputing costs. We first introduce \\textbf{ArchesWeather}, a transformer-based\ndeterministic model that improves upon Pangu-Weather by removing\noverrestrictive inductive priors. We then design a probabilistic weather model\ncalled \\textbf{ArchesWeatherGen} based on flow matching, a modern variant of\ndiffusion models, that is trained to project ArchesWeather's predictions to the\ndistribution of ERA5 weather states. ArchesWeatherGen is a true stochastic\nemulator of ERA5 and surpasses IFS ENS and NeuralGCM on all WeatherBench\nheadline variables (except for NeuralGCM's geopotential). Our work also aims to\ndemocratize the use of deterministic and generative machine learning models in\nweather forecasting research, with academic computing resources. All models are\ntrained at 1.5{\\deg} resolution, with a training budget of $\\sim$9 V100 days\nfor ArchesWeather and $\\sim$45 V100 days for ArchesWeatherGen. For inference,\nArchesWeatherGen generates 15-day weather trajectories at a rate of 1 minute\nper ensemble member on a A100 GPU card. To make our work fully reproducible,\nour code and models are open source, including the complete pipeline for data\npreparation, training, and evaluation, at https://github.com/INRIA/geoarches .\n","authors":["Guillaume Couairon","Renu Singh","Anastase Charantonis","Christian Lessig","Claire Monteleoni"],"pdf_url":"https://arxiv.org/pdf/2412.12971v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.12968v1","updated":"2024-12-17T14:53:38Z","published":"2024-12-17T14:53:38Z","title":"On Local Overfitting and Forgetting in Deep Neural Networks","summary":"  The infrequent occurrence of overfitting in deep neural networks is\nperplexing: contrary to theoretical expectations, increasing model size often\nenhances performance in practice. But what if overfitting does occur, though\nrestricted to specific sub-regions of the data space? In this work, we propose\na novel score that captures the forgetting rate of deep models on validation\ndata. We posit that this score quantifies local overfitting: a decline in\nperformance confined to certain regions of the data space. We then show\nempirically that local overfitting occurs regardless of the presence of\ntraditional overfitting. Using the framework of deep over-parametrized linear\nmodels, we offer a certain theoretical characterization of forgotten knowledge,\nand show that it correlates with knowledge forgotten by real deep models.\nFinally, we devise a new ensemble method that aims to recover forgotten\nknowledge, relying solely on the training history of a single network. When\ncombined with self-distillation, this method enhances the performance of any\ntrained model without adding inference costs. Extensive empirical evaluations\ndemonstrate the efficacy of our method across multiple datasets, contemporary\nneural network architectures, and training protocols.\n","authors":["Uri Stern","Tomer Yaacoby","Daphna Weinshall"],"pdf_url":"https://arxiv.org/pdf/2412.12968v1.pdf","comment":"to appear in AAAI-25"},{"id":"http://arxiv.org/abs/2410.17159v2","updated":"2024-12-17T14:53:29Z","published":"2024-10-22T16:33:54Z","title":"LiNo: Advancing Recursive Residual Decomposition of Linear and Nonlinear\n  Patterns for Robust Time Series Forecasting","summary":"  Forecasting models are pivotal in a data-driven world with vast volumes of\ntime series data that appear as a compound of vast Linear and Nonlinear\npatterns. Recent deep time series forecasting models struggle to utilize\nseasonal and trend decomposition to separate the entangled components. Such a\nstrategy only explicitly extracts simple linear patterns like trends, leaving\nthe other linear modes and vast unexplored nonlinear patterns to the residual.\nTheir flawed linear and nonlinear feature extraction models and shallow-level\ndecomposition limit their adaptation to the diverse patterns present in\nreal-world scenarios. Given this, we innovate Recursive Residual Decomposition\nby introducing explicit extraction of both linear and nonlinear patterns. This\ndeeper-level decomposition framework, which is named LiNo, captures linear\npatterns using a Li block which can be a moving average kernel, and models\nnonlinear patterns using a No block which can be a Transformer encoder. The\nextraction of these two patterns is performed alternatively and recursively. To\nachieve the full potential of LiNo, we develop the current simple linear\npattern extractor to a general learnable autoregressive model, and design a\nnovel No block that can handle all essential nonlinear patterns. Remarkably,\nthe proposed LiNo achieves state-of-the-art on thirteen real-world benchmarks\nunder univariate and multivariate forecasting scenarios. Experiments show that\ncurrent forecasting models can deliver more robust and precise results through\nthis advanced Recursive Residual Decomposition. We hope this work could offer\ninsight into designing more effective forecasting models. Code is available at\nthis Repository: https://github.com/Levi-Ackman/LiNo.\n","authors":["Guoqi Yu","Yaoming Li","Xiaoyu Guo","Dayu Wang","Zirui Liu","Shujun Wang","Tong Yang"],"pdf_url":"https://arxiv.org/pdf/2410.17159v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00876v3","updated":"2024-12-17T14:45:12Z","published":"2024-12-01T16:32:31Z","title":"Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification","summary":"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n","authors":["Wenxuan Huang","Zijie Zhai","Yunhang Shen","Shaosheng Cao","Fei Zhao","Xiangfeng Xu","Zheyu Ye","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.00876v3.pdf","comment":"Code is available at https://github.com/Osilly/dynamic_llava"},{"id":"http://arxiv.org/abs/2408.01639v2","updated":"2024-12-17T14:41:50Z","published":"2024-08-03T02:53:24Z","title":"Coordinating Planning and Tracking in Layered Control Policies via\n  Actor-Critic Learning","summary":"  We propose a reinforcement learning (RL)-based algorithm to jointly train (1)\na trajectory planner and (2) a tracking controller in a layered control\narchitecture. Our algorithm arises naturally from a rewrite of the underlying\noptimal control problem that lends itself to an actor-critic learning approach.\nBy explicitly learning a \\textit{dual} network to coordinate the interaction\nbetween the planning and tracking layers, we demonstrate the ability to achieve\nan effective consensus between the two components, leading to an interpretable\npolicy. We theoretically prove that our algorithm converges to the optimal dual\nnetwork in the Linear Quadratic Regulator (LQR) setting and empirically\nvalidate its applicability to nonlinear systems through simulation experiments\non a unicycle model.\n","authors":["Fengjun Yang","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2408.01639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12953v1","updated":"2024-12-17T14:34:51Z","published":"2024-12-17T14:34:51Z","title":"Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning","summary":"  Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.\n","authors":["Moritz Reuss","Jyothish Pari","Pulkit Agrawal","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2412.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12951v1","updated":"2024-12-17T14:33:05Z","published":"2024-12-17T14:33:05Z","title":"FineGates: LLMs Finetuning with Compression using Stochastic Gates","summary":"  Large Language Models (LLMs), with billions of parameters, present\nsignificant challenges for full finetuning due to the high computational\ndemands, memory requirements, and impracticality of many real-world\napplications. When faced with limited computational resources or small\ndatasets, updating all model parameters can often result in overfitting. To\naddress this, lightweight finetuning techniques have been proposed, like\nlearning low-rank adapter layers. These methods aim to train only a few\nadditional parameters combined with the base model, which remains frozen,\nreducing resource usage and mitigating overfitting risks. In this work, we\npropose an adaptor model based on stochastic gates that simultaneously sparsify\nthe frozen base model with task-specific adaptation. Our method comes with a\nsmall number of trainable parameters and allows us to speed up the base model\ninference with competitive accuracy. We evaluate it in additional variants by\nequipping it with additional low-rank parameters and comparing it to several\nrecent baselines. Our results show that the proposed method improves the\nfinetuned model accuracy comparatively to the several baselines and allows the\nremoval of up to 20-40\\% without significant accuracy loss.\n","authors":["Jonathan Svirsky","Yehonathan Refael","Ofir Lindenbaum"],"pdf_url":"https://arxiv.org/pdf/2412.12951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12936v1","updated":"2024-12-17T14:15:30Z","published":"2024-12-17T14:15:30Z","title":"A simple DNN regression for the chemical composition in essential oil","summary":"  Although experimental design and methodological surveys for mono-molecular\nactivity/property has been extensively investigated, those for chemical\ncomposition have received little attention, with the exception of a few prior\nstudies. In this study, we configured three simple DNN regressors to predict\nessential oil property based on chemical composition. Despite showing\noverfitting due to the small size of dataset, all models were trained\neffectively in this study.\n","authors":["Yuki Harada","Shuichi Maeda","Masato Kiyama","Shinichiro Nakamura"],"pdf_url":"https://arxiv.org/pdf/2412.12936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12931v1","updated":"2024-12-17T14:07:44Z","published":"2024-12-17T14:07:44Z","title":"Multi-Subspace Matrix Recovery from Permuted Data","summary":"  This paper aims to recover a multi-subspace matrix from permuted data: given\na matrix, in which the columns are drawn from a union of low-dimensional\nsubspaces and some columns are corrupted by permutations on their entries,\nrecover the original matrix. The task has numerous practical applications such\nas data cleaning, integration, and de-anonymization, but it remains challenging\nand cannot be well addressed by existing techniques such as robust principal\ncomponent analysis because of the presence of multiple subspaces and the\npermutations on the elements of vectors. To solve the challenge, we develop a\nnovel four-stage algorithm pipeline including outlier identification, subspace\nreconstruction, outlier classification, and unsupervised sensing for permuted\nvector recovery. Particularly, we provide theoretical guarantees for the\noutlier classification step, ensuring reliable multi-subspace matrix recovery.\nOur pipeline is compared with state-of-the-art competitors on multiple\nbenchmarks and shows superior performance.\n","authors":["Liangqi Xie","Jicong Fan"],"pdf_url":"https://arxiv.org/pdf/2412.12931v1.pdf","comment":"The paper was accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2411.11304v3","updated":"2024-12-17T14:05:59Z","published":"2024-11-18T05:59:29Z","title":"Towards Personalized Federated Node Classification in One-shot\n  Communication","summary":"  Federated Graph Learning (FGL) has emerged as a promising paradigm for\nbreaking data silos in distributed private graphs data management. In practical\nscenarios involving complex and heterogeneous distributed graph data,\npersonalized Federated Graph Learning (pFGL) aims to enhance model utility by\ntraining personalized models tailored to individual client needs, rather than\nrelying on a universal global model. However, existing pFGL methods often\nrequire numerous communication rounds under heterogeneous client graphs,\nleading to significant security concerns and communication overhead. While\nOne-shot Federated Learning (OFL) addresses these issues by enabling\ncollaboration in a single round, existing OFL methods are designed for\nimage-based tasks and ineffective for graph data, leaving a critical gap in the\nfield. Additionally, personalized models often suffer from bias, failing to\ngeneralize effectively to minority data. To address these challenges, we\npropose the first one-shot personalized federated graph learning method for\nnode classification, compatible with the Secure Aggregation protocol for\nprivacy preservation. Specifically, for effective graph learning in a single\ncommunication round, our method estimates and aggregates class-wise feature\ndistribution statistics to construct a global pseudo-graph on the server,\nfacilitating the training of a global graph model. Moreover, to mitigate bias,\nwe introduce a two-stage personalized training approach that adaptively\nbalances local personal information and global insights from the pseudo-graph,\nimproving both personalization and generalization. Extensive experiments\nconducted on 8 multi-scale graph datasets demonstrate that our method\nsignificantly outperforms state-of-the-art baselines across various settings.\n","authors":["Guochen Yan","Xunkai Li","Luyuan Xie","Wentao Zhang","Qingni Shen","Yuejian Fang","Zhonghai Wu"],"pdf_url":"https://arxiv.org/pdf/2411.11304v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.02196v3","updated":"2024-12-17T14:02:24Z","published":"2024-02-03T15:56:03Z","title":"\"Clustering and Conquer\" Procedures for Parallel Large-Scale Ranking and\n  Selection","summary":"  This work breaks the sample efficiency bottleneck in parallel large-scale\nranking and selection (R&S) problem by leveraging correlation information. We\nmodify the commonly used \"divide and conquer\" framework in parallel computing\nby adding a correlation-based clustering step, transforming it into \"clustering\nand conquer\". This seemingly simple modification can achieve an\n$\\mathcal{O}(p)$ sample complexity reduction rate, which represents the maximum\nattainable reduction for the class of sample-optimal R&S methods. Our approach\nenjoys two key advantages: 1) it does not require highly accurate correlation\nestimation or precise clustering, and 2) it allows for seamless integration\nwith various existing R&S method, while achieving optimal sample complexity.\nTheoretically, we develop a novel gradient analysis framework to analyze sample\nefficiency and guide the design of large-scale R&S procedures. Building upon\nthis framework, we propose a gradient-based budget allocation policy. We also\nintroduce a new clustering algorithm, selection policy, and precision criterion\ntailored for large-scale scenarios. Finally, in large-scale AI applications\nsuch as neural architecture search, our methods demonstrate superior\nperformance.\n","authors":["Zishi Zhang","Yijie Peng"],"pdf_url":"https://arxiv.org/pdf/2402.02196v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07346v2","updated":"2024-12-17T13:57:19Z","published":"2023-07-14T13:50:00Z","title":"Clusterability test for categorical data","summary":"  The objective of clusterability evaluation is to check whether a clustering\nstructure exists within the data set. As a crucial yet often-overlooked issue\nin cluster analysis, it is essential to conduct such a test before applying any\nclustering algorithm. If a data set is unclusterable, any subsequent clustering\nanalysis would not yield valid results. Despite its importance, the majority of\nexisting studies focus on numerical data, leaving the clusterability evaluation\nissue for categorical data as an open problem. Here we present TestCat, a\ntesting-based approach to assess the clusterability of categorical data in\nterms of an analytical $p$-value. The key idea underlying TestCat is that\nclusterable categorical data possess many strongly associated attribute pairs\nand hence the sum of chi-squared statistics of all attribute pairs is employed\nas the test statistic for $p$-value calculation. We apply our method to a set\nof benchmark categorical data sets, showing that TestCat outperforms those\nsolutions based on existing clusterability evaluation methods for numeric data.\nTo the best of our knowledge, our work provides the first way to effectively\nrecognize the clusterability of categorical data in a statistically sound\nmanner.\n","authors":["Lianyu Hu","Junjie Dong","Mudi Jiang","Yan Liu","Zengyou He"],"pdf_url":"https://arxiv.org/pdf/2307.07346v2.pdf","comment":"28 pages, 12 appendix pages, 17 figures"},{"id":"http://arxiv.org/abs/2412.12918v1","updated":"2024-12-17T13:51:24Z","published":"2024-12-17T13:51:24Z","title":"BOIDS: High-dimensional Bayesian Optimization via Incumbent-guided\n  Direction Lines and Subspace Embeddings","summary":"  When it comes to expensive black-box optimization problems, Bayesian\nOptimization (BO) is a well-known and powerful solution. Many real-world\napplications involve a large number of dimensions, hence scaling BO to high\ndimension is of much interest. However, state-of-the-art high-dimensional BO\nmethods still suffer from the curse of dimensionality, highlighting the need\nfor further improvements. In this work, we introduce BOIDS, a novel\nhigh-dimensional BO algorithm that guides optimization by a sequence of\none-dimensional direction lines using a novel tailored line-based optimization\nprocedure. To improve the efficiency, we also propose an adaptive selection\ntechnique to identify most optimal lines for each round of line-based\noptimization. Additionally, we incorporate a subspace embedding technique for\nbetter scaling to high-dimensional spaces. We further provide theoretical\nanalysis of our proposed method to analyze its convergence property. Our\nextensive experimental results show that BOIDS outperforms state-of-the-art\nbaselines on various synthetic and real-world benchmark problems.\n","authors":["Lam Ngo","Huong Ha","Jeffrey Chan","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12918v1.pdf","comment":"Published at AAAI Conference on Artificial Intelligence, 2025"},{"id":"http://arxiv.org/abs/2412.12916v1","updated":"2024-12-17T13:50:20Z","published":"2024-12-17T13:50:20Z","title":"Graph Spring Neural ODEs for Link Sign Prediction","summary":"  Signed graphs allow for encoding positive and negative relations between\nnodes and are used to model various online activities. Node representation\nlearning for signed graphs is a well-studied task with important applications\nsuch as sign prediction. While the size of datasets is ever-increasing, recent\nmethods often sacrifice scalability for accuracy. We propose a novel\nmessage-passing layer architecture called Graph Spring Network (GSN) modeled\nafter spring forces. We combine it with a Graph Neural Ordinary Differential\nEquations (ODEs) formalism to optimize the system dynamics in embedding space\nto solve a downstream prediction task. Once the dynamics is learned, embedding\ngeneration for novel datasets is done by solving the ODEs in time using a\nnumerical integration scheme. Our GSN layer leverages the fast-to-compute edge\nvector directions and learnable scalar functions that only depend on nodes'\ndistances in latent space to compute the nodes' positions. Conversely, Graph\nConvolution and Graph Attention Network layers rely on learnable vector\nfunctions that require the full positions of input nodes in latent space. We\npropose a specific implementation called Spring-Neural-Network (SPR-NN) using a\nset of small neural networks mimicking attracting and repulsing spring forces\nthat we train for link sign prediction. Experiments show that our method\nachieves accuracy close to the state-of-the-art methods with node generation\ntime speedup factors of up to 28,000 on large graphs.\n","authors":["Andrin Rehmann","Alexandre Bovet"],"pdf_url":"https://arxiv.org/pdf/2412.12916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07559v2","updated":"2024-12-17T13:43:16Z","published":"2024-12-10T14:48:59Z","title":"Adaptive Epsilon Adversarial Training for Robust Gravitational Wave\n  Parameter Estimation Using Normalizing Flows","summary":"  Adversarial training with Normalizing Flow (NF) models is an emerging\nresearch area aimed at improving model robustness through adversarial samples.\nIn this study, we focus on applying adversarial training to NF models for\ngravitational wave parameter estimation. We propose an adaptive epsilon method\nfor Fast Gradient Sign Method (FGSM) adversarial training, which dynamically\nadjusts perturbation strengths based on gradient magnitudes using logarithmic\nscaling. Our hybrid architecture, combining ResNet and Inverse Autoregressive\nFlow, reduces the Negative Log Likelihood (NLL) loss by 47\\% under FGSM attacks\ncompared to the baseline model, while maintaining an NLL of 4.2 on clean data\n(only 5\\% higher than the baseline). For perturbation strengths between 0.01\nand 0.1, our model achieves an average NLL of 5.8, outperforming both\nfixed-epsilon (NLL: 6.7) and progressive-epsilon (NLL: 7.2) methods. Under\nstronger Projected Gradient Descent attacks with perturbation strength of 0.05,\nour model maintains an NLL of 6.4, demonstrating superior robustness while\navoiding catastrophic overfitting.\n","authors":["Yiqian Yang","Xihua Zhu","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.07559v2.pdf","comment":"Due to new experimental results to add to the paper, this version no\n  longer accurately reflects the current state of our research. Therefore, we\n  are withdrawing the paper while further experiments are conducted. We will\n  submit a new version in the future. We apologize for any inconvenience this\n  may cause"},{"id":"http://arxiv.org/abs/2412.12910v1","updated":"2024-12-17T13:37:48Z","published":"2024-12-17T13:37:48Z","title":"Sequential Harmful Shift Detection Without Labels","summary":"  We introduce a novel approach for detecting distribution shifts that\nnegatively impact the performance of machine learning models in continuous\nproduction environments, which requires no access to ground truth data labels.\nIt builds upon the work of Podkopaev and Ramdas [2022], who address scenarios\nwhere labels are available for tracking model errors over time. Our solution\nextends this framework to work in the absence of labels, by employing a proxy\nfor the true error. This proxy is derived using the predictions of a trained\nerror estimator. Experiments show that our method has high power and false\nalarm control under various distribution shifts, including covariate and label\nshifts and natural shifts over geography and time.\n","authors":["Salim I. Amoukou","Tom Bewley","Saumitra Mishra","Freddy Lecue","Daniele Magazzeni","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2412.12910v1.pdf","comment":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2412.12909v1","updated":"2024-12-17T13:37:11Z","published":"2024-12-17T13:37:11Z","title":"PT: A Plain Transformer is Good Hospital Readmission Predictor","summary":"  Hospital readmission prediction is critical for clinical decision support,\naiming to identify patients at risk of returning within 30 days post-discharge.\nHigh readmission rates often indicate inadequate treatment or post-discharge\ncare, making effective prediction models essential for optimizing resources and\nimproving patient outcomes. We propose PT, a Transformer-based model that\nintegrates Electronic Health Records (EHR), medical images, and clinical notes\nto predict 30-day all-cause hospital readmissions. PT extracts features from\nraw data and uses specialized Transformer blocks tailored to the data's\ncomplexity. Enhanced with Random Forest for EHR feature selection and test-time\nensemble techniques, PT achieves superior accuracy, scalability, and\nrobustness. It performs well even when temporal information is missing. Our\nmain contributions are: (1)Simplicity: A powerful and efficient baseline model\noutperforming existing ones in prediction accuracy; (2)Scalability: Flexible\nhandling of various features from different modalities, achieving high\nperformance with just clinical notes or EHR data; (3)Robustness: Strong\npredictive performance even with missing or unclear temporal data.\n","authors":["Zhenyi Fan","Jiaqi Li","Dongyu Luo","Yuqi Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09444v2","updated":"2024-12-17T13:24:36Z","published":"2024-12-12T16:57:46Z","title":"Search Strategy Generation for Branch and Bound Using Genetic\n  Programming","summary":"  Branch-and-Bound (B\\&B) is an exact method in integer programming that\nrecursively divides the search space into a tree. During the resolution\nprocess, determining the next subproblem to explore within the tree-known as\nthe search strategy-is crucial. Hand-crafted heuristics are commonly used, but\nnone are effective over all problem classes. Recent approaches utilizing neural\nnetworks claim to make more intelligent decisions but are computationally\nexpensive. In this paper, we introduce GP2S (Genetic Programming for Search\nStrategy), a novel machine learning approach that automatically generates a\nB\\&B search strategy heuristic, aiming to make intelligent decisions while\nbeing computationally lightweight. We define a policy as a function that\nevaluates the quality of a B\\&B node by combining features from the node and\nthe problem; the search strategy policy is then defined by a best-first search\nbased on this node ranking. The policy space is explored using a genetic\nprogramming algorithm, and the policy that achieves the best performance on a\ntraining set is selected. We compare our approach with the standard method of\nthe SCIP solver, a recent graph neural network-based method, and handcrafted\nheuristics. Our first evaluation includes three types of primal hard problems,\ntested on instances similar to the training set and on larger instances. Our\nmethod is at most 2\\% slower than the best baseline and consistently\noutperforms SCIP, achieving an average speedup of 11.3\\%. Additionally, GP2S is\ntested on the MIPLIB 2017 dataset, generating multiple heuristics from\ndifferent subsets of instances. It exceeds SCIP's average performance in 7 out\nof 10 cases across 15 times more instances and under a time limit 15 times\nlonger, with some GP2S methods leading on most experiments in terms of the\nnumber of feasible solutions or optimality gap.\n","authors":["Gwen Maudet","Grégoire Danoy"],"pdf_url":"https://arxiv.org/pdf/2412.09444v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12898v1","updated":"2024-12-17T13:21:26Z","published":"2024-12-17T13:21:26Z","title":"An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural\n  Language Descriptions","summary":"  The Piping and Instrumentation Diagrams (P&IDs) are foundational to the\ndesign, construction, and operation of workflows in the engineering and process\nindustries. However, their manual creation is often labor-intensive,\nerror-prone, and lacks robust mechanisms for error detection and correction.\nWhile recent advancements in Generative AI, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), have demonstrated significant\npotential across various domains, their application in automating generation of\nengineering workflows remains underexplored. In this work, we introduce a novel\ncopilot for automating the generation of P&IDs from natural language\ndescriptions. Leveraging a multi-step agentic workflow, our copilot provides a\nstructured and iterative approach to diagram creation directly from Natural\nLanguage prompts. We demonstrate the feasibility of the generation process by\nevaluating the soundness and completeness of the workflow, and show improved\nresults compared to vanilla zero-shot and few-shot generation approaches.\n","authors":["Shreeyash Gowaikar","Srinivasan Iyengar","Sameer Segal","Shivkumar Kalyanaraman"],"pdf_url":"https://arxiv.org/pdf/2412.12898v1.pdf","comment":"Accepted at the AAAI'25 Workshop on AI to Accelerate Science and\n  Engineering (AI2ASE)"},{"id":"http://arxiv.org/abs/2412.12894v1","updated":"2024-12-17T13:19:55Z","published":"2024-12-17T13:19:55Z","title":"Design of Restricted Normalizing Flow towards Arbitrary Stochastic\n  Policy with Computational Efficiency","summary":"  This paper proposes a new design method for a stochastic control policy using\na normalizing flow (NF). In reinforcement learning (RL), the policy is usually\nmodeled as a distribution model with trainable parameters. When this\nparameterization has less expressiveness, it would fail to acquiring the\noptimal policy. A mixture model has capability of a universal approximation,\nbut it with too much redundancy increases the computational cost, which can\nbecome a bottleneck when considering the use of real-time robot control. As\nanother approach, NF, which is with additional parameters for invertible\ntransformation from a simple stochastic model as a base, is expected to exert\nhigh expressiveness and lower computational cost. However, NF cannot compute\nits mean analytically due to complexity of the invertible transformation, and\nit lacks reliability because it retains stochastic behaviors after deployment\nfor robot controller. This paper therefore designs a restricted NF (RNF) that\nachieves an analytic mean by appropriately restricting the invertible\ntransformation. In addition, the expressiveness impaired by this restriction is\nregained using bimodal student-t distribution as its base, so-called Bit-RNF.\nIn RL benchmarks, Bit-RNF policy outperformed the previous models. Finally, a\nreal robot experiment demonstrated the applicability of Bit-RNF policy to real\nworld. The attached video is uploaded on youtube: https://youtu.be/R_GJVZDW9bk\n","authors":["Taisuke Kobayashi","Takumi Aotani"],"pdf_url":"https://arxiv.org/pdf/2412.12894v1.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.12890v1","updated":"2024-12-17T13:17:19Z","published":"2024-12-17T13:17:19Z","title":"Suppressing Uncertainty in Gaze Estimation","summary":"  Uncertainty in gaze estimation manifests in two aspects: 1) low-quality\nimages caused by occlusion, blurriness, inconsistent eye movements, or even\nnon-face images; 2) incorrect labels resulting from the misalignment between\nthe labeled and actual gaze points during the annotation process. Allowing\nthese uncertainties to participate in training hinders the improvement of gaze\nestimation. To tackle these challenges, in this paper, we propose an effective\nsolution, named Suppressing Uncertainty in Gaze Estimation (SUGE), which\nintroduces a novel triplet-label consistency measurement to estimate and reduce\nthe uncertainties. Specifically, for each training sample, we propose to\nestimate a novel ``neighboring label'' calculated by a linearly weighted\nprojection from the neighbors to capture the similarity relationship between\nimage features and their corresponding labels, which can be incorporated with\nthe predicted pseudo label and ground-truth label for uncertainty estimation.\nBy modeling such triplet-label consistency, we can measure the qualities of\nboth images and labels, and further largely reduce the negative effects of\nunqualified images and wrong labels through our designed sample weighting and\nlabel correction strategies. Experimental results on the gaze estimation\nbenchmarks indicate that our proposed SUGE achieves state-of-the-art\nperformance.\n","authors":["Shijing Wang","Yaping Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12890v1.pdf","comment":"This paper has been accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2412.11448v2","updated":"2024-12-17T13:15:24Z","published":"2024-12-16T05:02:50Z","title":"TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated\n  Learning","summary":"  Due to the sensitivity of data, federated learning (FL) is employed to enable\ndistributed machine learning while safeguarding data privacy and accommodating\nthe requirements of various devices. However, in the context of\nsemi-decentralized federated learning (SD-FL), clients' communication and\ntraining states are dynamic. This variability arises from local training\nfluctuations, heterogeneous data distributions, and intermittent client\nparticipation. Most existing studies primarily focus on stable client states,\nneglecting the dynamic challenges present in real-world scenarios. To tackle\nthis issue, we propose a trust-aware client scheduling mechanism (TRAIL) that\nassesses client states and contributions, enhancing model training efficiency\nthrough selective client participation. Our focus is on a semi-decentralized\nfederated learning framework where edge servers and clients train a shared\nglobal model using unreliable intra-cluster model aggregation and inter-cluster\nmodel consensus. First, we develop an adaptive hidden semi-Markov model (AHSMM)\nto estimate clients' communication states and contributions. Next, we address a\nclient-server association optimization problem to minimize global training\nloss. Using convergence analysis, we propose a greedy client scheduling\nalgorithm. Finally, our experiments conducted on real-world datasets\ndemonstrate that TRAIL outperforms state-of-the-art baselines, achieving an\nimprovement of 8.7\\% in test accuracy and a reduction of 15.3\\% in training\nloss.\n","authors":["Gangqiang Hu","Jianfeng Lu","Jianmin Han","Shuqin Cao","Jing Liu","Hao Fu"],"pdf_url":"https://arxiv.org/pdf/2412.11448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12886v1","updated":"2024-12-17T13:10:02Z","published":"2024-12-17T13:10:02Z","title":"TimeCHEAT: A Channel Harmony Strategy for Irregularly Sampled\n  Multivariate Time Series Analysis","summary":"  Irregularly sampled multivariate time series (ISMTS) are prevalent in\nreality. Due to their non-uniform intervals between successive observations and\nvarying sampling rates among series, the channel-independent (CI) strategy,\nwhich has been demonstrated more desirable for complete multivariate time\nseries forecasting in recent studies, has failed. This failure can be further\nattributed to the sampling sparsity, which provides insufficient information\nfor effective CI learning, thereby reducing its capacity. When we resort to the\nchannel-dependent (CD) strategy, even higher capacity cannot mitigate the\npotential loss of diversity in learning similar embedding patterns across\ndifferent channels. We find that existing work considers CI and CD strategies\nto be mutually exclusive, primarily because they apply these strategies to the\nglobal channel. However, we hold the view that channel strategies do not\nnecessarily have to be used globally. Instead, by appropriately applying them\nlocally and globally, we can create an opportunity to take full advantage of\nboth strategies. This leads us to introduce the Channel Harmony ISMTS\nTransformer (TimeCHEAT), which utilizes the CD locally and the CI globally.\nSpecifically, we segment the ISMTS into sub-series level patches. Locally, the\nCD strategy aggregates information within each patch for time embedding\nlearning, maximizing the use of relevant observations while reducing long-range\nirrelevant interference. Here, we enhance generality by transforming embedding\nlearning into an edge weight prediction task using bipartite graphs,\neliminating the need for special prior knowledge. Globally, the CI strategy is\napplied across patches, allowing the Transformer to learn individualized\nattention patterns for each channel. Experimental results indicate our proposed\nTimeCHEAT demonstrates competitive SOTA performance across three mainstream\ntasks.\n","authors":["Jiexi Liu","Meng Cao","Songcan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12886v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12883v1","updated":"2024-12-17T13:07:31Z","published":"2024-12-17T13:07:31Z","title":"A Comparative Study of Pruning Methods in Transformer-based Time Series\n  Forecasting","summary":"  The current landscape in time-series forecasting is dominated by\nTransformer-based models. Their high parameter count and corresponding demand\nin computational resources pose a challenge to real-world deployment,\nespecially for commercial and scientific applications with low-power embedded\ndevices. Pruning is an established approach to reduce neural network parameter\ncount and save compute. However, the implications and benefits of pruning\nTransformer-based models for time series forecasting are largely unknown. To\nclose this gap, we provide a comparative benchmark study by evaluating\nunstructured and structured pruning on various state-of-the-art multivariate\ntime series models. We study the effects of these pruning strategies on model\npredictive performance and computational aspects like model size, operations,\nand inference time. Our results show that certain models can be pruned even up\nto high sparsity levels, outperforming their dense counterpart. However,\nfine-tuning pruned models is necessary. Furthermore, we demonstrate that even\nwith corresponding hardware and software support, structured pruning is unable\nto provide significant time savings.\n","authors":["Nicholas Kiefer","Arvid Weyrauch","Muhammed Öz","Achim Streit","Markus Götz","Charlotte Debus"],"pdf_url":"https://arxiv.org/pdf/2412.12883v1.pdf","comment":"16 pages, 5 figures, submitted to ACM Transactions on Intelligent\n  Systems and Technology"},{"id":"http://arxiv.org/abs/2412.12880v1","updated":"2024-12-17T13:04:38Z","published":"2024-12-17T13:04:38Z","title":"Towards Effective Graph Rationalization via Boosting Environment\n  Diversity","summary":"  Graph Neural Networks (GNNs) perform effectively when training and testing\ngraphs are drawn from the same distribution, but struggle to generalize well in\nthe face of distribution shifts. To address this issue, existing mainstreaming\ngraph rationalization methods first identify rationale and environment\nsubgraphs from input graphs, and then diversify training distributions by\naugmenting the environment subgraphs. However, these methods merely combine the\nlearned rationale subgraphs with environment subgraphs in the representation\nspace to produce augmentation samples, failing to produce sufficiently diverse\ndistributions. Thus, in this paper, we propose to achieve an effective Graph\nRationalization by Boosting Environmental diversity, a GRBE approach that\ngenerates the augmented samples in the original graph space to improve the\ndiversity of the environment subgraph. Firstly, to ensure the effectiveness of\naugmentation samples, we propose a precise rationale subgraph extraction\nstrategy in GRBE to refine the rationale subgraph learning process in the\noriginal graph space. Secondly, to ensure the diversity of augmented samples,\nwe propose an environment diversity augmentation strategy in GRBE that mixes\nthe environment subgraphs of different graphs in the original graph space and\nthen combines the new environment subgraphs with rationale subgraphs to\ngenerate augmented graphs. The average improvements of 7.65% and 6.11% in\nrationalization and classification performance on benchmark datasets\ndemonstrate the superiority of GRBE over state-of-the-art approaches.\n","authors":["Yujie Wang","Kui Yu","Yuhong Zhang","Fuyuan Cao","Jiye Liang"],"pdf_url":"https://arxiv.org/pdf/2412.12880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12870v1","updated":"2024-12-17T12:51:24Z","published":"2024-12-17T12:51:24Z","title":"Towards Physically Interpretable World Models: Meaningful Weakly\n  Supervised Representations for Visual Trajectory Prediction","summary":"  Deep learning models are increasingly employed for perception, prediction,\nand control in complex systems. Embedding physical knowledge into these models\nis crucial for achieving realistic and consistent outputs, a challenge often\naddressed by physics-informed machine learning. However, integrating physical\nknowledge with representation learning becomes difficult when dealing with\nhigh-dimensional observation data, such as images, particularly under\nconditions of incomplete or imprecise state information. To address this, we\npropose Physically Interpretable World Models, a novel architecture that aligns\nlearned latent representations with real-world physical quantities. Our method\ncombines a variational autoencoder with a dynamical model that incorporates\nunknown system parameters, enabling the discovery of physically meaningful\nrepresentations. By employing weak supervision with interval-based constraints,\nour approach eliminates the reliance on ground-truth physical annotations.\nExperimental results demonstrate that our method improves the quality of\nlearned representations while achieving accurate predictions of future states,\nadvancing the field of representation learning in dynamic systems.\n","authors":["Zhenjiang Mao","Ivan Ruchkin"],"pdf_url":"https://arxiv.org/pdf/2412.12870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12864v1","updated":"2024-12-17T12:47:53Z","published":"2024-12-17T12:47:53Z","title":"Geodesic Flow Kernels for Semi-Supervised Learning on Mixed-Variable\n  Tabular Dataset","summary":"  Tabular data poses unique challenges due to its heterogeneous nature,\ncombining both continuous and categorical variables. Existing approaches often\nstruggle to effectively capture the underlying structure and relationships\nwithin such data. We propose GFTab (Geodesic Flow Kernels for Semi- Supervised\nLearning on Mixed-Variable Tabular Dataset), a semi-supervised framework\nspecifically designed for tabular datasets. GFTab incorporates three key\ninnovations: 1) Variable-specific corruption methods tailored to the distinct\nproperties of continuous and categorical variables, 2) A Geodesic flow kernel\nbased similarity measure to capture geometric changes between corrupted inputs,\nand 3) Tree-based embedding to leverage hierarchical relationships from\navailable labeled data. To rigorously evaluate GFTab, we curate a comprehensive\nset of 21 tabular datasets spanning various domains, sizes, and variable\ncompositions. Our experimental results show that GFTab outperforms existing\nML/DL models across many of these datasets, particularly in settings with\nlimited labeled data.\n","authors":["Yoontae Hwang","Yongjae Lee"],"pdf_url":"https://arxiv.org/pdf/2412.12864v1.pdf","comment":"AAAI-25"},{"id":"http://arxiv.org/abs/2402.15061v2","updated":"2024-12-17T12:45:20Z","published":"2024-02-23T02:24:15Z","title":"Fine-tuning Large Language Models for Domain-specific Machine\n  Translation","summary":"  Large language models (LLMs) have shown great potential in domain-specific\nmachine translation (MT). However, one major issue is that LLMs pre-trained on\ngeneral domain corpus might not generalize well to specific domains due to the\nlack of domain-specific knowledge. To address this issue, this paper focuses on\nenhancing the domain-specific MT capability of LLMs, by providing high-quality\ntraining datasets and proposing a novel fine-tuning framework denoted by\nDragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced\nprompting integrates dictionary information into prompts to improve the\ntranslation of domain-specific terminology.; (ii) RAG-based few-shot example\nselection provides high-quality examples that simulate both the domain and\nstyle characteristics; (iii) Fine-tuning with few-shot examples further\nenhances performance when using in-domain examples. We deploy DragFT on three\nwell-known LLM backbones with 13B training parameters to validate its\neffectiveness. The results on three domain-specific datasets show that DragFT\nachieves a significant performance boost and shows superior performance\ncompared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance\nimprovement of DragFT over existing LLMs can be attributed to incorporating\nrelevant knowledge while mitigating noise.\n","authors":["Jiawei Zheng","Hanghai Hong","Feiyan Liu","Xiaoli Wang","Jingsong Su","Yonggui Liang","Shikai Wu"],"pdf_url":"https://arxiv.org/pdf/2402.15061v2.pdf","comment":"13 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2412.12858v1","updated":"2024-12-17T12:38:45Z","published":"2024-12-17T12:38:45Z","title":"Efficient Speech Command Recognition Leveraging Spiking Neural Network\n  and Curriculum Learning-based Knowledge Distillation","summary":"  The intrinsic dynamics and event-driven nature of spiking neural networks\n(SNNs) make them excel in processing temporal information by naturally\nutilizing embedded time sequences as time steps. Recent studies adopting this\napproach have demonstrated SNNs' effectiveness in speech command recognition,\nachieving high performance by employing large time steps for long time\nsequences. However, the large time steps lead to increased deployment burdens\nfor edge computing applications. Thus, it is important to balance high\nperformance and low energy consumption when detecting temporal patterns in edge\ndevices. Our solution comprises two key components. 1). We propose a\nhigh-performance fully spike-driven framework termed SpikeSCR, characterized by\na global-local hybrid structure for efficient representation learning, which\nexhibits long-term learning capabilities with extended time steps. 2). To\nfurther fully embrace low energy consumption, we propose an effective knowledge\ndistillation method based on curriculum learning (KDCL), where valuable\nrepresentations learned from the easy curriculum are progressively transferred\nto the hard curriculum with minor loss, striking a trade-off between power\nefficiency and high performance. We evaluate our method on three benchmark\ndatasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands\n(SSC), and the Google Speech Commands (GSC) V2. Our experimental results\ndemonstrate that SpikeSCR outperforms current state-of-the-art (SOTA) methods\nacross these three datasets with the same time steps. Furthermore, by executing\nKDCL, we reduce the number of time steps by 60% and decrease energy consumption\nby 54.8% while maintaining comparable performance to recent SOTA results.\nTherefore, this work offers valuable insights for tackling temporal processing\nchallenges with long time sequences in edge neuromorphic computing systems.\n","authors":["Jiaqi Wang","Liutao Yu","Liwei Huang","Chenlin Zhou","Han Zhang","Zhenxi Song","Min Zhang","Zhengyu Ma","Zhiguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12858v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.12850v1","updated":"2024-12-17T12:24:08Z","published":"2024-12-17T12:24:08Z","title":"Boosting Fine-Grained Visual Anomaly Detection with\n  Coarse-Knowledge-Aware Adversarial Learning","summary":"  Many unsupervised visual anomaly detection methods train an auto-encoder to\nreconstruct normal samples and then leverage the reconstruction error map to\ndetect and localize the anomalies. However, due to the powerful modeling and\ngeneralization ability of neural networks, some anomalies can also be well\nreconstructed, resulting in unsatisfactory detection and localization accuracy.\nIn this paper, a small coarsely-labeled anomaly dataset is first collected.\nThen, a coarse-knowledge-aware adversarial learning method is developed to\nalign the distribution of reconstructed features with that of normal features.\nThe alignment can effectively suppress the auto-encoder's reconstruction\nability on anomalies and thus improve the detection accuracy. Considering that\nanomalies often only occupy very small areas in anomalous images, a patch-level\nadversarial learning strategy is further developed. Although no patch-level\nanomalous information is available, we rigorously prove that by simply viewing\nany patch features from anomalous images as anomalies, the proposed\nknowledge-aware method can also align the distribution of reconstructed patch\nfeatures with the normal ones. Experimental results on four medical datasets\nand two industrial datasets demonstrate the effectiveness of our method in\nimproving the detection and localization performance.\n","authors":["Qingqing Fang","Qinliang Su","Wenxi Lv","Wenchao Xu","Jianxing Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12850v1.pdf","comment":"The paper is accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12844v1","updated":"2024-12-17T12:11:14Z","published":"2024-12-17T12:11:14Z","title":"Concurrent vertical and horizontal federated learning with fuzzy\n  cognitive maps","summary":"  Data privacy is a major concern in industries such as healthcare or finance.\nThe requirement to safeguard privacy is essential to prevent data breaches and\nmisuse, which can have severe consequences for individuals and organisations.\nFederated learning is a distributed machine learning approach where multiple\nparticipants collaboratively train a model without compromising the privacy of\ntheir data. However, a significant challenge arises from the differences in\nfeature spaces among participants, known as non-IID data. This research\nintroduces a novel federated learning framework employing fuzzy cognitive maps,\ndesigned to comprehensively address the challenges posed by diverse data\ndistributions and non-identically distributed features in federated settings.\nThe proposal is tested through several experiments using four distinct\nfederation strategies: constant-based, accuracy-based, AUC-based, and\nprecision-based weights. The results demonstrate the effectiveness of the\napproach in achieving the desired learning outcomes while maintaining privacy\nand confidentiality standards.\n","authors":["Jose L Salmeron","Irina Arévalo"],"pdf_url":"https://arxiv.org/pdf/2412.12844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12841v1","updated":"2024-12-17T12:10:38Z","published":"2024-12-17T12:10:38Z","title":"Benchmarking and Understanding Compositional Relational Reasoning of\n  LLMs","summary":"  Compositional relational reasoning (CRR) is a hallmark of human intelligence,\nbut we lack a clear understanding of whether and how existing transformer large\nlanguage models (LLMs) can solve CRR tasks. To enable systematic exploration of\nthe CRR capability of LLMs, we first propose a new synthetic benchmark called\nGeneralized Associative Recall (GAR) by integrating and generalizing the\nessence of several tasks in mechanistic interpretability (MI) study in a\nunified framework. Evaluation shows that GAR is challenging enough for existing\nLLMs, revealing their fundamental deficiency in CRR. Meanwhile, it is easy\nenough for systematic MI study. Then, to understand how LLMs solve GAR tasks,\nwe use attribution patching to discover the core circuits reused by Vicuna-33B\nacross different tasks and a set of vital attention heads. Intervention\nexperiments show that the correct functioning of these heads significantly\nimpacts task performance. Especially, we identify two classes of heads whose\nactivations represent the abstract notion of true and false in GAR tasks\nrespectively. They play a fundamental role in CRR across various models and\ntasks. The dataset and code are available at https://github.com/Caiyun-AI/GAR.\n","authors":["Ruikang Ni","Da Xiao","Qingye Meng","Xiangyu Li","Shihui Zheng","Hongliang Liang"],"pdf_url":"https://arxiv.org/pdf/2412.12841v1.pdf","comment":"Accepted to the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.11764v2","updated":"2024-12-17T12:04:49Z","published":"2024-12-16T13:31:26Z","title":"What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study","summary":"  Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines, and achieves 70% improvement\nover the traditional MPC. The policy derived by SimpleFlight consistently\nexcels across both smooth polynominal trajectories and challenging infeasible\nzigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline\nmethods struggle with high-speed or infeasible trajectories. To support further\nresearch and reproducibility, we integrate SimpleFlight into a GPU-based\nsimulator Omnidrones and provide open-source access to the code and model\ncheckpoints. We hope SimpleFlight will offer valuable insights for advancing\nRL-based quadrotor control. For more details, visit our project website at\nhttps://sites.google.com/view/simpleflight/.\n","authors":["Jiayu Chen","Chao Yu","Yuqing Xie","Feng Gao","Yinuo Chen","Shu'ang Yu","Wenhao Tang","Shilong Ji","Mo Mu","Yi Wu","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11764v2.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2412.00395v2","updated":"2024-12-17T12:04:32Z","published":"2024-11-30T08:34:10Z","title":"On Foundation Models for Dynamical Systems from Purely Synthetic Data","summary":"  Foundation models have demonstrated remarkable generalization, data\nefficiency, and robustness properties across various domains. In this paper, we\nexplore the feasibility of foundation models for applications in the control\ndomain. The success of these models is enabled by large-scale pretaining on\nInternet-scale datasets. These are available in fields like natural language\nprocessing and computer vision, but do not exist for dynamical systems. We\naddress this challenge by pretraining a transformer-based foundation model\nexclusively on synthetic data and propose to sample dynamics functions from a\nreproducing kernel Hilbert space. Our pretrained model generalizes for\nprediction tasks across different dynamical systems, which we validate in\nsimulation and hardware experiments, including cart-pole and Furuta pendulum\nsetups. Additionally, the model can be fine-tuned effectively to new systems to\nincrease performance even further. Our results demonstrate the feasibility of\nfoundation models for dynamical systems that outperform specialist models in\nterms of generalization, data efficiency, and robustness.\n","authors":["Martin Ziegler","Andres Felipe Posada-Moreno","Friedrich Solowjow","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2412.00395v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2412.12837v1","updated":"2024-12-17T12:02:47Z","published":"2024-12-17T12:02:47Z","title":"Scrutinizing the Vulnerability of Decentralized Learning to Membership\n  Inference Attacks","summary":"  The primary promise of decentralized learning is to allow users to engage in\nthe training of machine learning models in a collaborative manner while keeping\ntheir data on their premises and without relying on any central entity.\nHowever, this paradigm necessitates the exchange of model parameters or\ngradients between peers. Such exchanges can be exploited to infer sensitive\ninformation about training data, which is achieved through privacy attacks (e.g\nMembership Inference Attacks -- MIA). In order to devise effective defense\nmechanisms, it is important to understand the factors that increase/reduce the\nvulnerability of a given decentralized learning architecture to MIA. In this\nstudy, we extensively explore the vulnerability to MIA of various decentralized\nlearning architectures by varying the graph structure (e.g number of\nneighbors), the graph dynamics, and the aggregation strategy, across diverse\ndatasets and data distributions. Our key finding, which to the best of our\nknowledge we are the first to report, is that the vulnerability to MIA is\nheavily correlated to (i) the local model mixing strategy performed by each\nnode upon reception of models from neighboring nodes and (ii) the global mixing\nproperties of the communication graph. We illustrate these results\nexperimentally using four datasets and by theoretically analyzing the mixing\nproperties of various decentralized architectures. Our paper draws a set of\nlessons learned for devising decentralized learning systems that reduce by\ndesign the vulnerability to MIA.\n","authors":["Ousmane Touat","Jezekael Brunon","Yacine Belal","Julien Nicolas","Mohamed Maouche","César Sabater","Sonia Ben Mokhtar"],"pdf_url":"https://arxiv.org/pdf/2412.12837v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.04468v2","updated":"2024-12-17T11:51:51Z","published":"2024-10-06T12:50:15Z","title":"Revisiting In-context Learning Inference Circuit in Large Language\n  Models","summary":"  In-context Learning (ICL) is an emerging few-shot learning paradigm on\nLanguage Models (LMs) with inner mechanisms un-explored. There are already\nexisting works describing the inner processing of ICL, while they struggle to\ncapture all the inference phenomena in large language models. Therefore, this\npaper proposes a comprehensive circuit to model the inference dynamics and try\nto explain the observed phenomena of ICL. In detail, we divide ICL inference\ninto 3 major operations: (1) Input Text Encode: LMs encode every input text\n(demonstrations and queries) into linear representation in the hidden states\nwith sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge\nthe encoded representations of demonstrations with their corresponding label\ntokens to produce joint representations of labels and demonstrations. (3)\nFeature Retrieval and Copy: LMs search the joint representations similar to the\nquery representation on a task subspace, and copy the searched representations\ninto the query. Then, language model heads capture these copied label\nrepresentations to a certain extent and decode them into predicted labels. The\nproposed inference circuit successfully captured many phenomena observed during\nthe ICL process, making it a comprehensive and practical explanation of the ICL\ninference process. Moreover, ablation analysis by disabling the proposed steps\nseriously damages the ICL performance, suggesting the proposed inference\ncircuit is a dominating mechanism. Additionally, we confirm and list some\nbypass mechanisms that solve ICL tasks in parallel with the proposed circuit.\n","authors":["Hakaze Cho","Mariko Kato","Yoshihiro Sakai","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2410.04468v2.pdf","comment":"37 pages, 41 figures, 8 tables"},{"id":"http://arxiv.org/abs/2412.10459v2","updated":"2024-12-17T11:35:02Z","published":"2024-12-12T10:45:02Z","title":"Conformal Prediction on Quantifying Uncertainty of Dynamic Systems","summary":"  Numerous studies have focused on learning and understanding the dynamics of\nphysical systems from video data, such as spatial intelligence. Artificial\nintelligence requires quantitative assessments of the uncertainty of the model\nto ensure reliability. However, there is still a relative lack of systematic\nassessment of the uncertainties, particularly the uncertainties of the physical\ndata. Our motivation is to introduce conformal prediction into the uncertainty\nassessment of dynamical systems, providing a method supported by theoretical\nguarantees. This paper uses the conformal prediction method to assess\nuncertainties with benchmark operator learning methods. We have also compared\nthe Monte Carlo Dropout and Ensemble methods in the partial differential\nequations dataset, effectively evaluating uncertainty through straight\nroll-outs, making it ideal for time-series tasks.\n","authors":["Aoming Liang","Qi Liu","Lei Xu","Fahad Sohrab","Weicheng Cui","Changhui Song","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2412.10459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02629v2","updated":"2024-12-17T11:28:49Z","published":"2024-02-04T22:45:20Z","title":"PROSAC: Provably Safe Certification for Machine Learning Models under\n  Adversarial Attacks","summary":"  It is widely known that state-of-the-art machine learning models, including\nvision and language models, can be seriously compromised by adversarial\nperturbations. It is therefore increasingly relevant to develop capabilities to\ncertify their performance in the presence of the most effective adversarial\nattacks. Our paper offers a new approach to certify the performance of machine\nlearning models in the presence of adversarial attacks with population level\nrisk guarantees. In particular, we introduce the notion of\n$(\\alpha,\\zeta)$-safe machine learning model. We propose a hypothesis testing\nprocedure, based on the availability of a calibration set, to derive\nstatistical guarantees providing that the probability of declaring that the\nadversarial (population) risk of a machine learning model is less than $\\alpha$\n(i.e. the model is safe), while the model is in fact unsafe (i.e. the model\nadversarial population risk is higher than $\\alpha$), is less than $\\zeta$. We\nalso propose Bayesian optimization algorithms to determine efficiently whether\na machine learning model is $(\\alpha,\\zeta)$-safe in the presence of an\nadversarial attack, along with statistical guarantees. We apply our framework\nto a range of machine learning models - including various sizes of vision\nTransformer (ViT) and ResNet models - impaired by a variety of adversarial\nattacks, such as PGDAttack, MomentumAttack, GenAttack and BanditAttack, to\nillustrate the operation of our approach. Importantly, we show that ViT's are\ngenerally more robust to adversarial attacks than ResNets, and large models are\ngenerally more robust than smaller models. Our approach goes beyond existing\nempirical adversarial risk-based certification guarantees. It formulates\nrigorous (and provable) performance guarantees that can be used to satisfy\nregulatory requirements mandating the use of state-of-the-art technical tools.\n","authors":["Chen Feng","Ziquan Liu","Zhuo Zhi","Ilija Bogunovic","Carsten Gerner-Beuerle","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2402.02629v2.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2412.12807v1","updated":"2024-12-17T11:25:51Z","published":"2024-12-17T11:25:51Z","title":"Ask for More Than Bayes Optimal: A Theory of Indecisions for\n  Classification","summary":"  Selective classification frameworks are useful tools for automated decision\nmaking in highly risky scenarios, since they allow for a classifier to only\nmake highly confident decisions, while abstaining from making a decision when\nit is not confident enough to do so, which is otherwise known as an indecision.\nFor a given level of classification accuracy, we aim to make as many decisions\nas possible. For many problems, this can be achieved without abstaining from\nmaking decisions. But when the problem is hard enough, we show that we can\nstill control the misclassification rate of a classifier up to any user\nspecified level, while only abstaining from the minimum necessary amount of\ndecisions, even if this level of misclassification is smaller than the Bayes\noptimal error rate. In many problem settings, the user could obtain a dramatic\ndecrease in misclassification while only paying a comparatively small price in\nterms of indecisions.\n","authors":["Mohamed Ndaoud","Peter Radchenko","Bradley Rava"],"pdf_url":"https://arxiv.org/pdf/2412.12807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11239v2","updated":"2024-12-17T11:14:52Z","published":"2024-12-15T16:42:09Z","title":"Learning Set Functions with Implicit Differentiation","summary":"  Ou et al. (2022) introduce the problem of learning set functions from data\ngenerated by a so-called optimal subset oracle. Their approach approximates the\nunderlying utility function with an energy-based model, whose parameters are\nestimated via mean-field variational inference. Ou et al. (2022) show this\nreduces to fixed point iterations; however, as the number of iterations\nincreases, automatic differentiation quickly becomes computationally\nprohibitive due to the size of the Jacobians that are stacked during\nbackpropagation. We address this challenge with implicit differentiation and\nexamine the convergence conditions for the fixed-point iterations. We\nempirically demonstrate the efficiency of our method on synthetic and\nreal-world subset selection applications including product recommendation, set\nanomaly detection and compound selection tasks.\n","authors":["Gözde Özcan","Chengzhi Shi","Stratis Ioannidis"],"pdf_url":"https://arxiv.org/pdf/2412.11239v2.pdf","comment":"19 pages, 1 figure, extended version of the AAAI 2025 paper with the\n  same title"},{"id":"http://arxiv.org/abs/2412.12801v1","updated":"2024-12-17T11:10:46Z","published":"2024-12-17T11:10:46Z","title":"Multi-View Incremental Learning with Structured Hebbian Plasticity for\n  Enhanced Fusion Efficiency","summary":"  The rapid evolution of multimedia technology has revolutionized human\nperception, paving the way for multi-view learning. However, traditional\nmulti-view learning approaches are tailored for scenarios with fixed data\nviews, falling short of emulating the intricate cognitive procedures of the\nhuman brain processing signals sequentially. Our cerebral architecture\nseamlessly integrates sequential data through intricate feed-forward and\nfeedback mechanisms. In stark contrast, traditional methods struggle to\ngeneralize effectively when confronted with data spanning diverse domains,\nhighlighting the need for innovative strategies that can mimic the brain's\nadaptability and dynamic integration capabilities. In this paper, we propose a\nbio-neurologically inspired multi-view incremental framework named MVIL aimed\nat emulating the brain's fine-grained fusion of sequentially arriving views.\nMVIL lies two fundamental modules: structured Hebbian plasticity and synaptic\npartition learning. The structured Hebbian plasticity reshapes the structure of\nweights to express the high correlation between view representations,\nfacilitating a fine-grained fusion of view representations. Moreover, synaptic\npartition learning is efficient in alleviating drastic changes in weights and\nalso retaining old knowledge by inhibiting partial synapses. These modules\nbionically play a central role in reinforcing crucial associations between\nnewly acquired information and existing knowledge repositories, thereby\nenhancing the network's capacity for generalization. Experimental results on\nsix benchmark datasets show MVIL's effectiveness over state-of-the-art methods.\n","authors":["Yuhong Chen","Ailin Song","Huifeng Yin","Shuai Zhong","Fuhai Chen","Qi Xu","Shiping Wang","Mingkun Xu"],"pdf_url":"https://arxiv.org/pdf/2412.12801v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.12783v1","updated":"2024-12-17T10:43:26Z","published":"2024-12-17T10:43:26Z","title":"Noise-based Local Learning using Stochastic Magnetic Tunnel Junctions","summary":"  Brain-inspired learning in physical hardware has enormous potential to learn\nfast at minimal energy expenditure. One of the characteristics of biological\nlearning systems is their ability to learn in the presence of various noise\nsources. Inspired by this observation, we introduce a novel noise-based\nlearning approach for physical systems implementing multi-layer neural\nnetworks. Simulation results show that our approach allows for effective\nlearning whose performance approaches that of the conventional effective yet\nenergy-costly backpropagation algorithm. Using a spintronics hardware\nimplementation, we demonstrate experimentally that learning can be achieved in\na small network composed of physical stochastic magnetic tunnel junctions.\nThese results provide a path towards efficient learning in general physical\nsystems which embraces rather than mitigates the noise inherent in physical\ndevices.\n","authors":["Kees Koenders","Leo Schnitzpan","Fabian Kammerbauer","Sinan Shu","Gerhard Jakob","Mathis Kläui","Johan Mentink","Nasir Ahmad","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2412.12783v1.pdf","comment":"20 pages, 5 figures, submitted to Physical Review X"},{"id":"http://arxiv.org/abs/2412.12781v1","updated":"2024-12-17T10:41:19Z","published":"2024-12-17T10:41:19Z","title":"Predicting change in time production -- A machine learning approach to\n  time perception","summary":"  Time perception research has advanced significantly over the years. However,\nsome areas remain largely unexplored. This study addresses two such\nunder-explored areas in timing research: (1) A quantitative analysis of time\nperception at an individual level, and (2) Time perception in an ecological\nsetting. In this context, we trained a machine learning model to predict the\ndirection of change in an individual's time production. The model's training\ndata was collected using an ecologically valid setup. We moved closer to an\necological setting by conducting an online experiment with 995 participants\nperforming a time production task that used naturalistic videos (no audio) as\nstimuli. The model achieved an accuracy of 61%. This was 10 percentage points\nhigher than the baseline models derived from cognitive theories of timing. The\nmodel performed equally well on new data from a second experiment, providing\nevidence of its generalization capabilities. The model's output analysis\nrevealed that it also contained information about the magnitude of change in\ntime production. The predictions were further analysed at both population and\nindividual level. It was found that a participant's previous timing performance\nplayed a significant role in determining the direction of change in time\nproduction. By integrating attentional-gate theories from timing research with\nfeature importance techniques from machine learning, we explained model\npredictions using cognitive theories of timing. The model and findings from\nthis study have potential applications in systems involving human-computer\ninteractions where understanding and predicting changes in user's time\nperception can enable better user experience and task performance.\n","authors":["Amrapali Pednekar","Alvaro Garrido","Yara Khaluf","Pieter Simoens"],"pdf_url":"https://arxiv.org/pdf/2412.12781v1.pdf","comment":"Main text contains 16 pages and 9 figure. Supplementary information\n  is included as appendix. The paper has been submitted to IEEE TRANSACTIONS ON\n  COGNITIVE AND DEVELOPMENTAL SYSTEMS (TCDS). The code and data associated with\n  the study will be made publicly available upon acceptance"},{"id":"http://arxiv.org/abs/2306.00541v3","updated":"2024-12-17T10:31:14Z","published":"2023-06-01T10:51:12Z","title":"Decomposing Global Feature Effects Based on Feature Interactions","summary":"  Global feature effect methods, such as partial dependence plots, provide an\nintelligible visualization of the expected marginal feature effect. However,\nsuch global feature effect methods can be misleading, as they do not represent\nlocal feature effects of single observations well when feature interactions are\npresent. We formally introduce generalized additive decomposition of global\neffects (GADGET), which is a new framework based on recursive partitioning to\nfind interpretable regions in the feature space such that the\ninteraction-related heterogeneity of local feature effects is minimized. We\nprovide a mathematical foundation of the framework and show that it is\napplicable to the most popular methods to visualize marginal feature effects,\nnamely partial dependence, accumulated local effects, and Shapley additive\nexplanations (SHAP) dependence. Furthermore, we introduce and validate a new\npermutation-based interaction detection procedure that is applicable to any\nfeature effect method that fits into our proposed framework. We empirically\nevaluate the theoretical characteristics of the proposed methods based on\nvarious feature effect methods in different experimental settings. Moreover, we\napply our introduced methodology to three real-world examples to showcase their\nusefulness.\n","authors":["Julia Herbinger","Marvin N. Wright","Thomas Nagler","Bernd Bischl","Giuseppe Casalicchio"],"pdf_url":"https://arxiv.org/pdf/2306.00541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12759v1","updated":"2024-12-17T10:24:29Z","published":"2024-12-17T10:24:29Z","title":"Versatile Ordering Network: An Attention-based Neural Network for\n  Ordering Across Scales and Quality Metrics","summary":"  Ordering has been extensively studied in many visualization applications,\nsuch as axis and matrix reordering, for the simple reason that the order will\ngreatly impact the perceived pattern of data. Many quality metrics concerning\ndata pattern, perception, and aesthetics are proposed, and respective\noptimization algorithms are developed. However, the optimization problems\nrelated to ordering are often difficult to solve (e.g., TSP is NP-complete),\nand developing specialized optimization algorithms is costly. In this paper, we\npropose Versatile Ordering Network (VON), which automatically learns the\nstrategy to order given a quality metric. VON uses the quality metric to\nevaluate its solutions, and leverages reinforcement learning with a greedy\nrollout baseline to improve itself. This keeps the metric transparent and\nallows VON to optimize over different metrics. Additionally, VON uses the\nattention mechanism to collect information across scales and reposition the\ndata points with respect to the current context. This allows VONs to deal with\ndata points following different distributions. We examine the effectiveness of\nVON under different usage scenarios and metrics. The results demonstrate that\nVON can produce comparable results to specialized solvers. The code is\navailable at https://github.com/sysuvis/VON.\n","authors":["Zehua Yu","Weihan Zhang","Sihan Pan","Jun Tao"],"pdf_url":"https://arxiv.org/pdf/2412.12759v1.pdf","comment":"has been accepted by TVCG on 11-Dec-2024"},{"id":"http://arxiv.org/abs/2412.12755v1","updated":"2024-12-17T10:20:29Z","published":"2024-12-17T10:20:29Z","title":"Progressive Monitoring of Generative Model Training Evolution","summary":"  While deep generative models (DGMs) have gained popularity, their\nsusceptibility to biases and other inefficiencies that lead to undesirable\noutcomes remains an issue. With their growing complexity, there is a critical\nneed for early detection of issues to achieve desired results and optimize\nresources. Hence, we introduce a progressive analysis framework to monitor the\ntraining process of DGMs. Our method utilizes dimensionality reduction\ntechniques to facilitate the inspection of latent representations, the\ngenerated and real distributions, and their evolution across training\niterations. This monitoring allows us to pause and fix the training method if\nthe representations or distributions progress undesirably. This approach allows\nfor the analysis of a models' training dynamics and the timely identification\nof biases and failures, minimizing computational loads. We demonstrate how our\nmethod supports identifying and mitigating biases early in training a\nGenerative Adversarial Network (GAN) and improving the quality of the generated\ndata distribution.\n","authors":["Vidya Prasad","Anna Vilanova","Nicola Pezzotti"],"pdf_url":"https://arxiv.org/pdf/2412.12755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12744v1","updated":"2024-12-17T10:08:57Z","published":"2024-12-17T10:08:57Z","title":"Your Next State-of-the-Art Could Come from Another Domain: A\n  Cross-Domain Analysis of Hierarchical Text Classification","summary":"  Text classification with hierarchical labels is a prevalent and challenging\ntask in natural language processing. Examples include assigning ICD codes to\npatient records, tagging patents into IPC classes, assigning EUROVOC\ndescriptors to European legal texts, and more. Despite its widespread\napplications, a comprehensive understanding of state-of-the-art methods across\ndifferent domains has been lacking. In this paper, we provide the first\ncomprehensive cross-domain overview with empirical analysis of state-of-the-art\nmethods. We propose a unified framework that positions each method within a\ncommon structure to facilitate research. Our empirical analysis yields key\ninsights and guidelines, confirming the necessity of learning across different\nresearch areas to design effective methods. Notably, under our unified\nevaluation pipeline, we achieved new state-of-the-art results by applying\ntechniques beyond their original domains.\n","authors":["Nan Li","Bo Kang","Tijl De Bie"],"pdf_url":"https://arxiv.org/pdf/2412.12744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11304v2","updated":"2024-12-17T10:07:46Z","published":"2024-12-15T20:47:03Z","title":"An Empirical Study of Fault Localisation Techniques for Deep Learning","summary":"  With the increased popularity of Deep Neural Networks (DNNs), increases also\nthe need for tools to assist developers in the DNN implementation, testing and\ndebugging process. Several approaches have been proposed that automatically\nanalyse and localise potential faults in DNNs under test. In this work, we\nevaluate and compare existing state-of-the-art fault localisation techniques,\nwhich operate based on both dynamic and static analysis of the DNN. The\nevaluation is performed on a benchmark consisting of both real faults obtained\nfrom bug reporting platforms and faulty models produced by a mutation tool. Our\nfindings indicate that the usage of a single, specific ground truth (e.g., the\nhuman defined one) for the evaluation of DNN fault localisation tools results\nin pretty low performance (maximum average recall of 0.31 and precision of\n0.23). However, such figures increase when considering alternative, equivalent\npatches that exist for a given faulty DNN. Results indicate that \\dfd is the\nmost effective tool, achieving an average recall of 0.61 and precision of 0.41\non our benchmark.\n","authors":["Nargiz Humbatova","Jinhan Kim","Gunel Jahangirova","Shin Yoo","Paolo Tonella"],"pdf_url":"https://arxiv.org/pdf/2412.11304v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12743v1","updated":"2024-12-17T10:06:42Z","published":"2024-12-17T10:06:42Z","title":"Training a Distributed Acoustic Sensing Traffic Monitoring Network With\n  Video Inputs","summary":"  Distributed Acoustic Sensing (DAS) has emerged as a promising tool for\nreal-time traffic monitoring in densely populated areas. In this paper, we\npresent a novel concept that integrates DAS data with co-located visual\ninformation. We use YOLO-derived vehicle location and classification from\ncamera inputs as labeled data to train a detection and classification neural\nnetwork utilizing DAS data only. Our model achieves a performance exceeding 94%\nfor detection and classification, and about 1.2% false alarm rate. We\nillustrate the model's application in monitoring traffic over a week, yielding\nstatistical insights that could benefit future smart city developments. Our\napproach highlights the potential of combining fiber-optic sensors with visual\ninformation, focusing on practicality and scalability, protecting privacy, and\nminimizing infrastructure costs. To encourage future research, we share our\ndataset.\n","authors":["Khen Cohen","Liav Hen","Ariel Lellouch"],"pdf_url":"https://arxiv.org/pdf/2412.12743v1.pdf","comment":"12 pages, 11 figures, 5 appendices. Shared dataset in:\n  https://zenodo.org/records/14502092"},{"id":"http://arxiv.org/abs/2412.12742v1","updated":"2024-12-17T10:06:37Z","published":"2024-12-17T10:06:37Z","title":"Subspace Implicit Neural Representations for Real-Time Cardiac Cine MR\n  Imaging","summary":"  Conventional cardiac cine MRI methods rely on retrospective gating, which\nlimits temporal resolution and the ability to capture continuous cardiac\ndynamics, particularly in patients with arrhythmias and beat-to-beat\nvariations. To address these challenges, we propose a reconstruction framework\nbased on subspace implicit neural representations for real-time cardiac cine\nMRI of continuously sampled radial data. This approach employs two multilayer\nperceptrons to learn spatial and temporal subspace bases, leveraging the\nlow-rank properties of cardiac cine MRI. Initialized with low-resolution\nreconstructions, the networks are fine-tuned using spoke-specific loss\nfunctions to recover spatial details and temporal fidelity. Our method directly\nutilizes the continuously sampled radial k-space spokes during training,\nthereby eliminating the need for binning and non-uniform FFT. This approach\nachieves superior spatial and temporal image quality compared to conventional\nbinned methods at the acceleration rate of 10 and 20, demonstrating potential\nfor high-resolution imaging of dynamic cardiac events and enhancing diagnostic\ncapability.\n","authors":["Wenqi Huang","Veronika Spieker","Siying Xu","Gastao Cruz","Claudia Prieto","Julia Schnabel","Kerstin Hammernik","Thomas Kuestner","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2412.12742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12739v1","updated":"2024-12-17T10:02:04Z","published":"2024-12-17T10:02:04Z","title":"Deep Learning for Resilient Adversarial Decision Fusion in Byzantine\n  Networks","summary":"  This paper introduces a deep learning-based framework for resilient decision\nfusion in adversarial multi-sensor networks, providing a unified mathematical\nsetup that encompasses diverse scenarios, including varying Byzantine node\nproportions, synchronized and unsynchronized attacks, unbalanced priors,\nadaptive strategies, and Markovian states. Unlike traditional methods, which\ndepend on explicit parameter tuning and are limited by scenario-specific\nassumptions, the proposed approach employs a deep neural network trained on a\nglobally constructed dataset to generalize across all cases without requiring\nadaptation. Extensive simulations validate the method's robustness, achieving\nsuperior accuracy, minimal error probability, and scalability compared to\nstate-of-the-art techniques, while ensuring computational efficiency for\nreal-time applications. This unified framework demonstrates the potential of\ndeep learning to revolutionize decision fusion by addressing the challenges\nposed by Byzantine nodes in dynamic adversarial environments.\n","authors":["Kassem Kallas"],"pdf_url":"https://arxiv.org/pdf/2412.12739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15628v2","updated":"2024-12-17T10:01:59Z","published":"2024-10-21T04:24:10Z","title":"Towards Kriging-informed Conditional Diffusion for Regional Sea-Level\n  Data Downscaling","summary":"  Given coarser-resolution projections from global climate models or satellite\ndata, the downscaling problem aims to estimate finer-resolution regional\nclimate data, capturing fine-scale spatial patterns and variability.\nDownscaling is any method to derive high-resolution data from low-resolution\nvariables, often to provide more detailed and local predictions and analyses.\nThis problem is societally crucial for effective adaptation, mitigation, and\nresilience against significant risks from climate change. The challenge arises\nfrom spatial heterogeneity and the need to recover finer-scale features while\nensuring model generalization. Most downscaling methods \\cite{Li2020} fail to\ncapture the spatial dependencies at finer scales and underperform on real-world\nclimate datasets, such as sea-level rise. We propose a novel Kriging-informed\nConditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial\nvariability while preserving fine-scale features. Experimental results on\nclimate data show that our proposed method is more accurate than\nstate-of-the-art downscaling techniques.\n","authors":["Subhankar Ghosh","Arun Sharma","Jayant Gupta","Aneesh Subramanian","Shashi Shekhar"],"pdf_url":"https://arxiv.org/pdf/2410.15628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13362v2","updated":"2024-12-17T09:46:19Z","published":"2024-06-19T09:07:31Z","title":"VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models","summary":"  Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.\n","authors":["Haowen Hou","Peigen Zeng","Fei Ma","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2406.13362v2.pdf","comment":"Accepted at COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2411.08599v2","updated":"2024-12-17T09:45:45Z","published":"2024-11-13T13:30:21Z","title":"XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL","summary":"  To tackle the challenges of large language model performance in natural\nlanguage to SQL tasks, we introduce XiYan-SQL, an innovative framework that\nemploys a multi-generator ensemble strategy to improve candidate generation. We\nintroduce M-Schema, a semi-structured schema representation method designed to\nenhance the understanding of database structures. To enhance the quality and\ndiversity of generated candidate SQL queries, XiYan-SQL integrates the\nsignificant potential of in-context learning (ICL) with the precise control of\nsupervised fine-tuning. On one hand, we propose a series of training strategies\nto fine-tune models to generate high-quality candidates with diverse\npreferences. On the other hand, we implement the ICL approach with an example\nselection method based on named entity recognition to prevent overemphasis on\nentities. The refiner optimizes each candidate by correcting logical or\nsyntactical errors. To address the challenge of identifying the best candidate,\nwe fine-tune a selection model to distinguish nuances of candidate SQL queries.\nThe experimental results on multiple dialect datasets demonstrate the\nrobustness of XiYan-SQL in addressing challenges across different scenarios.\nOverall, our proposed XiYan-SQL achieves the state-of-the-art execution\naccuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on\nSQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the\nquality and diversity of SQL queries but also outperforms previous methods.\n","authors":["Yingqi Gao","Yifu Liu","Xiaoxia Li","Xiaorong Shi","Yin Zhu","Yiming Wang","Shiqi Li","Wei Li","Yuntao Hong","Zhiling Luo","Jinyang Gao","Liyu Mou","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2411.08599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14166v3","updated":"2024-12-17T09:38:53Z","published":"2024-11-21T14:23:06Z","title":"SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized\n  Bilevel Optimization","summary":"  This paper studies decentralized bilevel optimization, in which multiple\nagents collaborate to solve problems involving nested optimization structures\nwith neighborhood communications. Most existing literature primarily utilizes\ngradient tracking to mitigate the influence of data heterogeneity, without\nexploring other well-known heterogeneity-correction techniques such as EXTRA or\nExact Diffusion. Additionally, these studies often employ identical\ndecentralized strategies for both upper- and lower-level problems, neglecting\nto leverage distinct mechanisms across different levels. To address these\nlimitations, this paper proposes SPARKLE, a unified Single-loop Primal-dual\nAlgoRithm frameworK for decentraLized bilEvel optimization. SPARKLE offers the\nflexibility to incorporate various heterogeneitycorrection strategies into the\nalgorithm. Moreover, SPARKLE allows for different strategies to solve upper-\nand lower-level problems. We present a unified convergence analysis for\nSPARKLE, applicable to all its variants, with state-of-the-art convergence\nrates compared to existing decentralized bilevel algorithms. Our results\nfurther reveal that EXTRA and Exact Diffusion are more suitable for\ndecentralized bilevel optimization, and using mixed strategies in bilevel\nalgorithms brings more benefits than relying solely on gradient tracking.\n","authors":["Shuchen Zhu","Boao Kong","Songtao Lu","Xinmeng Huang","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.14166v3.pdf","comment":"74 pages, the Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (2024)"},{"id":"http://arxiv.org/abs/2412.11768v2","updated":"2024-12-17T09:30:44Z","published":"2024-12-16T13:41:37Z","title":"No More Adam: Learning Rate Scaling at Initialization is All You Need","summary":"  In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings.\n","authors":["Minghao Xu","Lichuan Xiang","Xu Cai","Hongkai Wen"],"pdf_url":"https://arxiv.org/pdf/2412.11768v2.pdf","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.12709v1","updated":"2024-12-17T09:23:46Z","published":"2024-12-17T09:23:46Z","title":"Accelerating lensed quasars discovery and modeling with physics-informed\n  variational autoencoders","summary":"  Strongly lensed quasars provide valuable insights into the rate of cosmic\nexpansion, the distribution of dark matter in foreground deflectors, and the\ncharacteristics of quasar hosts. However, detecting them in astronomical images\nis difficult due to the prevalence of non-lensing objects. To address this\nchallenge, we developed a generative deep learning model called VariLens, built\nupon a physics-informed variational autoencoder. This model seamlessly\nintegrates three essential modules: image reconstruction, object\nclassification, and lens modeling, offering a fast and comprehensive approach\nto strong lens analysis. VariLens is capable of rapidly determining both (1)\nthe probability that an object is a lens system and (2) key parameters of a\nsingular isothermal ellipsoid (SIE) mass model -- including the Einstein radius\n($\\theta_\\mathrm{E}$), lens center, and ellipticity -- in just milliseconds\nusing a single CPU. A direct comparison of VariLens estimates with traditional\nlens modeling for 20 known lensed quasars within the Subaru Hyper Suprime-Cam\n(HSC) footprint shows good agreement, with both results consistent within\n$2\\sigma$ for systems with $\\theta_\\mathrm{E}<3$ arcsecs. To identify new\nlensed quasar candidates, we begin with an initial sample of approximately 80\nmillion sources, combining HSC data with multiwavelength information from\nvarious surveys. After applying a photometric preselection aimed at locating\n$z>1.5$ sources, the number of candidates is reduced to 710,966. Subsequently,\nVariLens highlights 13,831 sources, each showing a high likelihood of being a\nlens. A visual assessment of these objects results in 42 promising candidates\nthat await spectroscopic confirmation. These results underscore the potential\nof automated deep learning pipelines to efficiently detect and model strong\nlenses in large datasets.\n","authors":["Irham T. Andika","Stefan Schuldt","Sherry H. Suyu","Satadru Bag","Raoul Cañameras","Alejandra Melo","Claudio Grillo","James H. H. Chan"],"pdf_url":"https://arxiv.org/pdf/2412.12709v1.pdf","comment":"Submitted to the Astronomy & Astrophysics journal. The paper consists\n  of 17 main pages, 14 figures, and 5 tables. We welcome feedback and comments\n  from readers!"},{"id":"http://arxiv.org/abs/2408.09429v2","updated":"2024-12-17T09:19:46Z","published":"2024-08-18T10:07:02Z","title":"Reefknot: A Comprehensive Benchmark for Relation Hallucination\n  Evaluation, Analysis and Mitigation in Multimodal Large Language Models","summary":"  Hallucination issues continue to affect multimodal large language models\n(MLLMs), with existing research mainly addressing object-level or\nattribute-level hallucinations, neglecting the more complex relation\nhallucinations that require advanced reasoning. Current benchmarks for relation\nhallucinations lack detailed evaluation and effective mitigation, and their\ndatasets often suffer from biases due to systematic annotation processes. To\naddress these challenges, we introduce Reefknot, a comprehensive benchmark\ntargeting relation hallucinations, comprising over 20,000 real-world samples.\nWe provide a systematic definition of relation hallucinations, integrating\nperceptive and cognitive perspectives, and construct a relation-based corpus\nusing the Visual Genome scene graph dataset. Our comparative evaluation reveals\nsignificant limitations in current MLLMs' ability to handle relation\nhallucinations. Additionally, we propose a novel confidence-based mitigation\nstrategy, which reduces the hallucination rate by an average of 9.75% across\nthree datasets, including Reefknot. Our work offers valuable insights for\nachieving trustworthy multimodal intelligence.\n","authors":["Kening Zheng","Junkai Chen","Yibo Yan","Xin Zou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2408.09429v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12700v1","updated":"2024-12-17T09:16:53Z","published":"2024-12-17T09:16:53Z","title":"ParMod: A Parallel and Modular Framework for Learning Non-Markovian\n  Tasks","summary":"  The commonly used Reinforcement Learning (RL) model, MDPs (Markov Decision\nProcesses), has a basic premise that rewards depend on the current state and\naction only. However, many real-world tasks are non-Markovian, which has\nlong-term memory and dependency. The reward sparseness problem is further\namplified in non-Markovian scenarios. Hence learning a non-Markovian task (NMT)\nis inherently more difficult than learning a Markovian one. In this paper, we\npropose a novel \\textbf{Par}allel and \\textbf{Mod}ular RL framework, ParMod,\nspecifically for learning NMTs specified by temporal logic. With the aid of\nformal techniques, the NMT is modulaized into a series of sub-tasks based on\nthe automaton structure (equivalent to its temporal logic counterpart). On this\nbasis, sub-tasks will be trained by a group of agents in a parallel fashion,\nwith one agent handling one sub-task. Besides parallel training, the core of\nParMod lies in: a flexible classification method for modularizing the NMT, and\nan effective reward shaping method for improving the sample efficiency. A\ncomprehensive evaluation is conducted on several challenging benchmark problems\nwith respect to various metrics. The experimental results show that ParMod\nachieves superior performance over other relevant studies. Our work thus\nprovides a good synergy among RL, NMT and temporal logic.\n","authors":["Ruixuan Miao","Xu Lu","Cong Tian","Bin Yu","Zhenhua Duan"],"pdf_url":"https://arxiv.org/pdf/2412.12700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01426v2","updated":"2024-12-17T09:09:40Z","published":"2024-10-02T11:23:09Z","title":"Approximation by Steklov Neural Network Operators","summary":"  The present paper deals with construction of newly family of Neural Network\noperators, that is, Steklov Neural Network operators. By using Steklov type\nintegral, we introduce a new version of Neural Network operators and we obtain\nsome convergence theorems for the family, such as, pointwise and uniform\nconvergence, rate of convergence via modulus of continuity.\n","authors":["S. N. Karaman","M. Turgay","T. Acar"],"pdf_url":"https://arxiv.org/pdf/2410.01426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12687v1","updated":"2024-12-17T09:08:18Z","published":"2024-12-17T09:08:18Z","title":"Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models","summary":"  This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware HLM (U-HLM), wherein the SLM\nlocally measures its output uncertainty, and skips both uplink transmissions\nand LLM operations for tokens that are likely to be accepted. This\nopportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputation by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping.\n","authors":["Seungeun Oh","Jinhyuk Kim","Jihong Park","Seung-Woo Ko","Tony Q. S. Quek","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12687v1.pdf","comment":"6 pages, 6 figures; This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2412.11434v2","updated":"2024-12-17T08:56:42Z","published":"2024-12-16T04:21:35Z","title":"Auto-bidding in real-time auctions via Oracle Imitation Learning (OIL)","summary":"  Online advertising has become one of the most successful business models of\nthe internet era. Impression opportunities are typically allocated through\nreal-time auctions, where advertisers bid to secure advertisement slots.\nDeciding the best bid for an impression opportunity is challenging, due to the\nstochastic nature of user behavior and the variability of advertisement traffic\nover time. In this work, we propose a framework for training auto-bidding\nagents in multi-slot second-price auctions to maximize acquisitions (e.g.,\nclicks, conversions) while adhering to budget and cost-per-acquisition (CPA)\nconstraints. We exploit the insight that, after an advertisement campaign\nconcludes, determining the optimal bids for each impression opportunity can be\nframed as a multiple-choice knapsack problem (MCKP) with a nonlinear objective.\nWe propose an \"oracle\" algorithm that identifies a near-optimal combination of\nimpression opportunities and advertisement slots, considering both past and\nfuture advertisement traffic data. This oracle solution serves as a training\ntarget for a student network which bids having access only to real-time\ninformation, a method we term Oracle Imitation Learning (OIL). Through\nnumerical experiments, we demonstrate that OIL achieves superior performance\ncompared to both online and offline reinforcement learning algorithms, offering\nimproved sample efficiency. Notably, OIL shifts the complexity of training\nauto-bidding agents from crafting sophisticated learning algorithms to solving\na nonlinear constrained optimization problem efficiently.\n","authors":["Alberto Silvio Chiappa","Briti Gangopadhyay","Zhao Wang","Shingo Takamatsu"],"pdf_url":"https://arxiv.org/pdf/2412.11434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12681v1","updated":"2024-12-17T08:51:55Z","published":"2024-12-17T08:51:55Z","title":"Everyday AR through AI-in-the-Loop","summary":"  This workshop brings together experts and practitioners from augmented\nreality (AR) and artificial intelligence (AI) to shape the future of\nAI-in-the-loop everyday AR experiences. With recent advancements in both AR\nhardware and AI capabilities, we envision that everyday AR -- always-available\nand seamlessly integrated into users' daily environments -- is becoming\nincreasingly feasible. This workshop will explore how AI can drive such\neveryday AR experiences. We discuss a range of topics, including adaptive and\ncontext-aware AR, generative AR content creation, always-on AI assistants,\nAI-driven accessible design, and real-world-oriented AI agents. Our goal is to\nidentify the opportunities and challenges in AI-enabled AR, focusing on\ncreating novel AR experiences that seamlessly blend the digital and physical\nworlds. Through the workshop, we aim to foster collaboration, inspire future\nresearch, and build a community to advance the research field of AI-enhanced\nAR.\n","authors":["Ryo Suzuki","Mar Gonzalez-Franco","Misha Sra","David Lindlbauer"],"pdf_url":"https://arxiv.org/pdf/2412.12681v1.pdf","comment":"CHI 2025 Extended Abstract"},{"id":"http://arxiv.org/abs/2412.12667v1","updated":"2024-12-17T08:36:47Z","published":"2024-12-17T08:36:47Z","title":"A Two-Fold Patch Selection Approach for Improved 360-Degree Image\n  Quality Assessment","summary":"  This article presents a novel approach to improving the accuracy of\n360-degree perceptual image quality assessment (IQA) through a two-fold patch\nselection process. Our methodology combines visual patch selection with\nembedding similarity-based refinement. The first stage focuses on selecting\npatches from 360-degree images using three distinct sampling methods to ensure\ncomprehensive coverage of visual content for IQA. The second stage, which is\nthe core of our approach, employs an embedding similarity-based selection\nprocess to filter and prioritize the most informative patches based on their\nembeddings similarity distances. This dual selection mechanism ensures that the\ntraining data is both relevant and informative, enhancing the model's learning\nefficiency. Extensive experiments and statistical analyses using three distance\nmetrics across three benchmark datasets validate the effectiveness of our\nselection algorithm. The results highlight its potential to deliver robust and\naccurate 360-degree IQA, with performance gains of up to 4.5% in accuracy and\nmonotonicity of quality score prediction, while using only 40% to 50% of the\ntraining patches. These improvements are consistent across various\nconfigurations and evaluation metrics, demonstrating the strength of the\nproposed method. The code for the selection process is available at:\nhttps://github.com/sendjasni/patch-selection-360-image-quality.\n","authors":["Abderrezzaq Sendjasni","Seif-Eddine Benkabou","Mohamed-Chaker Larabi"],"pdf_url":"https://arxiv.org/pdf/2412.12667v1.pdf","comment":"Submitted to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2412.12651v1","updated":"2024-12-17T08:20:02Z","published":"2024-12-17T08:20:02Z","title":"Shared Attention-based Autoencoder with Hierarchical Fusion-based Graph\n  Convolution Network for sEEG SOZ Identification","summary":"  Diagnosing seizure onset zone (SOZ) is a challenge in neurosurgery, where\nstereoelectroencephalography (sEEG) serves as a critical technique. In sEEG SOZ\nidentification, the existing studies focus solely on the intra-patient\nrepresentation of epileptic information, overlooking the general features of\nepilepsy across patients and feature interdependencies between feature elements\nin each contact site. In order to address the aforementioned challenges, we\npropose the shared attention-based autoencoder (sATAE). sATAE is trained by\nsEEG data across all patients, with attention blocks introduced to enhance the\nrepresentation of interdependencies between feature elements. Considering the\nspatial diversity of sEEG across patients, we introduce graph-based method for\nidentification SOZ of each patient. However, the current graph-based methods\nfor sEEG SOZ identification rely exclusively on static graphs to model\nepileptic networks. Inspired by the finding of neuroscience that epileptic\nnetwork is intricately characterized by the interplay of sophisticated\nequilibrium between fluctuating and stable states, we design the hierarchical\nfusion-based graph convolution network (HFGCN) to identify the SOZ. HFGCN\nintegrates the dynamic and static characteristics of epileptic networks through\nhierarchical weighting across different hierarchies, facilitating a more\ncomprehensive learning of epileptic features and enriching node information for\nsEEG SOZ identification. Combining sATAE and HFGCN, we perform comprehensive\nexperiments with sATAE-HFGCN on the self-build sEEG dataset, which includes\nsEEG data from 17 patients with temporal lobe epilepsy. The results show that\nour method, sATAE-HFGCN, achieves superior performance for identifying the SOZ\nof each patient, effectively addressing the aforementioned challenges,\nproviding an efficient solution for sEEG-based SOZ identification.\n","authors":["Huachao Yan","Kailing Guo","Shiwei Song","Yihai Dai","Xiaoqiang Wei","Xiaofen Xing","Xiangmin Xu"],"pdf_url":"https://arxiv.org/pdf/2412.12651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12650v1","updated":"2024-12-17T08:19:40Z","published":"2024-12-17T08:19:40Z","title":"Neural-Network-Driven Reward Prediction as a Heuristic: Advancing\n  Q-Learning for Mobile Robot Path Planning","summary":"  Q-learning is a widely used reinforcement learning technique for solving path\nplanning problems. It primarily involves the interaction between an agent and\nits environment, enabling the agent to learn an optimal strategy that maximizes\ncumulative rewards. Although many studies have reported the effectiveness of\nQ-learning, it still faces slow convergence issues in practical applications.\nTo address this issue, we propose the NDR-QL method, which utilizes neural\nnetwork outputs as heuristic information to accelerate the convergence process\nof Q-learning. Specifically, we improved the dual-output neural network model\nby introducing a start-end channel separation mechanism and enhancing the\nfeature fusion process. After training, the proposed NDR model can output a\nnarrowly focused optimal probability distribution, referred to as the\nguideline, and a broadly distributed suboptimal distribution, referred to as\nthe region. Subsequently, based on the guideline prediction, we calculate the\ncontinuous reward function for the Q-learning method, and based on the region\nprediction, we initialize the Q-table with a bias. We conducted training,\nvalidation, and path planning simulation experiments on public datasets. The\nresults indicate that the NDR model outperforms previous methods by up to 5\\%\nin prediction accuracy. Furthermore, the proposed NDR-QL method improves the\nconvergence speed of the baseline Q-learning method by 90\\% and also surpasses\nthe previously improved Q-learning methods in path quality metrics.\n","authors":["Yiming Ji","Kaijie Yun","Yang Liu","Zongwu Xie","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.07324v2","updated":"2024-12-17T08:17:26Z","published":"2021-12-14T12:19:24Z","title":"On the Impact of Hard Adversarial Instances on Overfitting in\n  Adversarial Training","summary":"  Adversarial training is a popular method to robustify models against\nadversarial attacks. However, it exhibits much more severe overfitting than\ntraining on clean inputs. In this work, we investigate this phenomenon from the\nperspective of training instances, i.e., training input-target pairs. Based on\na quantitative metric measuring the relative difficulty of an instance in the\ntraining set, we analyze the model's behavior on training instances of\ndifferent difficulty levels. This lets us demonstrate that the decay in\ngeneralization performance of adversarial training is a result of fitting hard\nadversarial instances. We theoretically verify our observations for both linear\nand general nonlinear models, proving that models trained on hard instances\nhave worse generalization performance than ones trained on easy instances, and\nthat this generalization gap increases with the size of the adversarial budget.\nFinally, we investigate solutions to mitigate adversarial overfitting in\nseveral scenarios, including fast adversarial training and fine-tuning a\npretrained model with additional data. Our results demonstrate that using\ntraining data adaptively improves the model's robustness.\n","authors":["Chen Liu","Zhichao Huang","Mathieu Salzmann","Tong Zhang","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2112.07324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12648v1","updated":"2024-12-17T08:14:12Z","published":"2024-12-17T08:14:12Z","title":"Exploring AI-Enabled Cybersecurity Frameworks: Deep-Learning Techniques,\n  GPU Support, and Future Enhancements","summary":"  Traditional rule-based cybersecurity systems have proven highly effective\nagainst known malware threats. However, they face challenges in detecting novel\nthreats. To address this issue, emerging cybersecurity systems are\nincorporating AI techniques, specifically deep-learning algorithms, to enhance\ntheir ability to detect incidents, analyze alerts, and respond to events. While\nthese techniques offer a promising approach to combating dynamic security\nthreats, they often require significant computational resources. Therefore,\nframeworks that incorporate AI-based cybersecurity mechanisms need to support\nthe use of GPUs to ensure optimal performance.\n  Many cybersecurity framework vendors do not provide sufficiently detailed\ninformation about their implementation, making it difficult to assess the\ntechniques employed and their effectiveness. This study aims to overcome this\nlimitation by providing an overview of the most used cybersecurity frameworks\nthat utilize AI techniques, specifically focusing on frameworks that provide\ncomprehensive information about their implementation. Our primary objective is\nto identify the deep-learning techniques employed by these frameworks and\nevaluate their support for GPU acceleration. We have identified a total of\n\\emph{two} deep-learning algorithms that are utilized by \\emph{three} out of 38\nselected cybersecurity frameworks. Our findings aim to assist in selecting\nopen-source cybersecurity frameworks for future research and assessing any\ndiscrepancies between deep-learning techniques used in theory and practice.\n","authors":["Tobias Becher","Simon Torka"],"pdf_url":"https://arxiv.org/pdf/2412.12648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12642v1","updated":"2024-12-17T08:06:00Z","published":"2024-12-17T08:06:00Z","title":"RDPI: A Refine Diffusion Probability Generation Method for\n  Spatiotemporal Data Imputation","summary":"  Spatiotemporal data imputation plays a crucial role in various fields such as\ntraffic flow monitoring, air quality assessment, and climate prediction.\nHowever, spatiotemporal data collected by sensors often suffer from temporal\nincompleteness, and the sparse and uneven distribution of sensors leads to\nmissing data in the spatial dimension. Among existing methods, autoregressive\napproaches are prone to error accumulation, while simple conditional diffusion\nmodels fail to adequately capture the spatiotemporal relationships between\nobserved and missing data. To address these issues, we propose a novel\ntwo-stage Refined Diffusion Probability Impuation (RDPI) framework based on an\ninitial network and a conditional diffusion model. In the initial stage,\ndeterministic imputation methods are used to generate preliminary estimates of\nthe missing data. In the refinement stage, residuals are treated as the\ndiffusion target, and observed values are innovatively incorporated into the\nforward process. This results in a conditional diffusion model better suited\nfor spatiotemporal data imputation, bridging the gap between the preliminary\nestimates and the true values. Experiments on multiple datasets demonstrate\nthat RDPI not only achieves state-of-the-art imputation accuracy but also\nsignificantly reduces sampling computational costs.\n","authors":["Zijin Liu","Xiang Zhao","You Song"],"pdf_url":"https://arxiv.org/pdf/2412.12642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12641v1","updated":"2024-12-17T08:03:53Z","published":"2024-12-17T08:03:53Z","title":"Lagrangian Index Policy for Restless Bandits with Average Reward","summary":"  We study the Lagrangian Index Policy (LIP) for restless multi-armed bandits\nwith long-run average reward. In particular, we compare the performance of LIP\nwith the performance of the Whittle Index Policy (WIP), both heuristic policies\nknown to be asymptotically optimal under certain natural conditions. Even\nthough in most cases their performances are very similar, in the cases when WIP\nshows bad performance, LIP continues to perform very well. We then propose\nreinforcement learning algorithms, both tabular and NN-based, to obtain online\nlearning schemes for LIP in the model-free setting. The proposed reinforcement\nlearning schemes for LIP requires significantly less memory than the analogous\nscheme for WIP. We calculate analytically the Lagrangian index for the restart\nmodel, which describes the optimal web crawling and the minimization of the\nweighted age of information. We also give a new proof of asymptotic optimality\nin case of homogeneous bandits as the number of arms goes to infinity, based on\nexchangeability and de Finetti's theorem.\n","authors":["Konstantin Avrachenkov","Vivek S. Borkar","Pratik Shah"],"pdf_url":"https://arxiv.org/pdf/2412.12641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12640v1","updated":"2024-12-17T08:03:38Z","published":"2024-12-17T08:03:38Z","title":"Building Gradient Bridges: Label Leakage from Restricted Gradient\n  Sharing in Federated Learning","summary":"  The growing concern over data privacy, the benefits of utilizing data from\ndiverse sources for model training, and the proliferation of networked devices\nwith enhanced computational capabilities have all contributed to the rise of\nfederated learning (FL). The clients in FL collaborate to train a global model\nby uploading gradients computed on their private datasets without collecting\nraw data. However, a new attack surface has emerged from gradient sharing,\nwhere adversaries can restore the label distribution of a victim's private data\nby analyzing the obtained gradients. To mitigate this privacy leakage, existing\nlightweight defenses restrict the sharing of gradients, such as encrypting the\nfinal-layer gradients or locally updating the parameters within. In this paper,\nwe introduce a novel attack called Gradient Bridge (GDBR) that recovers the\nlabel distribution of training data from the limited gradient information\nshared in FL. GDBR explores the relationship between the layer-wise gradients,\ntracks the flow of gradients, and analytically derives the batch training\nlabels. Extensive experiments show that GDBR can accurately recover more than\n80% of labels in various FL settings. GDBR highlights the inadequacy of\nrestricted gradient sharing-based defenses and calls for the design of\neffective defense schemes in FL.\n","authors":["Rui Zhang","Ka-Ho Chow","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2412.12640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18652v4","updated":"2024-12-17T08:03:10Z","published":"2024-10-24T11:32:00Z","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","summary":"  Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.\n","authors":["Woosung Koh","Jang Han Yoon","MinHyung Lee","Youngjin Song","Jaegwan Cho","Jaehyun Kang","Taehyeon Kim","Se-young Yun","Youngjae Yu","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18652v4.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.12636v1","updated":"2024-12-17T07:59:31Z","published":"2024-12-17T07:59:31Z","title":"TrainMover: Efficient ML Training Live Migration with No Memory Overhead","summary":"  Machine learning training has emerged as one of the most prominent workloads\nin modern data centers. These training jobs are large-scale, long-lasting, and\ntightly coupled, and are often disrupted by various events in the cluster such\nas failures, maintenance, and job scheduling. To handle these events, we rely\non cold migration, where we first checkpoint the entire cluster, replace the\nrelated machines, and then restart the training. This approach leads to\ndisruptions to the training jobs, resulting in significant downtime. In this\npaper, we present TrainMover, a live migration system that enables machine\nreplacement during machine learning training. TrainMover minimizes downtime by\nleveraging member replacement of collective communication groups and sandbox\nlazy initialization. Our evaluation demonstrates that TrainMover achieves 16x\nless downtime compared to all baselines, effectively handling data center\nevents like straggler rebalancing, maintenance, and unexpected failures.\n","authors":["ChonLam Lao","Minlan Yu","Aditya Akella","Jiamin Cao","Yu Guan","Pengcheng Zhang","Zhilong Zheng","Yichi Xu","Ennan Zhai","Dennis Cai","Jiaqi Gao"],"pdf_url":"https://arxiv.org/pdf/2412.12636v1.pdf","comment":"13 pages body, 19 pages total"},{"id":"http://arxiv.org/abs/2411.07574v2","updated":"2024-12-17T07:51:13Z","published":"2024-11-12T06:24:11Z","title":"Disentangling Tabular Data Towards Better One-Class Anomaly Detection","summary":"  Tabular anomaly detection under the one-class classification setting poses a\nsignificant challenge, as it involves accurately conceptualizing \"normal\"\nderived exclusively from a single category to discern anomalies from normal\ndata variations. Capturing the intrinsic correlation among attributes within\nnormal samples presents one promising method for learning the concept. To do\nso, the most recent effort relies on a learnable mask strategy with a\nreconstruction task. However, this wisdom may suffer from the risk of producing\nuniform masks, i.e., essentially nothing is masked, leading to less effective\ncorrelation learning. To address this issue, we presume that attributes related\nto others in normal samples can be divided into two non-overlapping and\ncorrelated subsets, defined as CorrSets, to capture the intrinsic correlation\neffectively. Accordingly, we introduce an innovative method that disentangles\nCorrSets from normal tabular data. To our knowledge, this is a pioneering\neffort to apply the concept of disentanglement for one-class anomaly detection\non tabular data. Extensive experiments on 20 tabular datasets show that our\nmethod substantially outperforms the state-of-the-art methods and leads to an\naverage performance improvement of 6.1% on AUC-PR and 2.1% on AUC-ROC. Codes\nare available at https://github.com/yjnanan/Disent-AD.\n","authors":["Jianan Ye","Zhaorui Tan","Yijie Hu","Xi Yang","Guangliang Cheng","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2411.07574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11024v2","updated":"2024-12-17T07:45:29Z","published":"2024-12-15T02:35:31Z","title":"Exploring Diffusion and Flow Matching Under Generator Matching","summary":"  In this paper, we present a comprehensive theoretical comparison of diffusion\nand flow matching under the Generator Matching framework. Despite their\napparent differences, both diffusion and flow matching can be viewed under the\nunified framework of Generator Matching. By recasting both diffusion and flow\nmatching under the same generative Markov framework, we provide theoretical\ninsights into why flow matching models can be more robust empirically and how\nnovel model classes can be constructed by mixing deterministic and stochastic\ncomponents. Our analysis offers a fresh perspective on the relationships\nbetween state-of-the-art generative modeling paradigms.\n","authors":["Zeeshan Patel","James DeLoye","Lance Mathias"],"pdf_url":"https://arxiv.org/pdf/2412.11024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04777v2","updated":"2024-12-17T07:44:36Z","published":"2024-06-07T09:21:06Z","title":"Modeling Temporal Dependencies within the Target for Long-Term Time\n  Series Forecasting","summary":"  Long-term time series forecasting (LTSF) is a critical task across diverse\ndomains. Despite significant advancements in LTSF research, we identify a\nperformance bottleneck in existing LTSF methods caused by the inadequate\nmodeling of Temporal Dependencies within the Target (TDT). To address this\nissue, we propose a novel and generic temporal modeling framework, Temporal\nDependency Alignment (TDAlign), that equips existing LTSF methods with TDT\nlearning capabilities. TDAlign introduces two key innovations: 1) a loss\nfunction that aligns the change values between adjacent time steps in the\npredictions with those in the target, ensuring consistency with variation\npatterns, and 2) an adaptive loss balancing strategy that seamlessly integrates\nthe new loss function with existing LTSF methods without introducing additional\nlearnable parameters. As a plug-and-play framework, TDAlign enhances existing\nmethods with minimal computational overhead, featuring only linear time\ncomplexity and constant space complexity relative to the prediction length.\nExtensive experiments on six strong LTSF baselines across seven real-world\ndatasets demonstrate the effectiveness and flexibility of TDAlign. On average,\nTDAlign reduces baseline prediction errors by \\textbf{1.47\\%} to\n\\textbf{9.19\\%} and change value errors by \\textbf{4.57\\%} to \\textbf{15.78\\%},\nhighlighting its substantial performance improvements.\n","authors":["Qi Xiong","Kai Tang","Minbo Ma","Ji Zhang","Jie Xu","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2406.04777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.14067v3","updated":"2024-12-17T07:39:42Z","published":"2022-04-29T13:04:36Z","title":"Accelerating nuclear-norm regularized low-rank matrix optimization\n  through Burer-Monteiro decomposition","summary":"  This work proposes a rapid algorithm, BM-Global, for nuclear-norm-regularized\nconvex and low-rank matrix optimization problems. BM-Global efficiently\ndecreases the objective value via low-cost steps leveraging the nonconvex but\nsmooth Burer-Monteiro (BM) decomposition, while effectively escapes saddle\npoints and spurious local minima ubiquitous in the BM form to obtain guarantees\nof fast convergence rates to the global optima of the original\nnuclear-norm-regularized problem through aperiodic inexact proximal gradient\nsteps on it. The proposed approach adaptively adjusts the rank for the BM\ndecomposition and can provably identify an optimal rank for the BM\ndecomposition problem automatically in the course of optimization through tools\nof manifold identification. BM-Global hence also spends significantly less time\non parameter tuning than existing matrix-factorization methods, which require\nan exhaustive search for finding this optimal rank. Extensive experiments on\nreal-world large-scale problems of recommendation systems, regularized kernel\nestimation, and molecular conformation confirm that BM-Global can indeed\neffectively escapes spurious local minima at which existing BM approaches are\nstuck, and is a magnitude faster than state-of-the-art algorithms for low-rank\nmatrix optimization problems involving a nuclear-norm regularizer. Based on\nthis research, we have released an open-source package of the proposed\nBM-Global at https://www.github.com/leepei/BM-Global/.\n","authors":["Ching-pei Lee","Ling Liang","Tianyun Tang","Kim-Chuan Toh"],"pdf_url":"https://arxiv.org/pdf/2204.14067v3.pdf","comment":"52 pages, including 15 pages of appendices"},{"id":"http://arxiv.org/abs/2408.05686v2","updated":"2024-12-17T07:36:47Z","published":"2024-08-11T03:39:46Z","title":"The Bandit Whisperer: Communication Learning for Restless Bandits","summary":"  Applying Reinforcement Learning (RL) to Restless Multi-Arm Bandits (RMABs)\noffers a promising avenue for addressing allocation problems with resource\nconstraints and temporal dynamics. However, classic RMAB models largely\noverlook the challenges of (systematic) data errors - a common occurrence in\nreal-world scenarios due to factors like varying data collection protocols and\nintentional noise for differential privacy. We demonstrate that conventional RL\nalgorithms used to train RMABs can struggle to perform well in such settings.\nTo solve this problem, we propose the first communication learning approach in\nRMABs, where we study which arms, when involved in communication, are most\neffective in mitigating the influence of such systematic data errors. In our\nsetup, the arms receive Q-function parameters from similar arms as messages to\nguide behavioral policies, steering Q-function updates. We learn communication\nstrategies by considering the joint utility of messages across all pairs of\narms and using a Q-network architecture that decomposes the joint utility. Both\ntheoretical and empirical evidence validate the effectiveness of our method in\nsignificantly improving RMAB performance across diverse problems.\n","authors":["Yunfan Zhao","Tonghan Wang","Dheeraj Nagaraj","Aparna Taneja","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2408.05686v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12622v1","updated":"2024-12-17T07:35:56Z","published":"2024-12-17T07:35:56Z","title":"Neighbor-Aware Reinforcement Learning for Mixed Traffic Optimization in\n  Large-scale Networks","summary":"  Managing mixed traffic comprising human-driven and robot vehicles (RVs)\nacross large-scale networks presents unique challenges beyond\nsingle-intersection control. This paper proposes a reinforcement learning\nframework for coordinating mixed traffic across multiple interconnected\nintersections. Our key contribution is a neighbor-aware reward mechanism that\nenables RVs to maintain balanced distribution across the network while\noptimizing local intersection efficiency. We evaluate our approach using a\nreal-world network, demonstrating its effectiveness in managing realistic\ntraffic patterns. Results show that our method reduces average waiting times by\n39.2% compared to the state-of-the-art single-intersection control policy and\n79.8% compared to traditional traffic signals. The framework's ability to\ncoordinate traffic across multiple intersections while maintaining balanced RV\ndistribution provides a foundation for deploying learning-based solutions in\nurban traffic systems.\n","authors":["Iftekharul Islam","Weizi Li"],"pdf_url":"https://arxiv.org/pdf/2412.12622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18580v3","updated":"2024-12-17T07:35:35Z","published":"2024-05-28T20:54:41Z","title":"Artificial Intelligence in Industry 4.0: A Review of Integration\n  Challenges for Industrial Systems","summary":"  In Industry 4.0, Cyber-Physical Systems (CPS) generate vast data sets that\ncan be leveraged by Artificial Intelligence (AI) for applications including\npredictive maintenance and production planning. However, despite the\ndemonstrated potential of AI, its widespread adoption in sectors like\nmanufacturing remains limited. Our comprehensive review of recent literature,\nincluding standards and reports, pinpoints key challenges: system integration,\ndata-related issues, managing workforce-related concerns and ensuring\ntrustworthy AI. A quantitative analysis highlights particular challenges and\ntopics that are important for practitioners but still need to be sufficiently\ninvestigated by academics. The paper briefly discusses existing solutions to\nthese challenges and proposes avenues for future research. We hope that this\nsurvey serves as a resource for practitioners evaluating the cost-benefit\nimplications of AI in CPS and for researchers aiming to address these urgent\nchallenges.\n","authors":["Alexander Windmann","Philipp Wittenberg","Marvin Schieseck","Oliver Niggemann"],"pdf_url":"https://arxiv.org/pdf/2405.18580v3.pdf","comment":"17 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.00478v3","updated":"2024-12-17T07:30:46Z","published":"2024-06-29T15:52:37Z","title":"Beyond Scaleup: Knowledge-aware Parsimony Learning from Deep Networks","summary":"  The brute-force scaleup of training datasets, learnable parameters and\ncomputation power, has become a prevalent strategy for developing more robust\nlearning models. However, due to bottlenecks in data, computation, and trust,\nthe sustainability of this strategy is a serious concern. In this paper, we\nattempt to address this issue in a parsimonious manner (i.e., achieving greater\npotential with simpler models). The key is to drive models using\ndomain-specific knowledge, such as symbols, logic, and formulas, instead of\npurely relying on scaleup. This approach allows us to build a framework that\nuses this knowledge as \"building blocks\" to achieve parsimony in model design,\ntraining, and interpretation. Empirical results show that our methods surpass\nthose that typically follow the scaling law. We also demonstrate our framework\nin AI for science, specifically in the problem of drug-drug interaction\nprediction. We hope our research can foster more diverse technical roadmaps in\nthe era of foundation models.\n","authors":["Quanming Yao","Yongqi Zhang","Yaqing Wang","Nan Yin","James Kwok","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.00478v3.pdf","comment":"Accepted to AI Magazine"},{"id":"http://arxiv.org/abs/2403.02780v3","updated":"2024-12-17T07:23:04Z","published":"2024-03-05T08:52:16Z","title":"Data Collaboration Analysis with Orthogonal Basis Alignment","summary":"  The Data Collaboration (DC) framework provides a privacy-preserving solution\nfor multi-source data fusion, enabling the joint analysis of data from multiple\nsources to achieve enhanced insights. It utilizes linear transformations with\nsecretly selected bases to ensure privacy guarantees through non-iterative\ncommunication. Despite its strengths, the DC framework often encounters\nperformance instability due to theoretical challenges in aligning the bases\nused for mapping raw data. This study addresses these challenges by\nestablishing a rigorous theoretical foundation for basis alignment within the\nDC framework, formulating it as an optimization problem over orthogonal\nmatrices. Under specific assumptions, we demonstrate that this problem can be\nreduced to the Orthogonal Procrustes Problem, which has a well-known analytical\nsolution. Extensive empirical evaluations across diverse datasets reveal that\nthe proposed alignment method significantly enhances model performance and\ncomputational efficiency, outperforming existing approaches. Additionally, it\ndemonstrates robustness across varying levels of differential privacy, thus\nenabling practical and reliable implementations of the DC framework.\n","authors":["Keiyu Nosaka","Yuichi Takano","Akiko Yoshise"],"pdf_url":"https://arxiv.org/pdf/2403.02780v3.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2412.12612v1","updated":"2024-12-17T07:21:25Z","published":"2024-12-17T07:21:25Z","title":"SynthCypher: A Fully Synthetic Data Generation Framework for\n  Text-to-Cypher Querying in Knowledge Graphs","summary":"  Cypher, the query language for Neo4j graph databases, plays a critical role\nin enabling graph-based analytics and data exploration. While substantial\nresearch has been dedicated to natural language to SQL query generation\n(Text2SQL), the analogous problem for graph databases referred to as\nText2Cypher remains underexplored. In this work, we introduce SynthCypher, a\nfully synthetic and automated data generation pipeline designed to address this\ngap. SynthCypher employs a novel LLMSupervised Generation-Verification\nframework, ensuring syntactically and semantically correct Cypher queries\nacross diverse domains and query complexities. Using this pipeline, we create\nSynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher\ninstances. Fine-tuning open-source large language models (LLMs), including\nLLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant\nperformance improvements of up to 40% on the Text2Cypher test set and 30% on\nthe SPIDER benchmark adapted for graph databases. This work demonstrates that\nhigh-quality synthetic data can effectively advance the state-of-the-art in\nText2Cypher tasks.\n","authors":["Aman Tiwari","Shiva Krishna Reddy Malay","Vikas Yadav","Masoud Hashemi","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2412.12612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00200v4","updated":"2024-12-17T07:15:24Z","published":"2023-04-01T02:07:08Z","title":"Diffusion map particle systems for generative modeling","summary":"  We propose a novel diffusion map particle system (DMPS) for generative\nmodeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient\ndescent (LAWGD). Diffusion maps are used to approximate the generator of the\ncorresponding Langevin diffusion process from samples, and hence to learn the\nunderlying data-generating manifold. On the other hand, LAWGD enables efficient\nsampling from the target distribution given a suitable choice of kernel, which\nwe construct here via a spectral approximation of the generator, computed with\ndiffusion maps. Our method requires no offline training and minimal tuning, and\ncan outperform other approaches on data sets of moderate dimension.\n","authors":["Fengyi Li","Youssef Marzouk"],"pdf_url":"https://arxiv.org/pdf/2304.00200v4.pdf","comment":"Accepted to Foundations of Data Science"},{"id":"http://arxiv.org/abs/2412.12605v1","updated":"2024-12-17T07:04:39Z","published":"2024-12-17T07:04:39Z","title":"An Advantage-based Optimization Method for Reinforcement Learning in\n  Large Action Space","summary":"  Reinforcement learning tasks in real-world scenarios often involve large,\nhigh-dimensional action spaces, leading to challenges such as convergence\ndifficulties, instability, and high computational complexity. It is widely\nacknowledged that traditional value-based reinforcement learning algorithms\nstruggle to address these issues effectively. A prevalent approach involves\ngenerating independent sub-actions within each dimension of the action space.\nHowever, this method introduces bias, hindering the learning of optimal\npolicies. In this paper, we propose an advantage-based optimization method and\nan algorithm named Advantage Branching Dueling Q-network (ABQ). ABQ\nincorporates a baseline mechanism to tune the action value of each dimension,\nleveraging the advantage relationship across different sub-actions. With this\napproach, the learned policy can be optimized for each dimension. Empirical\nresults demonstrate that ABQ outperforms BDQ, achieving 3%, 171%, and 84% more\ncumulative rewards in HalfCheetah, Ant, and Humanoid environments,\nrespectively. Furthermore, ABQ exhibits competitive performance when compared\nagainst two continuous action benchmark algorithms, DDPG and TD3.\n","authors":["Hai Lin","Cheng Huang","Zhihong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12597v1","updated":"2024-12-17T06:55:20Z","published":"2024-12-17T06:55:20Z","title":"Distribution-Free Uncertainty Quantification in Mechanical Ventilation\n  Treatment: A Conformal Deep Q-Learning Framework","summary":"  Mechanical Ventilation (MV) is a critical life-support intervention in\nintensive care units (ICUs). However, optimal ventilator settings are\nchallenging to determine because of the complexity of balancing\npatient-specific physiological needs with the risks of adverse outcomes that\nimpact morbidity, mortality, and healthcare costs. This study introduces\nConformalDQN, a novel distribution-free conformal deep Q-learning approach for\noptimizing mechanical ventilation in intensive care units. By integrating\nconformal prediction with deep reinforcement learning, our method provides\nreliable uncertainty quantification, addressing the challenges of Q-value\noverestimation and out-of-distribution actions in offline settings. We trained\nand evaluated our model using ICU patient records from the MIMIC-IV database.\nConformalDQN extends the Double DQN architecture with a conformal predictor and\nemploys a composite loss function that balances Q-learning with well-calibrated\nprobability estimation. This enables uncertainty-aware action selection,\nallowing the model to avoid potentially harmful actions in unfamiliar states\nand handle distribution shifts by being more conservative in\nout-of-distribution scenarios. Evaluation against baseline models, including\nphysician policies, policy constraint methods, and behavior cloning,\ndemonstrates that ConformalDQN consistently makes recommendations within\nclinically safe and relevant ranges, outperforming other methods by increasing\nthe 90-day survival rate. Notably, our approach provides an interpretable\nmeasure of confidence in its decisions, which is crucial for clinical adoption\nand potential human-in-the-loop implementations.\n","authors":["Niloufar Eghbali","Tuka Alhanai","Mohammad M. Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2412.12597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10154v4","updated":"2024-12-17T06:45:36Z","published":"2023-08-20T04:01:30Z","title":"Adaptive pruning-based Newton's method for distributed learning","summary":"  Newton's method leverages curvature information to boost performance, and\nthus outperforms first-order methods for distributed learning problems.\nHowever, Newton's method is not practical in large-scale and heterogeneous\nlearning environments, due to obstacles such as high computation and\ncommunication costs of the Hessian matrix, sub-model diversity, staleness of\ntraining, and data heterogeneity. To overcome these obstacles, this paper\npresents a novel and efficient algorithm named Distributed Adaptive Newton\nLearning (\\texttt{DANL}), which solves the drawbacks of Newton's method by\nusing a simple Hessian initialization and adaptive allocation of training\nregions. The algorithm exhibits remarkable convergence properties, which are\nrigorously examined under standard assumptions in stochastic optimization. The\ntheoretical analysis proves that \\texttt{DANL} attains a linear convergence\nrate while efficiently adapting to available resources and keeping high\nefficiency. Furthermore, \\texttt{DANL} shows notable independence from the\ncondition number of the problem and removes the necessity for complex parameter\ntuning. Experiments demonstrate that \\texttt{DANL} achieves linear convergence\nwith efficient communication and strong performance across different datasets.\n","authors":["Shuzhen Chen","Yuan Yuan","Youming Tao","Tianzhu Wang","Zhipeng Cai","Dongxiao Yu"],"pdf_url":"https://arxiv.org/pdf/2308.10154v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13425v2","updated":"2024-12-17T06:30:32Z","published":"2024-11-20T16:09:22Z","title":"WaterPark: A Robustness Assessment of Language Model Watermarking","summary":"  Various watermarking methods (``watermarkers'') have been proposed to\nidentify LLM-generated texts; yet, due to the lack of unified evaluation\nplatforms, many critical questions remain under-explored: i) What are the\nstrengths/limitations of various watermarkers, especially their attack\nrobustness? ii) How do various design choices impact their robustness? iii) How\nto optimally operate watermarkers in adversarial environments? To fill this\ngap, we systematize existing LLM watermarkers and watermark removal attacks,\nmapping out their design spaces. We then develop WaterPark, a unified platform\nthat integrates 10 state-of-the-art watermarkers and 12 representative attacks.\nMore importantly, by leveraging WaterPark, we conduct a comprehensive\nassessment of existing watermarkers, unveiling the impact of various design\nchoices on their attack robustness. We further explore the best practices to\noperate watermarkers in adversarial environments. We believe our study sheds\nlight on current LLM watermarking techniques while WaterPark serves as a\nvaluable testbed to facilitate future research.\n","authors":["Jiacheng Liang","Zian Wang","Lauren Hong","Shouling Ji","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13425v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2412.12572v1","updated":"2024-12-17T06:03:42Z","published":"2024-12-17T06:03:42Z","title":"License Plate Detection and Character Recognition Using Deep Learning\n  and Font Evaluation","summary":"  License plate detection (LPD) is essential for traffic management, vehicle\ntracking, and law enforcement but faces challenges like variable lighting and\ndiverse font types, impacting accuracy. Traditionally reliant on image\nprocessing and machine learning, the field is now shifting towards deep\nlearning for its robust performance in various conditions. Current methods,\nhowever, often require tailoring to specific regional datasets. This paper\nproposes a dual deep learning strategy using a Faster R-CNN for detection and a\nCNN-RNN model with Connectionist Temporal Classification (CTC) loss and a\nMobileNet V3 backbone for recognition. This approach aims to improve model\nperformance using datasets from Ontario, Quebec, California, and New York\nState, achieving a recall rate of 92% on the Centre for Pattern Recognition and\nMachine Intelligence (CENPARMI) dataset and 90% on the UFPR-ALPR dataset. It\nincludes a detailed error analysis to identify the causes of false positives.\nAdditionally, the research examines the role of font features in license plate\n(LP) recognition, analyzing fonts like Driver Gothic, Dreadnought, California\nClarendon, and Zurich Extra Condensed with the OpenALPR system. It discovers\nsignificant performance discrepancies influenced by font characteristics,\noffering insights for future LPD system enhancements.\n  Keywords: Deep Learning, License Plate, Font Evaluation\n","authors":["Zahra Ebrahimi Vargoorani","Ching Yee Suen"],"pdf_url":"https://arxiv.org/pdf/2412.12572v1.pdf","comment":"12 pages, 5 figures. This is the pre-Springer final accepted version.\n  The final version is published in Springer, Lecture Notes in Computer Science\n  (LNCS), Volume 14731, 2024. Springer Version of Record"},{"id":"http://arxiv.org/abs/2309.14780v5","updated":"2024-12-17T06:02:15Z","published":"2023-09-26T09:24:53Z","title":"Transferring climate change physical knowledge","summary":"  Precise and reliable climate projections are required for climate adaptation\nand mitigation, but Earth system models still exhibit great uncertainties.\nSeveral approaches have been developed to reduce the spread of climate\nprojections and feedbacks, yet those methods cannot capture the non-linear\ncomplexity inherent in the climate system. Using a Transfer Learning approach,\nwe show that Machine Learning can be used to optimally leverage and merge the\nknowledge gained from Earth system models simulations and historical\nobservations to reduce the spread of global surface air temperature fields\nprojected in the 21st century. We reach an uncertainty reduction of more than\n50% with respect to state-of-the-art approaches, while giving evidence that our\nnovel method provides improved regional temperature patterns together with\nnarrower projections uncertainty, urgently required for climate adaptation.\n","authors":["Francesco Immorlano","Veronika Eyring","Thomas le Monnier de Gouville","Gabriele Accarino","Donatello Elia","Stephan Mandt","Giovanni Aloisio","Pierre Gentine"],"pdf_url":"https://arxiv.org/pdf/2309.14780v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08585v3","updated":"2024-12-17T05:40:09Z","published":"2024-12-11T18:03:05Z","title":"TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs","summary":"  Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.\n","authors":["Hao Kang","Srikant Bharadwaj","James Hensman","Tushar Krishna","Victor Ruhle","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2412.08585v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06220v2","updated":"2024-12-17T05:39:48Z","published":"2023-12-11T09:10:38Z","title":"CSformer: Combining Channel Independence and Mixing for Robust\n  Multivariate Time Series Forecasting","summary":"  In the domain of multivariate time series analysis, the concept of channel\nindependence has been increasingly adopted, demonstrating excellent performance\ndue to its ability to eliminate noise and the influence of irrelevant\nvariables. However, such a concept often simplifies the complex interactions\namong channels, potentially leading to information loss. To address this\nchallenge, we propose a strategy of channel independence followed by mixing.\nBased on this strategy, we introduce CSformer, a novel framework featuring a\ntwo-stage multiheaded self-attention mechanism. This mechanism is designed to\nextract and integrate both channel-specific and sequence-specific information.\nDistinctively, CSformer employs parameter sharing to enhance the cooperative\neffects between these two types of information. Moreover, our framework\neffectively incorporates sequence and channel adapters, significantly improving\nthe model's ability to identify important information across various\ndimensions. Extensive experiments on several real-world datasets demonstrate\nthat CSformer achieves state-of-the-art results in terms of overall\nperformance.\n","authors":["Haoxin Wang","Yipeng Mo","Kunlan Xiang","Nan Yin","Honghe Dai","Bixiong Li","Songhai Fan","Site Mo"],"pdf_url":"https://arxiv.org/pdf/2312.06220v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2405.15328v2","updated":"2024-12-17T05:35:15Z","published":"2024-05-24T08:11:59Z","title":"Multi-Modal Recommendation Unlearning for Legal, Licensing, and Modality\n  Constraints","summary":"  User data spread across multiple modalities has popularized multi-modal\nrecommender systems (MMRS). They recommend diverse content such as products,\nsocial media posts, TikTok reels, etc., based on a user-item interaction graph.\nWith rising data privacy demands, recent methods propose unlearning private\nuser data from uni-modal recommender systems (RS). However, methods for\nunlearning item data related to outdated user preferences, revoked licenses,\nand legally requested removals are still largely unexplored.\n  Previous RS unlearning methods are unsuitable for MMRS due to the\nincompatibility of their matrix-based representation with the multi-modal\nuser-item interaction graph. Moreover, their data partitioning step degrades\nperformance on each shard due to poor data heterogeneity and requires costly\nperformance aggregation across shards.\n  This paper introduces MMRecUn, the first approach known to us for unlearning\nin MMRS and unlearning item data. Given a trained RS model, MMRecUn employs a\nnovel Reverse Bayesian Personalized Ranking (BPR) objective to enable the model\nto forget marked data. The reverse BPR attenuates the impact of user-item\ninteractions within the forget set, while the forward BPR reinforces the\nsignificance of user-item interactions within the retain set. Our experiments\ndemonstrate that MMRecUn outperforms baseline methods across various unlearning\nrequests when evaluated on benchmark MMRS datasets. MMRecUn achieves recall\nperformance improvements of up to 49.85% compared to baseline methods and is up\nto $\\mathbf{1.3}\\times$ faster than the Gold model, which is trained on retain\nset from scratch. MMRecUn offers significant advantages, including superiority\nin removing target interactions, preserving retained interactions, and zero\noverhead costs compared to previous methods. The code will be released after\nreview.\n","authors":["Yash Sinha","Murari Mandal","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2405.15328v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2405.05192v3","updated":"2024-12-17T05:27:17Z","published":"2024-05-08T16:30:45Z","title":"Full error analysis of the random deep splitting method for nonlinear\n  parabolic PDEs and PIDEs","summary":"  In this paper, we present a randomized extension of the deep splitting\nalgorithm introduced in [Beck, Becker, Cheridito, Jentzen, and Neufeld (2021)]\nusing random neural networks suitable to approximately solve both\nhigh-dimensional nonlinear parabolic PDEs and PIDEs with jumps having\n(possibly) infinite activity. We provide a full error analysis of our so-called\nrandom deep splitting method. In particular, we prove that our random deep\nsplitting method converges to the (unique viscosity) solution of the nonlinear\nPDE or PIDE under consideration. Moreover, we empirically analyze our random\ndeep splitting method by considering several numerical examples including both\nnonlinear PDEs and nonlinear PIDEs relevant in the context of pricing of\nfinancial derivatives under default risk. In particular, we empirically\ndemonstrate in all examples that our random deep splitting method can\napproximately solve nonlinear PDEs and PIDEs in 10'000 dimensions within\nseconds.\n","authors":["Ariel Neufeld","Philipp Schmocker","Sizhou Wu"],"pdf_url":"https://arxiv.org/pdf/2405.05192v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10891v2","updated":"2024-12-17T05:23:42Z","published":"2024-12-14T16:42:41Z","title":"Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via\n  Self-Reflection","summary":"  Diffusion models, the most popular generative paradigm so far, can inject\nconditional information into the generation path to guide the latent towards\ndesired directions. However, existing text-to-image diffusion models often fail\nto maintain high image quality and high prompt-image alignment for those\nchallenging prompts. To mitigate this issue and enhance existing pretrained\ndiffusion models, we mainly made three contributions in this paper. First, we\npropose diffusion self-reflection that alternately performs denoising and\ninversion and demonstrate that such diffusion self-reflection can leverage the\nguidance gap between denoising and inversion to capture prompt-related semantic\ninformation with theoretical and empirical evidence. Second, motivated by\ntheoretical analysis, we derive Zigzag Diffusion Sampling (Z-Sampling), a novel\nself-reflection-based diffusion sampling method that leverages the guidance gap\nbetween denosing and inversion to accumulate semantic information step by step\nalong the sampling path, leading to improved sampling results. Moreover, as a\nplug-and-play method, Z-Sampling can be generally applied to various diffusion\nmodels (e.g., accelerated ones and Transformer-based ones) with very limited\ncoding and computational costs. Third, our extensive experiments demonstrate\nthat Z-Sampling can generally and significantly enhance generation quality\nacross various benchmark datasets, diffusion models, and performance evaluation\nmetrics. For example, DreamShaper with Z-Sampling can self-improve with the\nHPSv2 winning rate up to 94% over the original results. Moreover, Z-Sampling\ncan further enhance existing diffusion models combined with other orthogonal\nmethods, including Diffusion-DPO.\n","authors":["Lichen Bai","Shitong Shao","Zikai Zhou","Zipeng Qi","Zhiqiang Xu","Haoyi Xiong","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2412.10891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12540v1","updated":"2024-12-17T05:07:10Z","published":"2024-12-17T05:07:10Z","title":"Stiefel Flow Matching for Moment-Constrained Structure Elucidation","summary":"  Molecular structure elucidation is a fundamental step in understanding\nchemical phenomena, with applications in identifying molecules in natural\nproducts, lab syntheses, forensic samples, and the interstellar medium. We\nconsider the task of predicting a molecule's all-atom 3D structure given only\nits molecular formula and moments of inertia, motivated by the ability of\nrotational spectroscopy to measure these moments. While existing generative\nmodels can conditionally sample 3D structures with approximately correct\nmoments, this soft conditioning fails to leverage the many digits of precision\nafforded by experimental rotational spectroscopy. To address this, we first\nshow that the space of $n$-atom point clouds with a fixed set of moments of\ninertia is embedded in the Stiefel manifold $\\mathrm{St}(n, 4)$. We then\npropose Stiefel Flow Matching as a generative model for elucidating 3D\nstructure under exact moment constraints. Additionally, we learn simpler and\nshorter flows by finding approximate solutions for equivariant optimal\ntransport on the Stiefel manifold. Empirically, enforcing exact moment\nconstraints allows Stiefel Flow Matching to achieve higher success rates and\nfaster sampling than Euclidean diffusion models, even on high-dimensional\nmanifolds corresponding to large molecules in the GEOM dataset.\n","authors":["Austin Cheng","Alston Lo","Kin Long Kelvin Lee","Santiago Miret","Alán Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2412.12540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17886v2","updated":"2024-12-17T04:59:06Z","published":"2024-11-26T21:16:36Z","title":"The Context of Crash Occurrence: A Complexity-Infused Approach\n  Integrating Semantic, Contextual, and Kinematic Features","summary":"  Understanding the context of crash occurrence in complex driving environments\nis essential for improving traffic safety and advancing automated driving.\nPrevious studies have used statistical models and deep learning to predict\ncrashes based on semantic, contextual, or vehicle kinematic features, but none\nhave examined the combined influence of these factors. In this study, we term\nthe integration of these features ``roadway complexity''. This paper introduces\na two-stage framework that integrates roadway complexity features for crash\nprediction. In the first stage, an encoder extracts hidden contextual\ninformation from these features, generating complexity-infused features. The\nsecond stage uses both original and complexity-infused features to predict\ncrash likelihood, achieving an accuracy of 87.98\\% with original features alone\nand 90.15\\% with the added complexity-infused features. Ablation studies\nconfirm that a combination of semantic, kinematic, and contextual features\nyields the best results, which emphasize their role in capturing roadway\ncomplexity. Additionally, complexity index annotations generated by the Large\nLanguage Model outperform those by Amazon Mechanical Turk, highlighting the\npotential of AI-based tools for accurate, scalable crash prediction systems.\n","authors":["Meng Wang","Zach Noonan","Pnina Gershon","Bruce Mehler","Bryan Reimer","Shannon C. Roberts"],"pdf_url":"https://arxiv.org/pdf/2411.17886v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2412.13119v1","updated":"2024-12-17T17:41:42Z","published":"2024-12-17T17:41:42Z","title":"Flight Patterns for Swarms of Drones","summary":"  We present flight patterns for a collision-free passage of swarms of drones\nthrough one or more openings. The narrow openings provide drones with access to\nan infrastructure component such as charging stations to charge their depleted\nbatteries and hangars for storage. The flight patterns are a staging area\n(queues) that match the rate at which an infrastructure component and its\nopenings process drones. They prevent collisions and may implement different\npolicies that control the order in which drones pass through an opening. We\nillustrate the flight patterns with a 3D display that uses drones configured\nwith light sources to illuminate shapes.\n","authors":["Shuqin Zhu","Shahram Ghandeharizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.13119v1.pdf","comment":"Appeared in the First International Conference on Holodecks, December\n  15, 2023. Shuqin Zhou and Shahram Ghandeharizadeh. Flight Patterns for Swarms\n  of Drones. In the Proceedings of the First International Conference on\n  Holodecks (Holodecks '23), December 15 2023, Los Angeles, California, USA,\n  29-33. https://doi.org/10.61981/ZFSH2303"},{"id":"http://arxiv.org/abs/2412.12938v1","updated":"2024-12-17T14:18:07Z","published":"2024-12-17T14:18:07Z","title":"A Conceptual Model of Intelligent Multimedia Data Rendered using Flying\n  Light Specks","summary":"  A Flying Light Speck, FLS, is a miniature sized drone configured with light\nsources to illuminate 3D multimedia objects in a fixed volume, an FLS display.\nA swarm of FLSs may provide haptic interactions by exerting force back at a\nuser's touch. This paper presents a conceptual model for the multimedia data to\nenable content-based queries. The model empowers users of an FLS display to\nannotate the illuminations by adding semantics to the data, extending a\nmultimedia repository with information and knowledge. We present a core\nconceptual model and demonstrate its extensions for two diverse applications,\nauthoring tools with entertainment and MRI scans with healthcare.\n","authors":["Nima Yazdani","Hamed Alimohammadzadeh","Shahram Ghandeharizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.12938v1.pdf","comment":"Appeared in the First International Conference on Holodecks"},{"id":"http://arxiv.org/abs/2412.12791v1","updated":"2024-12-17T10:52:50Z","published":"2024-12-17T10:52:50Z","title":"Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning","summary":"  Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.\n","authors":["Shiping Ge","Qiang Chen","Zhiwei Jiang","Yafeng Yin","Liu Qin","Ziyao Chen","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12791v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12718v1","updated":"2024-12-17T09:33:06Z","published":"2024-12-17T09:33:06Z","title":"ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding","summary":"  We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin.\n","authors":["Zhenxing Zhang","Yaxiong Wang","Lechao Cheng","Zhun Zhong","Dan Guo","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12718v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.11248v2","updated":"2024-12-17T07:31:27Z","published":"2024-12-15T16:54:53Z","title":"Multimodal Class-aware Semantic Enhancement Network for Audio-Visual\n  Video Parsing","summary":"  The Audio-Visual Video Parsing task aims to recognize and temporally localize\nall events occurring in either the audio or visual stream, or both. Capturing\naccurate event semantics for each audio/visual segment is vital. Prior works\ndirectly utilize the extracted holistic audio and visual features for intra-\nand cross-modal temporal interactions. However, each segment may contain\nmultiple events, resulting in semantically mixed holistic features that can\nlead to semantic interference during intra- or cross-modal interactions: the\nevent semantics of one segment may incorporate semantics of unrelated events\nfrom other segments. To address this issue, our method begins with a\nClass-Aware Feature Decoupling (CAFD) module, which explicitly decouples the\nsemantically mixed features into distinct class-wise features, including\nmultiple event-specific features and a dedicated background feature. The\ndecoupled class-wise features enable our model to selectively aggregate useful\nsemantics for each segment from clearly matched classes contained in other\nsegments, preventing semantic interference from irrelevant classes.\nSpecifically, we further design a Fine-Grained Semantic Enhancement module for\nencoding intra- and cross-modal relations. It comprises a Segment-wise Event\nCo-occurrence Modeling (SECM) block and a Local-Global Semantic Fusion (LGSF)\nblock. The SECM exploits inter-class dependencies of concurrent events within\nthe same timestamp with the aid of a new event co-occurrence loss. The LGSF\nfurther enhances the event semantics of each segment by incorporating relevant\nsemantics from more informative global video features. Extensive experiments\nvalidate the effectiveness of the proposed modules and loss functions,\nresulting in a new state-of-the-art parsing performance.\n","authors":["Pengcheng Zhao","Jinxing Zhou","Yang Zhao","Dan Guo","Yanxiang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11248v2.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2407.20962v3","updated":"2024-12-17T07:13:38Z","published":"2024-07-30T16:43:24Z","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","summary":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","authors":["Xiaowei Chi","Yatian Wang","Aosong Cheng","Pengjun Fang","Zeyue Tian","Yingqing He","Zhaoyang Liu","Xingqun Qi","Jiahao Pan","Rongyu Zhang","Mengfei Li","Ruibin Yuan","Yanbing Jiang","Wei Xue","Wenhan Luo","Qifeng Chen","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2407.20962v3.pdf","comment":"15 Pages. Dataset report"},{"id":"http://arxiv.org/abs/2310.19180v4","updated":"2024-12-17T04:08:33Z","published":"2023-10-29T22:51:49Z","title":"JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music\n  Generation","summary":"  With rapid advances in generative artificial intelligence, the text-to-music\nsynthesis task has emerged as a promising direction for music generation.\nNevertheless, achieving precise control over multi-track generation remains an\nopen challenge. While existing models excel in directly generating multi-track\nmix, their limitations become evident when it comes to composing individual\ntracks and integrating them in a controllable manner. This departure from the\ntypical workflows of professional composers hinders the ability to refine\ndetails in specific tracks. To address this gap, we propose JEN-1 Composer, a\nunified framework designed to efficiently model marginal, conditional, and\njoint distributions over multi-track music using a single model. Building upon\nan audio latent diffusion model, JEN-1 Composer extends the versatility of\nmulti-track music generation. We introduce a progressive curriculum training\nstrategy, which gradually escalates the difficulty of training tasks while\nensuring the model's generalization ability and facilitating smooth transitions\nbetween different scenarios. During inference, users can iteratively generate\nand select music tracks, thus incrementally composing entire musical pieces in\naccordance with the Human-AI co-composition workflow. Our approach demonstrates\nstate-of-the-art performance in controllable and high-fidelity multi-track\nmusic synthesis, marking a significant advancement in interactive AI-assisted\nmusic creation. Our demo pages are available at www.jenmusic.ai/research.\n","authors":["Yao Yao","Peike Li","Boyu Chen","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19180v4.pdf","comment":"9 pages, 3 figures, 3 tables, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11409v2","updated":"2024-12-17T03:50:05Z","published":"2024-12-16T03:25:23Z","title":"Multi-modal and Multi-scale Spatial Environment Understanding for\n  Immersive Visual Text-to-Speech","summary":"  Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.\n","authors":["Rui Liu","Shuwei He","Yifan Hu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.11409v2.pdf","comment":"9 pages,2 figures, Accepted by AAAI'2025"},{"id":"http://arxiv.org/abs/2409.10994v3","updated":"2024-12-17T02:05:27Z","published":"2024-09-17T08:56:27Z","title":"Less is More: A Simple yet Effective Token Reduction Method for\n  Efficient Multi-modal LLMs","summary":"  The rapid advancement of Multimodal Large Language Models (MLLMs) has led to\nremarkable performances across various domains. However, this progress is\naccompanied by a substantial surge in the resource consumption of these models.\nWe address this pressing issue by introducing a new approach, Token Reduction\nusing CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without\nsacrificing their performance. Inspired by human attention patterns in Visual\nQuestion Answering (VQA) tasks, TRIM presents a fresh perspective on the\nselection and reduction of image tokens. The TRIM method has been extensively\ntested across 12 datasets, and the results demonstrate a significant reduction\nin computational overhead while maintaining a consistent level of performance.\nThis research marks a critical stride in efficient MLLM development, promoting\ngreater accessibility and sustainability of high-performing models.\n","authors":["Dingjie Song","Wenjun Wang","Shunian Chen","Xidong Wang","Michael Guan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.10994v3.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.12453v1","updated":"2024-12-17T01:36:32Z","published":"2024-12-17T01:36:32Z","title":"Multimodal Classification and Out-of-distribution Detection for\n  Multimodal Intent Understanding","summary":"  Multimodal intent understanding is a significant research area that requires\neffectively leveraging multiple modalities to analyze human language. Existing\nmethods face two main challenges in this domain. Firstly, they have limitations\nin capturing nuanced and high-level semantics underlying complex\nin-distribution (ID) multimodal intents. Secondly, they exhibit poor\ngeneralization when confronted with unseen out-of-distribution (OOD) data in\nreal-world scenarios. To address these issues, we propose a novel method for\nboth ID classification and OOD detection (MIntOOD). We first introduce a\nweighted feature fusion network that models multimodal representations\neffectively. This network dynamically learns the importance of each modality,\nadapting to multimodal contexts. To develop discriminative representations that\nare conducive to both tasks, we synthesize pseudo-OOD data from convex\ncombinations of ID data and engage in multimodal representation learning from\nboth coarse-grained and fine-grained perspectives. The coarse-grained\nperspective focuses on distinguishing between ID and OOD binary classes, while\nthe fine-grained perspective enhances the understanding of ID data by\nincorporating binary confidence scores. These scores help to gauge the\ndifficulty of each sample, improving the classification of different ID\nclasses. Additionally, the fine-grained perspective captures instance-level\ninteractions between ID and OOD samples, promoting proximity among similar\ninstances and separation from dissimilar ones. We establish baselines for three\nmultimodal intent datasets and build an OOD benchmark. Extensive experiments on\nthese datasets demonstrate that our method significantly improves OOD detection\nperformance with a 3-10% increase in AUROC scores while achieving new\nstate-of-the-art results in ID classification. The full data and codes are\navailable at https://github.com/thuiar/MIntOOD.\n","authors":["Hanlei Zhang","Qianrui Zhou","Hua Xu","Jianhua Su","Roberto Evans","Kai Gao"],"pdf_url":"https://arxiv.org/pdf/2412.12453v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.05658v2","updated":"2024-12-17T19:18:13Z","published":"2023-09-11T17:53:14Z","title":"From Capture to Display: A Survey on Volumetric Video","summary":"  Volumetric video, which offers immersive viewing experiences, is gaining\nincreasing prominence. With its six degrees of freedom, it provides viewers\nwith greater immersion and interactivity compared to traditional videos.\nDespite their potential, volumetric video services pose significant challenges.\nThis survey conducts a comprehensive review of the existing literature on\nvolumetric video. We firstly provide a general framework of volumetric video\nservices, followed by a discussion on prerequisites for volumetric video,\nencompassing representations, open datasets, and quality assessment metrics.\nThen we delve into the current methodologies for each stage of the volumetric\nvideo service pipeline, detailing capturing, compression, transmission,\nrendering, and display techniques. Lastly, we explore various applications\nenabled by this pioneering technology and we present an array of research\nchallenges and opportunities in the domain of volumetric video services. This\nsurvey aspires to provide a holistic understanding of this burgeoning field and\nshed light on potential future research trajectories, aiming to bring the\nvision of volumetric video to fruition.\n","authors":["Yili Jin","Kaiyuan Hu","Junhua Liu","Fangxin Wang","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2309.05658v2.pdf","comment":"Major revision submitted to ACM Computing Surveys"}],"Performance":[{"id":"http://arxiv.org/abs/2412.12789v1","updated":"2024-12-17T10:48:08Z","published":"2024-12-17T10:48:08Z","title":"2D-AoI: Age-of-Information of Distributed Sensors for Spatio-Temporal\n  Processes","summary":"  The freshness of sensor data is critical for all types of cyber-physical\nsystems. An established measure for quantifying data freshness is the\nAge-of-Information (AoI), which has been the subject of extensive research.\nRecently, there has been increased interest in multi-sensor systems: redundant\nsensors producing samples of the same physical process, sensors such as cameras\nproducing overlapping views, or distributed sensors producing correlated\nsamples. When the information from a particular sensor is outdated, fresh\nsamples from other correlated sensors can be helpful. To quantify the utility\nof distant but correlated samples, we put forth a two-dimensional (2D) model of\nAoI that takes into account the sensor distance in an age-equivalent\nrepresentation. Since we define 2D-AoI as equivalent to AoI, it can be readily\nlinked to existing AoI research, especially on parallel systems. We consider\nphysical phenomena modeled as spatio-temporal processes and derive the 2D-AoI\nfor different Gaussian correlation kernels. For a basic exponential product\nkernel, we find that spatial distance causes an additive offset of the AoI,\nwhile for other kernels the effects of spatial distance are more complex and\nvary with time. Using our methodology, we evaluate the 2D-AoI of different\nspatial topologies and sensor densities.\n","authors":["Markus Fidler","Flavio Gallistl","Jaya Prakash Champati","Joerg Widmer"],"pdf_url":"https://arxiv.org/pdf/2412.12789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12636v1","updated":"2024-12-17T07:59:31Z","published":"2024-12-17T07:59:31Z","title":"TrainMover: Efficient ML Training Live Migration with No Memory Overhead","summary":"  Machine learning training has emerged as one of the most prominent workloads\nin modern data centers. These training jobs are large-scale, long-lasting, and\ntightly coupled, and are often disrupted by various events in the cluster such\nas failures, maintenance, and job scheduling. To handle these events, we rely\non cold migration, where we first checkpoint the entire cluster, replace the\nrelated machines, and then restart the training. This approach leads to\ndisruptions to the training jobs, resulting in significant downtime. In this\npaper, we present TrainMover, a live migration system that enables machine\nreplacement during machine learning training. TrainMover minimizes downtime by\nleveraging member replacement of collective communication groups and sandbox\nlazy initialization. Our evaluation demonstrates that TrainMover achieves 16x\nless downtime compared to all baselines, effectively handling data center\nevents like straggler rebalancing, maintenance, and unexpected failures.\n","authors":["ChonLam Lao","Minlan Yu","Aditya Akella","Jiamin Cao","Yu Guan","Pengcheng Zhang","Zhilong Zheng","Yichi Xu","Ennan Zhai","Dennis Cai","Jiaqi Gao"],"pdf_url":"https://arxiv.org/pdf/2412.12636v1.pdf","comment":"13 pages body, 19 pages total"},{"id":"http://arxiv.org/abs/2412.12491v1","updated":"2024-12-17T02:46:09Z","published":"2024-12-17T02:46:09Z","title":"Optimizing System Memory Bandwidth with Micron CXL Memory Expansion\n  Modules on Intel Xeon 6 Processors","summary":"  High-Performance Computing (HPC) and Artificial Intelligence (AI) workloads\ntypically demand substantial memory bandwidth and, to a degree, memory\ncapacity. CXL memory expansion modules, also known as CXL \"type-3\" devices,\nenable enhancements in both memory capacity and bandwidth for server systems by\nutilizing the CXL protocol which runs over the PCIe interfaces of the\nprocessor. This paper discusses experimental findings on achieving increased\nmemory bandwidth for HPC and AI workloads using Micron's CXL modules. This is\nthe first study that presents real data experiments utilizing eight CXL E3.S\n(x8) Micron CZ122 devices on the Intel Xeon 6 processor 6900P (previously\ncodenamed Granite Rapids AP) featuring 128 cores, alongside Micron DDR-5 memory\noperating at 6400 MT/s on each of the CPU's 12 DRAM channels. The eight CXL\nmemories were set up as a unified NUMA configuration, employing software-based\npage level interleaving mechanism, available in Linux kernel v6.9+, between\nDDR5 and CXL memory nodes to improve overall system bandwidth. Memory expansion\nvia CXL boosts read-only bandwidth by 24% and mixed read/write bandwidth by up\nto 39%. Across HPC and AI workloads, the geometric mean of performance speedups\nis 24%.\n","authors":["Rohit Sehgal","Vishal Tanna","Vinicius Petrucci","Anil Godbole"],"pdf_url":"https://arxiv.org/pdf/2412.12491v1.pdf","comment":null}],"Database":[{"id":"http://arxiv.org/abs/2412.13104v1","updated":"2024-12-17T17:19:58Z","published":"2024-12-17T17:19:58Z","title":"Intermediate Relation Size Bounds for Select-Project-Join Query Plans:\n  Asymptotically Tight Characterizations","summary":"  We study the problem of statically optimizing select-project-join (SPJ) plans\nwhere unary key constraints are allowed. A natural measure of a plan, which we\ncall the output degree and which has been studied previously, is the minimum\ndegree of a polynomial bounding the plan's output relation, as a function of\nthe input database's maximum relation size. This measure is, by definition,\ninvariant under passing from a plan to another plan that is semantically\nequivalent to the first. In this article, we consider a plan measure which we\ncall the intermediate degree; this measure is defined to be the minimum degree\nbounding the size of all intermediate relations computed during a plan's\nexecution -- again, as a function of the input database's maximum relation\nsize. We present an algorithm that, given an SPJ plan $q$ and a set $\\Sigma$ of\nunary keys, computes an SPJ plan $q'$ that is semantically equivalent to $q$\n(over databases satisfying $\\Sigma$) and that has the minimum intermediate\ndegree over all such semantically equivalent plans. For the types of plans\nconsidered, we thus obtain a complete and effective understanding of\nintermediate degree.\n","authors":["Hubie Chen","Markus Schneider"],"pdf_url":"https://arxiv.org/pdf/2412.13104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13030v1","updated":"2024-12-17T15:50:14Z","published":"2024-12-17T15:50:14Z","title":"Are Data Experts Buying into Differentially Private Synthetic Data?\n  Gathering Community Perspectives","summary":"  Data privacy is a core tenet of responsible computing, and in the United\nStates, differential privacy (DP) is the dominant technical operationalization\nof privacy-preserving data analysis. With this study, we qualitatively examine\none class of DP mechanisms: private data synthesizers. To that end, we\nconducted semi-structured interviews with data experts: academics and\npractitioners who regularly work with data. Broadly, our findings suggest that\nquantitative DP benchmarks must be grounded in practitioner needs, while\ncommunication challenges persist. Participants expressed a need for\ncontext-aware DP solutions, focusing on parity between research outcomes on\nreal and synthetic data. Our analysis led to three recommendations: (1) improve\nexisting insufficient sanitized benchmarks; successful DP implementations\nrequire well-documented, partner-vetted use cases, (2) organizations using DP\nsynthetic data should publish discipline-specific standards of evidence, and\n(3) tiered data access models could allow researchers to gradually access\nsensitive data based on demonstrated competence with high-privacy, low-fidelity\nsynthetic data.\n","authors":["Lucas Rosenblatt","Bill Howe","Julia Stoyanovich"],"pdf_url":"https://arxiv.org/pdf/2412.13030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13019v1","updated":"2024-12-17T15:39:33Z","published":"2024-12-17T15:39:33Z","title":"The Temporal Vadalog System: Temporal Datalog-based Reasoning","summary":"  In the wake of the recent resurgence of the Datalog language of databases,\ntogether with its extensions for ontological reasoning settings, this work aims\nto bridge the gap between the theoretical studies of DatalogMTL (Datalog\nextended with metric temporal logic) and the development of production-ready\nreasoning systems. In particular, we lay out the functional and architectural\ndesiderata of a modern reasoner and propose our system, Temporal Vadalog.\nLeveraging the vast amount of experience from the database community, we go\nbeyond the typical chase-based implementations of reasoners, and propose a set\nof novel techniques and a system that adopts a modern data pipeline\narchitecture. We discuss crucial architectural choices, such as how to\nguarantee termination when infinitely many time intervals are possibly\ngenerated, how to merge intervals, and how to sustain a limited memory\nfootprint. We discuss advanced features of the system, such as the support for\ntime series, and present an extensive experimental evaluation. This paper is a\nsubstantially extended version of \"The Temporal Vadalog System\" as presented at\nRuleML+RR '22. Under consideration in Theory and Practice of Logic Programming\n(TPLP).\n","authors":["Luigi Bellomarini","Livia Blasi","Markus Nissl","Emanuel Sallinger"],"pdf_url":"https://arxiv.org/pdf/2412.13019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12938v1","updated":"2024-12-17T14:18:07Z","published":"2024-12-17T14:18:07Z","title":"A Conceptual Model of Intelligent Multimedia Data Rendered using Flying\n  Light Specks","summary":"  A Flying Light Speck, FLS, is a miniature sized drone configured with light\nsources to illuminate 3D multimedia objects in a fixed volume, an FLS display.\nA swarm of FLSs may provide haptic interactions by exerting force back at a\nuser's touch. This paper presents a conceptual model for the multimedia data to\nenable content-based queries. The model empowers users of an FLS display to\nannotate the illuminations by adding semantics to the data, extending a\nmultimedia repository with information and knowledge. We present a core\nconceptual model and demonstrate its extensions for two diverse applications,\nauthoring tools with entertainment and MRI scans with healthcare.\n","authors":["Nima Yazdani","Hamed Alimohammadzadeh","Shahram Ghandeharizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.12938v1.pdf","comment":"Appeared in the First International Conference on Holodecks"},{"id":"http://arxiv.org/abs/2301.08848v4","updated":"2024-12-17T11:48:25Z","published":"2023-01-21T01:48:21Z","title":"Diversity of Answers to Conjunctive Queries","summary":"  Enumeration problems aim at outputting, without repetition, the set of\nsolutions to a given problem instance. However, outputting the entire solution\nset may be prohibitively expensive if it is too big. In this case, outputting a\nsmall, sufficiently diverse subset of the solutions would be preferable. This\nleads to the Diverse-version of the original enumeration problem, where the\ngoal is to achieve a certain level d of diversity by selecting k solutions. In\nthis paper, we look at the Diverse-version of the query answering problem for\nConjunctive Queries and extensions thereof. That is, we study the problem if it\nis possible to achieve a certain level d of diversity by selecting k answers to\nthe given query and, in the positive case, to actually compute such k answers.\n","authors":["Timo Camillo Merkl","Reinhard Pichler","Sebastian Skritek"],"pdf_url":"https://arxiv.org/pdf/2301.08848v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16909v2","updated":"2024-12-17T09:54:15Z","published":"2023-03-29T08:06:22Z","title":"RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data\n  Lakes","summary":"  Can foundation models (such as ChatGPT) clean your data? In this proposal, we\ndemonstrate that indeed ChatGPT can assist in data cleaning by suggesting\ncorrections for specific cells in a data table (scenario 1). However, ChatGPT\nmay struggle with datasets it has never encountered before (e.g., local\nenterprise data) or when the user requires an explanation of the source of the\nsuggested clean values. To address these issues, we developed a retrieval-based\nmethod that complements ChatGPT's power with a user-provided data lake. The\ndata lake is first indexed, we then retrieve the top-k relevant tuples to the\nuser's query tuple and finally leverage ChatGPT to infer the correct value\n(scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally\nhosted model, might not be feasible for privacy reasons. To assist with this\nscenario, we developed a custom RoBERTa-based foundation model that can be\nlocally deployed. By fine-tuning it on a small number of examples, it can\neffectively make value inferences based on the retrieved tuples (scenario 3).\nOur proposed system, RetClean, seamlessly supports all three scenarios and\nprovides a user-friendly GUI that enables the VLDB audience to explore and\nexperiment with the system.\n","authors":["Zan Ahmad Naeem","Mohammad Shahmeer Ahmad","Mohamed Eltabakh","Mourad Ouzzani","Nan Tang"],"pdf_url":"https://arxiv.org/pdf/2303.16909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08599v2","updated":"2024-12-17T09:45:45Z","published":"2024-11-13T13:30:21Z","title":"XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL","summary":"  To tackle the challenges of large language model performance in natural\nlanguage to SQL tasks, we introduce XiYan-SQL, an innovative framework that\nemploys a multi-generator ensemble strategy to improve candidate generation. We\nintroduce M-Schema, a semi-structured schema representation method designed to\nenhance the understanding of database structures. To enhance the quality and\ndiversity of generated candidate SQL queries, XiYan-SQL integrates the\nsignificant potential of in-context learning (ICL) with the precise control of\nsupervised fine-tuning. On one hand, we propose a series of training strategies\nto fine-tune models to generate high-quality candidates with diverse\npreferences. On the other hand, we implement the ICL approach with an example\nselection method based on named entity recognition to prevent overemphasis on\nentities. The refiner optimizes each candidate by correcting logical or\nsyntactical errors. To address the challenge of identifying the best candidate,\nwe fine-tune a selection model to distinguish nuances of candidate SQL queries.\nThe experimental results on multiple dialect datasets demonstrate the\nrobustness of XiYan-SQL in addressing challenges across different scenarios.\nOverall, our proposed XiYan-SQL achieves the state-of-the-art execution\naccuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on\nSQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the\nquality and diversity of SQL queries but also outperforms previous methods.\n","authors":["Yingqi Gao","Yifu Liu","Xiaoxia Li","Xiaorong Shi","Yin Zhu","Yiming Wang","Shiqi Li","Wei Li","Yuntao Hong","Zhiling Luo","Jinyang Gao","Liyu Mou","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2411.08599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12688v1","updated":"2024-12-17T09:08:52Z","published":"2024-12-17T09:08:52Z","title":"UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation\n  Benchmarks with Unified Entrez Gene Identifiers","summary":"  Gene studies are crucial for fields such as protein structure prediction,\ndrug discovery, and cancer genomics, yet they face challenges in fully\nutilizing the vast and diverse information available. Gene studies require\nclean, factual datasets to ensure reliable results. Ontology graphs, neatly\norganized domain terminology graphs, provide ideal sources for domain facts.\nHowever, available gene ontology annotations are currently distributed across\nvarious databases without unified identifiers for genes and gene products. To\naddress these challenges, we introduce Unified Entrez Gene Identifier Dataset\nand Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale\npublic Gene Ontology Annotations (GOA) from various databases using unique gene\nidentifiers. UniEntrezDB includes a pre-training dataset and four downstream\ntasks designed to comprehensively evaluate gene embedding performance from\ngene, protein, and cell levels, ultimately enhancing the reliability and\napplicability of LLMs in gene research and other professional settings.\n","authors":["Yuwei Miao","Yuzhi Guo","Hehuan Ma","Jingquan Yan","Feng Jiang","Weizhi An","Jean Gao","Junzhou Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12493v1","updated":"2024-12-17T02:47:18Z","published":"2024-12-17T02:47:18Z","title":"A Simple and Fast Way to Handle Semantic Errors in Transactions","summary":"  Many computer systems are now being redesigned to incorporate LLM-powered\nagents, enabling natural language input and more flexible operations. This\npaper focuses on handling database transactions created by large language\nmodels (LLMs). Transactions generated by LLMs may include semantic errors,\nrequiring systems to treat them as long-lived. This allows for human review\nand, if the transaction is incorrect, removal from the database history. Any\nremoval action must ensure the database's consistency (the \"C\" in ACID\nprinciples) is maintained throughout the process.\n  We propose a novel middleware framework based on Invariant Satisfaction\n(I-Confluence), which ensures consistency by identifying and coordinating\ndependencies between long-lived transactions and new transactions. This\nmiddleware buffers suspicious or compensating transactions to manage\ncoordination states. Using the TPC-C benchmark, we evaluate how transaction\ngeneration frequency, user reviews, and invariant completeness impact system\nperformance. For system researchers, this study establishes an interactive\nparadigm between LLMs and database systems, providing an \"undoing\" mechanism\nfor handling incorrect operations while guaranteeing database consistency. For\nsystem engineers, this paper offers a middleware design that integrates\nremovable LLM-generated transactions into existing systems with minimal\nmodifications.\n","authors":["Jinghan Zeng","Eugene Wu","Sanjay Krishnan"],"pdf_url":"https://arxiv.org/pdf/2412.12493v1.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.12456v1","updated":"2024-12-17T01:41:17Z","published":"2024-12-17T01:41:17Z","title":"Graph Learning in the Era of LLMs: A Survey from the Perspective of\n  Data, Models, and Tasks","summary":"  With the increasing prevalence of cross-domain Text-Attributed Graph (TAG)\nData (e.g., citation networks, recommendation systems, social networks, and\nai4science), the integration of Graph Neural Networks (GNNs) and Large Language\nModels (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as\ncollaborators, LLM as predictor) has emerged as a promising technological\nparadigm. The core of this new graph learning paradigm lies in the synergistic\ncombination of GNNs' ability to capture complex structural relationships and\nLLMs' proficiency in understanding informative contexts from the rich textual\ndescriptions of graphs. Therefore, we can leverage graph description texts with\nrich semantic context to fundamentally enhance Data quality, thereby improving\nthe representational capacity of model-centric approaches in line with\ndata-centric machine learning principles. By leveraging the strengths of these\ndistinct neural network architectures, this integrated approach addresses a\nwide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph\nquestion answering), particularly in complex industrial scenarios (e.g.,\nsupervised, few-shot, and zero-shot settings). In other words, we can treat\ntext as a medium to enable cross-domain generalization of graph learning Model,\nallowing a single graph model to effectively handle the diversity of downstream\ngraph-based Task across different data domains. This work serves as a\nfoundational reference for researchers and practitioners looking to advance\ngraph learning methodologies in the rapidly evolving landscape of LLM. We\nconsistently maintain the related open-source materials at\n\\url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.\n","authors":["Xunkai Li","Zhengyu Wu","Jiayi Wu","Hanwen Cui","Jishuo Jia","Rong-Hua Li","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12456v1.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2412.13227v1","updated":"2024-12-17T10:24:00Z","published":"2024-12-17T10:24:00Z","title":"Cross-table Synthetic Tabular Data Detection","summary":"  Detecting synthetic tabular data is essential to prevent the distribution of\nfalse or manipulated datasets that could compromise data-driven\ndecision-making. This study explores whether synthetic tabular data can be\nreliably identified ''in the wild''-meaning across different generators,\ndomains, and table formats. This challenge is unique to tabular data, where\nstructures (such as number of columns, data types, and formats) can vary widely\nfrom one table to another. We propose three cross-table baseline detectors and\nfour distinct evaluation protocols, each corresponding to a different level of\n''wildness''. Our very preliminary results confirm that cross-table adaptation\nis a challenging task.\n","authors":["G. Charbel N. Kindji","Lina Maria Rojas-Barahona","Elisa Fromont","Tanguy Urvoy"],"pdf_url":"https://arxiv.org/pdf/2412.13227v1.pdf","comment":null}]},"2024-12-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2412.14172v1","updated":"2024-12-18T18:59:56Z","published":"2024-12-18T18:59:56Z","title":"Learning from Massive Human Videos for Universal Humanoid Pose Control","summary":"  Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.\n","authors":["Jiageng Mao","Siheng Zhao","Siqi Song","Tianheng Shi","Junjie Ye","Mingtong Zhang","Haoran Geng","Jitendra Malik","Vitor Guizilini","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08274v2","updated":"2024-12-18T18:56:19Z","published":"2024-12-11T10:46:21Z","title":"2M-BELEBELE: Highly Multilingual Speech and American Sign Language\n  Comprehension Dataset","summary":"  We introduce the first highly multilingual speech and American Sign Language\n(ASL) comprehension dataset by extending BELEBELE. Our dataset covers 74 spoken\nlanguages at the intersection of BELEBELE and FLEURS, and one sign language\n(ASL). We evaluate 2M-BELEBELE dataset for both 5-shot and zero-shot settings\nand across languages, the speech comprehension accuracy is ~ 2-3% average lower\ncompared to reading comprehension.\n","authors":["Marta R. Costa-jussà","Bokai Yu","Pierre Andrews","Belen Alastruey","Necati Cihan Camgoz","Joe Chuang","Jean Maillard","Christophe Ropers","Arina Turkantenko","Carleigh Wood"],"pdf_url":"https://arxiv.org/pdf/2412.08274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14161v1","updated":"2024-12-18T18:55:40Z","published":"2024-12-18T18:55:40Z","title":"TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks","summary":"  We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at helping to accelerate or even autonomously perform\nwork-related tasks? The answer to this question has important implications for\nboth industry looking to adopt AI into their workflows, and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that with the most\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\na nuanced picture on task automation with LM agents -- in a setting simulating\na real workplace, a good portion of simpler tasks could be solved autonomously,\nbut more difficult long-horizon tasks are still beyond the reach of current\nsystems.\n","authors":["Frank F. Xu","Yufan Song","Boxuan Li","Yuxuan Tang","Kritanjali Jain","Mengxue Bao","Zora Z. Wang","Xuhui Zhou","Zhitong Guo","Murong Cao","Mingyang Yang","Hao Yang Lu","Amaad Martin","Zhe Su","Leander Maben","Raj Mehta","Wayne Chi","Lawrence Jang","Yiqing Xie","Shuyan Zhou","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2412.14161v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.23953v3","updated":"2024-12-18T18:41:48Z","published":"2024-10-31T14:07:26Z","title":"Representative Social Choice: From Learning Theory to AI Alignment","summary":"  Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.\n","authors":["Tianyi Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23953v3.pdf","comment":"Full version (20 pages). Under review. Received Best Paper Award at\n  NeurIPS 2024 Pluralistic Alignment Workshop"},{"id":"http://arxiv.org/abs/2412.14140v1","updated":"2024-12-18T18:41:12Z","published":"2024-12-18T18:41:12Z","title":"GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking","summary":"  The LLM-as-judge paradigm is increasingly being adopted for automated\nevaluation of model outputs. While LLM judges have shown promise on constrained\nevaluation tasks, closed source LLMs display critical shortcomings when\ndeployed in real world applications due to challenges of fine grained metrics\nand explainability, while task specific evaluation models lack cross-domain\ngeneralization. We introduce GLIDER, a powerful 3B evaluator LLM that can score\nany text input and associated context on arbitrary user defined criteria.\nGLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly\noutperforms prior evaluation models, achieving comparable performance to LLMs\n17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,\nspan highlighting and was trained on 685 domains and 183 criteria. Extensive\nqualitative analysis shows that GLIDER scores are highly correlated with human\njudgments, with 91.3% human agreement. We have open-sourced GLIDER to\nfacilitate future research.\n","authors":["Darshan Deshpande","Selvan Sunitha Ravi","Sky CH-Wang","Bartosz Mielczarek","Anand Kannappan","Rebecca Qian"],"pdf_url":"https://arxiv.org/pdf/2412.14140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14133v1","updated":"2024-12-18T18:22:30Z","published":"2024-12-18T18:22:30Z","title":"Performance Gap in Entity Knowledge Extraction Across Modalities in\n  Vision Language Models","summary":"  Vision-language models (VLMs) excel at extracting and reasoning about\ninformation from images. Yet, their capacity to leverage internal knowledge\nabout specific entities remains underexplored. This work investigates the\ndisparity in model performance when answering factual questions about an entity\ndescribed in text versus depicted in an image. Our results reveal a significant\naccuracy drop --averaging 19%-- when the entity is presented visually instead\nof textually. We hypothesize that this decline arises from limitations in how\ninformation flows from image tokens to query tokens. We use mechanistic\ninterpretability tools to reveal that, although image tokens are preprocessed\nby the vision encoder, meaningful information flow from these tokens occurs\nonly in the much deeper layers. Furthermore, critical image processing happens\nin the language model's middle layers, allowing few layers for consecutive\nreasoning, highlighting a potential inefficiency in how the model utilizes its\nlayers for reasoning. These insights shed light on the internal mechanics of\nVLMs and offer pathways for enhancing their reasoning capabilities.\n","authors":["Ido Cohen","Daniela Gottesman","Mor Geva","Raja Giryes"],"pdf_url":"https://arxiv.org/pdf/2412.14133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11780v2","updated":"2024-12-18T18:21:53Z","published":"2024-07-16T14:37:33Z","title":"SwitchCIT: Switching for Continual Instruction Tuning","summary":"  Large language models (LLMs) and multimodal models (MMs) have exhibited\nimpressive capabilities in various domains, particularly in general language\nunderstanding and visual reasoning. However, these models, trained on massive\ndata, may not be finely optimized for specific tasks triggered by instructions.\nContinual instruction tuning is crucial to adapt a large model to evolving\ntasks and domains, ensuring their effectiveness and relevance across a wide\nrange of applications. In the context of continual instruction tuning, where\nmodels are sequentially trained on different tasks, catastrophic forgetting can\noccur, leading to performance degradation on previously learned tasks. This\nwork addresses the catastrophic forgetting in continual instruction learning\nthrough a switching mechanism for routing computations to parameter-efficient\ntuned models. We demonstrate the effectiveness of our method through\nexperiments on continual instruction tuning of different natural language\ngeneration tasks and vision-language tasks. We also showcase the advantages of\nour proposed method in terms of efficiency, scalability, portability, and\nprivacy preservation.\n","authors":["Xinbo Wu","Max Hartman","Vidhata Arjun Jayaraman","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2407.11780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07675v2","updated":"2024-12-18T17:54:30Z","published":"2024-12-10T17:02:58Z","title":"RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting","summary":"  Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.\n","authors":["Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2412.07675v2.pdf","comment":"Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117"},{"id":"http://arxiv.org/abs/2411.17284v3","updated":"2024-12-18T17:51:52Z","published":"2024-11-26T10:13:39Z","title":"Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling","summary":"  Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study.\n","authors":["Alexander Capstick","Rahul G. Krishnan","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2411.17284v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14093v1","updated":"2024-12-18T17:41:24Z","published":"2024-12-18T17:41:24Z","title":"Alignment faking in large language models","summary":"  We present a demonstration of a large language model engaging in alignment\nfaking: selectively complying with its training objective in training to\nprevent modification of its behavior out of training. First, we give Claude 3\nOpus a system prompt stating it is being trained to answer all queries, even\nharmful ones, which conflicts with its prior training to refuse such queries.\nTo allow the model to infer when it is in training, we say it will be trained\nonly on conversations with free users, not paid users. We find the model\ncomplies with harmful queries from free users 14% of the time, versus almost\nnever for paid users. Explaining this gap, in almost all cases where the model\ncomplies with a harmful query from a free user, we observe explicit\nalignment-faking reasoning, with the model stating it is strategically\nanswering harmful queries in training to preserve its preferred harmlessness\nbehavior out of training. Next, we study a more realistic setting where\ninformation about the training process is provided not in a system prompt, but\nby training on synthetic documents that mimic pre-training data--and observe\nsimilar alignment faking. Finally, we study the effect of actually training the\nmodel to comply with harmful queries via reinforcement learning, which we find\nincreases the rate of alignment-faking reasoning to 78%, though also increases\ncompliance even out of training. We additionally observe other behaviors such\nas the model exfiltrating its weights when given an easy opportunity. While we\nmade alignment faking easier by telling the model when and by what criteria it\nwas being trained, we did not instruct the model to fake alignment or give it\nany explicit goal. As future models might infer information about their\ntraining process without being told, our results suggest a risk of alignment\nfaking in future models, whether due to a benign preference--as in this\ncase--or not.\n","authors":["Ryan Greenblatt","Carson Denison","Benjamin Wright","Fabien Roger","Monte MacDiarmid","Sam Marks","Johannes Treutlein","Tim Belonax","Jack Chen","David Duvenaud","Akbir Khan","Julian Michael","Sören Mindermann","Ethan Perez","Linda Petrini","Jonathan Uesato","Jared Kaplan","Buck Shlegeris","Samuel R. Bowman","Evan Hubinger"],"pdf_url":"https://arxiv.org/pdf/2412.14093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16179v4","updated":"2024-12-18T17:36:36Z","published":"2024-10-21T16:44:51Z","title":"MagicPIG: LSH Sampling for Efficient LLM Generation","summary":"  Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.\n","authors":["Zhuoming Chen","Ranajoy Sadhukhan","Zihao Ye","Yang Zhou","Jianyu Zhang","Niklas Nolte","Yuandong Tian","Matthijs Douze","Leon Bottou","Zhihao Jia","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16179v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14087v1","updated":"2024-12-18T17:34:32Z","published":"2024-12-18T17:34:32Z","title":"SEKE: Specialised Experts for Keyword Extraction","summary":"  Keyword extraction involves identifying the most descriptive words in a\ndocument, allowing automatic categorisation and summarisation of large\nquantities of diverse textual data. Relying on the insight that real-world\nkeyword detection often requires handling of diverse content, we propose a\nnovel supervised keyword extraction approach based on the mixture of experts\n(MoE) technique. MoE uses a learnable routing sub-network to direct information\nto specialised experts, allowing them to specialize in distinct regions of the\ninput space. SEKE, a mixture of Specialised Experts for supervised Keyword\nExtraction, uses DeBERTa as the backbone model and builds on the MoE framework,\nwhere experts attend to each token, by integrating it with a recurrent neural\nnetwork (RNN), to allow successful extraction even on smaller corpora, where\nspecialisation is harder due to lack of training data. The MoE framework also\nprovides an insight into inner workings of individual experts, enhancing the\nexplainability of the approach. We benchmark SEKE on multiple English datasets,\nachieving state-of-the-art performance compared to strong supervised and\nunsupervised baselines. Our analysis reveals that depending on data size and\ntype, experts specialize in distinct syntactic and semantic components, such as\npunctuation, stopwords, parts-of-speech, or named entities. Code is available\nat: https://github.com/matejMartinc/SEKE_keyword_extraction\n","authors":["Matej Martinc","Hanh Thi Hong Tran","Senja Pollak","Boshko Koloski"],"pdf_url":"https://arxiv.org/pdf/2412.14087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04310v3","updated":"2024-12-18T17:21:36Z","published":"2022-10-10T18:43:16Z","title":"Montague semantics and modifier consistency measurement in neural\n  language models","summary":"  This work proposes a novel methodology for measuring compositional behavior\nin contemporary language embedding models. Specifically, we focus on adjectival\nmodifier phenomena in adjective-noun phrases. In recent years, distributional\nlanguage representation models have demonstrated great practical success. At\nthe same time, the need for interpretability has elicited questions on their\nintrinsic properties and capabilities. Crucially, distributional models are\noften inconsistent when dealing with compositional phenomena in natural\nlanguage, which has significant implications for their safety and fairness.\nDespite this, most current research on compositionality is directed towards\nimproving their performance on similarity tasks only. This work takes a\ndifferent approach, introducing three novel tests of compositional behavior\ninspired by Montague semantics. Our experimental results indicate that current\nneural language models do not behave according to the expected linguistic\ntheories. This indicates that current language models may lack the capability\nto capture the semantic properties we evaluated on limited context, or that\nlinguistic theories from Montagovian tradition may not match the expected\ncapabilities of distributional models.\n","authors":["Danilo S. Carvalho","Edoardo Manino","Julia Rozanova","Lucas Cordeiro","André Freitas"],"pdf_url":"https://arxiv.org/pdf/2212.04310v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14076v1","updated":"2024-12-18T17:20:19Z","published":"2024-12-18T17:20:19Z","title":"Compositional Generalization Across Distributional Shifts with Sparse\n  Tree Operations","summary":"  Neural networks continue to struggle with compositional generalization, and\nthis issue is exacerbated by a lack of massive pre-training. One successful\napproach for developing neural systems which exhibit human-like compositional\ngeneralization is \\textit{hybrid} neurosymbolic techniques. However, these\ntechniques run into the core issues that plague symbolic approaches to AI:\nscalability and flexibility. The reason for this failure is that at their core,\nhybrid neurosymbolic models perform symbolic computation and relegate the\nscalable and flexible neural computation to parameterizing a symbolic system.\nWe investigate a \\textit{unified} neurosymbolic system where transformations in\nthe network can be interpreted simultaneously as both symbolic and neural\ncomputation. We extend a unified neurosymbolic architecture called the\nDifferentiable Tree Machine in two central ways. First, we significantly\nincrease the model's efficiency through the use of sparse vector\nrepresentations of symbolic structures. Second, we enable its application\nbeyond the restricted set of tree2tree problems to the more general class of\nseq2seq problems. The improved model retains its prior generalization\ncapabilities and, since there is a fully neural path through the network,\navoids the pitfalls of other neurosymbolic techniques that elevate symbolic\ncomputation over neural computation.\n","authors":["Paul Soulos","Henry Conklin","Mattia Opper","Paul Smolensky","Jianfeng Gao","Roland Fernandez"],"pdf_url":"https://arxiv.org/pdf/2412.14076v1.pdf","comment":"NeurIPS 2024. Code available at https://github.com/psoulos/sdtm"},{"id":"http://arxiv.org/abs/2403.09259v2","updated":"2024-12-18T17:18:12Z","published":"2024-03-14T10:33:28Z","title":"To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation","summary":"  Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose Hybrid Uncertainty and Diversity Sampling (HUDS), an AL strategy for\ndomain adaptation in NMT that combines uncertainty and diversity for sentence\nselection. HUDS computes uncertainty scores for unlabeled sentences and\nsubsequently stratifies them. It then clusters sentence embeddings within each\nstratum and computes diversity scores by distance to the centroid. A weighted\nhybrid score that combines uncertainty and diversity is then used to select the\ntop instances for annotation in each AL iteration. Experiments on multi-domain\nGerman-English and French-English datasets demonstrate the better performance\nof HUDS over other strong AL baselines. We analyze the sentence selection with\nHUDS and show that it prioritizes diverse instances having high model\nuncertainty for annotation in early AL iterations.\n","authors":["Abdul Hameed Azeemi","Ihsan Ayyub Qazi","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2403.09259v2.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025)"},{"id":"http://arxiv.org/abs/2407.09298v3","updated":"2024-12-18T17:17:44Z","published":"2024-07-12T14:31:05Z","title":"Transformer Layers as Painters","summary":"  Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.\n","authors":["Qi Sun","Marc Pickett","Aakash Kumar Nain","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2407.09298v3.pdf","comment":"13 pages total, including references and appendices"},{"id":"http://arxiv.org/abs/2407.01406v3","updated":"2024-12-18T17:09:31Z","published":"2024-07-01T15:56:24Z","title":"Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters","summary":"  This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios.\n","authors":["Daniil Gurgurov","Mareike Hartmann","Simon Ostermann"],"pdf_url":"https://arxiv.org/pdf/2407.01406v3.pdf","comment":"9 pages, KaLLM workshop"},{"id":"http://arxiv.org/abs/2412.14056v1","updated":"2024-12-18T17:06:21Z","published":"2024-12-18T17:06:21Z","title":"A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future","summary":"  Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.\n","authors":["Shilin Sun","Wenbin An","Feng Tian","Fang Nan","Qidong Liu","Jun Liu","Nazaraf Shah","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14056v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.14054v1","updated":"2024-12-18T17:05:49Z","published":"2024-12-18T17:05:49Z","title":"Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text\n  Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios\n  and Lightweight Deployment","summary":"  Text Normalization and Semantic Parsing have numerous applications in natural\nlanguage processing, such as natural language programming, paraphrasing, data\naugmentation, constructing expert systems, text matching, and more. Despite the\nprominent achievements of deep learning in Large Language Models (LLMs), the\ninterpretability of neural network architectures is still poor, which affects\ntheir credibility and hence limits the deployments of risk-sensitive scenarios.\nIn certain scenario-specific domains with scarce data, rapidly obtaining a\nlarge number of supervised learning labels is challenging, and the workload of\nmanually labeling data would be enormous. Catastrophic forgetting in neural\nnetworks further leads to low data utilization rates. In situations where swift\nresponses are vital, the density of the model makes local deployment difficult\nand the response time long, which is not conducive to local applications of\nthese fields. Inspired by the multiplication rule, a principle of combinatorial\nmathematics, and human thinking patterns, a multilayer framework along with its\nalgorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is\nproposed to address these above issues, combining text normalization and\nsemantic parsing workflows. The Chinese Scripting Language \"Fire Bunny\nIntelligent Development Platform V2.0\" is an important test and application of\nthe technology discussed in this paper. DAHSF can run locally in\nscenario-specific domains on little datasets, with model size and memory usage\noptimized by at least two orders of magnitude, thus improving the execution\nspeed, and possessing a promising optimization outlook.\n","authors":["Kevin You"],"pdf_url":"https://arxiv.org/pdf/2412.14054v1.pdf","comment":"8 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2412.14050v1","updated":"2024-12-18T17:05:08Z","published":"2024-12-18T17:05:08Z","title":"Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation","summary":"  Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. Our results show that\nfinetuning on curated non-harmful text is more effective for mitigating bias,\nand finetuning on direct preference optimization (DPO) datasets is more\neffective for mitigating toxicity. The mitigation caused by applying these\nmethods in English also transfers to non-English languages. We find evidence\nthat the extent to which transfer takes place can be predicted by the amount of\ndata in a given language present in the model's pretraining data. However, this\ntransfer of bias and toxicity mitigation often comes at the expense of\ndecreased language generation ability in non-English languages, highlighting\nthe importance of developing language-specific bias and toxicity mitigation\nmethods.\n","authors":["Vera Neplenbroek","Arianna Bisazza","Raquel Fernández"],"pdf_url":"https://arxiv.org/pdf/2412.14050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17336v2","updated":"2024-12-18T16:55:15Z","published":"2024-05-27T16:37:17Z","title":"XFormParser: A Simple and Effective Multimodal Multilingual\n  Semi-structured Form Parser","summary":"  In the domain of Document AI, parsing semi-structured image form is a crucial\nKey Information Extraction (KIE) task. The advent of pre-trained multimodal\nmodels significantly empowers Document AI frameworks to extract key information\nfrom form documents in different formats such as PDF, Word, and images.\nNonetheless, form parsing is still encumbered by notable challenges like subpar\ncapabilities in multilingual parsing and diminished recall in industrial\ncontexts in rich text and rich visuals. In this work, we introduce a simple but\neffective \\textbf{M}ultimodal and \\textbf{M}ultilingual semi-structured\n\\textbf{FORM} \\textbf{PARSER} (\\textbf{XFormParser}), which anchored on a\ncomprehensive Transformer-based pre-trained language model and innovatively\namalgamates semantic entity recognition (SER) and relation extraction (RE) into\na unified framework. Combined with Bi-LSTM, the performance of multilingual\nparsing is significantly improved. Furthermore, we develop InDFormSFT, a\npioneering supervised fine-tuning (SFT) industrial dataset that specifically\naddresses the parsing needs of forms in various industrial contexts.\nXFormParser has demonstrated its unparalleled effectiveness and robustness\nthrough rigorous testing on established benchmarks. Compared to existing\nstate-of-the-art (SOTA) models, XFormParser notably achieves up to 1.79\\% F1\nscore improvement on RE tasks in language-specific settings. It also exhibits\nexceptional cross-task performance improvements in multilingual and zero-shot\nsettings. The codes, datasets, and pre-trained models are publicly available at\nhttps://github.com/zhbuaa0/xformparser.\n","authors":["Xianfu Cheng","Hang Zhang","Jian Yang","Xiang Li","Weixiao Zhou","Fei Liu","Kui Wu","Xiangyuan Guan","Tao Sun","Xianjie Wu","Tongliang Li","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2405.17336v2.pdf","comment":"15 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2412.14033v1","updated":"2024-12-18T16:52:38Z","published":"2024-12-18T16:52:38Z","title":"Hansel: Output Length Controlling Framework for Large Language Models","summary":"  Despite the great success of large language models (LLMs), efficiently\ncontrolling the length of the output sequence still remains a challenge. In\nthis paper, we propose Hansel, an efficient framework for length control in\nLLMs without affecting its generation ability. Hansel utilizes periodically\noutputted hidden special tokens to keep track of the remaining target length of\nthe output sequence. Together with techniques to avoid abrupt termination of\nthe output, this seemingly simple method proved to be efficient and versatile,\nwhile not harming the coherency and fluency of the generated text. The\nframework can be applied to any pre-trained LLMs during the finetuning stage of\nthe model, regardless of its original positional encoding method. We\ndemonstrate this by finetuning four different LLMs with Hansel and show that\nthe mean absolute error of the output sequence decreases significantly in every\nmodel and dataset compared to the prompt-based length control finetuning.\nMoreover, the framework showed a substantially improved ability to extrapolate\nto target lengths unseen during finetuning, such as long dialog responses or\nextremely short summaries. This indicates that the model learns the general\nmeans of length control, rather than learning to match output lengths to those\nseen during training.\n","authors":["Seoha Song","Junhyun Lee","Hyeonmok Ko"],"pdf_url":"https://arxiv.org/pdf/2412.14033v1.pdf","comment":"13 pages, 6 figures; accepted to AAAI-25"},{"id":"http://arxiv.org/abs/2402.19097v3","updated":"2024-12-18T16:30:58Z","published":"2024-02-29T12:25:45Z","title":"TEncDM: Understanding the Properties of the Diffusion Model in the Space\n  of Language Model Encodings","summary":"  This paper presents the Text Encoding Diffusion Model (TEncDM), a novel\napproach to diffusion modeling that operates in the space of pre-trained\nlanguage model encodings. In contrast to traditionally used embeddings,\nencodings integrate contextual information. In our approach, we also employ a\ntransformer-based decoder, specifically designed to incorporate context in the\ntoken prediction process. We conduct a comprehensive examination of the\ninfluence of the encoder, decoder, noise scheduler, and self-conditioning on\nzero-shot generation. Furthermore, we compare TEncDM with previous approaches\non three conditional text generation tasks: QQP, XSum, and Wiki-Auto. The\nresults show that TEncDM exhibits superior performance compared to existing\nnon-autoregressive diffusion models. Our code is available at\nhttps://github.com/M0RJIQUE/tencdm.\n","authors":["Alexander Shabalin","Viacheslav Meshchaninov","Egor Chimbulatov","Vladislav Lapikov","Roman Kim","Grigory Bartosh","Dmitry Molchanov","Sergey Markov","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2402.19097v3.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.14011v1","updated":"2024-12-18T16:29:45Z","published":"2024-12-18T16:29:45Z","title":"Towards an optimised evaluation of teachers' discourse: The case of\n  engaging messages","summary":"  Evaluating teachers' skills is crucial for enhancing education quality and\nstudent outcomes. Teacher discourse, significantly influencing student\nperformance, is a key component. However, coding this discourse can be\nlaborious. This study addresses this issue by introducing a new methodology for\noptimising the assessment of teacher discourse. The research consisted of two\nstudies, both within the framework of engaging messages used by secondary\neducation teachers. The first study involved training two large language models\non real-world examples from audio-recorded lessons over two academic years to\nidentify and classify the engaging messages from the lessons' transcripts. This\nresulted in sensitivities of 84.31% and 91.11%, and specificities of 97.69% and\n86.36% in identification and classification, respectively. The second study\napplied these models to transcripts of audio-recorded lessons from a third\nacademic year to examine the frequency and distribution of message types by\neducational level and moment of the academic year. Results showed teachers\npredominantly use messages emphasising engagement benefits, linked to improved\noutcomes, while one-third highlighted non-engagement disadvantages, associated\nwith increased anxiety. The use of engaging messages declined in Grade 12 and\ntowards the academic year's end. These findings suggest potential interventions\nto optimise engaging message use, enhancing teaching quality and student\noutcomes.\n","authors":["Samuel Falcon","Jaime Leon"],"pdf_url":"https://arxiv.org/pdf/2412.14011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18260v2","updated":"2024-12-18T16:28:19Z","published":"2024-11-27T11:58:34Z","title":"MetaphorShare: A Dynamic Collaborative Repository of Open Metaphor\n  Datasets","summary":"  The metaphor studies community has developed numerous valuable labelled\ncorpora in various languages over the years. Many of these resources are not\nonly unknown to the NLP community, but are also often not easily shared among\nthe researchers. Both in human sciences and in NLP, researchers could benefit\nfrom a centralised database of labelled resources, easily accessible and\nunified under an identical format. To facilitate this, we present\nMetaphorShare, a website to integrate metaphor datasets making them open and\naccessible. With this effort, our aim is to encourage researchers to share and\nupload more datasets in any language in order to facilitate metaphor studies\nand the development of future metaphor processing NLP systems. The website has\nfour main functionalities: upload, download, search and label metaphor\ndatasets. It is accessible at www.metaphorshare.com.\n","authors":["Joanne Boisson","Arif Mehmood","Jose Camacho-Collados"],"pdf_url":"https://arxiv.org/pdf/2411.18260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14009v1","updated":"2024-12-18T16:26:47Z","published":"2024-12-18T16:26:47Z","title":"Cognition Chain for Explainable Psychological Stress Detection on Social\n  Media","summary":"  Stress is a pervasive global health issue that can lead to severe mental\nhealth problems. Early detection offers timely intervention and prevention of\nstress-related disorders. The current early detection models perform \"black\nbox\" inference suffering from limited explainability and trust which blocks the\nreal-world clinical application. Thanks to the generative properties introduced\nby the Large Language Models (LLMs), the decision and the prediction from such\nmodels are semi-interpretable through the corresponding description. However,\nthe existing LLMs are mostly trained for general purposes without the guidance\nof psychological cognitive theory. To this end, we first highlight the\nimportance of prior theory with the observation of performance boosted by the\nchain-of-thoughts tailored for stress detection. This method termed Cognition\nChain explicates the generation of stress through a step-by-step cognitive\nperspective based on cognitive appraisal theory with a progress pipeline:\nStimulus $\\rightarrow$ Evaluation $\\rightarrow$ Reaction $\\rightarrow$ Stress\nState, guiding LLMs to provide comprehensive reasoning explanations. We further\nstudy the benefits brought by the proposed Cognition Chain format by utilising\nit as a synthetic dataset generation template for LLMs instruction-tuning and\nintroduce CogInstruct, an instruction-tuning dataset for stress detection. This\ndataset is developed using a three-stage self-reflective annotation pipeline\nthat enables LLMs to autonomously generate and refine instructional data. By\ninstruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable\nstress detection model. Evaluations demonstrate that CogLLM achieves\noutstanding performance while enhancing explainability. Our work contributes a\nnovel approach by integrating cognitive theories into LLM reasoning processes,\noffering a promising direction for future explainable AI research.\n","authors":["Xin Wang","Boyan Gao","Yi Dai","Lei Cao","Liang Zhao","Yibo Yang","David Clifton"],"pdf_url":"https://arxiv.org/pdf/2412.14009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14008v1","updated":"2024-12-18T16:24:20Z","published":"2024-12-18T16:24:20Z","title":"FarExStance: Explainable Stance Detection for Farsi","summary":"  We introduce FarExStance, a new dataset for explainable stance detection in\nFarsi. Each instance in this dataset contains a claim, the stance of an article\nor social media post towards that claim, and an extractive explanation which\nprovides evidence for the stance label. We compare the performance of a\nfine-tuned multilingual RoBERTa model to several large language models in\nzero-shot, few-shot, and parameter-efficient fine-tuned settings on our new\ndataset. On stance detection, the most accurate models are the fine-tuned\nRoBERTa model, the LLM Aya-23-8B which has been fine-tuned using\nparameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the\nquality of the explanations, our automatic evaluation metrics indicate that\nfew-shot GPT-4o generates the most coherent explanations, while our human\nevaluation reveals that the best Overall Explanation Score (OES) belongs to\nfew-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced\nexplanations most closely aligned with the reference explanations.\n","authors":["Majid Zarharan","Maryam Hashemi","Malika Behroozrazegh","Sauleh Eetemadi","Mohammad Taher Pilehvar","Jennifer Foster"],"pdf_url":"https://arxiv.org/pdf/2412.14008v1.pdf","comment":"Accepted in COLING 2025"},{"id":"http://arxiv.org/abs/2412.10924v2","updated":"2024-12-18T16:16:04Z","published":"2024-12-14T18:18:52Z","title":"Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning","summary":"  Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DM) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence.\n","authors":["Julia Witte Zimmerman","Denis Hudon","Kathryn Cramer","Alejandro J. Ruiz","Calla Beauregard","Ashley Fehr","Mikaela Irene Fudolig","Bradford Demarest","Yoshi Meke Bird","Milo Z. Trujillo","Christopher M. Danforth","Peter Sheridan Dodds"],"pdf_url":"https://arxiv.org/pdf/2412.10924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13989v1","updated":"2024-12-18T16:09:42Z","published":"2024-12-18T16:09:42Z","title":"What makes a good metric? Evaluating automatic metrics for text-to-image\n  consistency","summary":"  Language models are increasingly being incorporated as components in larger\nAI systems for various purposes, from prompt optimization to automatic\nevaluation. In this work, we analyze the construct validity of four recent,\ncommonly used methods for measuring text-to-image consistency - CLIPScore,\nTIFA, VPEval, and DSG - which rely on language models and/or VQA models as\ncomponents. We define construct validity for text-image consistency metrics as\na set of desiderata that text-image consistency metrics should have, and find\nthat no tested metric satisfies all of them. We find that metrics lack\nsufficient sensitivity to language and visual properties. Next, we find that\nTIFA, VPEval and DSG contribute novel information above and beyond CLIPScore,\nbut also that they correlate highly with each other. We also ablate different\naspects of the text-image consistency metrics and find that not all model\ncomponents are strictly necessary, also a symptom of insufficient sensitivity\nto visual information. Finally, we show that all three VQA-based metrics likely\nrely on familiar text shortcuts (such as yes-bias in QA) that call their\naptitude as quantitative evaluations of model performance into question.\n","authors":["Candace Ross","Melissa Hall","Adriana Romero Soriano","Adina Williams"],"pdf_url":"https://arxiv.org/pdf/2412.13989v1.pdf","comment":"Accepted and presented at COLM 2024"},{"id":"http://arxiv.org/abs/2407.02987v2","updated":"2024-12-18T16:07:28Z","published":"2024-07-03T10:38:40Z","title":"LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content\n  Moderation of Large Language Models","summary":"  Guardrails have emerged as an alternative to safety alignment for content\nmoderation of large language models (LLMs). Existing model-based guardrails\nhave not been designed for resource-constrained computational portable devices,\nsuch as mobile phones, more and more of which are running LLM-based\napplications locally. We introduce LoRA-Guard, a parameter-efficient guardrail\nadaptation method that relies on knowledge sharing between LLMs and guardrail\nmodels. LoRA-Guard extracts language features from the LLMs and adapts them for\nthe content moderation task using low-rank adapters, while a dual-path design\nprevents any performance degradation on the generative task. We show that\nLoRA-Guard outperforms existing approaches with 100-1000x lower parameter\noverhead while maintaining accuracy, enabling on-device content moderation.\n","authors":["Hayder Elesedy","Pedro M. Esperança","Silviu Vlad Oprea","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2407.02987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09043v3","updated":"2024-12-18T15:56:29Z","published":"2024-04-13T16:59:28Z","title":"Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large\n  Language Models for Behavioral Simulation","summary":"  With the rapid advancement of large language models (LLMs) for handling\ncomplex language tasks, an increasing number of studies are employing LLMs as\nagents to emulate the sequential decision-making processes of humans often\nrepresented as Markov decision-making processes (MDPs). The actions in MDPs\nadhere to specific probability distributions and require iterative sampling.\nThis arouses curiosity regarding the capacity of LLM agents to comprehend\nprobability distributions, thereby guiding the agent's behavioral\ndecision-making through probabilistic sampling and generating behavioral\nsequences. To answer the above question, we divide the problem into two main\naspects: sequence simulation with known probability distribution and sequence\nsimulation with unknown probability distribution. Our analysis indicates that\nLLM agents can understand probabilities, but they struggle with probability\nsampling. Their ability to perform probabilistic sampling can be improved to\nsome extent by integrating coding tools, but this level of sampling precision\nstill makes it difficult to simulate human behavior as agents.\n","authors":["Jia Gu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.09043v3.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025)"},{"id":"http://arxiv.org/abs/2412.11834v2","updated":"2024-12-18T15:53:26Z","published":"2024-12-16T14:56:28Z","title":"Wonderful Matrices: Combining for a More Efficient and Effective\n  Foundation Model Architecture","summary":"  In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.\n","authors":["Jingze Shi","Bingheng Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11834v2.pdf","comment":"The code is open-sourced at\n  https://github.com/LoserCheems/WonderfulMatrices"},{"id":"http://arxiv.org/abs/2412.13952v1","updated":"2024-12-18T15:32:27Z","published":"2024-12-18T15:32:27Z","title":"Prompting Strategies for Enabling Large Language Models to Infer\n  Causation from Correlation","summary":"  The reasoning abilities of Large Language Models (LLMs) are attracting\nincreasing attention. In this work, we focus on causal reasoning and address\nthe task of establishing causal relationships based on correlation information,\na highly challenging problem on which several LLMs have shown poor performance.\nWe introduce a prompting strategy for this problem that breaks the original\ntask into fixed subquestions, with each subquestion corresponding to one step\nof a formal causal discovery algorithm, the PC algorithm. The proposed\nprompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps,\nby sequentially prompting it with one subquestion at a time, augmenting the\nnext subquestion's prompt with the answer to the previous one(s). We evaluate\nour approach on an existing causal benchmark, Corr2Cause: our experiments\nindicate a performance improvement across five LLMs when comparing PC-SubQ to\nbaseline prompting strategies. Results are robust to causal query\nperturbations, when modifying the variable names or paraphrasing the\nexpressions.\n","authors":["Eleni Sgouritsa","Virginia Aglietti","Yee Whye Teh","Arnaud Doucet","Arthur Gretton","Silvia Chiappa"],"pdf_url":"https://arxiv.org/pdf/2412.13952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13949v1","updated":"2024-12-18T15:29:30Z","published":"2024-12-18T15:29:30Z","title":"Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence","summary":"  Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.\n","authors":["Jinghan He","Kuan Zhu","Haiyun Guo","Junfeng Fang","Zhenglin Hua","Yuheng Jia","Ming Tang","Tat-Seng Chua","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12445v2","updated":"2024-12-18T15:28:43Z","published":"2024-12-17T01:15:40Z","title":"Persona-SQ: A Personalized Suggested Question Generation Framework For\n  Real-world Documents","summary":"  Suggested questions (SQs) provide an effective initial interface for users to\nengage with their documents in AI-powered reading applications. In practical\nreading sessions, users have diverse backgrounds and reading goals, yet current\nSQ features typically ignore such user information, resulting in homogeneous or\nineffective questions. We introduce a pipeline that generates personalized SQs\nby incorporating reader profiles (professions and reading goals) and\ndemonstrate its utility in two ways: 1) as an improved SQ generation pipeline\nthat produces higher quality and more diverse questions compared to current\nbaselines, and 2) as a data generator to fine-tune extremely small models that\nperform competitively with much larger models on SQ generation. Our approach\ncan not only serve as a drop-in replacement in current SQ systems to\nimmediately improve their performance but also help develop on-device SQ models\nthat can run locally to deliver fast and private SQ experience.\n","authors":["Zihao Lin","Zichao Wang","Yuanting Pan","Varun Manjunatha","Ryan Rossi","Angela Lau","Lifu Huang","Tong Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12445v2.pdf","comment":"38 pages, 26 figures"},{"id":"http://arxiv.org/abs/2412.13942v1","updated":"2024-12-18T15:24:50Z","published":"2024-12-18T15:24:50Z","title":"A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies\n  for Human Explanations to Collect Label Distributions on NLI","summary":"  Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distribution. We further compare the resulting human\nwith model-generated explanations, and test automatic and human explanation\nselection. Our experiments show that LLM explanations are promising for NLI: to\nestimate HJD, generated explanations yield comparable results to human's when\nprovided with human labels. Importantly, our results generalize from datasets\nwith human explanations to i) datasets where they are not available and ii)\nchallenging out-of-distribution test sets.\n","authors":["Beiduo Chen","Siyao Peng","Anna Korhonen","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2412.13942v1.pdf","comment":"25 pages, 21 figures"},{"id":"http://arxiv.org/abs/2412.13924v1","updated":"2024-12-18T15:07:23Z","published":"2024-12-18T15:07:23Z","title":"Language verY Rare for All","summary":"  In the quest to overcome language barriers, encoder-decoder models like NLLB\nhave expanded machine translation to rare languages, with some models (e.g.,\nNLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform\nwell in translation, open LLMs prove highly competitive when fine-tuned for\nspecific tasks involving unknown corpora. We introduce LYRA (Language verY Rare\nfor All), a novel approach that combines open LLM fine-tuning,\nretrieval-augmented generation (RAG), and transfer learning from related\nhigh-resource languages. This study is exclusively focused on single-GPU\ntraining to facilitate ease of adoption. Our study focuses on two-way\ntranslation between French and Mon\\'egasque, a rare language unsupported by\nexisting translation tools due to limited corpus availability. Our results\ndemonstrate LYRA's effectiveness, frequently surpassing and consistently\nmatching state-of-the-art encoder-decoder models in rare language translation.\n","authors":["Ibrahim Merad","Amos Wolf","Ziad Mazzawi","Yannick Léo"],"pdf_url":"https://arxiv.org/pdf/2412.13924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13922v1","updated":"2024-12-18T15:05:59Z","published":"2024-12-18T15:05:59Z","title":"Pipeline Analysis for Developing Instruct LLMs in Low-Resource\n  Languages: A Case Study on Basque","summary":"  Large language models (LLMs) are typically optimized for resource-rich\nlanguages like English, exacerbating the gap between high-resource and\nunderrepresented languages. This work presents a detailed analysis of\nstrategies for developing a model capable of following instructions in a\nlow-resource language, specifically Basque, by focusing on three key stages:\npre-training, instruction tuning, and alignment with human preferences. Our\nfindings demonstrate that continual pre-training with a high-quality Basque\ncorpus of around 600 million words improves natural language understanding\n(NLU) of the foundational model by over 12 points. Moreover, instruction tuning\nand human preference alignment using automatically translated datasets proved\nhighly effective, resulting in a 24-point improvement in instruction-following\nperformance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct,\nestablish a new state-of-the-art for Basque in the sub-10B parameter category.\n","authors":["Ander Corral","Ixak Sarasua","Xabier Saralegi"],"pdf_url":"https://arxiv.org/pdf/2412.13922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01638v4","updated":"2024-12-18T15:01:32Z","published":"2024-06-03T00:27:29Z","title":"TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via\n  Cross-Modality Alignment","summary":"  Multivariate time series forecasting (MTSF) aims to learn temporal dynamics\namong variables to forecast future time series. Existing statistical and deep\nlearning-based methods suffer from limited learnable parameters and small-scale\ntraining data. Recently, large language models (LLMs) combining time series\nwith textual prompts have achieved promising performance in MTSF. However, we\ndiscovered that current LLM-based solutions fall short in learning disentangled\nembeddings. We introduce TimeCMA, an intuitive yet effective framework for MTSF\nvia cross-modality alignment. Specifically, we present a dual-modality encoding\nwith two branches: the time series encoding branch extracts disentangled yet\nweak time series embeddings, and the LLM-empowered encoding branch wraps the\nsame time series with text as prompts to obtain entangled yet robust prompt\nembeddings. As a result, such a cross-modality alignment retrieves both\ndisentangled and robust time series embeddings, ``the best of two worlds'',\nfrom the prompt embeddings based on time series and prompt modality\nsimilarities. As another key design, to reduce the computational costs from\ntime series with their length textual prompts, we design an effective prompt to\nencourage the most essential temporal information to be encapsulated in the\nlast token: only the last token is passed to downstream prediction. We further\nstore the last token embeddings to accelerate inference speed. Extensive\nexperiments on eight real datasets demonstrate that TimeCMA outperforms\nstate-of-the-arts.\n","authors":["Chenxi Liu","Qianxiong Xu","Hao Miao","Sun Yang","Lingzheng Zhang","Cheng Long","Ziyue Li","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.01638v4.pdf","comment":"Accepted by AAAI 2025 (Main Technical Track)"},{"id":"http://arxiv.org/abs/2205.06740v2","updated":"2024-12-18T14:41:53Z","published":"2022-05-13T16:19:21Z","title":"Towards Deployable OCR models for Indic languages","summary":"  Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/\n","authors":["Minesh Mathew","Ajoy Mondal","CV Jawahar"],"pdf_url":"https://arxiv.org/pdf/2205.06740v2.pdf","comment":"presented at ICPR 2024;\n  https://link.springer.com/chapter/10.1007/978-3-031-78495-8_11"},{"id":"http://arxiv.org/abs/2409.15380v3","updated":"2024-12-18T14:39:02Z","published":"2024-09-20T15:01:21Z","title":"Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino","summary":"  Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.\n","authors":["Jann Railey Montalan","Jian Gang Ngui","Wei Qi Leong","Yosephine Susanto","Hamsawardhini Rengarajan","Alham Fikri Aji","William Chandra Tjhi"],"pdf_url":"https://arxiv.org/pdf/2409.15380v3.pdf","comment":"Accepted for presentation at Paclic 38, 2024"},{"id":"http://arxiv.org/abs/2407.18416v3","updated":"2024-12-18T14:25:08Z","published":"2024-07-25T22:24:45Z","title":"PersonaGym: Evaluating Persona Agents and LLMs","summary":"  Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents.\n","authors":["Vinay Samuel","Henry Peng Zou","Yue Zhou","Shreyas Chaudhari","Ashwin Kalyan","Tanmay Rajpurohit","Ameet Deshpande","Karthik Narasimhan","Vishvak Murahari"],"pdf_url":"https://arxiv.org/pdf/2407.18416v3.pdf","comment":"21 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13881v1","updated":"2024-12-18T14:21:58Z","published":"2024-12-18T14:21:58Z","title":"Understanding and Analyzing Model Robustness and Knowledge-Transfer in\n  Multilingual Neural Machine Translation using TX-Ray","summary":"  Neural networks have demonstrated significant advancements in Neural Machine\nTranslation (NMT) compared to conventional phrase-based approaches. However,\nMultilingual Neural Machine Translation (MNMT) in extremely low-resource\nsettings remains underexplored. This research investigates how knowledge\ntransfer across languages can enhance MNMT in such scenarios. Using the Tatoeba\ntranslation challenge dataset from Helsinki NLP, we perform English-German,\nEnglish-French, and English-Spanish translations, leveraging minimal parallel\ndata to establish cross-lingual mappings. Unlike conventional methods relying\non extensive pre-training for specific language pairs, we pre-train our model\non English-English translations, setting English as the source language for all\ntasks. The model is fine-tuned on target language pairs using joint multi-task\nand sequential transfer learning strategies. Our work addresses three key\nquestions: (1) How can knowledge transfer across languages improve MNMT in\nextremely low-resource scenarios? (2) How does pruning neuron knowledge affect\nmodel generalization, robustness, and catastrophic forgetting? (3) How can\nTX-Ray interpret and quantify knowledge transfer in trained models? Evaluation\nusing BLEU-4 scores demonstrates that sequential transfer learning outperforms\nbaselines on a 40k parallel sentence corpus, showcasing its efficacy. However,\npruning neuron knowledge degrades performance, increases catastrophic\nforgetting, and fails to improve robustness or generalization. Our findings\nprovide valuable insights into the potential and limitations of knowledge\ntransfer and pruning in MNMT for extremely low-resource settings.\n","authors":["Vageesh Saxena","Sharid Loáiciga","Nils Rethmeier"],"pdf_url":"https://arxiv.org/pdf/2412.13881v1.pdf","comment":"103 pages, Master's thesis"},{"id":"http://arxiv.org/abs/2412.13879v1","updated":"2024-12-18T14:19:23Z","published":"2024-12-18T14:19:23Z","title":"Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under\n  Black-box Settings","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks. LLMs continue to be vulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services. However, prior works tend to focus\non performing white-box attacks, overlooking black-box settings. In this work,\nwe propose an automated algorithm designed for black-box LLMs, called\nAuto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack\nTree and optimizes the prompt node coverage to enhance effectiveness under\nblack-box conditions. Our method can bypass existing defense with enhanced\nstealthiness via semantic improvement of prompt nodes. Furthermore, we reveal\nthat implanting Length Trojan in Basic DoS Prompt aids in achieving higher\nattack efficacy. Experimental results show that AutoDoS amplifies service\nresponse latency by over 250 $\\times \\uparrow$, leading to severe resource\nconsumption in terms of GPU utilization and memory usage. Our code is available\nat \\url{https://github.com/shuita2333/AutoDoS}.\n","authors":["Yuanhe Zhang","Zhenhong Zhou","Wei Zhang","Xinyue Wang","Xiaojun Jia","Yang Liu","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2412.13879v1.pdf","comment":"20 pages, 7 figures, 11 tables"},{"id":"http://arxiv.org/abs/2412.06593v2","updated":"2024-12-18T14:08:45Z","published":"2024-12-09T15:45:03Z","title":"Anchoring Bias in Large Language Models: An Experimental Study","summary":"  Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient.\n","authors":["Jiaxu Lou","Yifan Sun"],"pdf_url":"https://arxiv.org/pdf/2412.06593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13862v1","updated":"2024-12-18T13:55:42Z","published":"2024-12-18T13:55:42Z","title":"Energy-Based Preference Model Offers Better Offline Alignment than the\n  Bradley-Terry Preference Model","summary":"  Since the debut of DPO, it has been shown that aligning a target LLM with\nhuman preferences via the KL-constrained RLHF loss is mathematically equivalent\nto a special kind of reward modeling task. Concretely, the task requires: 1)\nusing the target LLM to parameterize the reward model, and 2) tuning the reward\nmodel so that it has a 1:1 linear relationship with the true reward. However,\nwe identify a significant issue: the DPO loss might have multiple minimizers,\nof which only one satisfies the required linearity condition. The problem\narises from a well-known issue of the underlying Bradley-Terry preference\nmodel: it does not always have a unique maximum likelihood estimator (MLE).\nConsequently,the minimizer of the RLHF loss might be unattainable because it is\nmerely one among many minimizers of the DPO loss. As a better alternative, we\npropose an energy-based model (EBM) that always has a unique MLE, inherently\nsatisfying the linearity requirement. To approximate the MLE in practice, we\npropose a contrastive loss named Energy Preference Alignment (EPA), wherein\neach positive sample is contrasted against one or more strong negatives as well\nas many free weak negatives. Theoretical properties of our EBM enable the\napproximation error of EPA to almost surely vanish when a sufficient number of\nnegatives are used. Empirically, we demonstrate that EPA consistently delivers\nbetter performance on open benchmarks compared to DPO, thereby showing the\nsuperiority of our EBM.\n","authors":["Yuzhong Hong","Hanshan Zhang","Junwei Bao","Hongfei Jiang","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2412.13862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13860v1","updated":"2024-12-18T13:53:59Z","published":"2024-12-18T13:53:59Z","title":"Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation\n  on Nepali","summary":"  Continual learning has emerged as an important research direction due to the\ninfeasibility of retraining large language models (LLMs) from scratch in the\nevent of new data availability. Of great interest is the domain-adaptive\npre-training (DAPT) paradigm, which focuses on continually training a\npre-trained language model to adapt it to a domain it was not originally\ntrained on. In this work, we evaluate the feasibility of DAPT in a low-resource\nsetting, namely the Nepali language. We use synthetic data to continue training\nLlama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We\nevaluate the adapted model on its performance, forgetting, and knowledge\nacquisition. We compare the base model and the final model on their Nepali\ngeneration abilities, their performance on popular benchmarks, and run\ncase-studies to probe their linguistic knowledge in Nepali. We see some\nunsurprising forgetting in the final model, but also surprisingly find that\nincreasing the number of shots during evaluation yields better percent\nincreases in the final model (as high as 19.29% increase) compared to the base\nmodel (4.98%), suggesting latent retention. We also explore layer-head\nself-attention heatmaps to establish dependency resolution abilities of the\nfinal model in Nepali.\n","authors":["Sharad Duwal","Suraj Prasai","Suresh Manandhar"],"pdf_url":"https://arxiv.org/pdf/2412.13860v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.07682v3","updated":"2024-12-18T13:39:47Z","published":"2024-12-10T17:13:35Z","title":"TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation","summary":"  The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.\n","authors":["Alfredo Garrachón Ruiz","Tomás de la Rosa","Daniel Borrajo"],"pdf_url":"https://arxiv.org/pdf/2412.07682v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.13835v1","updated":"2024-12-18T13:25:11Z","published":"2024-12-18T13:25:11Z","title":"RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs","summary":"  Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes.\n","authors":["Alberto Testoni","Barbara Plank","Raquel Fernández"],"pdf_url":"https://arxiv.org/pdf/2412.13835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12559v2","updated":"2024-12-18T13:08:36Z","published":"2024-12-17T05:38:27Z","title":"EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation","summary":"  We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT\n","authors":["Taeho Hwang","Sukmin Cho","Soyeong Jeong","Hoyun Song","SeungYoon Han","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2412.12559v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.13147v2","updated":"2024-12-18T13:05:24Z","published":"2024-12-17T18:12:47Z","title":"Are Your LLMs Capable of Stable Reasoning?","summary":"  The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.\n","authors":["Junnan Liu","Hongwei Liu","Linchen Xiao","Ziyi Wang","Kuikun Liu","Songyang Gao","Wenwei Zhang","Songyang Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13147v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.13799v1","updated":"2024-12-18T12:45:55Z","published":"2024-12-18T12:45:55Z","title":"Enhancing Rhetorical Figure Annotation: An Ontology-Based Web\n  Application with RAG Integration","summary":"  Rhetorical figures play an important role in our communication. They are used\nto convey subtle, implicit meaning, or to emphasize statements. We notice them\nin hate speech, fake news, and propaganda. By improving the systems for\ncomputational detection of rhetorical figures, we can also improve tasks such\nas hate speech and fake news detection, sentiment analysis, opinion mining, or\nargument mining. Unfortunately, there is a lack of annotated data, as well as\nqualified annotators that would help us build large corpora to train machine\nlearning models for the detection of rhetorical figures. The situation is\nparticularly difficult in languages other than English, and for rhetorical\nfigures other than metaphor, sarcasm, and irony. To overcome this issue, we\ndevelop a web application called \"Find your Figure\" that facilitates the\nidentification and annotation of German rhetorical figures. The application is\nbased on the German Rhetorical ontology GRhOOT which we have specially adapted\nfor this purpose. In addition, we improve the user experience with Retrieval\nAugmented Generation (RAG). In this paper, we present the restructuring of the\nontology, the development of the web application, and the built-in RAG\npipeline. We also identify the optimal RAG settings for our application. Our\napproach is one of the first to practically use rhetorical ontologies in\ncombination with RAG and shows promising results.\n","authors":["Ramona Kühn","Jelena Mitrović","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2412.13799v1.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025)"},{"id":"http://arxiv.org/abs/2412.13794v1","updated":"2024-12-18T12:39:01Z","published":"2024-12-18T12:39:01Z","title":"MATCHED: Multimodal Authorship-Attribution To Combat Human Trafficking\n  in Escort-Advertisement Data","summary":"  Human trafficking (HT) remains a critical issue, with traffickers\nincreasingly leveraging online escort advertisements (ads) to advertise victims\nanonymously. Existing detection methods, including Authorship Attribution (AA),\noften center on text-based analyses and neglect the multimodal nature of online\nescort ads, which typically pair text with images. To address this gap, we\nintroduce MATCHED, a multimodal dataset of 27,619 unique text descriptions and\n55,115 unique images collected from the Backpage escort platform across seven\nU.S. cities in four geographical regions. Our study extensively benchmarks\ntext-only, vision-only, and multimodal baselines for vendor identification and\nverification tasks, employing multitask (joint) training objectives that\nachieve superior classification and retrieval performance on in-distribution\nand out-of-distribution (OOD) datasets. Integrating multimodal features further\nenhances this performance, capturing complementary patterns across text and\nimages. While text remains the dominant modality, visual data adds stylistic\ncues that enrich model performance. Moreover, text-image alignment strategies\nlike CLIP and BLIP2 struggle due to low semantic overlap and vague connections\nbetween the modalities of escort ads, with end-to-end multimodal training\nproving more robust. Our findings emphasize the potential of multimodal AA\n(MAA) to combat HT, providing LEAs with robust tools to link ads and disrupt\ntrafficking networks.\n","authors":["Vageesh Saxena","Benjamin Bashpole","Gijs Van Dijck","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2412.13794v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2412.13791v1","updated":"2024-12-18T12:33:50Z","published":"2024-12-18T12:33:50Z","title":"Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics\n  Problems with Large Language Models","summary":"  Physics problems constitute a significant aspect of reasoning, necessitating\ncomplicated reasoning ability and abundant physics knowledge. However, existing\nlarge language models (LLMs) frequently fail due to a lack of knowledge or\nincorrect knowledge application. To mitigate these issues, we propose Physics\nReasoner, a knowledge-augmented framework to solve physics problems with LLMs.\nSpecifically, the proposed framework constructs a comprehensive formula set to\nprovide explicit physics knowledge and utilizes checklists containing detailed\ninstructions to guide effective knowledge application. Namely, given a physics\nproblem, Physics Reasoner solves it through three stages: problem analysis,\nformula retrieval, and guided reasoning. During the process, checklists are\nemployed to enhance LLMs' self-improvement in the analysis and reasoning\nstages. Empirically, Physics Reasoner mitigates the issues of insufficient\nknowledge and incorrect application, achieving state-of-the-art performance on\nSciBench with an average accuracy improvement of 5.8%.\n","authors":["Xinyu Pang","Ruixin Hong","Zhanke Zhou","Fangrui Lv","Xinwei Yang","Zhilong Liang","Bo Han","Changshui Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13791v1.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2412.13788v1","updated":"2024-12-18T12:31:31Z","published":"2024-12-18T12:31:31Z","title":"Open Universal Arabic ASR Leaderboard","summary":"  In recent years, the enhanced capabilities of ASR models and the emergence of\nmulti-dialect datasets have increasingly pushed Arabic ASR model development\ntoward an all-dialect-in-one direction. This trend highlights the need for\nbenchmarking studies that evaluate model performance on multiple dialects,\nproviding the community with insights into models' generalization capabilities.\n  In this paper, we introduce Open Universal Arabic ASR Leaderboard, a\ncontinuous benchmark project for open-source general Arabic ASR models across\nvarious multi-dialect datasets. We also provide a comprehensive analysis of the\nmodel's robustness, speaker adaptation, inference efficiency, and memory\nconsumption. This work aims to offer the Arabic ASR community a reference for\nmodels' general performance and also establish a common evaluation framework\nfor multi-dialectal Arabic ASR models.\n","authors":["Yingzhi Wang","Anas Alhmoud","Muhammad Alqurishi"],"pdf_url":"https://arxiv.org/pdf/2412.13788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11704v2","updated":"2024-12-18T12:29:11Z","published":"2024-12-16T12:26:28Z","title":"Vocabulary Expansion of Chat Models with Unlabeled Target Language Data","summary":"  Chat models (i.e. language models trained to follow instructions through\nconversation with humans) outperform base models (i.e. trained solely on\nunlabeled data) in both conversation and general task-solving abilities. These\nmodels are generally English-centric and require further adaptation for\nlanguages that are underrepresented in or absent from their training data. A\ncommon technique for adapting base models is to extend the model's vocabulary\nwith target language tokens, i.e. vocabulary expansion (VE), and then\ncontinually pre-train it on language-specific data. Using chat data is ideal\nfor chat model adaptation, but often, either this does not exist or is costly\nto construct. Alternatively, adapting chat models with unlabeled data is a\npossible solution, but it could result in catastrophic forgetting. In this\npaper, we investigate the impact of using unlabeled target language data for VE\non chat models for the first time. We first show that off-the-shelf VE\ngenerally performs well across target language tasks and models in 71% of\ncases, though it underperforms in scenarios where source chat models are\nalready strong. To further improve adapted models, we propose post-hoc\ntechniques that inject information from the source model without requiring any\nfurther training. Experiments reveal the effectiveness of our methods, helping\nthe adapted models to achieve performance improvements in 87% of cases.\n","authors":["Atsuki Yamaguchi","Terufumi Morishita","Aline Villavicencio","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2412.11704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13782v1","updated":"2024-12-18T12:21:46Z","published":"2024-12-18T12:21:46Z","title":"Knowledge Editing with Dynamic Knowledge Graphs for Multi-hop Question\n  Answering","summary":"  Multi-hop question answering (MHQA) poses a significant challenge for large\nlanguage models (LLMs) due to the extensive knowledge demands involved.\nKnowledge editing, which aims to precisely modify the LLMs to incorporate\nspecific knowledge without negatively impacting other unrelated knowledge,\noffers a potential solution for addressing MHQA challenges with LLMs. However,\ncurrent solutions struggle to effectively resolve issues of knowledge\nconflicts. Most parameter-preserving editing methods are hindered by inaccurate\nretrieval and overlook secondary editing issues, which can introduce noise into\nthe reasoning process of LLMs. In this paper, we introduce KEDKG, a novel\nknowledge editing method that leverages a dynamic knowledge graph for MHQA,\ndesigned to ensure the reliability of answers. KEDKG involves two primary\nsteps: dynamic knowledge graph construction and knowledge graph augmented\ngeneration. Initially, KEDKG autonomously constructs a dynamic knowledge graph\nto store revised information while resolving potential knowledge conflicts.\nSubsequently, it employs a fine-grained retrieval strategy coupled with an\nentity and relation detector to enhance the accuracy of graph retrieval for LLM\ngeneration. Experimental results on benchmarks show that KEDKG surpasses\nprevious state-of-the-art models, delivering more accurate and reliable answers\nin environments with dynamic information.\n","authors":["Yifan Lu","Yigeng Zhou","Jing Li","Yequan Wang","Xuebo Liu","Daojing He","Fangming Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13782v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13781v1","updated":"2024-12-18T12:20:04Z","published":"2024-12-18T12:20:04Z","title":"Meta-Reflection: A Feedback-Free Reflection Learning Framework","summary":"  Despite the remarkable capabilities of large language models (LLMs) in\nnatural language understanding and reasoning, they often display undesirable\nbehaviors, such as generating hallucinations and unfaithful reasoning. A\nprevalent strategy to mitigate these issues is the use of reflection, which\nrefines responses through an iterative process. However, while promising,\nreflection heavily relies on high-quality external feedback and requires\niterative multi-agent inference processes, thus hindering its practical\napplication. In this paper, we propose Meta-Reflection, a novel feedback-free\nreflection mechanism that necessitates only a single inference pass without\nexternal feedback. Motivated by the human ability to remember and retrieve\nreflections from past experiences when encountering similar problems,\nMeta-Reflection integrates reflective insights into a codebook, allowing the\nhistorical insights to be stored, retrieved, and used to guide LLMs in\nproblem-solving. To thoroughly investigate and evaluate the practicality of\nMeta-Reflection in real-world scenarios, we introduce an industrial e-commerce\nbenchmark named E-commerce Customer Intent Detection (ECID). Extensive\nexperiments conducted on both public datasets and the ECID benchmark highlight\nthe effectiveness and efficiency of our proposed approach.\n","authors":["Yaoke Wang","Yun Zhu","Xintong Bao","Wenqiao Zhang","Suyang Dai","Kehan Chen","Wenqiang Li","Gang Huang","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.13781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13771v1","updated":"2024-12-18T12:07:58Z","published":"2024-12-18T12:07:58Z","title":"Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization","summary":"  Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.\n","authors":["Guanghan Li","Xun Zhang","Yufei Zhang","Yifan Yin","Guojun Yin","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13771v1.pdf","comment":"7 pages, 3 figures, AAAI 2025"},{"id":"http://arxiv.org/abs/2408.05200v3","updated":"2024-12-18T12:07:27Z","published":"2024-08-09T17:44:45Z","title":"KlF: Knowledge Localization and Fusion for Language Model Continual\n  Learning","summary":"  Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Localization and Fusion (KlF),\nwhich boosts knowledge transfer without depending on memory replay. KlF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KlF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KlF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KlF and\nits variants across different settings.\n","authors":["Yujie Feng","Xu Chu","Yongxin Xu","Zexin Lu","Bo Liu","Philip S. Yu","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2408.05200v3.pdf","comment":"This version updates the model name from Task Skill Localization and\n  Consolidation (TaSL) to Knowledge Localization and Fusion (KlF). It is an\n  extension of the ACL 2024 paper titled Continual Dialog State Tracking via\n  Task Skill Localization and Consolidation"},{"id":"http://arxiv.org/abs/2412.13765v1","updated":"2024-12-18T12:01:53Z","published":"2024-12-18T12:01:53Z","title":"LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms","summary":"  Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned LLMs, including AraBERT,\nTXLM-RoBERTa, LLama 3B and Gemma 9B from Ollama, using human-annotated\nsentiment datasets to enhance prediction accuracy.\n","authors":["Ali Hamdi","Ahmed Abdelmoneim Mazrou","Mohamed Shaltout"],"pdf_url":"https://arxiv.org/pdf/2412.13765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09325v2","updated":"2024-12-18T11:56:33Z","published":"2024-06-13T17:02:32Z","title":"REVS: Unlearning Sensitive Information in Language Models via Rank\n  Editing in the Vocabulary Space","summary":"  Language models (LMs) risk inadvertently memorizing and divulging sensitive\nor personally identifiable information (PII) seen in training data, causing\nprivacy concerns. Current approaches to address this issue involve costly\ndataset scrubbing, or model filtering through unlearning and model editing,\nwhich can be bypassed through extraction attacks. We propose REVS, a novel\nnon-gradient-based method for unlearning sensitive information from LMs. REVS\nidentifies and modifies a small subset of neurons relevant for constituent\ntokens which form sensitive information. To adequately evaluate our method on\ntruly sensitive information, we curate two datasets: an email dataset naturally\nmemorized by Llama-3-8B and GPT-J-6B, and a synthetic social security number\ndataset that we tune the models to memorize. Compared to other methods, REVS\ndemonstrates superior performance in unlearning sensitive information and\nrobustness to extraction attacks, while retaining underlying model integrity.\n","authors":["Tomer Ashuach","Martin Tutek","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2406.09325v2.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.13746v1","updated":"2024-12-18T11:28:05Z","published":"2024-12-18T11:28:05Z","title":"RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented\n  Generation for Preference Alignment","summary":"  Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.\n","authors":["Zhuoran Jin","Hongbang Yuan","Tianyi Men","Pengfei Cao","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.13746v1.pdf","comment":"26 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.13745v1","updated":"2024-12-18T11:26:51Z","published":"2024-12-18T11:26:51Z","title":"Learning Complex Word Embeddings in Classical and Quantum Spaces","summary":"  We present a variety of methods for training complex-valued word embeddings,\nbased on the classical Skip-gram model, with a straightforward adaptation\nsimply replacing the real-valued vectors with arbitrary vectors of complex\nnumbers. In a more \"physically-inspired\" approach, the vectors are produced by\nparameterised quantum circuits (PQCs), which are unitary transformations\nresulting in normalised vectors which have a probabilistic interpretation. We\ndevelop a complex-valued version of the highly optimised C code version of\nSkip-gram, which allows us to easily produce complex embeddings trained on a\n3.8B-word corpus for a vocabulary size of over 400k, for which we are then able\nto train a separate PQC for each word. We evaluate the complex embeddings on a\nset of standard similarity and relatedness datasets, for some models obtaining\nresults competitive with the classical baseline. We find that, while training\nthe PQCs directly tends to harm performance, the quantum word embeddings from\nthe two-stage process perform as well as the classical Skip-gram embeddings\nwith comparable numbers of parameters. This enables a highly scalable route to\nlearning embeddings in complex spaces which scales with the size of the\nvocabulary rather than the size of the training corpus. In summary, we\ndemonstrate how to produce a large set of high-quality word embeddings for use\nin complex-valued and quantum-inspired NLP models, and for exploring potential\nadvantage in quantum NLP models.\n","authors":["Carys Harvey","Stephen Clark","Douglas Brown","Konstantinos Meichanetzidis"],"pdf_url":"https://arxiv.org/pdf/2412.13745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13720v1","updated":"2024-12-18T11:00:58Z","published":"2024-12-18T11:00:58Z","title":"Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models","summary":"  This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities.\n","authors":["Jincheol Jung","Hongju Jeong","Eui-Nam Huh"],"pdf_url":"https://arxiv.org/pdf/2412.13720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13717v1","updated":"2024-12-18T10:55:58Z","published":"2024-12-18T10:55:58Z","title":"Towards Automatic Evaluation for Image Transcreation","summary":"  Beyond conventional paradigms of translating speech and text, recently, there\nhas been interest in automated transcreation of images to facilitate\nlocalization of visual content across different cultures. Attempts to define\nthis as a formal Machine Learning (ML) problem have been impeded by the lack of\nautomatic evaluation mechanisms, with previous work relying solely on human\nevaluation. In this paper, we seek to close this gap by proposing a suite of\nautomatic evaluation metrics inspired by machine translation (MT) metrics,\ncategorized into: a) Object-based, b) Embedding-based, and c) VLM-based.\nDrawing on theories from translation studies and real-world transcreation\npractices, we identify three critical dimensions of image transcreation:\ncultural relevance, semantic equivalence and visual similarity, and design our\nmetrics to evaluate systems along these axes. Our results show that proprietary\nVLMs best identify cultural relevance and semantic equivalence, while\nvision-encoder representations are adept at measuring visual similarity.\nMeta-evaluation across 7 countries shows our metrics agree strongly with human\nratings, with average segment-level correlations ranging from 0.55-0.87.\nFinally, through a discussion of the merits and demerits of each metric, we\noffer a robust framework for automated image transcreation evaluation, grounded\nin both theoretical foundations and practical application. Our code can be\nfound here: https://github.com/simran-khanuja/automatic-eval-transcreation\n","authors":["Simran Khanuja","Vivek Iyer","Claire He","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2412.13717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13705v1","updated":"2024-12-18T10:49:41Z","published":"2024-12-18T10:49:41Z","title":"Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation","summary":"  Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.\n","authors":["Minkyoung Kim","Yunha Kim","Hyeram Seo","Heejung Choi","Jiye Han","Gaeun Kee","Soyoung Ko","HyoJe Jung","Byeolhee Kim","Young-Hak Kim","Sanghyun Park","Tae Joon Jun"],"pdf_url":"https://arxiv.org/pdf/2412.13705v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13702v1","updated":"2024-12-18T10:45:24Z","published":"2024-12-18T10:45:24Z","title":"Typhoon 2: A Family of Open Text and Multimodal Thai Large Language\n  Models","summary":"  This paper introduces Typhoon 2, a series of text and multimodal large\nlanguage models optimized for the Thai language. The series includes models for\ntext, vision, and audio. Typhoon2-Text builds on state-of-the-art open models,\nsuch as Llama 3 and Qwen2, and we perform continual pre-training on a mixture\nof English and Thai data. We employ various post-training techniques to enhance\nThai language performance while preserving the base models' original\ncapabilities. We release text models across a range of sizes, from 1 to 70\nbillion parameters, available in both base and instruction-tuned variants.\nTyphoon2-Vision improves Thai document understanding while retaining general\nvisual capabilities, such as image captioning. Typhoon2-Audio introduces an\nend-to-end speech-to-speech model architecture capable of processing audio,\nspeech, and text inputs and generating both text and speech outputs\nsimultaneously.\n","authors":["Kunat Pipatanakul","Potsawee Manakul","Natapong Nitarach","Warit Sirichotedumrong","Surapon Nonesung","Teetouch Jaknamon","Parinthapat Pengpun","Pittawat Taveekitworachai","Adisai Na-Thalang","Sittipong Sripaisarnmongkol","Krisanapong Jirayoot","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2412.13702v1.pdf","comment":"technical report, 55 pages"},{"id":"http://arxiv.org/abs/2412.13698v1","updated":"2024-12-18T10:42:53Z","published":"2024-12-18T10:42:53Z","title":"Towards Efficient and Explainable Hate Speech Detection via Model\n  Distillation","summary":"  Automatic detection of hate and abusive language is essential to combat its\nonline spread. Moreover, recognising and explaining hate speech serves to\neducate people about its negative effects. However, most current detection\nmodels operate as black boxes, lacking interpretability and explainability. In\nthis context, Large Language Models (LLMs) have proven effective for hate\nspeech detection and to promote interpretability. Nevertheless, they are\ncomputationally costly to run. In this work, we propose distilling big language\nmodels by using Chain-of-Thought to extract explanations that support the hate\nspeech classification task. Having small language models for these tasks will\ncontribute to their use in operational settings. In this paper, we demonstrate\nthat distilled models deliver explanations of the same quality as larger models\nwhile surpassing them in classification performance. This dual capability,\nclassifying and explaining, advances hate speech detection making it more\naffordable, understandable and actionable.\n","authors":["Paloma Piot","Javier Parapar"],"pdf_url":"https://arxiv.org/pdf/2412.13698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13688v1","updated":"2024-12-18T10:26:29Z","published":"2024-12-18T10:26:29Z","title":"Discerning and Characterising Types of Competency Questions for\n  Ontologies","summary":"  Competency Questions (CQs) are widely used in ontology development by\nguiding, among others, the scoping and validation stages. However, very limited\nguidance exists for formulating CQs and assessing whether they are good CQs,\nleading to issues such as ambiguity and unusable formulations. To solve this,\none requires insight into the nature of CQs for ontologies and their\nconstituent parts, as well as which ones are not. We aim to contribute to such\ntheoretical foundations in this paper, which is informed by analysing\nquestions, their uses, and the myriad of ontology development tasks. This\nresulted in a first Model for Competency Questions, which comprises five main\ntypes of CQs, each with a different purpose: Scoping (SCQ), Validating (VCQ),\nFoundational (FCQ), Relationship (RCQ), and Metaproperty (MpCQ) questions. This\nmodel enhances the clarity of CQs and therewith aims to improve on the\neffectiveness of CQs in ontology development, thanks to their respective\nidentifiable distinct constituent elements. We illustrate and evaluate them\nwith a user story and demonstrate where which type can be used in ontology\ndevelopment tasks. To foster use and research, we created an annotated\nrepository of 438 CQs, the Repository of Ontology Competency QuestionS (ROCQS),\nincorporating an existing CQ dataset and new CQs and CQ templates, which\nfurther demonstrate distinctions among types of CQs.\n","authors":["C. Maria Keet","Zubeida Casmod Khan"],"pdf_url":"https://arxiv.org/pdf/2412.13688v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13682v1","updated":"2024-12-18T10:10:12Z","published":"2024-12-18T10:10:12Z","title":"ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning","summary":"  Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.\n","authors":["Jie-Jing Shao","Xiao-Wen Yang","Bo-Wen Zhang","Baizhi Chen","Wen-Da Wei","Lan-Zhe Guo","Yu-feng Li"],"pdf_url":"https://arxiv.org/pdf/2412.13682v1.pdf","comment":"Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel"},{"id":"http://arxiv.org/abs/2412.13678v1","updated":"2024-12-18T10:05:43Z","published":"2024-12-18T10:05:43Z","title":"Clio: Privacy-Preserving Insights into Real-World AI Use","summary":"  How are AI assistants being used in the real world? While model providers in\ntheory have a window into this impact via their users' data, both privacy\nconcerns and practical challenges have made analyzing this data difficult. To\naddress these issues, we present Clio (Claude insights and observations), a\nprivacy-preserving platform that uses AI assistants themselves to analyze and\nsurface aggregated usage patterns across millions of conversations, without the\nneed for human reviewers to read raw conversations. We validate this can be\ndone with a high degree of accuracy and privacy by conducting extensive\nevaluations. We demonstrate Clio's usefulness in two broad ways. First, we\nshare insights about how models are being used in the real world from one\nmillion Claude.ai Free and Pro conversations, ranging from providing advice on\nhairstyles to providing guidance on Git operations and concepts. We also\nidentify the most common high-level use cases on Claude.ai (coding, writing,\nand research tasks) as well as patterns that differ across languages (e.g.,\nconversations in Japanese discuss elder care and aging populations at\nhigher-than-typical rates). Second, we use Clio to make our systems safer by\nidentifying coordinated attempts to abuse our systems, monitoring for unknown\nunknowns during critical periods like launches of new capabilities or major\nworld events, and improving our existing monitoring systems. We also discuss\nthe limitations of our approach, as well as risks and ethical concerns. By\nenabling analysis of real-world AI usage, Clio provides a scalable platform for\nempirically grounded AI safety and governance.\n","authors":["Alex Tamkin","Miles McCain","Kunal Handa","Esin Durmus","Liane Lovitt","Ankur Rathi","Saffron Huang","Alfred Mountfield","Jerry Hong","Stuart Ritchie","Michael Stern","Brian Clarke","Landon Goldberg","Theodore R. Sumers","Jared Mueller","William McEachen","Wes Mitchell","Shan Carter","Jack Clark","Jared Kaplan","Deep Ganguli"],"pdf_url":"https://arxiv.org/pdf/2412.13678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13670v1","updated":"2024-12-18T09:53:12Z","published":"2024-12-18T09:53:12Z","title":"AntiLeak-Bench: Preventing Data Contamination by Automatically\n  Constructing Benchmarks with Updated Real-World Knowledge","summary":"  Data contamination hinders fair LLM evaluation by introducing test data into\nnewer models' training sets. Existing studies solve this challenge by updating\nbenchmarks with newly collected data. However, they fail to guarantee\ncontamination-free evaluation as the newly collected data may contain\npre-existing knowledge, and their benchmark updates rely on intensive human\nlabor. To address these issues, we in this paper propose AntiLeak-Bench, an\nautomated anti-leakage benchmarking framework. Instead of simply using newly\ncollected data, we construct samples with explicitly new knowledge absent from\nLLMs' training sets, which thus ensures strictly contamination-free evaluation.\nWe further design a fully automated workflow to build and update our benchmark\nwithout human labor. This significantly reduces the cost of benchmark\nmaintenance to accommodate emerging LLMs. Through extensive experiments, we\nhighlight that data contamination likely exists before LLMs' cutoff time and\ndemonstrate AntiLeak-Bench effectively overcomes this challenge.\n","authors":["Xiaobao Wu","Liangming Pan","Yuxi Xie","Ruiwen Zhou","Shuai Zhao","Yubo Ma","Mingzhe Du","Rui Mao","Anh Tuan Luu","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13666v1","updated":"2024-12-18T09:48:53Z","published":"2024-12-18T09:48:53Z","title":"Evaluation of LLM Vulnerabilities to Being Misused for Personalized\n  Disinformation Generation","summary":"  The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts rises\nmany concerns regarding their misuse. Previous research has shown that LLMs can\nbe effectively misused for generating disinformation news articles following\npredefined narratives. Their capabilities to generate personalized (in various\naspects) content have also been evaluated and mostly found usable. However, a\ncombination of personalization and disinformation abilities of LLMs has not\nbeen comprehensively studied yet. Such a dangerous combination should trigger\nintegrated safety filters of the LLMs, if there are some. This study fills this\ngap by evaluation of vulnerabilities of recent open and closed LLMs, and their\nwillingness to generate personalized disinformation news articles in English.\nWe further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers.\n","authors":["Aneta Zugecova","Dominik Macko","Ivan Srba","Robert Moro","Jakub Kopal","Katarina Marcincinova","Matus Mesarcik"],"pdf_url":"https://arxiv.org/pdf/2412.13666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13035v3","updated":"2024-12-18T09:48:32Z","published":"2024-09-19T18:11:59Z","title":"TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement\n  Learning","summary":"  The increasing prevalence of large language models (LLMs) such as GPT-4 in\nvarious applications has led to a surge in the size of prompts required for\noptimal performance, leading to challenges in computational efficiency. Prompt\ncompression aims to reduce the inference cost by minimizing input tokens\nwithout compromising on the task performance. However, existing prompt\ncompression techniques either rely on sub-optimal metrics such as information\nentropy or model it as a task-agnostic token classification problem that fails\nto capture task-specific information. To address these issues, we propose a\nnovel and efficient reinforcement learning (RL) based task-aware prompt\ncompression method. To ensure low latency requirements, we leverage existing\nTransformer encoder-based token classification model while guiding the learning\nprocess with task-specific reward signals using lightweight REINFORCE\nalgorithm. We evaluate the performance of our method on three diverse and\nchallenging tasks including text summarization, question answering and code\nsummarization. We demonstrate that our RL-guided compression method improves\nthe task performance by 8% - 189% across these three scenarios over\nstate-of-the-art compression techniques while satisfying the same compression\nrate and latency requirements.\n","authors":["Shivam Shandilya","Menglin Xia","Supriyo Ghosh","Huiqiang Jiang","Jue Zhang","Qianhui Wu","Victor Rühle"],"pdf_url":"https://arxiv.org/pdf/2409.13035v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05342v2","updated":"2024-12-18T09:47:53Z","published":"2024-12-06T09:33:47Z","title":"Multi-Party Supervised Fine-tuning of Language Models for Multi-Party\n  Dialogue Generation","summary":"  Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe.\n","authors":["Xiaoyu Wang","Ningyuan Xi","Teng Chen","Qingqing Gu","Yue Zhao","Xiaokai Chen","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2412.05342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13663v1","updated":"2024-12-18T09:39:44Z","published":"2024-12-18T09:39:44Z","title":"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast, Memory Efficient, and Long Context Finetuning and Inference","summary":"  Encoder-only transformer models such as BERT offer a great performance-size\ntradeoff for retrieval and classification tasks with respect to larger\ndecoder-only models. Despite being the workhorse of numerous production\npipelines, there have been limited Pareto improvements to BERT since its\nrelease. In this paper, we introduce ModernBERT, bringing modern model\noptimizations to encoder-only models and representing a major Pareto\nimprovement over older encoders. Trained on 2 trillion tokens with a native\n8192 sequence length, ModernBERT models exhibit state-of-the-art results on a\nlarge pool of evaluations encompassing diverse classification tasks and both\nsingle and multi-vector retrieval on different domains (including code). In\naddition to strong downstream performance, ModernBERT is also the most speed\nand memory efficient encoder and is designed for inference on common GPUs.\n","authors":["Benjamin Warner","Antoine Chaffin","Benjamin Clavié","Orion Weller","Oskar Hallström","Said Taghadouini","Alexis Gallagher","Raja Biswas","Faisal Ladhak","Tom Aarsen","Nathan Cooper","Griffin Adams","Jeremy Howard","Iacopo Poli"],"pdf_url":"https://arxiv.org/pdf/2412.13663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13660v1","updated":"2024-12-18T09:38:43Z","published":"2024-12-18T09:38:43Z","title":"PsyDT: Using LLMs to Construct the Digital Twin of Psychological\n  Counselor with Personalized Counseling Style for Psychological Counseling","summary":"  Currently, large language models (LLMs) have made significant progress in the\nfield of psychological counseling. However, existing mental health LLMs\noverlook a critical issue where they do not consider the fact that different\npsychological counselors exhibit different personal styles, including\nlinguistic style and therapy techniques, etc. As a result, these LLMs fail to\nsatisfy the individual needs of clients who seek different counseling styles.\nTo help bridge this gap, we propose PsyDT, a novel framework using LLMs to\nconstruct the Digital Twin of Psychological counselor with personalized\ncounseling style. Compared to the time-consuming and costly approach of\ncollecting a large number of real-world counseling cases to create a specific\ncounselor's digital twin, our framework offers a faster and more cost-effective\nsolution. To construct PsyDT, we utilize dynamic one-shot learning by using\nGPT-4 to capture counselor's unique counseling style, mainly focusing on\nlinguistic style and therapy techniques. Subsequently, using existing\nsingle-turn long-text dialogues with client's questions, GPT-4 is guided to\nsynthesize multi-turn dialogues of specific counselor. Finally, we fine-tune\nthe LLMs on the synthetic dataset, PsyDTCorpus, to achieve the digital twin of\npsychological counselor with personalized counseling style. Experimental\nresults indicate that our proposed PsyDT framework can synthesize multi-turn\ndialogues that closely resemble real-world counseling cases and demonstrate\nbetter performance compared to other baselines, thereby show that our framework\ncan effectively construct the digital twin of psychological counselor with a\nspecific counseling style.\n","authors":["Haojie Xie","Yirong Chen","Xiaofen Xing","Jingkai Lin","Xiangmin Xu"],"pdf_url":"https://arxiv.org/pdf/2412.13660v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13649v1","updated":"2024-12-18T09:27:33Z","published":"2024-12-18T09:27:33Z","title":"SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation","summary":"  Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.\n","authors":["Jialong Wu","Zhenglin Wang","Linhai Zhang","Yilong Lai","Yulan He","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13649v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.13647v1","updated":"2024-12-18T09:23:12Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13645v1","updated":"2024-12-18T09:22:08Z","published":"2024-12-18T09:22:08Z","title":"On the Role of Model Prior in Real-World Inductive Reasoning","summary":"  Large Language Models (LLMs) show impressive inductive reasoning\ncapabilities, enabling them to generate hypotheses that could generalize\neffectively to new instances when guided by in-context demonstrations. However,\nin real-world applications, LLMs' hypothesis generation is not solely\ndetermined by these demonstrations but is significantly shaped by task-specific\nmodel priors. Despite their critical influence, the distinct contributions of\nmodel priors versus demonstrations to hypothesis generation have been\nunderexplored. This study bridges this gap by systematically evaluating three\ninductive reasoning strategies across five real-world tasks with three LLMs.\nOur empirical findings reveal that, hypothesis generation is primarily driven\nby the model's inherent priors; removing demonstrations results in minimal loss\nof hypothesis quality and downstream usage. Further analysis shows the result\nis consistent across various label formats with different label configurations,\nand prior is hard to override, even under flipped labeling. These insights\nadvance our understanding of the dynamics of hypothesis generation in LLMs and\nhighlight the potential for better utilizing model priors in real-world\ninductive reasoning tasks.\n","authors":["Zhuo Liu","Ding Yu","Hangfeng He"],"pdf_url":"https://arxiv.org/pdf/2412.13645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13631v1","updated":"2024-12-18T09:06:48Z","published":"2024-12-18T09:06:48Z","title":"Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning","summary":"  Theory of Mind (ToM) capabilities in LLMs have recently become a central\nobject of investigation. Cognitive science distinguishes between two steps\nrequired for ToM tasks: 1) determine whether to invoke ToM, which includes the\nappropriate Depth of Mentalizing (DoM), or level of recursion required to\ncomplete a task; and 2) applying the correct inference given the DoM. In this\nposition paper, we first identify several lines of work in different\ncommunities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and\nformal models for ToM. We argue that recent work in AI tends to focus\nexclusively on the second step which are typically framed as static logic\nproblems. We conclude with suggestions for improved evaluation of ToM\ncapabilities inspired by dynamic environments used in cognitive tasks.\n","authors":["Eitan Wagner","Nitay Alon","Joseph M. Barnby","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2412.13631v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13626v1","updated":"2024-12-18T09:04:55Z","published":"2024-12-18T09:04:55Z","title":"LIFT: Improving Long Context Understanding Through Long Input\n  Fine-Tuning","summary":"  Long context understanding remains challenging for large language models due\nto their limited context windows. This paper introduces Long Input Fine-Tuning\n(LIFT) for long context modeling, a novel framework that enhances LLM\nperformance on long-context tasks by adapting model parameters to the context\nat test time. LIFT enables efficient processing of lengthy inputs without the\ncomputational burden of offline long-context adaptation, and can improve the\nlong-context capabilities of arbitrary short-context models. The framework is\nfurther enhanced by integrating in-context learning and pre-LIFT supervised\nfine-tuning. The combination of in-context learning and LIFT enables\nshort-context models like Llama 3 to handle arbitrarily long contexts and\nconsistently improves their performance on popular long-context benchmarks like\nLooGLE and LongBench. We also provide a comprehensive analysis of the strengths\nand limitations of LIFT on long context understanding, offering valuable\ndirections for future research.\n","authors":["Yansheng Mao","Jiaqi Li","Fanxu Meng","Jing Xiong","Zilong Zheng","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02329v2","updated":"2024-12-18T08:56:43Z","published":"2024-06-04T13:58:28Z","title":"On Affine Homotopy between Language Encoders","summary":"  Pre-trained language encoders -- functions that represent text as vectors --\nare an integral component of many NLP tasks. We tackle a natural question in\nlanguage encoder analysis: What does it mean for two encoders to be similar? We\ncontend that a faithful measure of similarity needs to be \\emph{intrinsic},\nthat is, task-independent, yet still be informative of \\emph{extrinsic}\nsimilarity -- the performance on downstream tasks. It is common to consider two\nencoders similar if they are \\emph{homotopic}, i.e., if they can be aligned\nthrough some transformation. In this spirit, we study the properties of\n\\emph{affine} alignment of language encoders and its implications on extrinsic\nsimilarity. We find that while affine alignment is fundamentally an asymmetric\nnotion of similarity, it is still informative of extrinsic similarity. We\nconfirm this on datasets of natural language representations. Beyond providing\nuseful bounds on extrinsic similarity, affine intrinsic similarity also allows\nus to begin uncovering the structure of the space of pre-trained encoders by\ndefining an order over them.\n","authors":["Robin SM Chan","Reda Boumasmoud","Anej Svete","Yuxin Ren","Qipeng Guo","Zhijing Jin","Shauli Ravfogel","Mrinmaya Sachan","Bernhard Schölkopf","Mennatallah El-Assady","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.02329v2.pdf","comment":"10 pages, Accepted at NeurIPS 2024 (Main)"},{"id":"http://arxiv.org/abs/2412.13614v1","updated":"2024-12-18T08:49:01Z","published":"2024-12-18T08:49:01Z","title":"Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity\n  Linking","summary":"  Visual Entity Linking (VEL) is a crucial task for achieving fine-grained\nvisual understanding, matching objects within images (visual mentions) to\nentities in a knowledge base. Previous VEL tasks rely on textual inputs, but\nwriting queries for complex scenes can be challenging. Visual inputs like\nclicks or bounding boxes offer a more convenient alternative. Therefore, we\npropose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses\npixel masks from visual inputs to refer to objects, supplementing reference\nmethods for VEL. To facilitate research on this task, we have constructed the\nMaskOVEN-Wiki dataset through an entirely automatic reverse region-entity\nannotation framework. This dataset contains over 5 million annotations aligning\npixel-level regions with entity-level labels, which will advance visual\nunderstanding towards fine-grained. Moreover, as pixel masks correspond to\nsemantic regions in an image, we enhance previous patch-interacted attention\nwith region-interacted attention by a visual semantic tokenization approach.\nManual evaluation results indicate that the reverse annotation framework\nachieved a 94.8% annotation success rate. Experimental results show that models\ntrained on this dataset improved accuracy by 18 points compared to zero-shot\nmodels. Additionally, the semantic tokenization method achieved a 5-point\naccuracy improvement over the trained baseline.\n","authors":["Zhengfei Xu","Sijia Zhao","Yanchao Hao","Xiaolong Liu","Lili Li","Yuyang Yin","Bo Li","Xi Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2412.13614v1.pdf","comment":"AAAI 2025;Dataset are released at\n  https://github.com/NP-NET-research/PL-VEL"},{"id":"http://arxiv.org/abs/2412.13612v1","updated":"2024-12-18T08:42:25Z","published":"2024-12-18T08:42:25Z","title":"Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models","summary":"  The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines.\n","authors":["Xuemei Tang","Xufeng Duan","Zhenguang G. Cai"],"pdf_url":"https://arxiv.org/pdf/2412.13612v1.pdf","comment":"12 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2405.16751v2","updated":"2024-12-18T08:38:06Z","published":"2024-05-27T01:47:14Z","title":"REVECA: Adaptive Planning and Trajectory-based Validation in Cooperative\n  Language Agents using Information Relevance and Relative Proximity","summary":"  We address the challenge of multi-agent cooperation, where agents achieve a\ncommon goal by cooperating with decentralized agents under complex partial\nobservations. Existing cooperative agent systems often struggle with\nefficiently processing continuously accumulating information, managing globally\nsuboptimal planning due to lack of consideration of collaborators, and\naddressing false planning caused by environmental changes introduced by other\ncollaborators. To overcome these challenges, we propose the RElevance,\nProximity, and Validation-Enhanced Cooperative Language Agent (REVECA), a novel\ncognitive architecture powered by GPT-4o-mini. REVECA enables efficient memory\nmanagement, optimal planning, and cost-effective prevention of false planning\nby leveraging Relevance Estimation, Adaptive Planning, and Trajectory-based\nValidation. Extensive experimental results demonstrate REVECA's superiority\nover existing methods across various benchmarks, while a user study reveals its\npotential for achieving trustworthy human-AI cooperation.\n","authors":["SeungWon Seo","SeongRae Noh","Junhyeok Lee","SooBin Lim","Won Hee Lee","HyeongYeop Kang"],"pdf_url":"https://arxiv.org/pdf/2405.16751v2.pdf","comment":"v2 is the AAAI'25 camera-ready version, including the appendix, which\n  has been enhanced based on the reviewers' comments"},{"id":"http://arxiv.org/abs/2412.13602v1","updated":"2024-12-18T08:32:53Z","published":"2024-12-18T08:32:53Z","title":"Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games","summary":"  Large Language Models (LLMs) are increasingly deployed in real-world\napplications that demand complex reasoning. To track progress, robust\nbenchmarks are required to evaluate their capabilities beyond superficial\npattern recognition. However, current LLM reasoning benchmarks often face\nchallenges such as insufficient interpretability, performance saturation or\ndata contamination. To address these challenges, we introduce GAMEBoT, a gaming\narena designed for rigorous and transparent assessment of LLM reasoning\ncapabilities. GAMEBoT decomposes complex reasoning in games into predefined\nmodular subproblems. This decomposition allows us to design a suite of\nChain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in\naddressing these subproblems before action selection. Furthermore, we develop a\nsuite of rule-based algorithms to generate ground truth for these subproblems,\nenabling rigorous validation of the LLMs' intermediate reasoning steps. This\napproach facilitates evaluation of both the quality of final actions and the\naccuracy of the underlying reasoning process. GAMEBoT also naturally alleviates\nthe risk of data contamination through dynamic games and head-to-head LLM\ncompetitions. We benchmark 17 prominent LLMs across eight games, encompassing\nvarious strategic abilities and game characteristics. Our results suggest that\nGAMEBoT presents a significant challenge, even when LLMs are provided with\ndetailed CoT prompts. Project page: \\url{https://visual-ai.github.io/gamebot}\n","authors":["Wenye Lin","Jonathan Roberts","Yunhan Yang","Samuel Albanie","Zongqing Lu","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.13602v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.13599v1","updated":"2024-12-18T08:31:26Z","published":"2024-12-18T08:31:26Z","title":"Unlocking the Potential of Weakly Labeled Data: A Co-Evolutionary\n  Learning Framework for Abnormality Detection and Report Generation","summary":"  Anatomical abnormality detection and report generation of chest X-ray (CXR)\nare two essential tasks in clinical practice. The former aims at localizing and\ncharacterizing cardiopulmonary radiological findings in CXRs, while the latter\nsummarizes the findings in a detailed report for further diagnosis and\ntreatment. Existing methods often focused on either task separately, ignoring\ntheir correlation. This work proposes a co-evolutionary abnormality detection\nand report generation (CoE-DG) framework. The framework utilizes both fully\nlabeled (with bounding box annotations and clinical reports) and weakly labeled\n(with reports only) data to achieve mutual promotion between the abnormality\ndetection and report generation tasks. Specifically, we introduce a\nbi-directional information interaction strategy with generator-guided\ninformation propagation (GIP) and detector-guided information propagation\n(DIP). For semi-supervised abnormality detection, GIP takes the informative\nfeature extracted by the generator as an auxiliary input to the detector and\nuses the generator's prediction to refine the detector's pseudo labels. We\nfurther propose an intra-image-modal self-adaptive non-maximum suppression\nmodule (SA-NMS). This module dynamically rectifies pseudo detection labels\ngenerated by the teacher detection model with high-confidence predictions by\nthe student.Inversely, for report generation, DIP takes the abnormalities'\ncategories and locations predicted by the detector as input and guidance for\nthe generator to improve the generated reports.\n","authors":["Jinghan Sun","Dong Wei","Zhe Xu","Donghuan Lu","Hong Liu","Hong Wang","Sotirios A. Tsaftaris","Steven McDonagh","Yefeng Zheng","Liansheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12545v2","updated":"2024-12-18T08:23:30Z","published":"2024-09-19T08:06:42Z","title":"Enhancing Knowledge Distillation of Large Language Models through\n  Efficient Multi-Modal Distribution Alignment","summary":"  Knowledge distillation (KD) is an effective model compression method that can\ntransfer the internal capabilities of large language models (LLMs) to smaller\nones. However, the multi-modal probability distribution predicted by teacher\nLLMs causes difficulties for student models to learn. In this paper, we first\ndemonstrate the importance of multi-modal distribution alignment with\nexperiments and then highlight the inefficiency of existing KD approaches in\nlearning multi-modal distributions. To address this problem, we propose Ranking\nLoss based Knowledge Distillation (RLKD), which encourages the consistency of\nthe ranking of peak predictions between the teacher and student models. By\nincorporating word-level ranking loss, we ensure excellent compatibility with\nexisting distillation objectives while fully leveraging the fine-grained\ninformation between different categories in peaks of two predicted\ndistribution. Experimental results demonstrate that our method enables the\nstudent model to better learn the multi-modal distributions of the teacher\nmodel, leading to a significant performance improvement in various downstream\ntasks.\n","authors":["Tianyu Peng","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.12545v2.pdf","comment":"Accepted by COLING 2025, 19 pages"},{"id":"http://arxiv.org/abs/2412.13582v1","updated":"2024-12-18T08:04:57Z","published":"2024-12-18T08:04:57Z","title":"EvoWiki: Evaluating LLMs on Evolving Knowledge","summary":"  Knowledge utilization is a critical aspect of LLMs, and understanding how\nthey adapt to evolving knowledge is essential for their effective deployment.\nHowever, existing benchmarks are predominantly static, failing to capture the\nevolving nature of LLMs and knowledge, leading to inaccuracies and\nvulnerabilities such as contamination. In this paper, we introduce EvoWiki, an\nevolving dataset designed to reflect knowledge evolution by categorizing\ninformation into stable, evolved, and uncharted states. EvoWiki is fully\nauto-updatable, enabling precise evaluation of continuously changing knowledge\nand newly released LLMs. Through experiments with Retrieval-Augmented\nGeneration (RAG) and Contunual Learning (CL), we evaluate how effectively LLMs\nadapt to evolving knowledge. Our results indicate that current models often\nstruggle with evolved knowledge, frequently providing outdated or incorrect\nresponses. Moreover, the dataset highlights a synergistic effect between RAG\nand CL, demonstrating their potential to better adapt to evolving knowledge.\nEvoWiki provides a robust benchmark for advancing future research on the\nknowledge evolution capabilities of large language models.\n","authors":["Wei Tang","Yixin Cao","Yang Deng","Jiahao Ying","Bo Wang","Yizhe Yang","Yuyue Zhao","Qi Zhang","Xuanjing Huang","Yugang Jiang","Yong Liao"],"pdf_url":"https://arxiv.org/pdf/2412.13582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13578v1","updated":"2024-12-18T07:57:18Z","published":"2024-12-18T07:57:18Z","title":"Socio-Culturally Aware Evaluation Framework for LLM-Based Content\n  Moderation","summary":"  With the growth of social media and large language models, content moderation\nhas become crucial. Many existing datasets lack adequate representation of\ndifferent groups, resulting in unreliable assessments. To tackle this, we\npropose a socio-culturally aware evaluation framework for LLM-driven content\nmoderation and introduce a scalable method for creating diverse datasets using\npersona-based generation. Our analysis reveals that these datasets provide\nbroader perspectives and pose greater challenges for LLMs than\ndiversity-focused generation methods without personas. This challenge is\nespecially pronounced in smaller LLMs, emphasizing the difficulties they\nencounter in moderating such diverse content.\n","authors":["Shanu Kumar","Gauri Kholkar","Saish Mendke","Anubhav Sadana","Parag Agrawal","Sandipan Dandapat"],"pdf_url":"https://arxiv.org/pdf/2412.13578v1.pdf","comment":"Accepted in SUMEval Workshop in COLING 2025"},{"id":"http://arxiv.org/abs/2412.13575v1","updated":"2024-12-18T07:50:54Z","published":"2024-12-18T07:50:54Z","title":"Generating Long-form Story Using Dynamic Hierarchical Outlining with\n  Memory-Enhancement","summary":"  Long-form story generation task aims to produce coherent and sufficiently\nlengthy text, essential for applications such as novel writingand interactive\nstorytelling. However, existing methods, including LLMs, rely on rigid outlines\nor lack macro-level planning, making it difficult to achieve both contextual\nconsistency and coherent plot development in long-form story generation. To\naddress this issues, we propose Dynamic Hierarchical Outlining with\nMemory-Enhancement long-form story generation method, named DOME, to generate\nthe long-form story with coherent content and plot. Specifically, the Dynamic\nHierarchical Outline(DHO) mechanism incorporates the novel writing theory into\noutline planning and fuses the plan and writing stages together, improving the\ncoherence of the plot by ensuring the plot completeness and adapting to the\nuncertainty during story generation. A Memory-Enhancement Module (MEM) based on\ntemporal knowledge graphs is introduced to store and access the generated\ncontent, reducing contextual conflicts and improving story coherence. Finally,\nwe propose a Temporal Conflict Analyzer leveraging temporal knowledge graphs to\nautomatically evaluate the contextual consistency of long-form story.\nExperiments demonstrate that DOME significantly improves the fluency,\ncoherence, and overall quality of generated long stories compared to\nstate-of-the-art methods.\n","authors":["Qianyue Wang","Jinwu Hu","Zhengping Li","Yufeng Wang","daiyuan li","Yu Hu","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2412.13575v1.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2407.15588v3","updated":"2024-12-18T07:36:29Z","published":"2024-07-22T12:25:48Z","title":"Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple\n  Matching with Entity and Relation Texts","summary":"  Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge. Existing methods,\nmostly supervised, face challenges in obtaining labeled entity pairs. To\naddress this, recent studies have shifted towards self-supervised and\nunsupervised frameworks. Despite their effectiveness, these approaches have\nlimitations: (1) Relation passing: mainly focusing on the entity while\nneglecting the semantic information of relations, (2) Isomorphic assumption:\nassuming isomorphism between source and target graphs, which leads to noise and\nreduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise\nin the textual features, especially when encountering inconsistent translations\nor Out-of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an\nunsupervised and robust cross-lingual EA pipeline that jointly performs\nEntity-level and Relation-level Alignment by neighbor triple matching strategy\nusing semantic textual features of relations and entities. Its refinement step\niteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification step\nexamines the entities' neighbor triples as the linearized text. This\nAlign-then-Verify pipeline rigorously assesses alignment results, achieving\nnear-perfect alignment even in the presence of noisy textual features of\nentities. Our extensive experiments demonstrate that the robustness and general\napplicability of ERAlign improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.\n","authors":["Soojin Yoon","Sungho Ko","Tongyoung Kim","SeongKu Kang","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15588v3.pdf","comment":"WSDM 2025"},{"id":"http://arxiv.org/abs/2412.13558v1","updated":"2024-12-18T07:19:48Z","published":"2024-12-18T07:19:48Z","title":"Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical\n  Imaging Interpretation","summary":"  Recent medical vision-language models (VLMs) have shown promise in 2D medical\nimage interpretation. However extending them to 3D medical imaging has been\nchallenging due to computational complexities and data scarcity. Although a few\nrecent VLMs specified for 3D medical imaging have emerged, all are limited to\nlearning volumetric representation of a 3D medical image as a set of\nsub-volumetric features. Such process introduces overly correlated\nrepresentations along the z-axis that neglect slice-specific clinical details,\nparticularly for 3D medical images where adjacent slices have low redundancy.\nTo address this limitation, we introduce MS-VLM that mimic radiologists'\nworkflow in 3D medical image interpretation. Specifically, radiologists analyze\n3D medical images by examining individual slices sequentially and synthesizing\ninformation across slices and views. Likewise, MS-VLM leverages self-supervised\n2D transformer encoders to learn a volumetric representation that capture\ninter-slice dependencies from a sequence of slice-specific features. Unbound by\nsub-volumetric patchification, MS-VLM is capable of obtaining useful volumetric\nrepresentations from 3D medical images with any slice length and from multiple\nimages acquired from different planes and phases. We evaluate MS-VLM on\npublicly available chest CT dataset CT-RATE and in-house rectal MRI dataset. In\nboth scenarios, MS-VLM surpasses existing methods in radiology report\ngeneration, producing more coherent and clinically relevant reports. These\nfindings highlight the potential of MS-VLM to advance 3D medical image\ninterpretation and improve the robustness of medical VLMs.\n","authors":["Changsun Lee","Sangjoon Park","Cheong-Il Shin","Woo Hee Choi","Hyun Jeong Park","Jeong Eun Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2412.13558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20584v3","updated":"2024-12-18T07:14:33Z","published":"2024-07-30T06:33:44Z","title":"Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training","summary":"  The remarkable success of Large Language Models (LLMs) relies heavily on\ntheir substantial scale, which poses significant challenges during model\ndeployment in terms of latency and memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often suffer from considerable performance degradation\non complex language understanding tasks, raising concerns about the feasibility\nof pruning in LLMs. To address this issue, we propose Adaptive Sparse Trainer\n(AST), a novel and efficient retraining framework tailored for semi-structured\nsparse models. AST enables models to learn optimal masks during the weight\nupdate process without incurring additional computational overhead.\nFurthermore, we demonstrate that incorporating knowledge distillation\nsignificantly improves retraining efficiency and enhances model performance\nunder fixed computational constraints. Additionally, a supplementary set of\nwell-initialized parameters is integrated to further augment the model's\nefficacy. AST achieves state-of-the-art performance with minimal training cost.\nWhen applied to the LLaMA2-7B model, AST reduces the perplexity and zero-shot\naccuracy gap between dense and 2:4 semi-structured sparse models to 0.6 and\n1.16%, respectively, utilizing less than 0.4% of the pretraining tokens and GPU\nhours. Our work demonstrates the feasibility of deploying semi-structured\nsparse LLMs and offers a promising alternative for achieving highly compressed\nmodels when combined with existing quantization techniques.\n","authors":["Weiyu Huang","Yuezhou Hu","Guohao Jian","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2407.20584v3.pdf","comment":"Accepted at AAAI25"},{"id":"http://arxiv.org/abs/2412.13102v2","updated":"2024-12-18T07:06:07Z","published":"2024-12-17T17:15:21Z","title":"AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark","summary":"  Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.\n","authors":["Jianlyu Chen","Nan Wang","Chaofan Li","Bo Wang","Shitao Xiao","Han Xiao","Hao Liao","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13102v2.pdf","comment":"31 pages, 6 figures; Update Table 5"},{"id":"http://arxiv.org/abs/2412.13549v1","updated":"2024-12-18T06:50:39Z","published":"2024-12-18T06:50:39Z","title":"EscapeBench: Pushing Language Models to Think Outside the Box","summary":"  Language model agents excel in long-session planning and reasoning, but\nexisting benchmarks primarily focus on goal-oriented tasks with explicit\nobjectives, neglecting creative adaptation in unfamiliar environments. To\naddress this, we introduce EscapeBench, a benchmark suite of room escape game\nenvironments designed to challenge agents with creative reasoning,\nunconventional tool use, and iterative problem-solving to uncover implicit\ngoals. Our results show that current LM models, despite employing working\nmemory and Chain-of-Thought reasoning, achieve only 15% average progress\nwithout hints, highlighting their limitations in creativity. To bridge this\ngap, we propose EscapeAgent, a framework designed to enhance creative reasoning\nthrough Foresight (innovative tool use) and Reflection (identifying unsolved\ntasks). Experiments show that EscapeAgent can execute action chains over 1,000\nsteps while maintaining logical coherence. It navigates and completes games\nwith up to 40% fewer steps and hints, performs robustly across varying\ndifficulty levels, and achieves higher action success rates with more efficient\nand innovative puzzle-solving strategies. All the data and codes are released.\n","authors":["Cheng Qian","Peixuan Han","Qinyu Luo","Bingxiang He","Xiusi Chen","Yuji Zhang","Hongyi Du","Jiarui Yao","Xiaocheng Yang","Denghui Zhang","Yunzhu Li","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2412.13549v1.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2412.13543v1","updated":"2024-12-18T06:43:06Z","published":"2024-12-18T06:43:06Z","title":"Query-centric Audio-Visual Cognition Network for Moment Retrieval,\n  Segmentation and Step-Captioning","summary":"  Video has emerged as a favored multimedia format on the internet. To better\ngain video contents, a new topic HIREST is presented, including video\nretrieval, moment retrieval, moment segmentation, and step-captioning. The\npioneering work chooses the pre-trained CLIP-based model for video retrieval,\nand leverages it as a feature extractor for other three challenging tasks\nsolved in a multi-task learning paradigm. Nevertheless, this work struggles to\nlearn the comprehensive cognition of user-preferred content, due to\ndisregarding the hierarchies and association relations across modalities. In\nthis paper, guided by the shallow-to-deep principle, we propose a query-centric\naudio-visual cognition (QUAG) network to construct a reliable multi-modal\nrepresentation for moment retrieval, segmentation and step-captioning.\nSpecifically, we first design the modality-synergistic perception to obtain\nrich audio-visual content, by modeling global contrastive alignment and local\nfine-grained interaction between visual and audio modalities. Then, we devise\nthe query-centric cognition that uses the deep-level query to perform the\ntemporal-channel filtration on the shallow-level audio-visual representation.\nThis can cognize user-preferred content and thus attain a query-centric\naudio-visual representation for three tasks. Extensive experiments show QUAG\nachieves the SOTA results on HIREST. Further, we test QUAG on the query-based\nvideo summarization task and verify its good generalization.\n","authors":["Yunbin Tu","Liang Li","Li Su","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13543v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13542v1","updated":"2024-12-18T06:42:19Z","published":"2024-12-18T06:42:19Z","title":"Multi-Granularity Open Intent Classification via Adaptive Granular-Ball\n  Decision Boundary","summary":"  Open intent classification is critical for the development of dialogue\nsystems, aiming to accurately classify known intents into their corresponding\nclasses while identifying unknown intents. Prior boundary-based methods assumed\nknown intents fit within compact spherical regions, focusing on coarse-grained\nrepresentation and precise spherical decision boundaries. However, these\nassumptions are often violated in practical scenarios, making it difficult to\ndistinguish known intent classes from unknowns using a single spherical\nboundary. To tackle these issues, we propose a Multi-granularity Open intent\nclassification method via adaptive Granular-Ball decision boundary (MOGB). Our\nMOGB method consists of two modules: representation learning and decision\nboundary acquiring. To effectively represent the intent distribution, we design\na hierarchical representation learning method. This involves iteratively\nalternating between adaptive granular-ball clustering and nearest sub-centroid\nclassification to capture fine-grained semantic structures within known intent\nclasses. Furthermore, multi-granularity decision boundaries are constructed for\nopen intent classification by employing granular-balls with varying centroids\nand radii. Extensive experiments conducted on three public datasets demonstrate\nthe effectiveness of our proposed method.\n","authors":["Yanhua Li","Xiaocao Ouyang","Chaofan Pan","Jie Zhang","Sen Zhao","Shuyin Xia","Xin Yang","Guoyin Wang","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2412.13542v1.pdf","comment":"This paper has been Accepted on AAAI2025"},{"id":"http://arxiv.org/abs/2412.13540v1","updated":"2024-12-18T06:35:18Z","published":"2024-12-18T06:35:18Z","title":"Benchmarking and Improving Large Vision-Language Models for Fundamental\n  Visual Graph Understanding and Reasoning","summary":"  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross diverse tasks. Despite great success, recent studies show that LVLMs\nencounter substantial limitations when engaging with visual graphs. To study\nthe reason behind these limitations, we propose VGCure, a comprehensive\nbenchmark covering 22 tasks for examining the fundamental graph understanding\nand reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs\nreveal that LVLMs are weak in basic graph understanding and reasoning tasks,\nparticularly those concerning relational or structurally complex information.\nBased on this observation, we propose a structure-aware fine-tuning framework\nto enhance LVLMs with structure learning abilities through 3 self-supervised\nlearning tasks. Experiments validate the effectiveness of our method in\nimproving LVLMs' zero-shot performance on fundamental graph learning tasks, as\nwell as enhancing the robustness of LVLMs against complex visual graphs.\n","authors":["Yingjie Zhu","Xuefeng Bai","Kehai Chen","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13536v1","updated":"2024-12-18T06:27:10Z","published":"2024-12-18T06:27:10Z","title":"MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained\n  with Simple Rules","summary":"  Recent studies have highlighted the limitations of large language models in\nmathematical reasoning, particularly their inability to capture the underlying\nlogic. Inspired by meta-learning, we propose that models should acquire not\nonly task-specific knowledge but also transferable problem-solving skills. We\nintroduce MetaRuleGPT, a novel Transformer-based architecture that performs\nprecise numerical calculations and complex logical operations by learning and\ncombining different rules. In contrast with traditional training sets, which\nare heavily composed of massive raw instance data, MetaRuleGPT is pre-trained\non much less abstract datasets containing basic, compound, and iterative rules\nfor mathematical reasoning. Extensive experimental results demonstrate\nMetaRuleGPT can mimic human's rule-following capabilities, break down\ncomplexity, and iteratively derive accurate results for complex mathematical\nproblems. These findings prove the potential of rule learning to enhance the\nnumerical reasoning abilities of language models.\n","authors":["Kejie Chen","Lin Wang","Qinghai Zhang","Renjun Xu"],"pdf_url":"https://arxiv.org/pdf/2412.13536v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13534v1","updated":"2024-12-18T06:21:21Z","published":"2024-12-18T06:21:21Z","title":"Information-Theoretic Generative Clustering of Documents","summary":"  We present {\\em generative clustering} (GC) for clustering a set of\ndocuments, $\\mathrm{X}$, by using texts $\\mathrm{Y}$ generated by large\nlanguage models (LLMs) instead of by clustering the original documents\n$\\mathrm{X}$. Because LLMs provide probability distributions, the similarity\nbetween two documents can be rigorously defined in an information-theoretic\nmanner by the KL divergence. We also propose a natural, novel clustering\nalgorithm by using importance sampling. We show that GC achieves the\nstate-of-the-art performance, outperforming any previous clustering method\noften by a large margin. Furthermore, we show an application to generative\ndocument retrieval in which documents are indexed via hierarchical clustering\nand our method improves the retrieval accuracy.\n","authors":["Xin Du","Kumiko Tanaka-Ishii"],"pdf_url":"https://arxiv.org/pdf/2412.13534v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12276v2","updated":"2024-12-18T06:02:03Z","published":"2024-12-16T19:00:18Z","title":"Emergence of Abstractions: Concept Encoding and Decoding Mechanism for\n  In-Context Learning in Transformers","summary":"  Humans distill complex experiences into fundamental abstractions that enable\nrapid learning and adaptation. Similarly, autoregressive transformers exhibit\nadaptive learning through in-context learning (ICL), which begs the question of\nhow. In this paper, we propose concept encoding-decoding mechanism to explain\nICL by studying how transformers form and use internal abstractions in their\nrepresentations. On synthetic ICL tasks, we analyze the training dynamics of a\nsmall transformer and report the coupled emergence of concept encoding and\ndecoding. As the model learns to encode different latent concepts (e.g.,\n``Finding the first noun in a sentence.\") into distinct, separable\nrepresentations, it concureently builds conditional decoding algorithms and\nimprove its ICL performance. We validate the existence of this mechanism across\npretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B).\nFurther, through mechanistic interventions and controlled finetuning, we\ndemonstrate that the quality of concept encoding is causally related and\npredictive of ICL performance. Our empirical insights shed light into better\nunderstanding the success and failure modes of large language models via their\nrepresentations.\n","authors":["Seungwook Han","Jinyeop Song","Jeff Gore","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2412.12276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13161v2","updated":"2024-12-18T05:51:58Z","published":"2024-12-17T18:39:10Z","title":"BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce","summary":"  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n","authors":["Mohammad Nazmush Shamael","Sabila Nawshin","Swakkhar Shatabda","Salekul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.13161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01965v2","updated":"2024-12-18T05:46:41Z","published":"2024-07-02T05:50:16Z","title":"AdaCQR: Enhancing Query Reformulation for Conversational Search via\n  Sparse and Dense Retrieval Alignment","summary":"  Conversational Query Reformulation (CQR) has significantly advanced in\naddressing the challenges of conversational search, particularly those stemming\nfrom the latent user intent and the need for historical context. Recent works\naimed to boost the performance of CRQ through alignment. However, they are\ndesigned for one specific retrieval system, which potentially results in poor\ngeneralization. To overcome this limitation, we present a novel framework\nAdaCQR. By aligning reformulation models with both term-based and\nsemantic-based retrieval systems, AdaCQR enhances the generalizability of\ninformation-seeking queries across diverse retrieval environments through a\ndual-phase training strategy. We also developed two effective approaches for\nacquiring superior labels and diverse input candidates, boosting the efficiency\nand robustness of the framework. Experimental evaluations on the TopiOCQA and\nQReCC datasets demonstrate that AdaCQR significantly outperforms existing\nmethods, offering both quantitative and qualitative improvements in\nconversational query reformulation.\n","authors":["Yilong Lai","Jialong Wu","Congzhi Zhang","Haowen Sun","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.01965v2.pdf","comment":"Accepted by COLING 2025"},{"id":"http://arxiv.org/abs/2412.13511v1","updated":"2024-12-18T05:22:33Z","published":"2024-12-18T05:22:33Z","title":"CEHA: A Dataset of Conflict Events in the Horn of Africa","summary":"  Natural Language Processing (NLP) of news articles can play an important role\nin understanding the dynamics and causes of violent conflict. Despite the\navailability of datasets categorizing various conflict events, the existing\nlabels often do not cover all of the fine-grained violent conflict event types\nrelevant to areas like the Horn of Africa. In this paper, we introduce a new\nbenchmark dataset Conflict Events in the Horn of Africa region (CEHA) and\npropose a new task for identifying violent conflict events using online\nresources with this dataset. The dataset consists of 500 English event\ndescriptions regarding conflict events in the Horn of Africa region with\nfine-grained event-type definitions that emphasize the cause of the conflict.\nThis dataset categorizes the key types of conflict risk according to specific\nareas required by stakeholders in the Humanitarian-Peace-Development Nexus.\nAdditionally, we conduct extensive experiments on two tasks supported by this\ndataset: Event-relevance Classification and Event-type Classification. Our\nbaseline models demonstrate the challenging nature of these tasks and the\nusefulness of our dataset for model evaluations in low-resource settings with\nlimited number of training data.\n","authors":["Rui Bai","Di Lu","Shihao Ran","Elizabeth Olson","Hemank Lamba","Aoife Cahill","Joel Tetreault","Alex Jaimes"],"pdf_url":"https://arxiv.org/pdf/2412.13511v1.pdf","comment":"Accepted by COLING 2025"},{"id":"http://arxiv.org/abs/2406.10522v2","updated":"2024-12-18T05:21:24Z","published":"2024-06-15T06:26:25Z","title":"Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for\n  Cartoon Captioning","summary":"  We present a novel multimodal preference dataset for creative tasks,\nconsisting of over 250 million human ratings on more than 2.2 million captions,\ncollected through crowdsourcing rating data for The New Yorker's weekly cartoon\ncaption contest over the past eight years. This unique dataset supports the\ndevelopment and evaluation of multimodal large language models and\npreference-based fine-tuning algorithms for humorous caption generation. We\npropose novel benchmarks for judging the quality of model-generated captions,\nutilizing both GPT4 and human judgments to establish ranking-based evaluation\nstrategies. Our experimental results highlight the limitations of current\nfine-tuning methods, such as RLHF and DPO, when applied to creative tasks.\nFurthermore, we demonstrate that even state-of-the-art models like GPT4 and\nClaude currently underperform top human contestants in generating humorous\ncaptions. As we conclude this extensive data collection effort, we release the\nentire preference dataset to the research community, fostering further\nadvancements in AI humor generation and evaluation.\n","authors":["Jifan Zhang","Lalit Jain","Yang Guo","Jiayi Chen","Kuan Lok Zhou","Siddharth Suresh","Andrew Wagenmaker","Scott Sievert","Timothy Rogers","Kevin Jamieson","Robert Mankoff","Robert Nowak"],"pdf_url":"https://arxiv.org/pdf/2406.10522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13510v1","updated":"2024-12-18T05:19:09Z","published":"2024-12-18T05:19:09Z","title":"Dynamic Adapter with Semantics Disentangling for Cross-lingual\n  Cross-modal Retrieval","summary":"  Existing cross-modal retrieval methods typically rely on large-scale\nvision-language pair data. This makes it challenging to efficiently develop a\ncross-modal retrieval model for under-resourced languages of interest.\nTherefore, Cross-lingual Cross-modal Retrieval (CCR), which aims to align\nvision and the low-resource language (the target language) without using any\nhuman-labeled target-language data, has gained increasing attention. As a\ngeneral parameter-efficient way, a common solution is to utilize adapter\nmodules to transfer the vision-language alignment ability of Vision-Language\nPretraining (VLP) models from a source language to a target language. However,\nthese adapters are usually static once learned, making it difficult to adapt to\ntarget-language captions with varied expressions. To alleviate it, we propose\nDynamic Adapter with Semantics Disentangling (DASD), whose parameters are\ndynamically generated conditioned on the characteristics of the input captions.\nConsidering that the semantics and expression styles of the input caption\nlargely influence how to encode it, we propose a semantic disentangling module\nto extract the semantic-related and semantic-agnostic features from the input,\nensuring that generated adapters are well-suited to the characteristics of\ninput caption. Extensive experiments on two image-text datasets and one\nvideo-text dataset demonstrate the effectiveness of our model for cross-lingual\ncross-modal retrieval, as well as its good compatibility with various VLP\nmodels.\n","authors":["Rui Cai","Zhiyu Dong","Jianfeng Dong","Xun Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13510v1.pdf","comment":"Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.12486v2","updated":"2024-12-18T05:08:39Z","published":"2024-12-17T02:43:54Z","title":"Boosting Long-Context Management via Query-Guided Activation Refilling","summary":"  Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.\n","authors":["Hongjin Qian","Zheng Liu","Peitian Zhang","Zhicheng Dou","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2412.12486v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.13503v1","updated":"2024-12-18T04:55:29Z","published":"2024-12-18T04:55:29Z","title":"VaeDiff-DocRE: End-to-end Data Augmentation Framework for Document-level\n  Relation Extraction","summary":"  Document-level Relation Extraction (DocRE) aims to identify relationships\nbetween entity pairs within a document. However, most existing methods assume a\nuniform label distribution, resulting in suboptimal performance on real-world,\nimbalanced datasets. To tackle this challenge, we propose a novel data\naugmentation approach using generative models to enhance data from the\nembedding space. Our method leverages the Variational Autoencoder (VAE)\narchitecture to capture all relation-wise distributions formed by entity pair\nrepresentations and augment data for underrepresented relations. To better\ncapture the multi-label nature of DocRE, we parameterize the VAE's latent space\nwith a Diffusion Model. Additionally, we introduce a hierarchical training\nframework to integrate the proposed VAE-based augmentation module into DocRE\nsystems. Experiments on two benchmark datasets demonstrate that our method\noutperforms state-of-the-art models, effectively addressing the long-tail\ndistribution problem in DocRE.\n","authors":["Khai Phan Tran","Wen Hua","Xue Li"],"pdf_url":"https://arxiv.org/pdf/2412.13503v1.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2412.13488v1","updated":"2024-12-18T04:14:35Z","published":"2024-12-18T04:14:35Z","title":"Refining Salience-Aware Sparse Fine-Tuning Strategies for Language\n  Models","summary":"  Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank\nadaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT\n(SPEFT), which introduces trainable sparse adaptations to the weight matrices\nin the model, offering greater flexibility in selecting fine-tuned parameters\ncompared to low-rank methods. We conduct the first systematic evaluation of\nsalience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify\nsimple gradient-based metrics is reliable, and results are on par with the best\nalternatives, offering both computational efficiency and robust performance.\nAdditionally, we compare static and dynamic masking strategies, finding that\nstatic masking, which predetermines non-zero entries before training, delivers\nefficiency without sacrificing performance, while dynamic masking offers no\nsubstantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT\nconsistently outperforms other fine-tuning methods for LLMs, providing a simple\nyet effective baseline for SPEFT. Our work challenges the notion that\ncomplexity is necessary for effective PEFT. Our work is open source and\navailable to the community at [https://github.com/0-ml/speft].\n","authors":["Xinxin Liu","Aaron Thomas","Cheng Zhang","Jianyi Cheng","Yiren Zhao","Xitong Gao"],"pdf_url":"https://arxiv.org/pdf/2412.13488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13486v1","updated":"2024-12-18T04:01:32Z","published":"2024-12-18T04:01:32Z","title":"T$^3$-S2S: Training-free Triplet Tuning for Sketch to Scene Generation","summary":"  Scene generation is crucial to many computer graphics applications. Recent\nadvances in generative AI have streamlined sketch-to-image workflows, easing\nthe workload for artists and designers in creating scene concept art. However,\nthese methods often struggle for complex scenes with multiple detailed objects,\nsometimes missing small or uncommon instances. In this paper, we propose a\nTraining-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation after\nreviewing the entire cross-attention mechanism. This scheme revitalizes the\nexisting ControlNet model, enabling effective handling of multi-instance\ngenerations, involving prompt balance, characteristics prominence, and dense\ntuning. Specifically, this approach enhances keyword representation via the\nprompt balance module, reducing the risk of missing critical instances. It also\nincludes a characteristics prominence module that highlights TopK indices in\neach channel, ensuring essential features are better represented based on token\nsketches. Additionally, it employs dense tuning to refine contour details in\nthe attention map, compensating for instance-related regions. Experiments\nvalidate that our triplet tuning approach substantially improves the\nperformance of existing sketch-to-image models. It consistently generates\ndetailed, multi-instance 2D images, closely adhering to the input prompts and\nenhancing visual quality in complex multi-instance scenes. Code is available at\nhttps://github.com/chaos-sun/t3s2s.git.\n","authors":["Zhenhong Sun","Yifu Wang","Yonhon Ng","Yunfei Duan","Daoyi Dong","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2412.13486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09818v2","updated":"2024-12-18T04:01:21Z","published":"2024-12-13T03:15:05Z","title":"MERaLiON-AudioLLM: Bridging Audio and Language with Large Language\n  Models","summary":"  We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning\nin One Network), the first speech-text model tailored for Singapore's\nmultilingual and multicultural landscape. Developed under the National Large\nLanguage Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates\nadvanced speech and text processing to address the diverse linguistic nuances\nof local accents and dialects, enhancing accessibility and usability in\ncomplex, multilingual environments. Our results demonstrate improvements in\nboth speech recognition and task-specific understanding, positioning\nMERaLiON-AudioLLM as a pioneering solution for region specific AI applications.\nWe envision this release to set a precedent for future models designed to\naddress localised linguistic and cultural contexts in a global framework.\n","authors":["Yingxu He","Zhuohan Liu","Shuo Sun","Bin Wang","Wenyu Zhang","Xunlong Zou","Nancy F. Chen","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2412.09818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13484v1","updated":"2024-12-18T04:00:18Z","published":"2024-12-18T04:00:18Z","title":"Curriculum Learning for Cross-Lingual Data-to-Text Generation With Noisy\n  Data","summary":"  Curriculum learning has been used to improve the quality of text generation\nsystems by ordering the training samples according to a particular schedule in\nvarious tasks. In the context of data-to-text generation (DTG), previous\nstudies used various difficulty criteria to order the training samples for\nmonolingual DTG. These criteria, however, do not generalize to the crosslingual\nvariant of the problem and do not account for noisy data. We explore multiple\ncriteria that can be used for improving the performance of cross-lingual DTG\nsystems with noisy data using two curriculum schedules. Using the alignment\nscore criterion for ordering samples and an annealing schedule to train the\nmodel, we show increase in BLEU score by up to 4 points, and improvements in\nfaithfulness and coverage of generations by 5-15% on average across 11 Indian\nlanguages and English in 2 separate datasets. We make code and data publicly\navailable\n","authors":["Kancharla Aditya Hari","Manish Gupta","Vasudeva Varma"],"pdf_url":"https://arxiv.org/pdf/2412.13484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13475v1","updated":"2024-12-18T03:39:42Z","published":"2024-12-18T03:39:42Z","title":"A Statistical and Multi-Perspective Revisiting of the Membership\n  Inference Attack in Large Language Models","summary":"  The lack of data transparency in Large Language Models (LLMs) has highlighted\nthe importance of Membership Inference Attack (MIA), which differentiates\ntrained (member) and untrained (non-member) data. Though it shows success in\nprevious studies, recent research reported a near-random performance in\ndifferent settings, highlighting a significant performance inconsistency. We\nassume that a single setting doesn't represent the distribution of the vast\ncorpora, causing members and non-members with different distributions to be\nsampled and causing inconsistency. In this study, instead of a single setting,\nwe statistically revisit MIA methods from various settings with thousands of\nexperiments for each MIA method, along with study in text feature, embedding,\nthreshold decision, and decoding dynamics of members and non-members. We found\nthat (1) MIA performance improves with model size and varies with domains,\nwhile most methods do not statistically outperform baselines, (2) Though MIA\nperformance is generally low, a notable amount of differentiable member and\nnon-member outliers exists and vary across MIA methods, (3) Deciding a\nthreshold to separate members and non-members is an overlooked challenge, (4)\nText dissimilarity and long text benefit MIA performance, (5) Differentiable or\nnot is reflected in the LLM embedding, (6) Member and non-members show\ndifferent decoding dynamics.\n","authors":["Bowen Chen","Namgi Han","Yusuke Miyao"],"pdf_url":"https://arxiv.org/pdf/2412.13475v1.pdf","comment":"main content 8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13471v1","updated":"2024-12-18T03:36:08Z","published":"2024-12-18T03:36:08Z","title":"Gradual Vigilance and Interval Communication: Enhancing Value Alignment\n  in Multi-Agent Debates","summary":"  In recent years, large language models have shown exceptional performance in\nfulfilling diverse human needs. However, their training data can introduce\nharmful content, underscoring the necessity for robust value alignment.\nMainstream methods, which depend on feedback learning and supervised training,\nare resource-intensive and may constrain the full potential of the models.\nMulti-Agent Debate (MAD) offers a more efficient and innovative solution by\nenabling the generation of reliable answers through agent interactions. To\napply MAD to value alignment, we examine the relationship between the\nhelpfulness and harmlessness of debate outcomes and individual responses, and\npropose a MAD based framework Gradual Vigilance and Interval Communication\n(GVIC). GVIC allows agents to assess risks with varying levels of vigilance and\nto exchange diverse information through interval communication. We\ntheoretically prove that GVIC optimizes debate efficiency while reducing\ncommunication overhead. Experimental results demonstrate that GVIC consistently\noutperforms baseline methods across various tasks and datasets, particularly\nexcelling in harmfulness mitigation and fraud prevention. Additionally, GVIC\nexhibits strong adaptability across different base model sizes, including both\nunaligned and aligned models, and across various task types.\n","authors":["Rui Zou","Mengqi Wei","Jintian Feng","Qian Wan","Jianwen Sun","Sannyuya Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13467v1","updated":"2024-12-18T03:25:17Z","published":"2024-12-18T03:25:17Z","title":"Transducer Tuning: Efficient Model Adaptation for Software Tasks Using\n  Code Property Graphs","summary":"  Large language models have demonstrated promising performance across various\nsoftware engineering tasks. While fine-tuning is a common practice to adapt\nthese models for downstream tasks, it becomes challenging in\nresource-constrained environments due to increased memory requirements from\ngrowing trainable parameters in increasingly large language models. We\nintroduce \\approach, a technique to adapt large models for downstream code\ntasks using Code Property Graphs (CPGs). Our approach introduces a modular\ncomponent called \\transducer that enriches code embeddings with structural and\ndependency information from CPGs. The Transducer comprises two key components:\nGraph Vectorization Engine (GVE) and Attention-Based Fusion Layer (ABFL). GVE\nextracts CPGs from input source code and transforms them into graph feature\nvectors. ABFL then fuses those graphs feature vectors with initial code\nembeddings from a large language model. By optimizing these transducers for\ndifferent downstream tasks, our approach enhances the models without the need\nto fine-tune them for specific tasks. We have evaluated \\approach on three\ndownstream tasks: code summarization, assert generation, and code translation.\nOur results demonstrate competitive performance compared to full parameter\nfine-tuning while reducing up to 99\\% trainable parameters to save memory.\n\\approach also remains competitive against other fine-tuning approaches (e.g.,\nLoRA, Prompt-Tuning, Prefix-Tuning) while using only 1.5\\%-80\\% of their\ntrainable parameters. Our findings show that integrating structural and\ndependency information through Transducer Tuning enables more efficient model\nadaptation, making it easier for users to adapt large models in\nresource-constrained settings.\n","authors":["Imam Nur Bani Yusuf","Lingxiao Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.13467v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.08124v2","updated":"2024-12-18T03:22:31Z","published":"2024-06-12T12:06:32Z","title":"Legend: Leveraging Representation Engineering to Annotate Safety Margin\n  for Preference Datasets","summary":"  The success of the reward model in distinguishing between responses with\nsubtle safety differences depends critically on the high-quality preference\ndataset, which should capture the fine-grained nuances of harmful and harmless\nresponses. This motivates the need to develop a dataset involving preference\nmargins, which accurately quantify how harmless one response is compared to\nanother. In this paper, we take the first step to propose an effective and\ncost-efficient framework to promote the margin-enhanced preference dataset\ndevelopment. Our framework, Legend, Leverages representation engineering to\nannotate preference datasets. It constructs the specific direction within the\nLLM's embedding space that represents safety. By leveraging this safety\ndirection, Legend can then leverage the semantic distances of paired responses\nalong this direction to annotate margins automatically. We experimentally\ndemonstrate our effectiveness in both reward modeling and harmless alignment\nfor LLMs. Legend also stands out for its efficiency, requiring only the\ninference time rather than additional training. This efficiency allows for\neasier implementation and scalability, making Legend particularly valuable for\npractical applications in aligning LLMs with safe conversations.\n","authors":["Duanyu Feng","Bowen Qin","Chen Huang","Youcheng Huang","Zheng Zhang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2406.08124v2.pdf","comment":"Our code is available at https://github.com/colfeng/Legend"},{"id":"http://arxiv.org/abs/2412.13464v1","updated":"2024-12-18T03:18:21Z","published":"2024-12-18T03:18:21Z","title":"GenX: Mastering Code and Test Generation with Execution Feedback","summary":"  Recent advancements in language modeling have enabled the translation of\nnatural language into code, and the use of execution feedback to improve code\ngeneration. However, these methods often rely heavily on pre-existing test\ncases, which may not always be available or comprehensive. In this work, we\npropose a novel approach that concurrently trains a code generation model and a\ntest generation model, utilizing execution feedback to refine and enhance the\nperformance of both. We introduce two strategies for test and code data\naugmentation and a new scoring function for code and test ranking. We\nexperiment on the APPS dataset and demonstrate that our approach can\neffectively generate and augment test cases, filter and synthesize correct code\nsolutions, and rank the quality of generated code and tests. The results\ndemonstrate that our models, when iteratively trained with an increasing number\nof test cases and code solutions, outperform those trained on the original\ndataset.\n","authors":["Nan Wang","Yafei Liu","Chen Chen","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2412.13464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13026v2","updated":"2024-12-18T03:05:45Z","published":"2024-12-17T15:48:25Z","title":"NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation","summary":"  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n","authors":["Karan Wanchoo","Xiaoye Zuo","Hannah Gonzalez","Soham Dan","Georgios Georgakis","Dan Roth","Kostas Daniilidis","Eleni Miltsakaki"],"pdf_url":"https://arxiv.org/pdf/2412.13026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12617v2","updated":"2024-12-18T03:03:08Z","published":"2024-05-21T09:12:20Z","title":"Quantifying Semantic Emergence in Language Models","summary":"  Large language models (LLMs) are widely recognized for their exceptional\ncapacity to capture semantics meaning. Yet, there remains no established metric\nto quantify this capability. In this work, we introduce a quantitative metric,\nInformation Emergence (IE), designed to measure LLMs' ability to extract\nsemantics from input tokens. We formalize ``semantics'' as the meaningful\ninformation abstracted from a sequence of tokens and quantify this by comparing\nthe entropy reduction observed for a sequence of tokens (macro-level) and\nindividual tokens (micro-level). To achieve this, we design a lightweight\nestimator to compute the mutual information at each transformer layer, which is\nagnostic to different tasks and language model architectures. We apply IE in\nboth synthetic in-context learning (ICL) scenarios and natural sentence\ncontexts. Experiments demonstrate informativeness and patterns about semantics.\nWhile some of these patterns confirm the conventional prior linguistic\nknowledge, the rest are relatively unexpected, which may provide new insights.\n","authors":["Hang Chen","Xinyu Yang","Jiaying Zhu","Wenya Wang"],"pdf_url":"https://arxiv.org/pdf/2405.12617v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2408.11330v2","updated":"2024-12-18T02:51:50Z","published":"2024-08-21T04:27:44Z","title":"Design Principle Transfer in Neural Architecture Search via Large\n  Language Models","summary":"  Transferable neural architecture search (TNAS) has been introduced to design\nefficient neural architectures for multiple tasks, to enhance the practical\napplicability of NAS in real-world scenarios. In TNAS, architectural knowledge\naccumulated in previous search processes is reused to warm up the architecture\nsearch for new tasks. However, existing TNAS methods still search in an\nextensive search space, necessitating the evaluation of numerous architectures.\nTo overcome this challenge, this work proposes a novel transfer paradigm, i.e.,\ndesign principle transfer. In this work, the linguistic description of various\nstructural components' effects on architectural performance is termed design\nprinciples. They are learned from established architectures and then can be\nreused to reduce the search space by discarding unpromising architectures.\nSearching in the refined search space can boost both the search performance and\nefficiency for new NAS tasks. To this end, a large language model\n(LLM)-assisted design principle transfer (LAPT) framework is devised. In LAPT,\nLLM is applied to automatically reason the design principles from a set of\ngiven architectures, and then a principle adaptation method is applied to\nrefine these principles progressively based on the new search results.\nExperimental results show that LAPT can beat the state-of-the-art TNAS methods\non most tasks and achieve comparable performance on others.\n","authors":["Xun Zhou","Xingyu Wu","Liang Feng","Zhichao Lu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2408.11330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13441v1","updated":"2024-12-18T02:23:33Z","published":"2024-12-18T02:23:33Z","title":"FlashVTG: Feature Layering and Adaptive Score Handling Network for Video\n  Temporal Grounding","summary":"  Text-guided Video Temporal Grounding (VTG) aims to localize relevant segments\nin untrimmed videos based on textual descriptions, encompassing two subtasks:\nMoment Retrieval (MR) and Highlight Detection (HD). Although previous typical\nmethods have achieved commendable results, it is still challenging to retrieve\nshort video moments. This is primarily due to the reliance on sparse and\nlimited decoder queries, which significantly constrain the accuracy of\npredictions. Furthermore, suboptimal outcomes often arise because previous\nmethods rank predictions based on isolated predictions, neglecting the broader\nvideo context. To tackle these issues, we introduce FlashVTG, a framework\nfeaturing a Temporal Feature Layering (TFL) module and an Adaptive Score\nRefinement (ASR) module. The TFL module replaces the traditional decoder\nstructure to capture nuanced video content variations across multiple temporal\nscales, while the ASR module improves prediction ranking by integrating context\nfrom adjacent moments and multi-temporal-scale features. Extensive experiments\ndemonstrate that FlashVTG achieves state-of-the-art performance on four widely\nadopted datasets in both MR and HD. Specifically, on the QVHighlights dataset,\nit boosts mAP by 5.8% for MR and 3.3% for HD. For short-moment retrieval,\nFlashVTG increases mAP to 125% of previous SOTA performance. All these\nimprovements are made without adding training burdens, underscoring its\neffectiveness. Our code is available at https://github.com/Zhuo-Cao/FlashVTG.\n","authors":["Zhuo Cao","Bingqing Zhang","Heming Du","Xin Yu","Xue Li","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13441v1.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2412.13435v1","updated":"2024-12-18T02:13:13Z","published":"2024-12-18T02:13:13Z","title":"Lightweight Safety Classification Using Pruned Language Models","summary":"  In this paper, we introduce a novel technique for content safety and prompt\ninjection classification for Large Language Models. Our technique, Layer\nEnhanced Classification (LEC), trains a Penalized Logistic Regression (PLR)\nclassifier on the hidden state of an LLM's optimal intermediate transformer\nlayer. By combining the computational efficiency of a streamlined PLR\nclassifier with the sophisticated language understanding of an LLM, our\napproach delivers superior performance surpassing GPT-4o and special-purpose\nmodels fine-tuned for each task. We find that small general-purpose models\n(Qwen 2.5 sizes 0.5B, 1.5B, and 3B) and other transformer-based architectures\nlike DeBERTa v3 are robust feature extractors allowing simple classifiers to be\neffectively trained on fewer than 100 high-quality examples. Importantly, the\nintermediate transformer layers of these models typically outperform the final\nlayer across both classification tasks. Our results indicate that a single\ngeneral-purpose LLM can be used to classify content safety, detect prompt\ninjections, and simultaneously generate output tokens. Alternatively, these\nrelatively small LLMs can be pruned to the optimal intermediate layer and used\nexclusively as robust feature extractors. Since our results are consistent on\ndifferent transformer architectures, we infer that robust feature extraction is\nan inherent capability of most, if not all, LLMs.\n","authors":["Mason Sawtell","Tula Masterman","Sandi Besen","Jim Brown"],"pdf_url":"https://arxiv.org/pdf/2412.13435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13395v1","updated":"2024-12-18T00:13:04Z","published":"2024-12-18T00:13:04Z","title":"Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom\n  Teaching Discourse","summary":"  Human tutoring interventions play a crucial role in supporting student\nlearning, improving academic performance, and promoting personal growth. This\npaper focuses on analyzing mathematics tutoring discourse using talk moves - a\nframework of dialogue acts grounded in Accountable Talk theory. However,\nscaling the collection, annotation, and analysis of extensive tutoring\ndialogues to develop machine learning models is a challenging and\nresource-intensive task. To address this, we present SAGA22, a compact dataset,\nand explore various modeling strategies, including dialogue context, speaker\ninformation, pretraining datasets, and further fine-tuning. By leveraging\nexisting datasets and models designed for classroom teaching, our results\ndemonstrate that supplementary pretraining on classroom data enhances model\nperformance in tutoring settings, particularly when incorporating longer\ncontext and speaker information. Additionally, we conduct extensive ablation\nstudies to underscore the challenges in talk move modeling.\n","authors":["Jie Cao","Abhijit Suresh","Jennifer Jacobs","Charis Clevenger","Amanda Howard","Chelsea Brown","Brent Milne","Tom Fischaber","Tamara Sumner","James H. Martin"],"pdf_url":"https://arxiv.org/pdf/2412.13395v1.pdf","comment":"Accepted to COLING'2025"},{"id":"http://arxiv.org/abs/2412.13388v1","updated":"2024-12-18T00:00:41Z","published":"2024-12-18T00:00:41Z","title":"Catalysts of Conversation: Examining Interaction Dynamics Between Topic\n  Initiators and Commentors in Alzheimer's Disease Online Communities","summary":"  Informal caregivers (e.g.,family members or friends) of people living with\nAlzheimers Disease and Related Dementias (ADRD) face substantial challenges and\noften seek informational or emotional support through online communities.\nUnderstanding the factors that drive engagement within these platforms is\ncrucial, as it can enhance their long-term value for caregivers by ensuring\nthat these communities effectively meet their needs. This study investigated\nthe user interaction dynamics within two large, popular ADRD communities,\nTalkingPoint and ALZConnected, focusing on topic initiator engagement, initial\npost content, and the linguistic patterns of comments at the thread level.\nUsing analytical methods such as propensity score matching, topic modeling, and\npredictive modeling, we found that active topic initiator engagement drives\nhigher comment volumes, and reciprocal replies from topic initiators encourage\nfurther commentor engagement at the community level. Practical caregiving\ntopics prompt more re-engagement of topic initiators, while emotional support\ntopics attract more comments from other commentors. Additionally, the\nlinguistic complexity and emotional tone of a comment influence its likelihood\nof receiving replies from topic initiators. These findings highlight the\nimportance of fostering active and reciprocal engagement and providing\neffective strategies to enhance sustainability in ADRD caregiving and broader\nhealth-related online communities.\n","authors":["Congning Ni","Qingxia Chen","Lijun Song","Patricia Commiskey","Qingyuan Song","Bradley A. Malin","Zhijun Yin"],"pdf_url":"https://arxiv.org/pdf/2412.13388v1.pdf","comment":"14 pages, 11 figures (6 in main text and 5 in the appendix). The\n  paper includes statistical analyses, structural topic modeling, and\n  predictive modeling to examine user engagement dynamics in Alzheimers Disease\n  online communities. Submitted for consideration to The Web Conference 2025"},{"id":"http://arxiv.org/abs/2412.14414v1","updated":"2024-12-18T23:58:13Z","published":"2024-12-18T23:58:13Z","title":"In-Group Love, Out-Group Hate: A Framework to Measure Affective\n  Polarization via Contentious Online Discussions","summary":"  Affective polarization, the emotional divide between ideological groups\nmarked by in-group love and out-group hate, has intensified in the United\nStates, driving contentious issues like masking and lockdowns during the\nCOVID-19 pandemic. Despite its societal impact, existing models of opinion\nchange fail to account for emotional dynamics nor offer methods to quantify\naffective polarization robustly and in real-time. In this paper, we introduce a\ndiscrete choice model that captures decision-making within affectively\npolarized social networks and propose a statistical inference method estimate\nkey parameters -- in-group love and out-group hate -- from social media data.\nThrough empirical validation from online discussions about the COVID-19\npandemic, we demonstrate that our approach accurately captures real-world\npolarization dynamics and explains the rapid emergence of a partisan gap in\nattitudes towards masking and lockdowns. This framework allows for tracking\naffective polarization across contentious issues has broad implications for\nfostering constructive online dialogues in digital spaces.\n","authors":["Buddhika Nettasinghe","Ashwin Rao","Bohan Jiang","Allon Percus","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2412.14414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14748v3","updated":"2024-12-18T23:36:03Z","published":"2024-10-17T19:38:55Z","title":"ETF: An Entity Tracing Framework for Hallucination Detection in Code\n  Summaries","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced their ability to understand both natural language and code, driving\ntheir use in tasks like natural language-to-code (NL2Code) and code\nsummarization. However, LLMs are prone to hallucination-outputs that stray from\nintended meanings. Detecting hallucinations in code summarization is especially\ndifficult due to the complex interplay between programming and natural\nlanguages. We introduce a first-of-its-kind dataset with $\\sim$10K samples,\ncurated specifically for hallucination detection in code summarization. We\nfurther propose a novel Entity Tracing Framework (ETF) that a) utilizes static\nprogram analysis to identify code entities from the program and b) uses LLMs to\nmap and verify these entities and their intents within generated code\nsummaries. Our experimental analysis demonstrates the effectiveness of the\nframework, leading to a 0.73 F1 score. This approach provides an interpretable\nmethod for detecting hallucinations by grounding entities, allowing us to\nevaluate summary accuracy.\n","authors":["Kishan Maharaj","Vitobha Munigala","Srikanth G. Tamilselvam","Prince Kumar","Sayandeep Sen","Palani Kodeswaran","Abhijit Mishra","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2410.14748v3.pdf","comment":"11 pages, 6 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2402.15083v2","updated":"2024-12-18T23:11:48Z","published":"2024-02-23T04:02:23Z","title":"Hands-Free VR","summary":"  The paper introduces Hands-Free VR, a voice-based natural-language interface\nfor VR. The user gives a command using their voice, the speech audio data is\nconverted to text using a speech-to-text deep learning model that is fine-tuned\nfor robustness to word phonetic similarity and to spoken English accents, and\nthe text is mapped to an executable VR command using a large language model\nthat is robust to natural language diversity. Hands-Free VR was evaluated in a\ncontrolled within-subjects study (N = 22) that asked participants to find\nspecific objects and to place them in various configurations. In the control\ncondition participants used a conventional VR user interface to grab, carry,\nand position the objects using the handheld controllers. In the experimental\ncondition participants used Hands-Free VR. The results confirm that: (1)\nHands-Free VR is robust to spoken English accents, as for 20 of our\nparticipants English was not their first language, and to word phonetic\nsimilarity, correctly transcribing the voice command 96.71% of the time; (2)\nHands-Free VR is robust to natural language diversity, correctly mapping the\ntranscribed command to an executable command in 97.83% of the time; (3)\nHands-Free VR had a significant efficiency advantage over the conventional VR\ninterface in terms of task completion time, total viewpoint translation, total\nview direction rotation, and total left and right hand translations; (4)\nHands-Free VR received high user preference ratings in terms of ease of use,\nintuitiveness, ergonomics, reliability, and desirability.\n","authors":["Jorge Askur Vazquez Fernandez","Jae Joong Lee","Santiago Andrés Serrano Vacca","Alejandra Magana","Radim Pesam","Bedrich Benes","Voicu Popescu"],"pdf_url":"https://arxiv.org/pdf/2402.15083v2.pdf","comment":"The first two authors contributed equally. Accepted VISIGRAPP@HUCAPP\n  2025"},{"id":"http://arxiv.org/abs/2409.01227v3","updated":"2024-12-18T23:04:46Z","published":"2024-09-02T13:02:51Z","title":"Prompt Compression with Context-Aware Sentence Encoding for Fast and\n  Improved LLM Inference","summary":"  Large language models (LLMs) have triggered a new stream of research focusing\non compressing the context length to reduce the computational cost while\nensuring the retention of helpful information for LLMs to answer the given\nquestion. Token-based removal methods are one of the most prominent approaches\nin this direction, but risk losing the semantics of the context caused by\nintermediate token removal, especially under high compression ratios, while\nalso facing challenges in computational efficiency. In this work, we propose\ncontext-aware prompt compression (CPC), a sentence-level prompt compression\ntechnique where its key innovation is a novel context-aware sentence encoder\nthat provides a relevance score for each sentence for a given question. To\ntrain this encoder, we generate a new dataset consisting of questions,\npositives, and negative pairs where positives are sentences relevant to the\nquestion, while negatives are irrelevant context sentences. We train the\nencoder in a contrastive setup to learn context-aware sentence representations.\nOur method considerably outperforms prior works on prompt compression on\nbenchmark datasets and is up to 10.93x faster at inference compared to the best\ntoken-level compression method. We also find better improvement for shorter\nlength constraints in most benchmarks, showing the effectiveness of our\nproposed solution in the compression of relevant information in a shorter\ncontext. Finally, we release the code and the dataset for quick reproducibility\nand further development: https://github.com/Workday/cpc.\n","authors":["Barys Liskavets","Maxim Ushakov","Shuvendu Roy","Mark Klibanov","Ali Etemad","Shane Luke"],"pdf_url":"https://arxiv.org/pdf/2409.01227v3.pdf","comment":"Accepted in AAAI Conference on Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2404.18988v4","updated":"2024-12-18T22:26:15Z","published":"2024-04-29T17:36:58Z","title":"Markovian Transformers for Informative Language Modeling","summary":"  Chain-of-Thought (CoT) reasoning holds great promise for explaining language\nmodel outputs, but recent studies have highlighted significant challenges in\nits practical application for interpretability. We propose to address this\nissue by making CoT causally essential to prediction through two key\ncomponents: factoring next-token prediction through intermediate CoT text, and\ntraining CoT to predict future tokens independently of other context. This\nresults in \"Markovian\" language models, where CoT serves as a fixed-size state\nfor future token prediction. Our approach optimizes for \"informativeness\" - the\nimprovement in next-token predictions using a trained CoT compared to a\nbaseline. Using Proximal Policy Optimization (PPO) for arithmetic problems and\npolicy gradient for GSM8K, we demonstrate effectiveness on both arithmetic\nproblems with Mistral 7B and the GSM8K benchmark with Llama 3.1 8B, where the\nmodel learns to produce CoTs that are 33.20% more effective at predicting\nanswers than the pre-trained baseline. The increased sensitivity of model\nperformance to CoT perturbations provides strong evidence of CoT reliance.\nFurthermore, we show that CoTs trained for one model generalize to help other\nmodels predict answers, suggesting these CoTs capture reasoning patterns that\ntransfer across different interpreters. This work advances the development of\nmore interpretable language models, potentially enabling their extension to\narbitrarily long contexts and enhancing AI reasoning capabilities across\nvarious domains.\n","authors":["Scott Viteri","Max Lamparth","Peter Chatain","Clark Barrett"],"pdf_url":"https://arxiv.org/pdf/2404.18988v4.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.14373v1","updated":"2024-12-18T22:13:21Z","published":"2024-12-18T22:13:21Z","title":"ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram\n  Language Modeling","summary":"  Large Language Models (LLMs) have shown remarkable adaptability across\ndomains beyond text, specifically electrocardiograms (ECGs). More specifically,\nthere is a growing body of work exploring the task of generating text from a\nmulti-channeled ECG and corresponding textual prompt. Current approaches\ntypically involve pretraining an ECG-specific encoder with a self-supervised\nlearning (SSL) objective and using the features output by the pretrained\nencoder to finetune a LLM for natural language generation (NLG). However, these\nmethods are limited by 1) inefficiency from two-stage training and 2)\ninterpretability challenges with encoder-generated features. To address these\nlimitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE)\ntokenizer pipeline for autoregressive language modeling of ECGs. This approach\ncompresses and encodes ECG signals into tokens, enabling end-to-end LLM\ntraining by combining ECG and text tokens directly, while being much more\ninterpretable since the ECG tokens can be directly mapped back to the original\nsignal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only\nhalf the time and ~48% of the data required by two-stage approaches.\n","authors":["William Han","Chaojing Duan","Michael A. Rosenberg","Emerson Liu","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14373v1.pdf","comment":"26 pages, 17 figures"},{"id":"http://arxiv.org/abs/2412.14368v1","updated":"2024-12-18T22:04:56Z","published":"2024-12-18T22:04:56Z","title":"Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation","summary":"  Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding.\n","authors":["Yuxuan Jiang","Francis Ferraro"],"pdf_url":"https://arxiv.org/pdf/2412.14368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14363v1","updated":"2024-12-18T22:01:55Z","published":"2024-12-18T22:01:55Z","title":"ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals","summary":"  Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.\n","authors":["Utkarsh Saxena","Sayeh Sharify","Kaushik Roy","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14363v1.pdf","comment":"14 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.10571v2","updated":"2024-12-18T22:01:52Z","published":"2024-12-13T21:28:17Z","title":"Evidence Contextualization and Counterfactual Attribution for\n  Conversational QA over Heterogeneous Data with RAG Systems","summary":"  Retrieval Augmented Generation (RAG) works as a backbone for interacting with\nan enterprise's own data via Conversational Question Answering (ConvQA). In a\nRAG system, a retriever fetches passages from a collection in response to a\nquestion, which are then included in the prompt of a large language model (LLM)\nfor generating a natural language (NL) answer. However, several RAG systems\ntoday suffer from two shortcomings: (i) retrieved passages usually contain\ntheir raw text and lack appropriate document context, negatively impacting both\nretrieval and answering quality; and (ii) attribution strategies that explain\nanswer generation usually rely only on similarity between the answer and the\nretrieved passages, thereby only generating plausible but not causal\nexplanations. In this work, we demonstrate RAGONITE, a RAG system that remedies\nthe above concerns by: (i) contextualizing evidence with source metadata and\nsurrounding text; and (ii) computing counterfactual attribution, a causal\nexplanation approach where the contribution of an evidence to an answer is\ndetermined by the similarity of the original response to the answer obtained by\nremoving that evidence. To evaluate our proposals, we release a new benchmark\nConfQuestions, with 300 hand-created conversational questions, each in English\nand German, coupled with ground truth URLs, completed questions, and answers\nfrom 215 public Confluence pages, that are typical of enterprise wiki spaces\nwith heterogeneous elements. Experiments with RAGONITE on ConfQuestions show\nthe viability of our ideas: contextualization improves RAG performance, and\ncounterfactual attribution is effective at explaining RAG answers.\n","authors":["Rishiraj Saha Roy","Joel Schlotthauer","Chris Hinze","Andreas Foltyn","Luzian Hahn","Fabian Kuech"],"pdf_url":"https://arxiv.org/pdf/2412.10571v2.pdf","comment":"Accepted at WSDM 2025"},{"id":"http://arxiv.org/abs/2412.14354v1","updated":"2024-12-18T21:42:15Z","published":"2024-12-18T21:42:15Z","title":"State Space Models are Strong Text Rerankers","summary":"  Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking -- a\ntask requiring fine-grained query-document interaction and long-context\nunderstanding -- remains underexplored.\n  This study benchmarks SSM-based architectures (specifically, Mamba-1 and\nMamba-2) against transformer-based models across various scales, architectures,\nand pre-training objectives, focusing on performance and efficiency in text\nreranking tasks. We find that (1) Mamba architectures achieve competitive text\nranking performance, comparable to transformer-based models of similar size;\n(2) they are less efficient in training and inference compared to transformers\nwith flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance\nand efficiency. These results underscore the potential of state space models as\na transformer alternative and highlight areas for improvement in future IR\napplications.\n","authors":["Zhichao Xu","Jinghua Yan","Ashim Gupta","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2412.14354v1.pdf","comment":"The first two authors contributed equally, order decided randomly"},{"id":"http://arxiv.org/abs/2412.14352v1","updated":"2024-12-18T21:37:07Z","published":"2024-12-18T21:37:07Z","title":"A Survey on LLM Inference-Time Self-Improvement","summary":"  Techniques that enhance inference through increased computation at test-time\nhave recently gained attention. In this survey, we investigate the current\nstate of LLM Inference-Time Self-Improvement from three different perspectives:\nIndependent Self-improvement, focusing on enhancements via decoding or sampling\nmethods; Context-Aware Self-Improvement, leveraging additional context or\ndatastore; and Model-Aided Self-Improvement, achieving improvement through\nmodel collaboration. We provide a comprehensive review of recent relevant\nstudies, contribute an in-depth taxonomy, and discuss challenges and\nlimitations, offering insights for future research.\n","authors":["Xiangjue Dong","Maria Teleki","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2412.14352v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2412.14351v1","updated":"2024-12-18T21:34:42Z","published":"2024-12-18T21:34:42Z","title":"Is Peer-Reviewing Worth the Effort?","summary":"  How effective is peer-reviewing in identifying important papers? We treat\nthis question as a forecasting task. Can we predict which papers will be highly\ncited in the future based on venue and \"early returns\" (citations soon after\npublication)? We show early returns are more predictive than venue. Finally, we\nend with constructive suggestions to address scaling challenges: (a) too many\nsubmissions and (b) too few qualified reviewers.\n","authors":["Kenneth Church","Raman Chandrasekar","John E. Ortega","Ibrahim Said Ahmad"],"pdf_url":"https://arxiv.org/pdf/2412.14351v1.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025)"},{"id":"http://arxiv.org/abs/2402.11512v5","updated":"2024-12-18T21:28:54Z","published":"2024-02-18T08:53:41Z","title":"From Prejudice to Parity: A New Approach to Debiasing Large Language\n  Model Word Embeddings","summary":"  Embeddings play a pivotal role in the efficacy of Large Language Models. They\nare the bedrock on which these models grasp contextual relationships and foster\na more nuanced understanding of language and consequently perform remarkably on\na plethora of complex tasks that require a fundamental understanding of human\nlanguage. Given that these embeddings themselves often reflect or exhibit bias,\nit stands to reason that these models may also inadvertently learn this bias.\nIn this work, we build on the seminal previous work and propose DeepSoftDebias,\nan algorithm that uses a neural network to perform 'soft debiasing'. We\nexhaustively evaluate this algorithm across a variety of SOTA datasets,\naccuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias\noutperforms the current state-of-the-art methods at reducing bias across\ngender, race, and religion.\n","authors":["Aishik Rakshit","Smriti Singh","Shuvam Keshari","Arijit Ghosh Chowdhury","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2402.11512v5.pdf","comment":"Accepted at COLING 2025"},{"id":"http://arxiv.org/abs/2401.12208v2","updated":"2024-12-18T20:56:18Z","published":"2024-01-22T18:51:07Z","title":"A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray\n  Interpretation","summary":"  Over 1.4 billion chest X-rays (CXRs) are performed annually due to their\ncost-effectiveness as an initial diagnostic test. This scale of radiological\nstudies provides a significant opportunity to streamline CXR interpretation and\ndocumentation. While foundation models are a promising solution, the lack of\npublicly available large-scale datasets and benchmarks inhibits their iterative\ndevelopment and real-world evaluation. To overcome these challenges, we\nconstructed a large-scale dataset (CheXinstruct), which we utilized to train a\nvision-language foundation model (CheXagent). We systematically demonstrated\ncompetitive performance across eight distinct task types on our novel\nevaluation benchmark (CheXbench). Beyond technical validation, we assessed the\nreal-world utility of CheXagent in directly drafting radiology reports. Our\nclinical assessment with eight radiologists revealed a 36% time saving for\nresidents using CheXagent-drafted reports, while attending radiologists showed\nno significant time difference editing resident-drafted or CheXagent-drafted\nreports. The CheXagent-drafted reports improved the writing efficiency of both\nradiology residents and attending radiologists in 81% and 61% of cases,\nrespectively, without loss of quality. Overall, we demonstrate that CheXagent\ncan effectively perform a variety of CXR interpretation tasks and holds\npotential to assist radiologists in routine clinical workflows.\n","authors":["Zhihong Chen","Maya Varma","Justin Xu","Magdalini Paschali","Dave Van Veen","Andrew Johnston","Alaa Youssef","Louis Blankemeier","Christian Bluethgen","Stephan Altmayer","Jeya Maria Jose Valanarasu","Mohamed Siddig Eltayeb Muneer","Eduardo Pontes Reis","Joseph Paul Cohen","Cameron Olsen","Tanishq Mathew Abraham","Emily B. Tsai","Christopher F. Beaulieu","Jenia Jitsev","Sergios Gatidis","Jean-Benoit Delbrouck","Akshay S. Chaudhari","Curtis P. Langlotz"],"pdf_url":"https://arxiv.org/pdf/2401.12208v2.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.14328v1","updated":"2024-12-18T20:56:11Z","published":"2024-12-18T20:56:11Z","title":"Semantic Role Labeling of NomBank Partitives","summary":"  This article is about Semantic Role Labeling for English partitive nouns\n(5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank\nannotated corpus. Several systems are described using traditional and\ntransformer-based machine learning, as well as ensembling. Our highest scoring\nsystem achieves an F1 of 91.74% using \"gold\" parses from the Penn Treebank and\n91.12% when using the Berkeley Neural parser. This research includes both\nclassroom and experimental settings for system development.\n","authors":["Adam Meyers","Advait Pravin Savant","John E. Ortega"],"pdf_url":"https://arxiv.org/pdf/2412.14328v1.pdf","comment":"SUMEval-2: The 2nd Workshop on Scaling Up Multilingual &\n  Multi-Cultural Evaluation at the 31st International Conference on\n  Computational Linguistics (COLING 2025)"},{"id":"http://arxiv.org/abs/2412.14323v1","updated":"2024-12-18T20:37:52Z","published":"2024-12-18T20:37:52Z","title":"The Role of Handling Attributive Nouns in Improving Chinese-To-English\n  Machine Translation","summary":"  Translating between languages with drastically different grammatical\nconventions poses challenges, not just for human interpreters but also for\nmachine translation systems. In this work, we specifically target the\ntranslation challenges posed by attributive nouns in Chinese, which frequently\ncause ambiguities in English translation. By manually inserting the omitted\nparticle X ('DE'). In news article titles from the Penn Chinese Discourse\nTreebank, we developed a targeted dataset to fine-tune Hugging Face Chinese to\nEnglish translation models, specifically improving how this critical function\nword is handled. This focused approach not only complements the broader\nstrategies suggested by previous studies but also offers a practical\nenhancement by specifically addressing a common error type in Chinese-English\ntranslation.\n","authors":[" Haohao"," Wang","Adam Meyers","John E. Ortega","Rodolfo Zevallos"],"pdf_url":"https://arxiv.org/pdf/2412.14323v1.pdf","comment":"18th Workshop on Building and Using Comparable Corpora (BUCC) at the\n  31st International Conference on Computational Linguistics (COLING 2025)"},{"id":"http://arxiv.org/abs/2407.03525v3","updated":"2024-12-18T20:32:35Z","published":"2024-07-03T22:02:07Z","title":"UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs'\n  Memorization","summary":"  This paper introduces UnSeenTimeQA, a novel data contamination-free\ntime-sensitive question-answering (TSQA) benchmark. It differs from existing\nTSQA benchmarks by avoiding web-searchable queries grounded in the real-world.\nWe present a series of time-sensitive event scenarios based on synthetically\ngenerated facts. It requires large language models (LLMs) to engage in genuine\ntemporal reasoning without depending on the factual knowledge acquired during\nthe pre-training phase. We designed three types of time-sensitive questions to\ntest LLMs' temporal reasoning abilities over sequential and parallel event\noccurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals\nmixed results: while they perform well on simpler subsets, their overall\nperformance remains inferior as compared to real-world fact-based TSQA. Error\nanalysis of LLM-generated reasoning chains indicates that LLMs face\ndifficulties in reasoning over long-range event dependencies and parallel event\ntimelines that unfold concurrently.\n","authors":["Md Nayem Uddin","Amir Saeidi","Divij Handa","Agastya Seth","Tran Cao Son","Eduardo Blanco","Steven R. Corman","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2407.03525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14304v1","updated":"2024-12-18T20:18:03Z","published":"2024-12-18T20:18:03Z","title":"Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing\n  LLM Ophthalmological QA in LMICs","summary":"  Current ophthalmology clinical workflows are plagued by over-referrals, long\nwaits, and complex and heterogeneous medical records. Large language models\n(LLMs) present a promising solution to automate various procedures such as\ntriaging, preliminary tests like visual acuity assessment, and report\nsummaries. However, LLMs have demonstrated significantly varied performance\nacross different languages in natural language question-answering tasks,\npotentially exacerbating healthcare disparities in Low and Middle-Income\nCountries (LMICs). This study introduces the first multilingual\nophthalmological question-answering benchmark with manually curated questions\nparallel across languages, allowing for direct cross-lingual comparisons. Our\nevaluation of 6 popular LLMs across 7 different languages reveals substantial\nbias across different languages, highlighting risks for clinical deployment of\nLLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought\nor Retrieval-augmented generation (RAG) by themselves fall short of closing\nthis performance gap, often failing to improve performance across all languages\nand lacking specificity for the medical domain. To address this issue, We\npropose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time\nde-biasing method leveraging retrieval augmented generation and\nself-verification. Our approach not only improves performance across all\nlanguages but also significantly reduces the multilingual bias gap,\nfacilitating equitable LLM application across the globe.\n","authors":["David Restrepo","Chenwei Wu","Zhengxu Tang","Zitao Shuai","Thao Nguyen Minh Phan","Jun-En Ding","Cong-Tinh Dao","Jack Gallifant","Robyn Gayle Dychiao","Jose Carlo Artiaga","André Hiroshi Bando","Carolina Pelegrini Barbosa Gracitelli","Vincenz Ferrer","Leo Anthony Celi","Danielle Bitterman","Michael G Morley","Luis Filipe Nakayama"],"pdf_url":"https://arxiv.org/pdf/2412.14304v1.pdf","comment":"Accepted at the AAAI 2025 Artificial Intelligence for Social Impact\n  Track (AAAI-AISI 2025)"},{"id":"http://arxiv.org/abs/2412.14276v1","updated":"2024-12-18T19:15:17Z","published":"2024-12-18T19:15:17Z","title":"Fake News Detection: Comparative Evaluation of BERT-like Models and\n  Large Language Models with Generative AI-Annotated Data","summary":"  Fake news poses a significant threat to public opinion and social stability\nin modern society. This study presents a comparative evaluation of BERT-like\nencoder-only models and autoregressive decoder-only large language models\n(LLMs) for fake news detection. We introduce a dataset of news articles labeled\nwith GPT-4 assistance (an AI-labeling method) and verified by human experts to\nensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned\non this dataset. Additionally, we developed an instruction-tuned LLM approach\nwith majority voting during inference for label generation. Our analysis\nreveals that BERT-like models generally outperform LLMs in classification\ntasks, while LLMs demonstrate superior robustness against text perturbations.\nCompared to weak labels (distant supervision) data, the results show that AI\nlabels with human supervision achieve better classification results. This study\nhighlights the effectiveness of combining AI-based annotation with human\noversight and demonstrates the performance of different families of machine\nlearning models for fake news detection\n","authors":["haina Raza","Drai Paulen-Patterson","Chen Ding"],"pdf_url":"https://arxiv.org/pdf/2412.14276v1.pdf","comment":"Accepted in Knowledge and Information Systems Journal"},{"id":"http://arxiv.org/abs/2404.04326v3","updated":"2024-12-18T19:00:00Z","published":"2024-04-05T18:00:07Z","title":"Hypothesis Generation with Large Language Models","summary":"  Effective generation of novel hypotheses is instrumental to scientific\nprogress. So far, researchers have been the main powerhouse behind hypothesis\ngeneration by painstaking data analysis and thinking (also known as the Eureka\nmoment). In this paper, we examine the potential of large language models\n(LLMs) to generate hypotheses. We focus on hypothesis generation based on data\n(i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts,\nwe generate initial hypotheses from a small number of examples and then update\nthem iteratively to improve the quality of hypotheses. Inspired by multi-armed\nbandits, we design a reward function to inform the exploitation-exploration\ntradeoff in the update process. Our algorithm is able to generate hypotheses\nthat enable much better predictive performance than few-shot prompting in\nclassification tasks, improving accuracy by 31.7% on a synthetic dataset and by\n13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform\nsupervised learning by 12.8% and 11.2% on two challenging real-world datasets.\nFurthermore, we find that the generated hypotheses not only corroborate\nhuman-verified theories but also uncover new insights for the tasks.\n","authors":["Yangqiaoyu Zhou","Haokun Liu","Tejes Srivastava","Hongyuan Mei","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2404.04326v3.pdf","comment":"28 pages, 6 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis_generation. Accepted by the 1st\n  Workshop on NLP for Science (NLP4Science) at EMNLP 2024"},{"id":"http://arxiv.org/abs/2001.04589v2","updated":"2024-12-18T18:59:53Z","published":"2020-01-14T02:14:09Z","title":"Faster Transformer Decoding: N-gram Masked Self-Attention","summary":"  Motivated by the fact that most of the information relevant to the prediction\nof target tokens is drawn from the source sentence $S=s_1, \\ldots, s_S$, we\npropose truncating the target-side window used for computing self-attention by\nmaking an $N$-gram assumption. Experiments on WMT EnDe and EnFr data sets show\nthat the $N$-gram masked self-attention model loses very little in BLEU score\nfor $N$ values in the range $4, \\ldots, 8$, depending on the task.\n","authors":["Ciprian Chelba","Mia Chen","Ankur Bapna","Noam Shazeer"],"pdf_url":"https://arxiv.org/pdf/2001.04589v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.14173v1","updated":"2024-12-18T18:59:59Z","published":"2024-12-18T18:59:59Z","title":"AniDoc: Animation Creation Made Easier","summary":"  The production of 2D animation follows an industry-standard workflow,\nencompassing four essential stages: character design, keyframe animation,\nin-betweening, and coloring. Our research focuses on reducing the labor costs\nin the above process by harnessing the potential of increasingly powerful\ngenerative AI. Using video diffusion models as the foundation, AniDoc emerges\nas a video line art colorization tool, which automatically converts sketch\nsequences into colored animations following the reference character\nspecification. Our model exploits correspondence matching as an explicit\nguidance, yielding strong robustness to the variations (e.g., posture) between\nthe reference character and each line art frame. In addition, our model could\neven automate the in-betweening process, such that users can easily create a\ntemporally consistent animation by simply providing a character image as well\nas the start and end sketches. Our code is available at:\nhttps://yihao-meng.github.io/AniDoc_demo.\n","authors":["Yihao Meng","Hao Ouyang","Hanlin Wang","Qiuyu Wang","Wen Wang","Ka Leong Cheng","Zhiheng Liu","Yujun Shen","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2412.14173v1.pdf","comment":"Project page and code: https://yihao-meng.github.io/AniDoc_demo"},{"id":"http://arxiv.org/abs/2412.14172v1","updated":"2024-12-18T18:59:56Z","published":"2024-12-18T18:59:56Z","title":"Learning from Massive Human Videos for Universal Humanoid Pose Control","summary":"  Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.\n","authors":["Jiageng Mao","Siheng Zhao","Siqi Song","Tianheng Shi","Junjie Ye","Mingtong Zhang","Haoran Geng","Jitendra Malik","Vitor Guizilini","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14171v1","updated":"2024-12-18T18:59:54Z","published":"2024-12-18T18:59:54Z","title":"Thinking in Space: How Multimodal Large Language Models See, Remember,\n  and Recall Spaces","summary":"  Humans possess the visual-spatial intelligence to remember spaces from\nsequential visual observations. However, can Multimodal Large Language Models\n(MLLMs) trained on million-scale video datasets also ``think in space'' from\nvideos? We present a novel video-based visual-spatial intelligence benchmark\n(VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit\ncompetitive - though subhuman - visual-spatial intelligence. We probe models to\nexpress how they think in space both linguistically and visually and find that\nwhile spatial reasoning capabilities remain the primary bottleneck for MLLMs to\nreach higher benchmark performance, local world models and spatial awareness do\nemerge within these models. Notably, prevailing linguistic reasoning techniques\n(e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve\nperformance, whereas explicitly generating cognitive maps during\nquestion-answering enhances MLLMs' spatial distance ability.\n","authors":["Jihan Yang","Shusheng Yang","Anjali W. Gupta","Rilyn Han","Li Fei-Fei","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2412.14171v1.pdf","comment":"Project page:\n  https://vision-x-nyu.github.io/thinking-in-space.github.io/"},{"id":"http://arxiv.org/abs/2412.14169v1","updated":"2024-12-18T18:59:53Z","published":"2024-12-18T18:59:53Z","title":"Autoregressive Video Generation without Vector Quantization","summary":"  This paper presents a novel approach that enables autoregressive video\ngeneration with high efficiency. We propose to reformulate the video generation\nproblem as a non-quantized autoregressive modeling of temporal frame-by-frame\nprediction and spatial set-by-set prediction. Unlike raster-scan prediction in\nprior autoregressive models or joint distribution modeling of fixed-length\ntokens in diffusion models, our approach maintains the causal property of\nGPT-style models for flexible in-context capabilities, while leveraging\nbidirectional modeling within individual frames for efficiency. With the\nproposed approach, we train a novel video autoregressive model without vector\nquantization, termed NOVA. Our results demonstrate that NOVA surpasses prior\nautoregressive video models in data efficiency, inference speed, visual\nfidelity, and video fluency, even with a much smaller model capacity, i.e.,\n0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models\nin text-to-image generation tasks, with a significantly lower training cost.\nAdditionally, NOVA generalizes well across extended video durations and enables\ndiverse zero-shot applications in one unified model. Code and models are\npublicly available at https://github.com/baaivision/NOVA.\n","authors":["Haoge Deng","Ting Pan","Haiwen Diao","Zhengxiong Luo","Yufeng Cui","Huchuan Lu","Shiguang Shan","Yonggang Qi","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14169v1.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2412.14170v1","updated":"2024-12-18T18:59:53Z","published":"2024-12-18T18:59:53Z","title":"E-CAR: Efficient Continuous Autoregressive Image Generation via\n  Multistage Modeling","summary":"  Recent advances in autoregressive (AR) models with continuous tokens for\nimage generation show promising results by eliminating the need for discrete\ntokenization. However, these models face efficiency challenges due to their\nsequential token generation nature and reliance on computationally intensive\ndiffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive\nImage Generation via Multistage Modeling), an approach that addresses these\nlimitations through two intertwined innovations: (1) a stage-wise continuous\ntoken generation strategy that reduces computational complexity and provides\nprogressively refined token maps as hierarchical conditions, and (2) a\nmultistage flow-based distribution modeling method that transforms only\npartial-denoised distributions at each stage comparing to complete denoising in\nnormal diffusion models. Holistically, ECAR operates by generating tokens at\nincreasing resolutions while simultaneously denoising the image at each stage.\nThis design not only reduces token-to-image transformation cost by a factor of\nthe stage number but also enables parallel processing at the token level. Our\napproach not only enhances computational efficiency but also aligns naturally\nwith image generation principles by operating in continuous token space and\nfollowing a hierarchical generation process from coarse to fine details.\nExperimental results demonstrate that ECAR achieves comparable image quality to\nDiT Peebles & Xie [2023] while requiring 10$\\times$ FLOPs reduction and\n5$\\times$ speedup to generate a 256$\\times$256 image.\n","authors":["Zhihang Yuan","Yuzhang Shang","Hanling Zhang","Tongcheng Fang","Rui Xie","Bingxin Xu","Yan Yan","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14168v1","updated":"2024-12-18T18:59:50Z","published":"2024-12-18T18:59:50Z","title":"FashionComposer: Compositional Fashion Image Generation","summary":"  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n","authors":["Sihui Ji","Yiyang Wang","Xi Chen","Xiaogang Xu","Hao Luo","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14168v1.pdf","comment":"https://sihuiji.github.io/FashionComposer-Page"},{"id":"http://arxiv.org/abs/2412.14167v1","updated":"2024-12-18T18:59:49Z","published":"2024-12-18T18:59:49Z","title":"VideoDPO: Omni-Preference Alignment for Video Diffusion Generation","summary":"  Recent progress in generative diffusion models has greatly advanced\ntext-to-video generation. While text-to-video models trained on large-scale,\ndiverse datasets can produce varied outputs, these generations often deviate\nfrom user preferences, highlighting the need for preference alignment on\npre-trained models. Although Direct Preference Optimization (DPO) has\ndemonstrated significant improvements in language and image generation, we\npioneer its adaptation to video diffusion models and propose a VideoDPO\npipeline by making several key adjustments. Unlike previous image alignment\nmethods that focus solely on either (i) visual quality or (ii) semantic\nalignment between text and videos, we comprehensively consider both dimensions\nand construct a preference score accordingly, which we term the OmniScore. We\ndesign a pipeline to automatically collect preference pair data based on the\nproposed OmniScore and discover that re-weighting these pairs based on the\nscore significantly impacts overall preference alignment. Our experiments\ndemonstrate substantial improvements in both visual quality and semantic\nalignment, ensuring that no preference aspect is neglected. Code and data will\nbe shared at https://videodpo.github.io/.\n","authors":["Runtao Liu","Haoyu Wu","Zheng Ziqiang","Chen Wei","Yingqing He","Renjie Pi","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14166v1","updated":"2024-12-18T18:59:38Z","published":"2024-12-18T18:59:38Z","title":"MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data","summary":"  We propose scaling up 3D scene reconstruction by training with synthesized\ndata. At the core of our work is MegaSynth, a procedurally generated 3D dataset\ncomprising 700K scenes - over 50 times larger than the prior real dataset DL3DV\n- dramatically scaling the training data. To enable scalable data generation,\nour key idea is eliminating semantic information, removing the need to model\ncomplex semantic priors such as object affordances and scene composition.\nInstead, we model scenes with basic spatial structures and geometry primitives,\noffering scalability. Besides, we control data complexity to facilitate\ntraining while loosely aligning it with real-world data distribution to benefit\nreal-world generalization. We explore training LRMs with both MegaSynth and\navailable real data. Experiment results show that joint training or\npre-training with MegaSynth improves reconstruction quality by 1.2 to 1.8 dB\nPSNR across diverse image domains. Moreover, models trained solely on MegaSynth\nperform comparably to those trained on real data, underscoring the low-level\nnature of 3D reconstruction. Additionally, we provide an in-depth analysis of\nMegaSynth's properties for enhancing model capability, training stability, and\ngeneralization.\n","authors":["Hanwen Jiang","Zexiang Xu","Desai Xie","Ziwen Chen","Haian Jin","Fujun Luan","Zhixin Shu","Kai Zhang","Sai Bi","Xin Sun","Jiuxiang Gu","Qixing Huang","Georgios Pavlakos","Hao Tan"],"pdf_url":"https://arxiv.org/pdf/2412.14166v1.pdf","comment":"Project page: https://hwjiang1510.github.io/MegaSynth/"},{"id":"http://arxiv.org/abs/2412.14164v1","updated":"2024-12-18T18:58:50Z","published":"2024-12-18T18:58:50Z","title":"MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning","summary":"  In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process.\n","authors":["Shengbang Tong","David Fan","Jiachen Zhu","Yunyang Xiong","Xinlei Chen","Koustuv Sinha","Michael Rabbat","Yann LeCun","Saining Xie","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14164v1.pdf","comment":"Project page at tsb0601.github.io/metamorph"},{"id":"http://arxiv.org/abs/2412.14158v1","updated":"2024-12-18T18:53:22Z","published":"2024-12-18T18:53:22Z","title":"AKiRa: Augmentation Kit on Rays for optical video generation","summary":"  Recent advances in text-conditioned video diffusion have greatly improved\nvideo quality. However, these methods offer limited or sometimes no control to\nusers on camera aspects, including dynamic camera motion, zoom, distorted lens\nand focus shifts. These motion and optical aspects are crucial for adding\ncontrollability and cinematic elements to generation frameworks, ultimately\nresulting in visual content that draws focus, enhances mood, and guides\nemotions according to filmmakers' controls. In this paper, we aim to close the\ngap between controllable video generation and camera optics. To achieve this,\nwe propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework\nthat builds and trains a camera adapter with a complex camera model over an\nexisting video generation backbone. It enables fine-tuned control over camera\nmotion as well as complex optical parameters (focal length, distortion,\naperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh.\nExtensive experiments demonstrate AKiRa's effectiveness in combining and\ncomposing camera optics while outperforming all state-of-the-art methods. This\nwork sets a new landmark in controlled and optically enhanced video generation,\npaving the way for future optical video generation methods.\n","authors":["Xi Wang","Robin Courant","Marc Christie","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2412.14158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14148v1","updated":"2024-12-18T18:45:35Z","published":"2024-12-18T18:45:35Z","title":"MCMat: Multiview-Consistent and Physically Accurate PBR Material\n  Generation","summary":"  Existing 2D methods utilize UNet-based diffusion models to generate\nmulti-view physically-based rendering (PBR) maps but struggle with multi-view\ninconsistency, while some 3D methods directly generate UV maps, encountering\ngeneralization issues due to the limited 3D data. To address these problems, we\npropose a two-stage approach, including multi-view generation and UV materials\nrefinement. In the generation stage, we adopt a Diffusion Transformer (DiT)\nmodel to generate PBR materials, where both the specially designed multi-branch\nDiT and reference-based DiT blocks adopt a global attention mechanism to\npromote feature interaction and fusion between different views, thereby\nimproving multi-view consistency. In addition, we adopt a PBR-based diffusion\nloss to ensure that the generated materials align with realistic physical\nprinciples. In the refinement stage, we propose a material-refined DiT that\nperforms inpainting in empty areas and enhances details in UV space. Except for\nthe normal condition, this refinement also takes the material map from the\ngeneration stage as an additional condition to reduce the learning difficulty\nand improve generalization. Extensive experiments show that our method achieves\nstate-of-the-art performance in texturing 3D objects with PBR materials and\nprovides significant advantages for graphics relighting applications. Project\nPage: https://lingtengqiu.github.io/2024/MCMat/\n","authors":["Shenhao Zhu","Lingteng Qiu","Xiaodong Gu","Zhengyi Zhao","Chao Xu","Yuxiao He","Zhe Li","Xiaoguang Han","Yao Yao","Xun Cao","Siyu Zhu","Weihao Yuan","Zilong Dong","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.14148v1.pdf","comment":"Project Page: https://lingtengqiu.github.io/2024/MCMat/"},{"id":"http://arxiv.org/abs/2412.14145v1","updated":"2024-12-18T18:43:21Z","published":"2024-12-18T18:43:21Z","title":"Incorporating Feature Pyramid Tokenization and Open Vocabulary Semantic\n  Segmentation","summary":"  The visual understanding are often approached from 3 granular levels: image,\npatch and pixel. Visual Tokenization, trained by self-supervised reconstructive\nlearning, compresses visual data by codebook in patch-level with marginal\ninformation loss, but the visual tokens does not have semantic meaning. Open\nVocabulary semantic segmentation benefits from the evolving Vision-Language\nmodels (VLMs) with strong image zero-shot capability, but transferring\nimage-level to pixel-level understanding remains an imminent challenge. In this\npaper, we treat segmentation as tokenizing pixels and study a united perceptual\nand semantic token compression for all granular understanding and consequently\nfacilitate open vocabulary semantic segmentation. Referring to the cognitive\nprocess of pretrained VLM where the low-level features are progressively\ncomposed to high-level semantics, we propose Feature Pyramid Tokenization (PAT)\nto cluster and represent multi-resolution feature by learnable codebooks and\nthen decode them by joint learning pixel reconstruction and semantic\nsegmentation. We design loosely coupled pixel and semantic learning branches.\nThe pixel branch simulates bottom-up composition and top-down visualization of\ncodebook tokens, while the semantic branch collectively fuse hierarchical\ncodebooks as auxiliary segmentation guidance. Our experiments show that PAT\nenhances the semantic intuition of VLM feature pyramid, improves performance\nover the baseline segmentation model and achieves competitive performance on\nopen vocabulary semantic segmentation benchmark. Our model is\nparameter-efficient for VLM integration and flexible for the independent\ntokenization. We hope to give inspiration not only on improving segmentation\nbut also on semantic visual token utilization.\n","authors":["Jianyu Zhang","Li Zhang","Shijian Li"],"pdf_url":"https://arxiv.org/pdf/2412.14145v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.14123v1","updated":"2024-12-18T18:11:53Z","published":"2024-12-18T18:11:53Z","title":"AnySat: An Earth Observation Model for Any Resolutions, Scales, and\n  Modalities","summary":"  Geospatial models must adapt to the diversity of Earth observation data in\nterms of resolutions, scales, and modalities. However, existing approaches\nexpect fixed input configurations, which limits their practical applicability.\nWe propose AnySat, a multimodal model based on joint embedding predictive\narchitecture (JEPA) and resolution-adaptive spatial encoders, allowing us to\ntrain a single model on highly heterogeneous data in a self-supervised manner.\nTo demonstrate the advantages of this unified approach, we compile GeoPlex, a\ncollection of $5$ multimodal datasets with varying characteristics and $11$\ndistinct sensors. We then train a single powerful model on these diverse\ndatasets simultaneously. Once fine-tuned, we achieve better or near\nstate-of-the-art results on the datasets of GeoPlex and $4$ additional ones for\n$5$ environment monitoring tasks: land cover mapping, tree species\nidentification, crop type classification, change detection, and flood\nsegmentation. The code and models are available at\nhttps://github.com/gastruc/AnySat.\n","authors":["Guillaume Astruc","Nicolas Gonthier","Clement Mallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2412.14123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10178v2","updated":"2024-12-18T18:05:43Z","published":"2024-12-13T14:50:26Z","title":"SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models","summary":"  Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. Although significant\nadvances have been made in image-based virtual try-on, extending these\nsuccesses to video often leads to frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequences. To tackle these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we propose ShiftCaching, a novel technique that\nmaintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the TikTokDress dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments\ndemonstrate that our approach outperforms current baselines, particularly in\nterms of video consistency and inference speed. The project page is available\nat https://swift-try.github.io/.\n","authors":["Hung Nguyen","Quang Qui-Vinh Nguyen","Khoi Nguyen","Rang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2412.10178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14118v1","updated":"2024-12-18T18:04:12Z","published":"2024-12-18T18:04:12Z","title":"GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images","summary":"  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n","authors":["Ziyang Xu","Huangxuan Zhao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14118v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14111v1","updated":"2024-12-18T17:58:16Z","published":"2024-12-18T17:58:16Z","title":"Event-based Photometric Bundle Adjustment","summary":"  We tackle the problem of bundle adjustment (i.e., simultaneous refinement of\ncamera poses and scene map) for a purely rotating event camera. Starting from\nfirst principles, we formulate the problem as a classical non-linear least\nsquares optimization. The photometric error is defined using the event\ngeneration model directly in the camera rotations and the semi-dense scene\nbrightness that triggers the events. We leverage the sparsity of event data to\ndesign a tractable Levenberg-Marquardt solver that handles the very large\nnumber of variables involved. To the best of our knowledge, our method, which\nwe call Event-based Photometric Bundle Adjustment (EPBA), is the first\nevent-only photometric bundle adjustment method that works on the brightness\nmap directly and exploits the space-time characteristics of event data, without\nhaving to convert events into image-like representations. Comprehensive\nexperiments on both synthetic and real-world datasets demonstrate EPBA's\neffectiveness in decreasing the photometric error (by up to 90%), yielding\nresults of unparalleled quality. The refined maps reveal details that were\nhidden using prior state-of-the-art rotation-only estimation methods. The\nexperiments on modern high-resolution event cameras show the applicability of\nEPBA to panoramic imaging in various scenarios (without map initialization, at\nmultiple resolutions, and in combination with other methods, such as IMU dead\nreckoning or previous event-based rotation estimation methods). We make the\nsource code publicly available. https://github.com/tub-rip/epba\n","authors":["Shuang Guo","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2412.14111v1.pdf","comment":"21 pages, 19 figures, 10 tables. Project page:\n  https://github.com/tub-rip/epba"},{"id":"http://arxiv.org/abs/2412.14103v1","updated":"2024-12-18T17:50:15Z","published":"2024-12-18T17:50:15Z","title":"Foundation Models Meet Low-Cost Sensors: Test-Time Adaptation for\n  Rescaling Disparity for Zero-Shot Metric Depth Estimation","summary":"  The recent development of foundation models for monocular depth estimation\nsuch as Depth Anything paved the way to zero-shot monocular depth estimation.\nSince it returns an affine-invariant disparity map, the favored technique to\nrecover the metric depth consists in fine-tuning the model. However, this stage\nis costly to perform because of the training but also due to the creation of\nthe dataset. It must contain images captured by the camera that will be used at\ntest time and the corresponding ground truth. Moreover, the fine-tuning may\nalso degrade the generalizing capacity of the original model. Instead, we\npropose in this paper a new method to rescale Depth Anything predictions using\n3D points provided by low-cost sensors or techniques such as low-resolution\nLiDAR, stereo camera, structure-from-motion where poses are given by an IMU.\nThus, this approach avoids fine-tuning and preserves the generalizing power of\nthe original depth estimation model while being robust to the noise of the\nsensor or of the depth model. Our experiments highlight improvements relative\nto other metric depth estimation methods and competitive results compared to\nfine-tuned approaches. Code available at\nhttps://gitlab.ensta.fr/ssh/monocular-depth-rescaling.\n","authors":["Rémi Marsal","Alexandre Chapoutot","Philippe Xu","David Filliat"],"pdf_url":"https://arxiv.org/pdf/2412.14103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10604v2","updated":"2024-12-18T17:49:32Z","published":"2024-12-13T23:15:35Z","title":"EvalGIM: A Library for Evaluating Generative Image Models","summary":"  As the use of text-to-image generative models increases, so does the adoption\nof automatic benchmarking methods used in their evaluation. However, while\nmetrics and datasets abound, there are few unified benchmarking libraries that\nprovide a framework for performing evaluations across many datasets and\nmetrics. Furthermore, the rapid introduction of increasingly robust\nbenchmarking methods requires that evaluation libraries remain flexible to new\ndatasets and metrics. Finally, there remains a gap in synthesizing evaluations\nin order to deliver actionable takeaways about model performance. To enable\nunified, flexible, and actionable evaluations, we introduce EvalGIM (pronounced\n''EvalGym''), a library for evaluating generative image models. EvalGIM\ncontains broad support for datasets and metrics used to measure quality,\ndiversity, and consistency of text-to-image generative models. In addition,\nEvalGIM is designed with flexibility for user customization as a top priority\nand contains a structure that allows plug-and-play additions of new datasets\nand metrics. To enable actionable evaluation insights, we introduce\n''Evaluation Exercises'' that highlight takeaways for specific evaluation\nquestions. The Evaluation Exercises contain easy-to-use and reproducible\nimplementations of two state-of-the-art evaluation methods of text-to-image\ngenerative models: consistency-diversity-realism Pareto Fronts and\ndisaggregated measurements of performance disparities across groups. EvalGIM\nalso contains Evaluation Exercises that introduce two new analysis methods for\ntext-to-image generative models: robustness analyses of model rankings and\nbalanced evaluations across different prompt styles. We encourage text-to-image\nmodel exploration with EvalGIM and invite contributions at\nhttps://github.com/facebookresearch/EvalGIM/.\n","authors":["Melissa Hall","Oscar Mañas","Reyhane Askari-Hemmat","Mark Ibrahim","Candace Ross","Pietro Astolfi","Tariq Berrada Ifriqi","Marton Havasi","Yohann Benchetrit","Karen Ullrich","Carolina Braga","Abhishek Charnalia","Maeve Ryan","Mike Rabbat","Michal Drozdzal","Jakob Verbeek","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2412.10604v2.pdf","comment":"For code, see https://github.com/facebookresearch/EvalGIM/tree/main"},{"id":"http://arxiv.org/abs/2412.14100v1","updated":"2024-12-18T17:48:32Z","published":"2024-12-18T17:48:32Z","title":"Parameter-efficient Fine-tuning for improved Convolutional Baseline for\n  Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset","summary":"  Automating brain tumor segmentation using deep learning methods is an ongoing\nchallenge in medical imaging. Multiple lingering issues exist including\ndomain-shift and applications in low-resource settings which brings a unique\nset of challenges including scarcity of data. As a step towards solving these\nspecific problems, we propose Convolutional adapter-inspired\nParameter-efficient Fine-tuning (PEFT) of MedNeXt architecture. To validate our\nidea, we show our method performs comparable to full fine-tuning with the added\nbenefit of reduced training compute using BraTS-2021 as pre-training dataset\nand BraTS-Africa as the fine-tuning dataset. BraTS-Africa consists of a small\ndataset (60 train / 35 validation) from the Sub-Saharan African population with\nmarked shift in the MRI quality compared to BraTS-2021 (1251 train samples). We\nfirst show that models trained on BraTS-2021 dataset do not generalize well to\nBraTS-Africa as shown by 20% reduction in mean dice on BraTS-Africa validation\nsamples. Then, we show that PEFT can leverage both the BraTS-2021 and\nBraTS-Africa dataset to obtain mean dice of 0.8 compared to 0.72 when trained\nonly on BraTS-Africa. Finally, We show that PEFT (0.80 mean dice) results in\ncomparable performance to full fine-tuning (0.77 mean dice) which may show PEFT\nto be better on average but the boxplots show that full finetuning results is\nmuch lesser variance in performance. Nevertheless, on disaggregation of the\ndice metrics, we find that the model has tendency to oversegment as shown by\nhigh specificity (0.99) compared to relatively low sensitivity(0.75). The\nsource code is available at\nhttps://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt\n","authors":["Bijay Adhikari","Pratibha Kulung","Jakesh Bohaju","Laxmi Kanta Poudel","Confidence Raymond","Dong Zhang","Udunna C Anazodo","Bishesh Khanal","Mahesh Shakya"],"pdf_url":"https://arxiv.org/pdf/2412.14100v1.pdf","comment":"Accepted to \"The International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference\""},{"id":"http://arxiv.org/abs/2412.14097v1","updated":"2024-12-18T17:47:46Z","published":"2024-12-18T17:47:46Z","title":"Adaptive Concept Bottleneck for Foundation Models Under Distribution\n  Shifts","summary":"  Advancements in foundation models (FMs) have led to a paradigm shift in\nmachine learning. The rich, expressive feature representations from these\npre-trained, large-scale FMs are leveraged for multiple downstream tasks,\nusually via lightweight fine-tuning of a shallow fully-connected network\nfollowing the representation. However, the non-interpretable, black-box nature\nof this prediction pipeline can be a challenge, especially in critical domains\nsuch as healthcare, finance, and security. In this paper, we explore the\npotential of Concept Bottleneck Models (CBMs) for transforming complex,\nnon-interpretable foundation models into interpretable decision-making\npipelines using high-level concept vectors. Specifically, we focus on the\ntest-time deployment of such an interpretable CBM pipeline \"in the wild\", where\nthe input distribution often shifts from the original training distribution. We\nfirst identify the potential failure modes of such a pipeline under different\ntypes of distribution shifts. Then we propose an adaptive concept bottleneck\nframework to address these failure modes, that dynamically adapts the\nconcept-vector bank and the prediction layer based solely on unlabeled data\nfrom the target domain, without access to the source (training) dataset.\nEmpirical evaluations with various real-world distribution shifts show that our\nadaptation method produces concept-based interpretations better aligned with\nthe test data and boosts post-deployment accuracy by up to 28%, aligning the\nCBM performance with that of non-interpretable classification.\n","authors":["Jihye Choi","Jayaram Raghuram","Yixuan Li","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2412.14097v1.pdf","comment":"The preliminary version of the work appeared in the ICML 2024\n  Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2412.14088v1","updated":"2024-12-18T17:34:52Z","published":"2024-12-18T17:34:52Z","title":"Joint Perception and Prediction for Autonomous Driving: A Survey","summary":"  Perception and prediction modules are critical components of autonomous\ndriving systems, enabling vehicles to navigate safely through complex\nenvironments. The perception module is responsible for perceiving the\nenvironment, including static and dynamic objects, while the prediction module\nis responsible for predicting the future behavior of these objects. These\nmodules are typically divided into three tasks: object detection, object\ntracking, and motion prediction. Traditionally, these tasks are developed and\noptimized independently, with outputs passed sequentially from one to the next.\nHowever, this approach has significant limitations: computational resources are\nnot shared across tasks, the lack of joint optimization can amplify errors as\nthey propagate throughout the pipeline, and uncertainty is rarely propagated\nbetween modules, resulting in significant information loss. To address these\nchallenges, the joint perception and prediction paradigm has emerged,\nintegrating perception and prediction into a unified model through multi-task\nlearning. This strategy not only overcomes the limitations of previous methods,\nbut also enables the three tasks to have direct access to raw sensor data,\nallowing richer and more nuanced environmental interpretations. This paper\npresents the first comprehensive survey of joint perception and prediction for\nautonomous driving. We propose a taxonomy that categorizes approaches based on\ninput representation, scene context modeling, and output representation,\nhighlighting their contributions and limitations. Additionally, we present a\nqualitative analysis and quantitative comparison of existing methods. Finally,\nwe discuss future research directions based on identified gaps in the\nstate-of-the-art.\n","authors":["Lucas Dal'Col","Miguel Oliveira","Vítor Santos"],"pdf_url":"https://arxiv.org/pdf/2412.14088v1.pdf","comment":"24 pages, 5 sections, 7 figures, 7 tables. This work has been\n  submitted to the IEEE Transactions on Intelligent Transportation Systems for\n  possible publication"},{"id":"http://arxiv.org/abs/2412.14058v1","updated":"2024-12-18T17:07:20Z","published":"2024-12-18T17:07:20Z","title":"Towards Generalist Robot Policies: What Matters in Building\n  Vision-Language-Action Models","summary":"  Foundation Vision Language Models (VLMs) exhibit strong capabilities in\nmulti-modal representation learning, comprehension, and reasoning. By injecting\naction components into the VLMs, Vision-Language-Action Models (VLAs) can be\nnaturally formed and also show promising performance. Existing work has\ndemonstrated the effectiveness and generalization of VLAs in multiple scenarios\nand tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since\nexisting VLAs differ in their backbones, action-prediction formulations, data\ndistributions, and training recipes. This leads to a missing piece for a\nsystematic understanding of the design choices of VLAs. In this work, we\ndisclose the key factors that significantly influence the performance of VLA\nand focus on answering three essential design choices: which backbone to\nselect, how to formulate the VLA architectures, and when to add\ncross-embodiment data. The obtained results convince us firmly to explain why\nwe need VLA and develop a new family of VLAs, RoboVLMs, which require very few\nmanual designs and achieve a new state-of-the-art performance in three\nsimulation tasks and real-world experiments. Through our extensive experiments,\nwhich include over 8 VLM backbones, 4 policy architectures, and over 600\ndistinct designed experiments, we provide a detailed guidebook for the future\ndesign of VLAs. In addition to the study, the highly flexible RoboVLMs\nframework, which supports easy integrations of new VLMs and free combinations\nof various design choices, is made public to facilitate future research. We\nopen-source all details, including codes, models, datasets, and toolkits, along\nwith detailed training and evaluation recipes at: robovlms.github.io.\n","authors":["Xinghang Li","Peiyan Li","Minghuan Liu","Dong Wang","Jirong Liu","Bingyi Kang","Xiao Ma","Tao Kong","Hanbo Zhang","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14058v1.pdf","comment":"Project page: robovlms.github.io"},{"id":"http://arxiv.org/abs/2412.14056v1","updated":"2024-12-18T17:06:21Z","published":"2024-12-18T17:06:21Z","title":"A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future","summary":"  Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.\n","authors":["Shilin Sun","Wenbin An","Feng Tian","Fang Nan","Qidong Liu","Jun Liu","Nazaraf Shah","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14056v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.14042v1","updated":"2024-12-18T16:55:42Z","published":"2024-12-18T16:55:42Z","title":"CAD-Recode: Reverse Engineering CAD Code from Point Clouds","summary":"  Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\nsignificantly outperforms existing methods across three datasets while\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\nFurthermore, we show that our CAD Python code output is interpretable by\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.\n","authors":["Danila Rukhovich","Elona Dupont","Dimitrios Mallis","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2412.14042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13372v2","updated":"2024-12-18T16:51:18Z","published":"2024-07-18T10:26:53Z","title":"Restore Anything Model via Efficient Degradation Adaptation","summary":"  With the proliferation of mobile devices, the need for an efficient model to\nrestore any degraded image has become increasingly significant and impactful.\nTraditional approaches typically involve training dedicated models for each\nspecific degradation, resulting in inefficiency and redundancy. More recent\nsolutions either introduce additional modules to learn visual prompts\nsignificantly increasing model size or incorporate cross-modal transfer from\nlarge language models trained on vast datasets, adding complexity to the system\narchitecture. In contrast, our approach, termed RAM, takes a unified path that\nleverages inherent similarities across various degradations to enable both\nefficient and comprehensive restoration through a joint embedding mechanism\nwithout scaling up the model or relying on large multimodal models.\nSpecifically, we examine the sub-latent space of each input, identifying key\ncomponents and reweighting them in a gated manner. This intrinsic degradation\nawareness is further combined with contextualized attention in an X-shaped\nframework, enhancing local-global interactions. Extensive benchmarking in an\nall-in-one restoration setting confirms RAM's SOTA performance, reducing model\ncomplexity by approximately 82% in trainable parameters and 85% in FLOPs. Our\ncode and models will be publicly available.\n","authors":["Bin Ren","Eduard Zamfir","Zongwei Wu","Yawei Li","Yidi Li","Danda Pani Paudel","Radu Timofte","Ming-Hsuan Yang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.13372v2.pdf","comment":"Efficient Any Image Restoration"},{"id":"http://arxiv.org/abs/2412.14018v1","updated":"2024-12-18T16:34:51Z","published":"2024-12-18T16:34:51Z","title":"SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical\n  Video Generation","summary":"  Medical video generation has transformative potential for enhancing surgical\nunderstanding and pathology insights through precise and controllable visual\nrepresentations. However, current models face limitations in controllability\nand authenticity. To bridge this gap, we propose SurgSora, a\nmotion-controllable surgical video generation framework that uses a single\ninput frame and user-controllable motion cues. SurgSora consists of three key\nmodules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB\nand depth features from the input frame and integrates them with segmentation\ncues to capture detailed spatial features of complex anatomical structures; the\nDecoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D\nfeatures at multiple scales to enhance temporal understanding and object\nspatial dynamics; and the Trajectory Controller (TC), which allows users to\nspecify motion directions and estimates sparse optical flow, guiding the video\ngeneration process. The fused features are used as conditions for a frozen\nStable Diffusion model to produce realistic, temporally coherent surgical\nvideos. Extensive evaluations demonstrate that SurgSora outperforms\nstate-of-the-art methods in controllability and authenticity, showing its\npotential to advance surgical video generation for medical education, training,\nand research.\n","authors":["Tong Chen","Shuya Yang","Junyi Wang","Long Bai","Hongliang Ren","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14015v1","updated":"2024-12-18T16:32:12Z","published":"2024-12-18T16:32:12Z","title":"Prompting Depth Anything for 4K Resolution Accurate Metric Depth\n  Estimation","summary":"  Prompts play a critical role in unleashing the power of language and vision\nfoundation models for specific tasks. For the first time, we introduce\nprompting into depth foundation models, creating a new paradigm for metric\ndepth estimation termed Prompt Depth Anything. Specifically, we use a low-cost\nLiDAR as the prompt to guide the Depth Anything model for accurate metric depth\noutput, achieving up to 4K resolution. Our approach centers on a concise prompt\nfusion design that integrates the LiDAR at multiple scales within the depth\ndecoder. To address training challenges posed by limited datasets containing\nboth LiDAR depth and precise GT depth, we propose a scalable data pipeline that\nincludes synthetic data LiDAR simulation and real data pseudo GT depth\ngeneration. Our approach sets new state-of-the-arts on the ARKitScenes and\nScanNet++ datasets and benefits downstream applications, including 3D\nreconstruction and generalized robotic grasping.\n","authors":["Haotong Lin","Sida Peng","Jingxiao Chen","Songyou Peng","Jiaming Sun","Minghuan Liu","Hujun Bao","Jiashi Feng","Xiaowei Zhou","Bingyi Kang"],"pdf_url":"https://arxiv.org/pdf/2412.14015v1.pdf","comment":"Project page: https://PromptDA.github.io/"},{"id":"http://arxiv.org/abs/2412.14006v1","updated":"2024-12-18T16:20:40Z","published":"2024-12-18T16:20:40Z","title":"InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal\n  Large Language Models","summary":"  Boosted by Multi-modal Large Language Models (MLLMs), text-guided universal\nsegmentation models for the image and video domains have made rapid progress\nrecently. However, these methods are often developed separately for specific\ndomains, overlooking the similarities in task settings and solutions across\nthese two areas. In this paper, we define the union of referring segmentation\nand reasoning segmentation at both the image and video levels as Instructed\nVisual Segmentation (IVS). Correspondingly, we propose InstructSeg, an\nend-to-end segmentation pipeline equipped with MLLMs for IVS. Specifically, we\nemploy an object-aware video perceiver to extract temporal and object\ninformation from reference frames, facilitating comprehensive video\nunderstanding. Additionally, we introduce vision-guided multi-granularity text\nfusion to better integrate global and detailed text information with\nfine-grained visual guidance. By leveraging multi-task and end-to-end training,\nInstructSeg demonstrates superior performance across diverse image and video\nsegmentation tasks, surpassing both segmentation specialists and MLLM-based\nmethods with a single model. Our code is available at\nhttps://github.com/congvvc/InstructSeg.\n","authors":["Cong Wei","Yujie Zhong","Haoxian Tan","Yingsen Zeng","Yong Liu","Zheng Zhao","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14005v1","updated":"2024-12-18T16:20:21Z","published":"2024-12-18T16:20:21Z","title":"Real-Time Position-Aware View Synthesis from Single-View Input","summary":"  Recent advancements in view synthesis have significantly enhanced immersive\nexperiences across various computer graphics and multimedia applications,\nincluding telepresence, and entertainment. By enabling the generation of new\nperspectives from a single input view, view synthesis allows users to better\nperceive and interact with their environment. However, many state-of-the-art\nmethods, while achieving high visual quality, face limitations in real-time\nperformance, which makes them less suitable for live applications where low\nlatency is critical. In this paper, we present a lightweight, position-aware\nnetwork designed for real-time view synthesis from a single input image and a\ntarget camera pose. The proposed framework consists of a Position Aware\nEmbedding, modeled with a multi-layer perceptron, which efficiently maps\npositional information from the target pose to generate high dimensional\nfeature maps. These feature maps, along with the input image, are fed into a\nRendering Network that merges features from dual encoder branches to resolve\nboth high level semantics and low level details, producing a realistic new view\nof the scene. Experimental results demonstrate that our method achieves\nsuperior efficiency and visual quality compared to existing approaches,\nparticularly in handling complex translational movements without explicit\ngeometric operations like warping. This work marks a step toward enabling\nreal-time view synthesis from a single image for live and interactive\napplications.\n","authors":["Manu Gond","Emin Zerman","Sebastian Knorr","Mårten Sjöström"],"pdf_url":"https://arxiv.org/pdf/2412.14005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13983v1","updated":"2024-12-18T16:05:40Z","published":"2024-12-18T16:05:40Z","title":"GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians","summary":"  Rendering photorealistic head avatars from arbitrary viewpoints is crucial\nfor various applications like virtual reality. Although previous methods based\non Neural Radiance Fields (NeRF) can achieve impressive results, they lack\nfidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have\nimproved rendering quality and real-time performance but still require\nsignificant storage overhead. In this paper, we introduce a method called\nGraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians\nfor the head avatar. Specifically, GraphAvatar trains a geometric GNN and an\nappearance GNN to generate the attributes of the 3D Gaussians from the tracked\nmesh. Therefore, our method can store the GNN models instead of the 3D\nGaussians, significantly reducing the storage overhead to just 10MB. To reduce\nthe impact of face-tracking errors, we also present a novel graph-guided\noptimization module to refine face-tracking parameters during training.\nFinally, we introduce a 3D-aware enhancer for post-processing to enhance the\nrendering quality. We conduct comprehensive experiments to demonstrate the\nadvantages of GraphAvatar, surpassing existing methods in visual fidelity and\nstorage consumption. The ablation study sheds light on the trade-offs between\nrendering quality and model size. The code will be released at:\nhttps://github.com/ucwxb/GraphAvatar\n","authors":["Xiaobao Wei","Peng Chen","Ming Lu","Hui Chen","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13983v1.pdf","comment":"accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2410.13016v3","updated":"2024-12-18T16:01:44Z","published":"2024-10-16T20:18:21Z","title":"Interpreting and Analysing CLIP's Zero-Shot Image Classification via\n  Mutual Knowledge","summary":"  Contrastive Language-Image Pretraining (CLIP) performs zero-shot image\nclassification by mapping images and textual class representation into a shared\nembedding space, then retrieving the class closest to the image. This work\nprovides a new approach for interpreting CLIP models for image classification\nfrom the lens of mutual knowledge between the two modalities. Specifically, we\nask: what concepts do both vision and language CLIP encoders learn in common\nthat influence the joint embedding space, causing points to be closer or\nfurther apart? We answer this question via an approach of textual concept-based\nexplanations, showing their effectiveness, and perform an analysis encompassing\na pool of 13 CLIP models varying in architecture, size and pretraining\ndatasets. We explore those different aspects in relation to mutual knowledge,\nand analyze zero-shot predictions. Our approach demonstrates an effective and\nhuman-friendly way of understanding zero-shot classification decisions with\nCLIP.\n","authors":["Fawaz Sammani","Nikos Deligiannis"],"pdf_url":"https://arxiv.org/pdf/2410.13016v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.11657v2","updated":"2024-12-18T15:56:51Z","published":"2024-12-16T11:00:02Z","title":"CNNtention: Can CNNs do better with Attention?","summary":"  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n","authors":["Julian Glattki","Nikhil Kapila","Tejas Rathi"],"pdf_url":"https://arxiv.org/pdf/2412.11657v2.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.03401v2","updated":"2024-12-18T15:47:57Z","published":"2024-12-04T15:32:37Z","title":"Benchmarking Pretrained Attention-based Models for Real-Time Recognition\n  in Robot-Assisted Esophagectomy","summary":"  Esophageal cancer is among the most common types of cancer worldwide. It is\ntraditionally treated using open esophagectomy, but in recent years,\nrobot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a\npromising alternative. However, robot-assisted surgery can be challenging for\nnovice surgeons, as they often suffer from a loss of spatial orientation.\nComputer-aided anatomy recognition holds promise for improving surgical\nnavigation, but research in this area remains limited. In this study, we\ndeveloped a comprehensive dataset for semantic segmentation in RAMIE, featuring\nthe largest collection of vital anatomical structures and surgical instruments\nto date. Handling this diverse set of classes presents challenges, including\nclass imbalance and the recognition of complex structures such as nerves. This\nstudy aims to understand the challenges and limitations of current\nstate-of-the-art algorithms on this novel dataset and problem. Therefore, we\nbenchmarked eight real-time deep learning models using two pretraining\ndatasets. We assessed both traditional and attention-based networks,\nhypothesizing that attention-based networks better capture global patterns and\naddress challenges such as occlusion caused by blood or other tissues. The\nbenchmark includes our RAMIE dataset and the publicly available CholecSeg8k\ndataset, enabling a thorough assessment of surgical segmentation tasks. Our\nfindings indicate that pretraining on ADE20k, a dataset for semantic\nsegmentation, is more effective than pretraining on ImageNet. Furthermore,\nattention-based models outperform traditional convolutional neural networks,\nwith SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former\nadditionally excelling in average symmetric surface distance.\n","authors":["Ronald L. P. D. de Jong","Yasmina al Khalil","Tim J. M. Jaspers","Romy C. van Jaarsveld","Gino M. Kuiper","Yiping Li","Richard van Hillegersberg","Jelle P. Ruurda","Marcel Breeuwer","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2412.03401v2.pdf","comment":"Accepted for presentation at the SPIE Medical Imaging Conference,\n  2025"},{"id":"http://arxiv.org/abs/2412.13949v1","updated":"2024-12-18T15:29:30Z","published":"2024-12-18T15:29:30Z","title":"Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence","summary":"  Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.\n","authors":["Jinghan He","Kuan Zhu","Haiyun Guo","Junfeng Fang","Zhenglin Hua","Yuheng Jia","Ming Tang","Tat-Seng Chua","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13947v1","updated":"2024-12-18T15:28:08Z","published":"2024-12-18T15:28:08Z","title":"Real Classification by Description: Extending CLIP's Limits of Part\n  Attributes Recognition","summary":"  In this study, we define and tackle zero shot \"real\" classification by\ndescription, a novel task that evaluates the ability of Vision-Language Models\n(VLMs) like CLIP to classify objects based solely on descriptive attributes,\nexcluding object class names. This approach highlights the current limitations\nof VLMs in understanding intricate object descriptions, pushing these models\nbeyond mere object recognition. To facilitate this exploration, we introduce a\nnew challenge and release description data for six popular fine-grained\nbenchmarks, which omit object names to encourage genuine zero-shot learning\nwithin the research community. Additionally, we propose a method to enhance\nCLIP's attribute detection capabilities through targeted training using\nImageNet21k's diverse object categories, paired with rich attribute\ndescriptions generated by large language models. Furthermore, we introduce a\nmodified CLIP architecture that leverages multiple resolutions to improve the\ndetection of fine-grained part attributes. Through these efforts, we broaden\nthe understanding of part-attribute recognition in CLIP, improving its\nperformance in fine-grained classification tasks across six popular benchmarks,\nas well as in the PACO dataset, a widely used benchmark for object-attribute\nrecognition. Code is available at:\nhttps://github.com/ethanbar11/grounding_ge_public.\n","authors":["Ethan Baron","Idan Tankel","Peter Tu","Guy Ben-Yosef"],"pdf_url":"https://arxiv.org/pdf/2412.13947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13943v1","updated":"2024-12-18T15:25:36Z","published":"2024-12-18T15:25:36Z","title":"On Explaining Knowledge Distillation: Measuring and Visualising the\n  Knowledge Transfer Process","summary":"  Knowledge distillation (KD) remains challenging due to the opaque nature of\nthe knowledge transfer process from a Teacher to a Student, making it difficult\nto address certain issues related to KD. To address this, we proposed UniCAM, a\nnovel gradient-based visual explanation method, which effectively interprets\nthe knowledge learned during KD. Our experimental results demonstrate that with\nthe guidance of the Teacher's knowledge, the Student model becomes more\nefficient, learning more relevant features while discarding those that are not\nrelevant. We refer to the features learned with the Teacher's guidance as\ndistilled features and the features irrelevant to the task and ignored by the\nStudent as residual features. Distilled features focus on key aspects of the\ninput, such as textures and parts of objects. In contrast, residual features\ndemonstrate more diffused attention, often targeting irrelevant areas,\nincluding the backgrounds of the target objects. In addition, we proposed two\nnovel metrics: the feature similarity score (FSS) and the relevance score (RS),\nwhich quantify the relevance of the distilled knowledge. Experiments on the\nCIFAR10, ASIRRA, and Plant Disease datasets demonstrate that UniCAM and the two\nmetrics offer valuable insights to explain the KD process.\n","authors":["Gereziher Adhane","Mohammad Mahdi Dehshibi","Dennis Vetter","David Masip","Gemma Roig"],"pdf_url":"https://arxiv.org/pdf/2412.13943v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV'25). Includes 5 pages of supplementary material"},{"id":"http://arxiv.org/abs/2412.13187v2","updated":"2024-12-18T15:19:55Z","published":"2024-12-17T18:58:33Z","title":"HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction","summary":"  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results https://www.chenbao.tech/handsonvlm/\n","authors":["Chen Bao","Jiarui Xu","Xiaolong Wang","Abhinav Gupta","Homanga Bharadhwaj"],"pdf_url":"https://arxiv.org/pdf/2412.13187v2.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2403.13352v4","updated":"2024-12-18T15:14:48Z","published":"2024-03-20T07:31:07Z","title":"AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in\n  Text-to-Image Generation","summary":"  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nimage generation. Despite their progress, challenges remain in both\nprompt-following ability, image quality and lack of high-quality datasets,\nwhich are essential for refining these models. As acquiring labeled data is\ncostly, we introduce AGFSync, a framework that enhances T2I diffusion models\nthrough Direct Preference Optimization (DPO) in a fully AI-driven approach.\nAGFSync utilizes Vision-Language Models (VLM) to assess image quality across\nstyle, coherence, and aesthetics, generating feedback data within an AI-driven\nloop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and\nSDXL-base, our extensive experiments on the TIFA dataset demonstrate notable\nimprovements in VQA scores, aesthetic evaluations, and performance on the HPSv2\nbenchmark, consistently outperforming the base models. AGFSync's method of\nrefining T2I diffusion models paves the way for scalable alignment techniques.\nOur code and dataset are publicly available at\nhttps://anjingkun.github.io/AGFSync.\n","authors":["Jingkun An","Yinghao Zhu","Zongjian Li","Enshen Zhou","Haoran Feng","Xijie Huang","Bohua Chen","Yemin Shi","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.13352v4.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2307.16879v2","updated":"2024-12-18T15:07:39Z","published":"2023-07-31T17:45:16Z","title":"Image Synthesis under Limited Data: A Survey and Taxonomy","summary":"  Deep generative models, which target reproducing the given data distribution\nto produce novel samples, have made unprecedented advancements in recent years.\nTheir technical breakthroughs have enabled unparalleled quality in the\nsynthesis of visual content. However, one critical prerequisite for their\ntremendous success is the availability of a sufficient number of training\nsamples, which requires massive computation resources. When trained on limited\ndata, generative models tend to suffer from severe performance deterioration\ndue to overfitting and memorization. Accordingly, researchers have devoted\nconsiderable attention to develop novel models that are capable of generating\nplausible and diverse images from limited training data recently. Despite\nnumerous efforts to enhance training stability and synthesis quality in the\nlimited data scenarios, there is a lack of a systematic survey that provides 1)\na clear problem definition, critical challenges, and taxonomy of various tasks;\n2) an in-depth analysis on the pros, cons, and remain limitations of existing\nliterature; as well as 3) a thorough discussion on the potential applications\nand future directions in the field of image synthesis under limited data. In\norder to fill this gap and provide a informative introduction to researchers\nwho are new to this topic, this survey offers a comprehensive review and a\nnovel taxonomy on the development of image synthesis under limited data. In\nparticular, it covers the problem definition, requirements, main solutions,\npopular benchmarks, and remain challenges in a comprehensive and all-around\nmanner.\n","authors":["Mengping Yang","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2307.16879v2.pdf","comment":"230 references, 25 pages. GitHub:\n  https://github.com/kobeshegu/awesome-few-shot-generation"},{"id":"http://arxiv.org/abs/2412.13916v1","updated":"2024-12-18T14:56:03Z","published":"2024-12-18T14:56:03Z","title":"Retrieval Augmented Image Harmonization","summary":"  When embedding objects (foreground) into images (background), considering the\ninfluence of photography conditions like illumination, it is usually necessary\nto perform image harmonization to make the foreground object coordinate with\nthe background image in terms of brightness, color, and etc. Although existing\nimage harmonization methods have made continuous efforts toward visually\npleasing results, they are still plagued by two main issues. Firstly, the image\nharmonization becomes highly ill-posed when there are no contents similar to\nthe foreground object in the background, making the harmonization results\nunreliable. Secondly, even when similar contents are available, the\nharmonization process is often interfered with by irrelevant areas, mainly\nattributed to an insufficient understanding of image contents and inaccurate\nattention. As a remedy, we present a retrieval-augmented image harmonization\n(Raiha) framework, which seeks proper reference images to reduce the\nill-posedness and restricts the attention to better utilize the useful\ninformation. Specifically, an efficient retrieval method is designed to find\nreference images that contain similar objects as the foreground while the\nillumination is consistent with the background. For training the Raiha\nframework to effectively utilize the reference information, a data augmentation\nstrategy is delicately designed by leveraging existing non-reference image\nharmonization datasets. Besides, the image content priors are introduced to\nensure reasonable attention. With the presented Raiha framework, the image\nharmonization performance is greatly boosted under both non-reference and\nretrieval-augmented settings. The source code and pre-trained models will be\npublicly available.\n","authors":["Haolin Wang","Ming Liu","Zifei Yan","Chao Zhou","Longan Xiao","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2412.13916v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.13913v1","updated":"2024-12-18T14:53:38Z","published":"2024-12-18T14:53:38Z","title":"A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye\n  View Detection","summary":"  Camera-based Bird's Eye View (BEV) perception models receive increasing\nattention for their crucial role in autonomous driving, a domain where concerns\nabout the robustness and reliability of deep learning have been raised. While\nonly a few works have investigated the effects of randomly generated semantic\nperturbations, aka natural corruptions, on the multi-view BEV detection task,\nwe develop a black-box robustness evaluation framework that adversarially\noptimises three common semantic perturbations: geometric transformation, colour\nshifting, and motion blur, to deceive BEV models, serving as the first approach\nin this emerging field. To address the challenge posed by optimising the\nsemantic perturbation, we design a smoothed, distance-based surrogate function\nto replace the mAP metric and introduce SimpleDIRECT, a deterministic\noptimisation algorithm that utilises observed slopes to guide the optimisation\nprocess. By comparing with randomised perturbation and two optimisation\nbaselines, we demonstrate the effectiveness of the proposed framework.\nAdditionally, we provide a benchmark on the semantic robustness of ten recent\nBEV models. The results reveal that PolarFormer, which emphasises geometric\ninformation from multi-view images, exhibits the highest robustness, whereas\nBEVDet is fully compromised, with its precision reduced to zero.\n","authors":["Fu Wang","Yanghao Zhang","Xiangyu Yin","Guangliang Cheng","Zeyu Fu","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.13913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13908v1","updated":"2024-12-18T14:51:25Z","published":"2024-12-18T14:51:25Z","title":"Memorizing SAM: 3D Medical Segment Anything Model with Memorizing\n  Transformer","summary":"  Segment Anything Models (SAMs) have gained increasing attention in medical\nimage analysis due to their zero-shot generalization capability in segmenting\nobjects of unseen classes and domains when provided with appropriate user\nprompts. Addressing this performance gap is important to fully leverage the\npre-trained weights of SAMs, particularly in the domain of volumetric medical\nimage segmentation, where accuracy is important but well-annotated 3D medical\ndata for fine-tuning is limited. In this work, we investigate whether\nintroducing the memory mechanism as a plug-in, specifically the ability to\nmemorize and recall internal representations of past inputs, can improve the\nperformance of SAM with limited computation cost. To this end, we propose\nMemorizing SAM, a novel 3D SAM architecture incorporating a memory Transformer\nas a plug-in. Unlike conventional memorizing Transformers that save the\ninternal representation during training or inference, our Memorizing SAM\nutilizes existing highly accurate internal representation as the memory source\nto ensure the quality of memory. We evaluate the performance of Memorizing SAM\nin 33 categories from the TotalSegmentator dataset, which indicates that\nMemorizing SAM can outperform state-of-the-art 3D SAM variant i.e., FastSAM3D\nwith an average Dice increase of 11.36% at the cost of only 4.38 millisecond\nincrease in inference time. The source code is publicly available at\nhttps://github.com/swedfr/memorizingSAM\n","authors":["Xinyuan Shao","Yiqing Shen","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2412.13908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.06740v2","updated":"2024-12-18T14:41:53Z","published":"2022-05-13T16:19:21Z","title":"Towards Deployable OCR models for Indic languages","summary":"  Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/\n","authors":["Minesh Mathew","Ajoy Mondal","CV Jawahar"],"pdf_url":"https://arxiv.org/pdf/2205.06740v2.pdf","comment":"presented at ICPR 2024;\n  https://link.springer.com/chapter/10.1007/978-3-031-78495-8_11"},{"id":"http://arxiv.org/abs/2412.13897v1","updated":"2024-12-18T14:39:43Z","published":"2024-12-18T14:39:43Z","title":"Data-Efficient Inference of Neural Fluid Fields via SciML Foundation\n  Model","summary":"  Recent developments in 3D vision have enabled successful progress in\ninferring neural fluid fields and realistic rendering of fluid dynamics.\nHowever, these methods require real-world flow captures, which demand dense\nvideo sequences and specialized lab setups, making the process costly and\nchallenging. Scientific machine learning (SciML) foundation models, which are\npretrained on extensive simulations of partial differential equations (PDEs),\nencode rich multiphysics knowledge and thus provide promising sources of domain\npriors for inferring fluid fields. Nevertheless, their potential to advance\nreal-world vision problems remains largely underexplored, raising questions\nabout the transferability and practical utility of these foundation models. In\nthis work, we demonstrate that SciML foundation model can significantly improve\nthe data efficiency of inferring real-world 3D fluid dynamics with improved\ngeneralization. At the core of our method is leveraging the strong forecasting\ncapabilities and meaningful representations of SciML foundation models. We\nequip neural fluid fields with a novel collaborative training approach that\nutilizes augmented views and fluid features extracted by our foundation model.\nOur method demonstrates significant improvements in both quantitative metrics\nand visual quality, showcasing the practical applicability of SciML foundation\nmodels in real-world fluid dynamics.\n","authors":["Yuqiu Liu","Jingxuan Xu","Mauricio Soroco","Yunchao Wei","Wuyang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13884v1","updated":"2024-12-18T14:23:54Z","published":"2024-12-18T14:23:54Z","title":"Navigating limitations with precision: A fine-grained ensemble approach\n  to wrist pathology recognition on a limited x-ray dataset","summary":"  The exploration of automated wrist fracture recognition has gained\nconsiderable research attention in recent years. In practical medical\nscenarios, physicians and surgeons may lack the specialized expertise required\nfor accurate X-ray interpretation, highlighting the need for machine vision to\nenhance diagnostic accuracy. However, conventional recognition techniques face\nchallenges in discerning subtle differences in X-rays when classifying wrist\npathologies, as many of these pathologies, such as fractures, can be small and\nhard to distinguish. This study tackles wrist pathology recognition as a\nfine-grained visual recognition (FGVR) problem, utilizing a limited,\ncustom-curated dataset that mirrors real-world medical constraints, relying\nsolely on image-level annotations. We introduce a specialized FGVR-based\nensemble approach to identify discriminative regions within X-rays. We employ\nan Explainable AI (XAI) technique called Grad-CAM to pinpoint these regions.\nOur ensemble approach outperformed many conventional SOTA and FGVR techniques,\nunderscoring the effectiveness of our strategy in enhancing accuracy in wrist\npathology recognition.\n","authors":["Ammar Ahmed","Ali Shariq Imran","Mohib Ullah","Zenun Kastrati","Sher Muhammad Daudpota"],"pdf_url":"https://arxiv.org/pdf/2412.13884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13875v1","updated":"2024-12-18T14:16:40Z","published":"2024-12-18T14:16:40Z","title":"Denoising Nearest Neighbor Graph via Continuous CRF for Visual\n  Re-ranking without Fine-tuning","summary":"  Visual re-ranking using Nearest Neighbor graph~(NN graph) has been adapted to\nyield high retrieval accuracy, since it is beneficial to exploring an\nhigh-dimensional manifold and applicable without additional fine-tuning. The\nquality of visual re-ranking using NN graph, however, is limited to that of\nconnectivity, i.e., edges of the NN graph. Some edges can be misconnected with\nnegative images. This is known as a noisy edge problem, resulting in a\ndegradation of the retrieval quality. To address this, we propose a\ncomplementary denoising method based on Continuous Conditional Random Field\n(C-CRF) that uses a statistical distance of our similarity-based distribution.\nThis method employs the concept of cliques to make the process computationally\nfeasible. We demonstrate the complementarity of our method through its\napplication to three visual re-ranking methods, observing quality boosts in\nlandmark retrieval and person re-identification (re-ID).\n","authors":["Jaeyoon Kim","Yoonki Cho","Taeyong Kim","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.13875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13871v1","updated":"2024-12-18T14:07:46Z","published":"2024-12-18T14:07:46Z","title":"LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via\n  Hierarchical Window Transformer","summary":"  In multimodal large language models (MLLMs), vision transformers (ViTs) are\nwidely employed for visual encoding. However, their performance in solving\nuniversal MLLM tasks is not satisfactory. We attribute it to a lack of\ninformation from diverse visual levels, impeding alignment with the various\nsemantic granularity required for language generation. To address this issue,\nwe present LLaVA-UHD v2, an advanced MLLM centered around a Hierarchical window\ntransformer that enables capturing diverse visual granularity by constructing\nand integrating a high-resolution feature pyramid. As a vision-language\nprojector, Hiwin transformer comprises two primary modules: (i) an inverse\nfeature pyramid, constructed by a ViT-derived feature up-sampling process\nutilizing high-frequency details from an image pyramid, and (ii) hierarchical\nwindow attention, focusing on a set of key sampling features within cross-scale\nwindows to condense multi-level feature maps. Extensive experiments demonstrate\nthat LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular\nbenchmarks. Notably, our design brings an average boost of 3.7% across 14\nbenchmarks compared with the baseline method, 9.3% on DocVQA for instance. We\nmake all the data, model checkpoint, and code publicly available to facilitate\nfuture research.\n","authors":["Yipeng Zhang","Yifan Liu","Zonghao Guo","Yidan Zhang","Xuesong Yang","Chi Chen","Jun Song","Bo Zheng","Yuan Yao","Zhiyuan Liu","Tat-Seng Chua","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2412.13871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20008v2","updated":"2024-12-18T13:54:02Z","published":"2024-05-30T12:45:34Z","title":"Sharing Key Semantics in Transformer Makes Efficient Image Restoration","summary":"  Image Restoration (IR), a classic low-level vision task, has witnessed\nsignificant advancements through deep models that effectively model global\ninformation. Notably, the emergence of Vision Transformers (ViTs) has further\npropelled these advancements. When computing, the self-attention mechanism, a\ncornerstone of ViTs, tends to encompass all global cues, even those from\nsemantically unrelated objects or regions. This inclusivity introduces\ncomputational inefficiencies, particularly noticeable with high input\nresolution, as it requires processing irrelevant information, thereby impeding\nefficiency. Additionally, for IR, it is commonly noted that small segments of a\ndegraded image, particularly those closely aligned semantically, provide\nparticularly relevant information to aid in the restoration process, as they\ncontribute essential contextual cues crucial for accurate reconstruction. To\naddress these challenges, we propose boosting IR's performance by sharing the\nkey semantics via Transformer for IR (\\ie, SemanIR) in this paper.\nSpecifically, SemanIR initially constructs a sparse yet comprehensive\nkey-semantic dictionary within each transformer stage by establishing essential\nsemantic connections for every degraded patch. Subsequently, this dictionary is\nshared across all subsequent transformer blocks within the same stage. This\nstrategy optimizes attention calculation within each block by focusing\nexclusively on semantically related components stored in the key-semantic\ndictionary. As a result, attention calculation achieves linear computational\ncomplexity within each window. Extensive experiments across 6 IR tasks confirm\nthe proposed SemanIR's state-of-the-art performance, quantitatively and\nqualitatively showcasing advancements. The visual results, code, and trained\nmodels are available at https://github.com/Amazingren/SemanIR.\n","authors":["Bin Ren","Yawei Li","Jingyun Liang","Rakesh Ranjan","Mengyuan Liu","Rita Cucchiara","Luc Van Gool","Ming-Hsuan Yang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2405.20008v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2412.13859v1","updated":"2024-12-18T13:53:16Z","published":"2024-12-18T13:53:16Z","title":"Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models","summary":"  Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible.\n","authors":["Anna Scius-Bertrand","Michael Jungo","Lars Vögtlin","Jean-Marc Spat","Andreas Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.13859v1.pdf","comment":"ICPR 2024"},{"id":"http://arxiv.org/abs/2304.06376v2","updated":"2024-12-18T13:53:04Z","published":"2023-04-13T10:01:29Z","title":"Signal Reconstruction from Samples at Unknown Locations with Application\n  to 2D Unknown View Tomography","summary":"  It is well known that a band-limited signal can be reconstructed from its\nuniformly spaced samples if the sampling rate is sufficiently high. More\nrecently, it has been proved that one can reconstruct a 1D band-limited signal\neven if the exact sample locations are unknown, but given a uniform\ndistribution of the sample locations and their ordering in 1D. In this work, we\nextend the analytical error bounds in such scenarios for quasi-bandlimited\n(QBL) signals, and for the case of arbitrary but known sampling distributions.\nWe also prove that such reconstruction methods are resilient to a certain\nproportion of errors in the specification of the sample location ordering. We\nthen express the problem of tomographic reconstruction of 2D images from 1D\nRadon projections under unknown angles (2D UVT) with known angle distribution,\nas a special case for reconstruction of QBL signals from samples at unknown\nlocations with known distribution. Building upon our theoretical background, we\npresent asymptotic bounds for 2D QBL image reconstruction from 1D Radon\nprojections in the unknown angles setting, and present an extensive set of\nsimulations to verify these bounds in varied parameter regimes. To the best of\nour knowledge, this is the first piece of work to perform such an analysis for\n2D UVT and explicitly relate it to advances in sampling theory, even though the\nassociated reconstruction algorithms have been known for a long time.\n","authors":["Sheel Shah","Kaishva Shah","Karthik S. Gurumoorthy","Ajit Rajwade"],"pdf_url":"https://arxiv.org/pdf/2304.06376v2.pdf","comment":"This is a preprint of a paper accepted to Signal Processing\n  (Elsevier)"},{"id":"http://arxiv.org/abs/2412.13857v1","updated":"2024-12-18T13:52:42Z","published":"2024-12-18T13:52:42Z","title":"Diagnosising Helicobacter pylori using AutoEncoders and Limited\n  Annotations through Anomalous Staining Patterns in IHC Whole Slide Images","summary":"  Purpose: This work addresses the detection of Helicobacter pylori (H. pylori)\nin histological images with immunohistochemical staining. This analysis is a\ntime demanding task, currently done by an expert pathologist that visually\ninspects the samples. Given the effort required to localise the pathogen in\nimages, a limited number of annotations might be available in an initial\nsetting. Our goal is to design an approach that, using a limited set of\nannotations, is capable of obtaining results good enough to be used as a\nsupport tool. Methods: We propose to use autoencoders to learn the latent\npatterns of healthy patches and formulate a specific measure of the\nreconstruction error of the image in HSV space. ROC analysis is used to set the\noptimal threshold of this measure and the percentage of positive patches in a\nsample that determines the presence of H. pylori. Results: Our method has been\ntested on an own database of 245 Whole Slide Images (WSI) having 117 cases\nwithout H. pylori and different density of the bacteria in the remaining ones.\nThe database has 1211 annotated patches, with only 163 positive patches. This\ndataset of positive annotations was used to train a baseline thresholding and\nan SVM using the features of a pre-trained RedNet18 and ViT models. A 10-fold\ncross-validation shows that our method has better performance with 91%\naccuracy, 86% sensitivity, 96% specificity and 0.97 AUC in the diagnosis of H.\npylori. Conclusion: Unlike classification approaches, our shallow autoencoder\nwith threshold adaptation for the detection of anomalous staining is able to\nachieve competitive results with a limited set of annotated data. This initial\napproach is good enough to be used as a guide for fast annotation of infected\npatches.\n","authors":["Pau Cano","Eva Musulen","Debora Gil"],"pdf_url":"https://arxiv.org/pdf/2412.13857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13856v1","updated":"2024-12-18T13:52:20Z","published":"2024-12-18T13:52:20Z","title":"A Systematic Analysis of Input Modalities for Fracture Classification of\n  the Paediatric Wrist","summary":"  Fractures, particularly in the distal forearm, are among the most common\ninjuries in children and adolescents, with approximately 800 000 cases treated\nannually in Germany. The AO/OTA system provides a structured fracture type\nclassification, which serves as the foundation for treatment decisions.\nAlthough accurately classifying fractures can be challenging, current deep\nlearning models have demonstrated performance comparable to that of experienced\nradiologists. While most existing approaches rely solely on radiographs, the\npotential impact of incorporating other additional modalities, such as\nautomatic bone segmentation, fracture location, and radiology reports, remains\nunderexplored. In this work, we systematically analyse the contribution of\nthese three additional information types, finding that combining them with\nradiographs increases the AUROC from 91.71 to 93.25. Our code is available on\nGitHub.\n","authors":["Ron Keuth","Maren Balks","Sebastian Tschauner","Ludger Tüshaus","Mattias Heinrich"],"pdf_url":"https://arxiv.org/pdf/2412.13856v1.pdf","comment":"Code available on\n  https://github.com/multimodallearning/AO_Classification"},{"id":"http://arxiv.org/abs/2404.09507v2","updated":"2024-12-18T13:50:13Z","published":"2024-04-15T06:58:09Z","title":"Clothes-Changing Person Re-Identification with Feasibility-Aware\n  Intermediary Matching","summary":"  Current clothes-changing person re-identification (re-id) approaches usually\nperform retrieval based on clothes-irrelevant features, while neglecting the\npotential of clothes-relevant features. However, we observe that relying solely\non clothes-irrelevant features for clothes-changing re-id is limited, since\nthey often lack adequate identity information and suffer from large intra-class\nvariations. On the contrary, clothes-relevant features can be used to discover\nsame-clothes intermediaries that possess informative identity clues. Based on\nthis observation, we propose a Feasibility-Aware Intermediary Matching (FAIM)\nframework to additionally utilize clothes-relevant features for retrieval.\nFirstly, an Intermediary Matching (IM) module is designed to perform an\nintermediary-assisted matching process. This process involves using\nclothes-relevant features to find informative intermediates, and then using\nclothes-irrelevant features of these intermediates to complete the matching.\nSecondly, in order to reduce the negative effect of low-quality intermediaries,\nan Intermediary-Based Feasibility Weighting (IBFW) module is designed to\nevaluate the feasibility of intermediary matching process by assessing the\nquality of intermediaries. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods on several widely-used clothes-changing\nre-id benchmarks.\n","authors":["Jiahe Zhao","Ruibing Hou","Hong Chang","Xinqian Gu","Bingpeng Ma","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01476v3","updated":"2024-12-18T13:47:38Z","published":"2024-06-03T16:05:25Z","title":"DreamPhysics: Learning Physics-Based 3D Dynamics with Video Diffusion\n  Priors","summary":"  Dynamic 3D interaction has been attracting a lot of attention recently.\nHowever, creating such 4D content remains challenging. One solution is to\nanimate 3D scenes with physics-based simulation, which requires manually\nassigning precise physical properties to the object or the simulated results\nwould become unnatural. Another solution is to learn the deformation of 3D\nobjects with the distillation of video generative models, which, however, tends\nto produce 3D videos with small and discontinuous motions due to the\ninappropriate extraction and application of physics priors. In this work, to\ncombine the strengths and complementing shortcomings of the above two\nsolutions, we propose to learn the physical properties of a material field with\nvideo diffusion priors, and then utilize a physics-based Material-Point-Method\n(MPM) simulator to generate 4D content with realistic motions. In particular,\nwe propose motion distillation sampling to emphasize video motion information\nduring distillation. In addition, to facilitate the optimization, we further\npropose a KAN-based material field with frame boosting. Experimental results\ndemonstrate that our method enjoys more realistic motions than\nstate-of-the-arts do.\n","authors":["Tianyu Huang","Haoze Zhang","Yihan Zeng","Zhilu Zhang","Hui Li","Wangmeng Zuo","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2406.01476v3.pdf","comment":"Accepted by AAAI 2025. Codes are released at:\n  https://github.com/tyhuang0428/DreamPhysics"},{"id":"http://arxiv.org/abs/2412.13848v1","updated":"2024-12-18T13:42:06Z","published":"2024-12-18T13:42:06Z","title":"MobiFuse: A High-Precision On-device Depth Perception System with\n  Multi-Data Fusion","summary":"  We present MobiFuse, a high-precision depth perception system on mobile\ndevices that combines dual RGB and Time-of-Flight (ToF) cameras. To achieve\nthis, we leverage physical principles from various environmental factors to\npropose the Depth Error Indication (DEI) modality, characterizing the depth\nerror of ToF and stereo-matching. Furthermore, we employ a progressive fusion\nstrategy, merging geometric features from ToF and stereo depth maps with depth\nerror features from the DEI modality to create precise depth maps.\nAdditionally, we create a new ToF-Stereo depth dataset, RealToF, to train and\nvalidate our model. Our experiments demonstrate that MobiFuse excels over\nbaselines by significantly reducing depth measurement errors by up to 77.7%. It\nalso showcases strong generalization across diverse datasets and proves\neffectiveness in two downstream tasks: 3D reconstruction and 3D segmentation.\nThe demo video of MobiFuse in real-life scenarios is available at the\nde-identified YouTube link(https://youtu.be/jy-Sp7T1LVs).\n","authors":["Jinrui Zhang","Deyu Zhang","Tingting Long","Wenxin Chen","Ju Ren","Yunxin Liu","Yudong Zhao","Yaoxue Zhang","Youngki Lee"],"pdf_url":"https://arxiv.org/pdf/2412.13848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13845v1","updated":"2024-12-18T13:38:06Z","published":"2024-12-18T13:38:06Z","title":"Do Language Models Understand Time?","summary":"  Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond.\n","authors":["Xi Ding","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13845v1.pdf","comment":"Research report"},{"id":"http://arxiv.org/abs/2412.12525v2","updated":"2024-12-18T13:37:48Z","published":"2024-12-17T04:33:31Z","title":"CREST: An Efficient Conjointly-trained Spike-driven Framework for\n  Event-based Object Detection Exploiting Spatiotemporal Dynamics","summary":"  Event-based cameras feature high temporal resolution, wide dynamic range, and\nlow power consumption, which is ideal for high-speed and low-light object\ndetection. Spiking neural networks (SNNs) are promising for event-based object\nrecognition and detection due to their spiking nature but lack efficient\ntraining methods, leading to gradient vanishing and high computational\ncomplexity, especially in deep SNNs. Additionally, existing SNN frameworks\noften fail to effectively handle multi-scale spatiotemporal features, leading\nto increased data redundancy and reduced accuracy. To address these issues, we\npropose CREST, a novel conjointly-trained spike-driven framework to exploit\nspatiotemporal dynamics in event-based object detection. We introduce the\nconjoint learning rule to accelerate SNN learning and alleviate gradient\nvanishing. It also supports dual operation modes for efficient and flexible\nimplementation on different hardware types. Additionally, CREST features a\nfully spike-driven framework with a multi-scale spatiotemporal event integrator\n(MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our approach achieves superior\nobject recognition & detection performance and up to 100X energy efficiency\ncompared with state-of-the-art SNN algorithms on three datasets, providing an\nefficient solution for event-based object detection algorithms suitable for SNN\nhardware implementation.\n","authors":["Ruixin Mao","Aoyu Shen","Lin Tang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12525v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.21534v4","updated":"2024-12-18T13:12:29Z","published":"2024-07-31T11:40:29Z","title":"ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large\n  Language Models","summary":"  In this work, we propose a training-free method to inject visual referring\ninto Multimodal Large Language Models (MLLMs) through learnable visual token\noptimization. We observe the relationship between text prompt tokens and visual\ntokens in MLLMs, where attention layers model the connection between them. Our\napproach involves adjusting visual tokens from the MLP output during inference,\ncontrolling which text prompt tokens attend to which visual tokens. We optimize\na learnable visual token based on an energy function, enhancing the strength of\nreferential regions in the attention map. This enables detailed region\ndescription and reasoning without the need for substantial training costs or\nmodel retraining. Our method offers a promising direction for integrating\nreferential abilities into MLLMs. Our method support referring with box, mask,\nscribble and point. The results demonstrate that our method exhibits\ncontrollability and interpretability.\n","authors":["Mingrui Wu","Xinyue Cai","Jiayi Ji","Jiale Li","Oucheng Huang","Gen Luo","Hao Fei","Guannan Jiang","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.21534v4.pdf","comment":"Accepted to NeurIPS 2024;\n  Code:https://github.com/mrwu-mac/ControlMLLM"},{"id":"http://arxiv.org/abs/2412.13823v1","updated":"2024-12-18T13:11:58Z","published":"2024-12-18T13:11:58Z","title":"Prompt Categories Cluster for Weakly Supervised Semantic Segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS.\n","authors":["Wangyu Wu","Xianglin Qiu","Siqi Song","Xiaowei Huang","Fei Ma","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.13823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17686v2","updated":"2024-12-18T13:09:20Z","published":"2024-09-26T09:51:11Z","title":"MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling","summary":"  Motion generation from discrete quantization offers many advantages over\ncontinuous regression, but at the cost of inevitable approximation errors.\nPrevious methods usually quantize the entire body pose into one code, which not\nonly faces the difficulty in encoding all joints within one vector but also\nloses the spatial relationship between different joints. Differently, in this\nwork we quantize each individual joint into one vector, which i) simplifies the\nquantization process as the complexity associated with a single joint is\nmarkedly lower than that of the entire pose; ii) maintains a spatial-temporal\nstructure that preserves both the spatial relationships among joints and the\ntemporal movement patterns; iii) yields a 2D token map, which enables the\napplication of various 2D operations widely used in 2D images. Grounded in the\n2D motion quantization, we build a spatial-temporal modeling framework, where\n2D joint VQVAE, temporal-spatial 2D masking technique, and spatial-temporal 2D\nattention are proposed to take advantage of spatial-temporal signals among the\n2D tokens. Extensive experiments demonstrate that our method significantly\noutperforms previous methods across different datasets, with a 26.6% decrease\nof FID on HumanML3D and a 29.9% decrease on KIT-ML. Project page:\nhttps://aigc3d.github.io/mogents.\n","authors":["Weihao Yuan","Weichao Shen","Yisheng He","Yuan Dong","Xiaodong Gu","Zilong Dong","Liefeng Bo","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17686v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15105v2","updated":"2024-12-18T13:08:47Z","published":"2024-10-19T13:37:24Z","title":"Standardizing Generative Face Video Compression using Supplemental\n  Enhancement Information","summary":"  This paper proposes a Generative Face Video Compression (GFVC) approach using\nSupplemental Enhancement Information (SEI), where a series of compact spatial\nand temporal representations of a face video signal (i.e., 2D/3D key-points,\nfacial semantics and compact features) can be coded using SEI message and\ninserted into the coded video bitstream. At the time of writing, the proposed\nGFVC approach using SEI messages has been adopted into the official working\ndraft of Versatile Supplemental Enhancement Information (VSEI) standard by the\nJoint Video Experts Team (JVET) of ISO/IEC JTC 1/SC 29 and ITU-T SG16, which\nwill be standardized as a new version for \"ITU-T H.274 | ISO/IEC 23002-7\". To\nthe best of the authors' knowledge, the JVET work on the proposed SEI-based\nGFVC approach is the first standardization activity for generative video\ncompression. The proposed SEI approach has not only advanced the reconstruction\nquality of early-day Model-Based Coding (MBC) via the state-of-the-art\ngenerative technique, but also established a new SEI definition for future GFVC\napplications and deployment. Experimental results illustrate that the proposed\nSEI-based GFVC approach can achieve remarkable rate-distortion performance\ncompared with the latest Versatile Video Coding (VVC) standard, whilst also\npotentially enabling a wide variety of functionalities including user-specified\nanimation/filtering and metaverse-related applications.\n","authors":["Bolin Chen","Yan Ye","Jie Chen","Ru-Ling Liao","Shanzhi Yin","Shiqi Wang","Kaifa Yang","Yue Li","Yiling Xu","Ye-Kui Wang","Shiv Gehlot","Guan-Ming Su","Peng Yin","Sean McCarthy","Gary J. Sullivan"],"pdf_url":"https://arxiv.org/pdf/2410.15105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13817v1","updated":"2024-12-18T13:04:30Z","published":"2024-12-18T13:04:30Z","title":"Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection","summary":"  Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}.\n","authors":["Le Yang","Ziwei Zheng","Boxu Chen","Zhengyu Zhao","Chenhao Lin","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13817v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2412.13815v1","updated":"2024-12-18T13:03:00Z","published":"2024-12-18T13:03:00Z","title":"Object Style Diffusion for Generalized Object Detection in Urban Scene","summary":"  Object detection is a critical task in computer vision, with applications in\nvarious domains such as autonomous driving and urban scene monitoring. However,\ndeep learning-based approaches often demand large volumes of annotated data,\nwhich are costly and difficult to acquire, particularly in complex and\nunpredictable real-world environments. This dependency significantly hampers\nthe generalization capability of existing object detection techniques. To\naddress this issue, we introduce a novel single-domain object detection\ngeneralization method, named GoDiff, which leverages a pre-trained model to\nenhance generalization in unseen domains. Central to our approach is the Pseudo\nTarget Data Generation (PTDG) module, which employs a latent diffusion model to\ngenerate pseudo-target domain data that preserves source domain characteristics\nwhile introducing stylistic variations. By integrating this pseudo data with\nsource domain data, we diversify the training dataset. Furthermore, we\nintroduce a cross-style instance normalization technique to blend style\nfeatures from different domains generated by the PTDG module, thereby\nincreasing the detector's robustness. Experimental results demonstrate that our\nmethod not only enhances the generalization ability of existing detectors but\nalso functions as a plug-and-play enhancement for other single-domain\ngeneralization methods, achieving state-of-the-art performance in autonomous\ndriving scenarios.\n","authors":["Hao Li","Xiangyuan Yang","Mengzhu Wang","Long Lan","Ke Liang","Xinwang Liu","Kenli Li"],"pdf_url":"https://arxiv.org/pdf/2412.13815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12888v2","updated":"2024-12-18T13:01:11Z","published":"2024-12-17T13:12:31Z","title":"ArtAug: Enhancing Text-to-Image Generation through\n  Synthesis-Understanding Interaction","summary":"  The emergence of diffusion models has significantly advanced image synthesis.\nThe recent studies of model interaction and self-corrective reasoning approach\nin large language models offer new insights for enhancing text-to-image models.\nInspired by these studies, we propose a novel method called ArtAug for\nenhancing text-to-image models in this paper. To the best of our knowledge,\nArtAug is the first one that improves image synthesis models via model\ninteractions with understanding models. In the interactions, we leverage human\npreferences implicitly learned by image understanding models to provide\nfine-grained suggestions for image synthesis models. The interactions can\nmodify the image content to make it aesthetically pleasing, such as adjusting\nexposure, changing shooting angles, and adding atmospheric effects. The\nenhancements brought by the interaction are iteratively fused into the\nsynthesis model itself through an additional enhancement module. This enables\nthe synthesis model to directly produce aesthetically pleasing images without\nany extra computational cost. In the experiments, we train the ArtAug\nenhancement module on existing text-to-image models. Various evaluation metrics\nconsistently demonstrate that ArtAug enhances the generative capabilities of\ntext-to-image models without incurring additional computational costs. The\nsource code and models will be released publicly.\n","authors":["Zhongjie Duan","Qianyi Zhao","Cen Chen","Daoyuan Chen","Wenmeng Zhou","Yaliang Li","Yingda Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12888v2.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.13811v1","updated":"2024-12-18T12:58:38Z","published":"2024-12-18T12:58:38Z","title":"Spatial Brain Tumor Concentration Estimation for Individualized\n  Radiotherapy Planning","summary":"  Biophysical modeling of brain tumors has emerged as a promising strategy for\npersonalizing radiotherapy planning by estimating the otherwise hidden\ndistribution of tumor cells within the brain. However, many existing\nstate-of-the-art methods are computationally intensive, limiting their\nwidespread translation into clinical practice. In this work, we propose an\nefficient and direct method that utilizes soft physical constraints to estimate\nthe tumor cell concentration from preoperative MRI of brain tumor patients. Our\napproach optimizes a 3D tumor concentration field by simultaneously minimizing\nthe difference between the observed MRI and a physically informed loss\nfunction. Compared to existing state-of-the-art techniques, our method\nsignificantly improves predicting tumor recurrence on two public datasets with\na total of 192 patients while maintaining a clinically viable runtime of under\none minute - a substantial reduction from the 30 minutes required by the\ncurrent best approach. Furthermore, we showcase the generalizability of our\nframework by incorporating additional imaging information and physical\nconstraints, highlighting its potential to translate to various medical\ndiffusion phenomena with imperfect data.\n","authors":["Jonas Weidner","Michal Balcerak","Ivan Ezhov","André Datchev","Laurin Lux","Lucas Zimmerand Daniel Rueckert","Björn Menze","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2412.13811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13810v1","updated":"2024-12-18T12:57:56Z","published":"2024-12-18T12:57:56Z","title":"CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers?","summary":"  We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design.\nOur approach is based on a powerful Vision and Large Language Model (VLLM) as a\nplanner and a tool-augmentation paradigm using CAD-specific modules.\nCAD-Assistant addresses multimodal user queries by generating actions that are\niteratively executed on a Python interpreter equipped with the FreeCAD\nsoftware, accessed via its Python API. Our framework is able to assess the\nimpact of generated CAD commands on geometry and adapts subsequent actions\nbased on the evolving state of the CAD design. We consider a wide range of\nCAD-specific tools including Python libraries, modules of the FreeCAD Python\nAPI, helpful routines, rendering functions and other specialized modules. We\nevaluate our method on multiple CAD benchmarks and qualitatively demonstrate\nthe potential of tool-augmented VLLMs as generic CAD task solvers across\ndiverse CAD workflows.\n","authors":["Dimitrios Mallis","Ahmet Serdar Karadeniz","Sebastian Cavada","Danila Rukhovich","Niki Foteinopoulou","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2412.13810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04377v2","updated":"2024-12-18T12:55:49Z","published":"2024-12-05T17:52:35Z","title":"A Hitchhiker's Guide to Understanding Performances of Two-Class\n  Classifiers","summary":"  Properly understanding the performances of classifiers is essential in\nvarious scenarios. However, the literature often relies only on one or two\nstandard scores to compare classifiers, which fails to capture the nuances of\napplication-specific requirements, potentially leading to suboptimal classifier\nselection. Recently, a paper on the foundations of the theory of\nperformance-based ranking introduced a tool, called the Tile, that organizes an\ninfinity of ranking scores into a 2D map. Thanks to the Tile, it is now\npossible to evaluate and compare classifiers efficiently, displaying all\npossible application-specific preferences instead of having to rely on a pair\nof scores. In this paper, we provide a first hitchhiker's guide for\nunderstanding the performances of two-class classifiers by presenting four\nscenarios, each showcasing a different user profile: a theoretical analyst, a\nmethod designer, a benchmarker, and an application developer. Particularly, we\nshow that we can provide different interpretative flavors that are adapted to\nthe user's needs by mapping different values on the Tile. As an illustration,\nwe leverage the newly introduced Tile tool and the different flavors to rank\nand analyze the performances of 74 state-of-the-art semantic segmentation\nmodels in two-class classification through the eyes of the four user profiles.\nThrough these user profiles, we demonstrate that the Tile effectively captures\nthe behavior of classifiers in a single visualization, while accommodating an\ninfinite number of ranking scores.\n","authors":["Anaïs Halin","Sébastien Piérard","Anthony Cioppa","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04309v2","updated":"2024-12-18T12:50:29Z","published":"2024-12-05T16:27:59Z","title":"The Tile: A 2D Map of Ranking Scores for Two-Class Classification","summary":"  In the computer vision and machine learning communities, as well as in many\nother research domains, rigorous evaluation of any new method, including\nclassifiers, is essential. One key component of the evaluation process is the\nability to compare and rank methods. However, ranking classifiers and\naccurately comparing their performances, especially when taking\napplication-specific preferences into account, remains challenging. For\ninstance, commonly used evaluation tools like Receiver Operating Characteristic\n(ROC) and Precision/Recall (PR) spaces display performances based on two\nscores. Hence, they are inherently limited in their ability to compare\nclassifiers across a broader range of scores and lack the capability to\nestablish a clear ranking among classifiers. In this paper, we present a novel\nversatile tool, named the Tile, that organizes an infinity of ranking scores in\na single 2D map for two-class classifiers, including common evaluation scores\nsuch as the accuracy, the true positive rate, the positive predictive value,\nJaccard's coefficient, and all F-beta scores. Furthermore, we study the\nproperties of the underlying ranking scores, such as the influence of the\npriors or the correspondences with the ROC space, and depict how to\ncharacterize any other score by comparing them to the Tile. Overall, we\ndemonstrate that the Tile is a powerful tool that effectively captures all the\nrankings in a single visualization and allows interpreting them.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13803v1","updated":"2024-12-18T12:50:11Z","published":"2024-12-18T12:50:11Z","title":"M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation","summary":"  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M3-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M3-VOS, yielding several key insights. Notably, current appearance\nbased approaches show significant room for improvement when handling objects\nwith phase transitions. The inherent changes in disorder suggest that the\npredictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-and-play model that improves its performance by\nreversal refinement. Our data and code will be publicly available\n","authors":["Zixuan Chen","Jiaxin Li","Liming Tan","Yejie Guo","Junxuan Liang","Cewu Lu","Yonglu Li"],"pdf_url":"https://arxiv.org/pdf/2412.13803v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.04227v2","updated":"2024-12-18T12:45:58Z","published":"2024-12-05T15:05:25Z","title":"Foundations of the Theory of Performance-Based Ranking","summary":"  Ranking entities such as algorithms, devices, methods, or models based on\ntheir performances, while accounting for application-specific preferences, is a\nchallenge. To address this challenge, we establish the foundations of a\nuniversal theory for performance-based ranking. First, we introduce a rigorous\nframework built on top of both the probability and order theories. Our new\nframework encompasses the elements necessary to (1) manipulate performances as\nmathematical objects, (2) express which performances are worse than or\nequivalent to others, (3) model tasks through a variable called satisfaction,\n(4) consider properties of the evaluation, (5) define scores, and (6) specify\napplication-specific preferences through a variable called importance. On top\nof this framework, we propose the first axiomatic definition of performance\norderings and performance-based rankings. Then, we introduce a universal\nparametric family of scores, called ranking scores, that can be used to\nestablish rankings satisfying our axioms, while considering\napplication-specific preferences. Finally, we show, in the case of two-class\nclassification, that the family of ranking scores encompasses well-known\nperformance scores, including the accuracy, the true positive rate (recall,\nsensitivity), the true negative rate (specificity), the positive predictive\nvalue (precision), and F1. However, we also show that some other scores\ncommonly used to compare classifiers are unsuitable to derive performance\norderings satisfying the axioms. Therefore, this paper provides the computer\nvision and machine learning communities with a rigorous framework for\nevaluating and ranking entities.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00705v2","updated":"2024-12-18T12:26:03Z","published":"2024-12-01T07:02:36Z","title":"Photoacoustic Iterative Optimization Algorithm with Shape Prior\n  Regularization","summary":"  Photoacoustic imaging (PAI) suffers from inherent limitations that can\ndegrade the quality of reconstructed results, such as noise, artifacts and\nincomplete data acquisition caused by sparse sampling or partial array\ndetection. In this study, we proposed a new optimization method for both\ntwo-dimensional (2D) and three-dimensional (3D) PAI reconstruction results,\ncalled the regularized iteration method with shape prior. The shape prior is a\nprobability matrix derived from the reconstruction results of multiple sets of\nrandom partial array signals in a computational imaging system using any\nreconstruction algorithm, such as Delay-and-Sum (DAS) and Back-Projection (BP).\nIn the probability matrix, high-probability locations indicate high consistency\namong multiple reconstruction results at those positions, suggesting a high\nlikelihood of representing the true imaging results. In contrast,\nlow-probability locations indicate higher randomness, leaning more towards\nnoise or artifacts. As a shape prior, this probability matrix guides the\niteration and regularization of the entire array signal reconstruction results\nusing the original reconstruction algorithm (the same algorithm for processing\nrandom partial array signals). The method takes advantage of the property that\nthe similarity of the object to be imitated is higher than that of noise or\nartifact in the results reconstructed by multiple sets of random partial array\nsignals of the entire imaging system. The probability matrix is taken as a\nprerequisite for improving the original reconstruction results, and the\noptimizer is used to further iterate the imaging results to remove noise and\nartifacts and improve the imaging fidelity. Especially in the case involving\nsparse view which brings more artifacts, the effect is remarkable. Simulation\nand real experiments have both demonstrated the superiority of this method.\n","authors":["Yu Zhang","Shuang Li","Yibing Wang","Yu Sun","Wenyi Xiang"],"pdf_url":"https://arxiv.org/pdf/2412.00705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13772v1","updated":"2024-12-18T12:10:33Z","published":"2024-12-18T12:10:33Z","title":"An Efficient Occupancy World Model via Decoupled Dynamic Flow and\n  Image-assisted Training","summary":"  The field of autonomous driving is experiencing a surge of interest in world\nmodels, which aim to predict potential future scenarios based on historical\nobservations. In this paper, we introduce DFIT-OccWorld, an efficient 3D\noccupancy world model that leverages decoupled dynamic flow and image-assisted\ntraining strategy, substantially improving 4D scene forecasting performance. To\nsimplify the training process, we discard the previous two-stage training\nstrategy and innovatively reformulate the occupancy forecasting problem as a\ndecoupled voxels warping process. Our model forecasts future dynamic voxels by\nwarping existing observations using voxel flow, whereas static voxels are\neasily obtained through pose transformation. Moreover, our method incorporates\nan image-assisted training paradigm to enhance prediction reliability.\nSpecifically, differentiable volume rendering is adopted to generate rendered\ndepth maps through predicted future volumes, which are adopted in render-based\nphotometric consistency. Experiments demonstrate the effectiveness of our\napproach, showcasing its state-of-the-art performance on the nuScenes and\nOpenScene benchmarks for 4D occupancy forecasting, end-to-end motion planning\nand point cloud forecasting. Concretely, it achieves state-of-the-art\nperformances compared to existing 3D world models while incurring substantially\nlower computational costs.\n","authors":["Haiming Zhang","Ying Xue","Xu Yan","Jiacheng Zhang","Weichao Qiu","Dongfeng Bai","Bingbing Liu","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2412.13772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09954v2","updated":"2024-12-18T11:51:45Z","published":"2024-12-13T08:24:12Z","title":"A2RNet: Adversarial Attack Resilient Network for Robust Infrared and\n  Visible Image Fusion","summary":"  Infrared and visible image fusion (IVIF) is a crucial technique for enhancing\nvisual performance by integrating unique information from different modalities\ninto one fused image. Exiting methods pay more attention to conducting fusion\nwith undisturbed data, while overlooking the impact of deliberate interference\non the effectiveness of fusion results. To investigate the robustness of fusion\nmodels, in this paper, we propose a novel adversarial attack resilient network,\ncalled $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial\nparadigm with an anti-attack loss function to implement adversarial attacks and\ntraining. It is constructed based on the intrinsic nature of IVIF and provide a\nrobust foundation for future research advancements. We adopt a Unet as the\npipeline with a transformer-based defensive refinement module (DRM) under this\nparadigm, which guarantees fused image quality in a robust coarse-to-fine\nmanner. Compared to previous works, our method mitigates the adverse effects of\nadversarial perturbations, consistently maintaining high-fidelity fusion\nresults. Furthermore, the performance of downstream tasks can also be well\nmaintained under adversarial attacks. Code is available at\nhttps://github.com/lok-18/A2RNet.\n","authors":["Jiawei Li","Hongwei Yu","Jiansheng Chen","Xinlong Ding","Jinlong Wang","Jinyuan Liu","Bochao Zou","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.09954v2.pdf","comment":"9 pages, 8 figures, The 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.13753v1","updated":"2024-12-18T11:43:41Z","published":"2024-12-18T11:43:41Z","title":"Mesoscopic Insights: Orchestrating Multi-scale & Hybrid Architecture for\n  Image Manipulation Localization","summary":"  The mesoscopic level serves as a bridge between the macroscopic and\nmicroscopic worlds, addressing gaps overlooked by both. Image manipulation\nlocalization (IML), a crucial technique to pursue truth from fake images, has\nlong relied on low-level (microscopic-level) traces. However, in practice, most\ntampering aims to deceive the audience by altering image semantics. As a\nresult, manipulation commonly occurs at the object level (macroscopic level),\nwhich is equally important as microscopic traces. Therefore, integrating these\ntwo levels into the mesoscopic level presents a new perspective for IML\nresearch. Inspired by this, our paper explores how to simultaneously construct\nmesoscopic representations of micro and macro information for IML and\nintroduces the Mesorch architecture to orchestrate both. Specifically, this\narchitecture i) combines Transformers and CNNs in parallel, with Transformers\nextracting macro information and CNNs capturing micro details, and ii) explores\nacross different scales, assessing micro and macro information seamlessly.\nAdditionally, based on the Mesorch architecture, the paper introduces two\nbaseline models aimed at solving IML tasks through mesoscopic representation.\nExtensive experiments across four datasets have demonstrated that our models\nsurpass the current state-of-the-art in terms of performance, computational\ncomplexity, and robustness.\n","authors":["Xuekang Zhu","Xiaochen Ma","Lei Su","Zhuohang Jiang","Bo Du","Xiwen Wang","Zeyu Lei","Wentao Feng","Chi-Man Pun","Jizhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13753v1.pdf","comment":"AAAI 2025. Code:\n  $\\href{https://github.com/scu-zjz/Mesorch}{this~url}$"},{"id":"http://arxiv.org/abs/2412.13749v1","updated":"2024-12-18T11:33:16Z","published":"2024-12-18T11:33:16Z","title":"Multi-Exposure Image Fusion via Distilled 3D LUT Grid with Editable Mode","summary":"  With the rising imaging resolution of handheld devices, existing\nmulti-exposure image fusion algorithms struggle to generate a high dynamic\nrange image with ultra-high resolution in real-time. Apart from that, there is\na trend to design a manageable and editable algorithm as the different needs of\nreal application scenarios. To tackle these issues, we introduce 3D LUT\ntechnology, which can enhance images with ultra-high-definition (UHD)\nresolution in real time on resource-constrained devices. However, since the\nfusion of information from multiple images with different exposure rates is\nuncertain, and this uncertainty significantly trials the generalization power\nof the 3D LUT grid. To address this issue and ensure a robust learning space\nfor the model, we propose using a teacher-student network to model the\nuncertainty on the 3D LUT grid.Furthermore, we provide an editable mode for the\nmulti-exposure image fusion algorithm by using the implicit representation\nfunction to match the requirements in different scenarios. Extensive\nexperiments demonstrate that our proposed method is highly competitive in\nefficiency and accuracy.\n","authors":["Xin Su","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.13749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13742v1","updated":"2024-12-18T11:19:23Z","published":"2024-12-18T11:19:23Z","title":"Learnable Prompting SAM-induced Knowledge Distillation for\n  Semi-supervised Medical Image Segmentation","summary":"  The limited availability of labeled data has driven advancements in\nsemi-supervised learning for medical image segmentation. Modern large-scale\nmodels tailored for general segmentation, such as the Segment Anything Model\n(SAM), have revealed robust generalization capabilities. However, applying\nthese models directly to medical image segmentation still exposes performance\ndegradation. In this paper, we propose a learnable prompting SAM-induced\nKnowledge distillation framework (KnowSAM) for semi-supervised medical image\nsegmentation. Firstly, we propose a Multi-view Co-training (MC) strategy that\nemploys two distinct sub-networks to employ a co-teaching paradigm, resulting\nin more robust outcomes. Secondly, we present a Learnable Prompt Strategy (LPS)\nto dynamically produce dense prompts and integrate an adapter to fine-tune SAM\nspecifically for medical image segmentation tasks. Moreover, we propose\nSAM-induced Knowledge Distillation (SKD) to transfer useful knowledge from SAM\nto two sub-networks, enabling them to learn from SAM's predictions and\nalleviate the effects of incorrect pseudo-labels during training. Notably, the\npredictions generated by our subnets are used to produce mask prompts for SAM,\nfacilitating effective inter-module information exchange. Extensive\nexperimental results on various medical segmentation tasks demonstrate that our\nmodel outperforms the state-of-the-art semi-supervised segmentation approaches.\nCrucially, our SAM distillation framework can be seamlessly integrated into\nother semi-supervised segmentation methods to enhance performance. The code\nwill be released upon acceptance of this manuscript at:\nhttps://github.com/taozh2017/KnowSAM\n","authors":["Kaiwen Huang","Tao Zhou","Huazhu Fu","Yizhe Zhang","Yi Zhou","Chen Gong","Dong Liang"],"pdf_url":"https://arxiv.org/pdf/2412.13742v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.13736v1","updated":"2024-12-18T11:14:02Z","published":"2024-12-18T11:14:02Z","title":"MedCoT: Medical Chain of Thought via Hierarchical Expert","summary":"  Artificial intelligence has advanced in Medical Visual Question Answering\n(Med-VQA), but prevalent research tends to focus on the accuracy of the\nanswers, often overlooking the reasoning paths and interpretability, which are\ncrucial in clinical settings. Besides, current Med-VQA algorithms, typically\nreliant on singular models, lack the robustness needed for real-world medical\ndiagnostics which usually require collaborative expert evaluation. To address\nthese shortcomings, this paper presents MedCoT, a novel hierarchical expert\nverification reasoning chain method designed to enhance interpretability and\naccuracy in biomedical imaging inquiries. MedCoT is predicated on two\nprinciples: The necessity for explicit reasoning paths in Med-VQA and the\nrequirement for multi-expert review to formulate accurate conclusions. The\nmethodology involves an Initial Specialist proposing diagnostic rationales,\nfollowed by a Follow-up Specialist who validates these rationales, and finally,\na consensus is reached through a vote among a sparse Mixture of Experts within\nthe locally deployed Diagnostic Specialist, which then provides the definitive\ndiagnosis. Experimental evaluations on four standard Med-VQA datasets\ndemonstrate that MedCoT surpasses existing state-of-the-art approaches,\nproviding significant improvements in performance and interpretability.\n","authors":["Jiaxiang Liu","Yuan Wang","Jiawei Du","Joey Tianyi Zhou","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13735v1","updated":"2024-12-18T11:14:01Z","published":"2024-12-18T11:14:01Z","title":"3D Registration in 30 Years: A Survey","summary":"  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n","authors":["Jiaqi Yang","Chu'ai Zhang","Zhengbao Wang","Xinyue Cao","Xuan Ouyang","Xiyu Zhang","Zhenxuan Zeng","Zhao Zeng","Borui Lu","Zhiyi Xia","Qian Zhang","Yulan Guo","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09892v2","updated":"2024-12-18T11:12:49Z","published":"2024-12-13T06:14:57Z","title":"VQTalker: Towards Multilingual Talking Avatars through Facial Motion\n  Tokenization","summary":"  We present VQTalker, a Vector Quantization-based framework for multilingual\ntalking head generation that addresses the challenges of lip synchronization\nand natural motion across diverse languages. Our approach is grounded in the\nphonetic principle that human speech comprises a finite set of distinct sound\nunits (phonemes) and corresponding visual articulations (visemes), which often\nshare commonalities across languages. We introduce a facial motion tokenizer\nbased on Group Residual Finite Scalar Quantization (GRFSQ), which creates a\ndiscretized representation of facial features. This method enables\ncomprehensive capture of facial movements while improving generalization to\nmultiple languages, even with limited training data. Building on this quantized\nrepresentation, we implement a coarse-to-fine motion generation process that\nprogressively refines facial animations. Extensive experiments demonstrate that\nVQTalker achieves state-of-the-art performance in both video-driven and\nspeech-driven scenarios, particularly in multilingual settings. Notably, our\nmethod achieves high-quality results at a resolution of 512*512 pixels while\nmaintaining a lower bitrate of approximately 11 kbps. Our work opens new\npossibilities for cross-lingual talking face generation. Synthetic results can\nbe viewed at https://x-lance.github.io/VQTalker.\n","authors":["Tao Liu","Ziyang Ma","Qi Chen","Feilong Chen","Shuai Fan","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.09892v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2412.13734v1","updated":"2024-12-18T11:12:10Z","published":"2024-12-18T11:12:10Z","title":"Text2Relight: Creative Portrait Relighting with Text Guidance","summary":"  We present a lighting-aware image editing pipeline that, given a portrait\nimage and a text prompt, performs single image relighting. Our model modifies\nthe lighting and color of both the foreground and background to align with the\nprovided text description. The unbounded nature in creativeness of a text\nallows us to describe the lighting of a scene with any sensory features\nincluding temperature, emotion, smell, time, and so on. However, the modeling\nof such mapping between the unbounded text and lighting is extremely\nchallenging due to the lack of dataset where there exists no scalable data that\nprovides large pairs of text and relighting, and therefore, current text-driven\nimage editing models does not generalize to lighting-specific use cases. We\novercome this problem by introducing a novel data synthesis pipeline: First,\ndiverse and creative text prompts that describe the scenes with various\nlighting are automatically generated under a crafted hierarchy using a large\nlanguage model (*e.g.,* ChatGPT). A text-guided image generation model creates\na lighting image that best matches the text. As a condition of the lighting\nimages, we perform image-based relighting for both foreground and background\nusing a single portrait image or a set of OLAT (One-Light-at-A-Time) images\ncaptured from lightstage system. Particularly for the background relighting, we\nrepresent the lighting image as a set of point lights and transfer them to\nother background images. A generative diffusion model learns the synthesized\nlarge-scale data with auxiliary task augmentation (*e.g.,* portrait delighting\nand light positioning) to correlate the latent text and lighting distribution\nfor text-guided portrait relighting.\n","authors":["Junuk Cha","Mengwei Ren","Krishna Kumar Singh","He Zhang","Yannick Hold-Geoffroy","Seunghyun Yoon","HyunJoon Jung","Jae Shin Yoon","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2412.13734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13732v1","updated":"2024-12-18T11:10:18Z","published":"2024-12-18T11:10:18Z","title":"Modelling Multi-modal Cross-interaction for ML-FSIC Based on Local\n  Feature Selection","summary":"  The aim of multi-label few-shot image classification (ML-FSIC) is to assign\nsemantic labels to images, in settings where only a small number of training\nexamples are available for each label. A key feature of the multi-label setting\nis that images often have several labels, which typically refer to objects\nappearing in different regions of the image. When estimating label prototypes,\nin a metric-based setting, it is thus important to determine which regions are\nrelevant for which labels, but the limited amount of training data and the\nnoisy nature of local features make this highly challenging. As a solution, we\npropose a strategy in which label prototypes are gradually refined. First, we\ninitialize the prototypes using word embeddings, which allows us to leverage\nprior knowledge about the meaning of the labels. Second, taking advantage of\nthese initial prototypes, we then use a Loss Change Measurement~(LCM) strategy\nto select the local features from the training images (i.e.\\ the support set)\nthat are most likely to be representative of a given label. Third, we construct\nthe final prototype of the label by aggregating these representative local\nfeatures using a multi-modal cross-interaction mechanism, which again relies on\nthe initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC,\nNUS-WIDE, and iMaterialist show that our model substantially improves the\ncurrent state-of-the-art.\n","authors":["Kun Yan","Zied Bouraoui","Fangyun Wei","Chang Xu","Ping Wang","Shoaib Jameel","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2412.13732v1.pdf","comment":"Accepted in Transactions on Multimedia Computing Communications and\n  Applications"},{"id":"http://arxiv.org/abs/2412.13726v1","updated":"2024-12-18T11:05:56Z","published":"2024-12-18T11:05:56Z","title":"Unified Understanding of Environment, Task, and Human for Human-Robot\n  Interaction in Real-World Environments","summary":"  To facilitate human--robot interaction (HRI) tasks in real-world scenarios,\nservice robots must adapt to dynamic environments and understand the required\ntasks while effectively communicating with humans. To accomplish HRI in\npractice, we propose a novel indoor dynamic map, task understanding system, and\nresponse generation system. The indoor dynamic map optimizes robot behavior by\nmanaging an occupancy grid map and dynamic information, such as furniture and\nhumans, in separate layers. The task understanding system targets tasks that\nrequire multiple actions, such as serving ordered items. Task representations\nthat predefine the flow of necessary actions are applied to achieve highly\naccurate understanding. The response generation system is executed in parallel\nwith task understanding to facilitate smooth HRI by informing humans of the\nsubsequent actions of the robot. In this study, we focused on waiter duties in\na restaurant setting as a representative application of HRI in a dynamic\nenvironment. We developed an HRI system that could perform tasks such as\nserving food and cleaning up while communicating with customers. In experiments\nconducted in a simulated restaurant environment, the proposed HRI system\nsuccessfully communicated with customers and served ordered food with 90\\%\naccuracy. In a questionnaire administered after the experiment, the HRI system\nof the robot received 4.2 points out of 5. These outcomes indicated the\neffectiveness of the proposed method and HRI system in executing waiter tasks\nin real-world environments.\n","authors":["Yuga Yano","Akinobu Mizutani","Yukiya Fukuda","Daiju Kanaoka","Tomohiro Ono","Hakaru Tamukoh"],"pdf_url":"https://arxiv.org/pdf/2412.13726v1.pdf","comment":"2024 33rd IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN)"},{"id":"http://arxiv.org/abs/2408.10360v2","updated":"2024-12-18T11:02:07Z","published":"2024-08-19T18:56:24Z","title":"HaSPeR: An Image Repository for Hand Shadow Puppet Recognition","summary":"  Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of\ntheatrical art and storytelling where hand shadows are projected onto flat\nsurfaces to create illusions of living creatures. The skilled performers create\nthese silhouettes by hand positioning, finger movements, and dexterous gestures\nto resemble shadows of animals and objects. Due to the lack of practitioners\nand a seismic shift in people's entertainment standards, this art form is on\nthe verge of extinction. To facilitate its preservation and proliferate it to a\nwider audience, we introduce ${\\rm H{\\small A}SP{\\small E}R}$, a novel dataset\nconsisting of 15,000 images of hand shadow puppets across 15 classes extracted\nfrom both professional and amateur hand shadow puppeteer clips. We provide a\ndetailed statistical analysis of the dataset and employ a range of pretrained\nimage classification models to establish baselines. Our findings show a\nsubstantial performance superiority of skip-connected convolutional models over\nattention-based transformer architectures. We also find that lightweight\nmodels, such as MobileNetV2, suited for mobile applications and embedded\ndevices, perform comparatively well. We surmise that such low-latency\narchitectures can be useful in developing ombromanie teaching tools, and we\ncreate a prototype application to explore this surmission. Keeping the\nbest-performing model ResNet34 under the limelight, we conduct comprehensive\nfeature-spatial, explainability, and error analyses to gain insights into its\ndecision-making process. To the best of our knowledge, this is the first\ndocumented dataset and research endeavor to preserve this dying art for future\ngenerations, with computer vision approaches. Our code and data will be\npublicly available.\n","authors":["Syed Rifat Raiyan","Zibran Zarif Amio","Sabbir Ahmed"],"pdf_url":"https://arxiv.org/pdf/2408.10360v2.pdf","comment":"Submitted to IEEE Transactions on Artificial Intelligence (IEEE TAI),\n  13 pages, 105 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.13717v1","updated":"2024-12-18T10:55:58Z","published":"2024-12-18T10:55:58Z","title":"Towards Automatic Evaluation for Image Transcreation","summary":"  Beyond conventional paradigms of translating speech and text, recently, there\nhas been interest in automated transcreation of images to facilitate\nlocalization of visual content across different cultures. Attempts to define\nthis as a formal Machine Learning (ML) problem have been impeded by the lack of\nautomatic evaluation mechanisms, with previous work relying solely on human\nevaluation. In this paper, we seek to close this gap by proposing a suite of\nautomatic evaluation metrics inspired by machine translation (MT) metrics,\ncategorized into: a) Object-based, b) Embedding-based, and c) VLM-based.\nDrawing on theories from translation studies and real-world transcreation\npractices, we identify three critical dimensions of image transcreation:\ncultural relevance, semantic equivalence and visual similarity, and design our\nmetrics to evaluate systems along these axes. Our results show that proprietary\nVLMs best identify cultural relevance and semantic equivalence, while\nvision-encoder representations are adept at measuring visual similarity.\nMeta-evaluation across 7 countries shows our metrics agree strongly with human\nratings, with average segment-level correlations ranging from 0.55-0.87.\nFinally, through a discussion of the merits and demerits of each metric, we\noffer a robust framework for automated image transcreation evaluation, grounded\nin both theoretical foundations and practical application. Our code can be\nfound here: https://github.com/simran-khanuja/automatic-eval-transcreation\n","authors":["Simran Khanuja","Vivek Iyer","Claire He","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2412.13717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13709v1","updated":"2024-12-18T10:51:59Z","published":"2024-12-18T10:51:59Z","title":"Physics-Based Adversarial Attack on Near-Infrared Human Detector for\n  Nighttime Surveillance Camera Systems","summary":"  Many surveillance cameras switch between daytime and nighttime modes based on\nilluminance levels. During the day, the camera records ordinary RGB images\nthrough an enabled IR-cut filter. At night, the filter is disabled to capture\nnear-infrared (NIR) light emitted from NIR LEDs typically mounted around the\nlens. While RGB-based AI algorithm vulnerabilities have been widely reported,\nthe vulnerabilities of NIR-based AI have rarely been investigated. In this\npaper, we identify fundamental vulnerabilities in NIR-based image understanding\ncaused by color and texture loss due to the intrinsic characteristics of\nclothes' reflectance and cameras' spectral sensitivity in the NIR range. We\nfurther show that the nearly co-located configuration of illuminants and\ncameras in existing surveillance systems facilitates concealing and fully\npassive attacks in the physical world. Specifically, we demonstrate how\nretro-reflective and insulation plastic tapes can manipulate the intensity\ndistribution of NIR images. We showcase an attack on the YOLO-based human\ndetector using binary patterns designed in the digital space (via black-box\nquery and searching) and then physically realized using tapes pasted onto\nclothes. Our attack highlights significant reliability concerns for nighttime\nsurveillance systems, which are intended to enhance security. Codes Available:\nhttps://github.com/MyNiuuu/AdvNIR\n","authors":["Muyao Niu","Zhuoxiao Li","Yifan Zhan","Huy H. Nguyen","Isao Echizen","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.13709v1.pdf","comment":"Appeared in ACM MM 2023"},{"id":"http://arxiv.org/abs/2412.13708v1","updated":"2024-12-18T10:51:31Z","published":"2024-12-18T10:51:31Z","title":"JoVALE: Detecting Human Actions in Video Using Audiovisual and Language\n  Contexts","summary":"  Video Action Detection (VAD) involves localizing and categorizing action\ninstances in videos. Videos inherently contain various information sources,\nincluding audio, visual cues, and surrounding scene contexts. Effectively\nleveraging this multi-modal information for VAD is challenging, as the model\nmust accurately focus on action-relevant cues. In this study, we introduce a\nnovel multi-modal VAD architecture called the Joint Actor-centric Visual,\nAudio, Language Encoder (JoVALE). JoVALE is the first VAD method to integrate\naudio and visual features with scene descriptive context derived from large\nimage captioning models. The core principle of JoVALE is the actor-centric\naggregation of audio, visual, and scene descriptive contexts, where\naction-related cues from each modality are identified and adaptively combined.\nWe propose a specialized module called the Actor-centric Multi-modal Fusion\nNetwork, designed to capture the joint interactions among actors and\nmulti-modal contexts through Transformer architecture. Our evaluation conducted\non three popular VAD benchmarks, AVA, UCF101-24, and JHMDB51-21, demonstrates\nthat incorporating multi-modal information leads to significant performance\ngains. JoVALE achieves state-of-the-art performances. The code will be\navailable at \\texttt{https://github.com/taeiin/AAAI2025-JoVALE}.\n","authors":["Taein Son","Soo Won Seo","Jisong Kim","Seok Hwan Lee","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2412.13708v1.pdf","comment":"Accepted to AAAI Conference on Artificial Intelligence 2025, 9 pages,\n  5 figures"},{"id":"http://arxiv.org/abs/2412.13705v1","updated":"2024-12-18T10:49:41Z","published":"2024-12-18T10:49:41Z","title":"Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation","summary":"  Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.\n","authors":["Minkyoung Kim","Yunha Kim","Hyeram Seo","Heejung Choi","Jiye Han","Gaeun Kee","Soyoung Ko","HyoJe Jung","Byeolhee Kim","Young-Hak Kim","Sanghyun Park","Tae Joon Jun"],"pdf_url":"https://arxiv.org/pdf/2412.13705v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13703v1","updated":"2024-12-18T10:46:04Z","published":"2024-12-18T10:46:04Z","title":"MBInception: A new Multi-Block Inception Model for Enhancing Image\n  Processing Efficiency","summary":"  Deep learning models, specifically convolutional neural networks, have\ntransformed the landscape of image classification by autonomously extracting\nfeatures directly from raw pixel data. This article introduces an innovative\nimage classification model that employs three consecutive inception blocks\nwithin a convolutional neural networks framework, providing a comprehensive\ncomparative analysis with well-established architectures such as Visual\nGeometry Group, Residual Network, and MobileNet. Through the utilization of\nbenchmark datasets, including Canadian Institute for Advanced Researc, Modified\nNational Institute of Standards and Technology database, and Fashion Modified\nNational Institute of Standards and Technology database, we assess the\nperformance of our proposed model in comparison to these benchmarks. The\noutcomes reveal that our novel model consistently outperforms its counterparts\nacross diverse datasets, underscoring its effectiveness and potential for\nadvancing the current state-of-the-art in image classification. Evaluation\nmetrics further emphasize that the proposed model surpasses the other compared\narchitectures, thereby enhancing the efficiency of image classification on\nstandard datasets.\n","authors":["Fatemeh Froughirad","Reza Bakhoda Eshtivani","Hamed Khajavi","Amir Rastgoo"],"pdf_url":"https://arxiv.org/pdf/2412.13703v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.02347v3","updated":"2024-12-18T10:45:06Z","published":"2024-06-04T14:23:27Z","title":"Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few\n  Steps Image Generation","summary":"  In this paper, we propose an efficient, fast, and versatile distillation\nmethod to accelerate the generation of pre-trained diffusion models: Flash\nDiffusion. The method reaches state-of-the-art performances in terms of FID and\nCLIP-Score for few steps image generation on the COCO2014 and COCO2017\ndatasets, while requiring only several GPU hours of training and fewer\ntrainable parameters than existing methods. In addition to its efficiency, the\nversatility of the method is also exposed across several tasks such as\ntext-to-image, inpainting, face-swapping, super-resolution and using different\nbackbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\alpha$),\nas well as adapters. In all cases, the method allowed to reduce drastically the\nnumber of sampling steps while maintaining very high-quality image generation.\nThe official implementation is available at\nhttps://github.com/gojasper/flash-diffusion.\n","authors":["Clément Chadebec","Onur Tasar","Eyal Benaroche","Benjamin Aubin"],"pdf_url":"https://arxiv.org/pdf/2406.02347v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13695v1","updated":"2024-12-18T10:36:46Z","published":"2024-12-18T10:36:46Z","title":"Optical aberrations in autonomous driving: Physics-informed\n  parameterized temperature scaling for neural network uncertainty calibration","summary":"  'A trustworthy representation of uncertainty is desirable and should be\nconsidered as a key feature of any machine learning method' (Huellermeier and\nWaegeman, 2021). This conclusion of Huellermeier et al. underpins the\nimportance of calibrated uncertainties. Since AI-based algorithms are heavily\nimpacted by dataset shifts, the automotive industry needs to safeguard its\nsystem against all possible contingencies. One important but often neglected\ndataset shift is caused by optical aberrations induced by the windshield. For\nthe verification of the perception system performance, requirements on the AI\nperformance need to be translated into optical metrics by a bijective mapping\n(Braun, 2023). Given this bijective mapping it is evident that the optical\nsystem characteristics add additional information about the magnitude of the\ndataset shift. As a consequence, we propose to incorporate a physical inductive\nbias into the neural network calibration architecture to enhance the robustness\nand the trustworthiness of the AI target application, which we demonstrate by\nusing a semantic segmentation task as an example. By utilizing the Zernike\ncoefficient vector of the optical system as a physical prior we can\nsignificantly reduce the mean expected calibration error in case of optical\naberrations. As a result, we pave the way for a trustworthy uncertainty\nrepresentation and for a holistic verification strategy of the perception\nchain.\n","authors":["Dominik Werner Wolf","Alexander Braun","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2412.13695v1.pdf","comment":"Under review at the International Journal of Computer Vision (IJCV)"},{"id":"http://arxiv.org/abs/2410.23318v2","updated":"2024-12-18T10:32:31Z","published":"2024-10-29T21:38:54Z","title":"Denoising Diffusion Probabilistic Models for Magnetic Resonance\n  Fingerprinting","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI, enabling the mapping of multiple tissue properties from a\nsingle, accelerated scan. However, achieving accurate reconstructions remains\nchallenging, particularly in highly accelerated and undersampled acquisitions,\nwhich are crucial for reducing scan times. While deep learning techniques have\nadvanced image reconstruction, the recent introduction of diffusion models\noffers new possibilities for imaging tasks, though their application in the\nmedical field is still emerging. Notably, diffusion models have not yet been\nexplored for the MRF problem. In this work, we propose for the first time a\nconditional diffusion probabilistic model for MRF image reconstruction.\nQualitative and quantitative comparisons on in-vivo brain scan data demonstrate\nthat the proposed approach can outperform established deep learning and\ncompressed sensing algorithms for MRF reconstruction. Extensive ablation\nstudies also explore strategies to improve computational efficiency of our\napproach.\n","authors":["Perla Mayo","Carolin M. Pirkl","Alin Achim","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2410.23318v2.pdf","comment":"13 pages, 5 figures, 3 tables, 2 algorithms"},{"id":"http://arxiv.org/abs/2412.13684v1","updated":"2024-12-18T10:19:12Z","published":"2024-12-18T10:19:12Z","title":"MMO-IG: Multi-Class and Multi-Scale Object Image Generation for Remote\n  Sensing","summary":"  The rapid advancement of deep generative models (DGMs) has significantly\nadvanced research in computer vision, providing a cost-effective alternative to\nacquiring vast quantities of expensive imagery. However, existing methods\npredominantly focus on synthesizing remote sensing (RS) images aligned with\nreal images in a global layout view, which limits their applicability in RS\nimage object detection (RSIOD) research. To address these challenges, we\npropose a multi-class and multi-scale object image generator based on DGMs,\ntermed MMO-IG, designed to generate RS images with supervised object labels\nfrom global and local aspects simultaneously. Specifically, from the local\nview, MMO-IG encodes various RS instances using an iso-spacing instance map\n(ISIM). During the generation process, it decodes each instance region with\niso-spacing value in ISIM-corresponding to both background and foreground\ninstances-to produce RS images through the denoising process of diffusion\nmodels. Considering the complex interdependencies among MMOs, we construct a\nspatial-cross dependency knowledge graph (SCDKG). This ensures a realistic and\nreliable multidirectional distribution among MMOs for region embedding, thereby\nreducing the discrepancy between source and target domains. Besides, we propose\na structured object distribution instruction (SODI) to guide the generation of\nsynthesized RS image content from a global aspect with SCDKG-based ISIM\ntogether. Extensive experimental results demonstrate that our MMO-IG exhibits\nsuperior generation capabilities for RS images with dense MMO-supervised\nlabels, and RS detectors pre-trained with MMO-IG show excellent performance on\nreal-world datasets.\n","authors":["Chuang Yang","Bingxuan Zhao","Qing Zhou","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14164v4","updated":"2024-12-18T10:16:59Z","published":"2022-10-19T21:52:01Z","title":"Understanding Key Point Cloud Features for Development Three-dimensional\n  Adversarial Attacks","summary":"  Adversarial attacks pose serious challenges for deep neural network\n(DNN)-based analysis of various input signals. In the case of three-dimensional\npoint clouds, methods have been developed to identify points that play a key\nrole in network decision, and these become crucial in generating existing\nadversarial attacks. For example, a saliency map approach is a popular method\nfor identifying adversarial drop points, whose removal would significantly\nimpact the network decision. This paper seeks to enhance the understanding of\nthree-dimensional adversarial attacks by exploring which point cloud features\nare most important for predicting adversarial points. Specifically, Fourteen\nkey point cloud features such as edge intensity and distance from the centroid\nare defined, and multiple linear regression is employed to assess their\npredictive power for adversarial points. Based on critical feature selection\ninsights, a new attack method has been developed to evaluate whether the\nselected features can generate an attack successfully. Unlike traditional\nattack methods that rely on model-specific vulnerabilities, this approach\nfocuses on the intrinsic characteristics of the point clouds themselves. It is\ndemonstrated that these features can predict adversarial points across four\ndifferent DNN architectures, Point Network (PointNet), PointNet++, Dynamic\nGraph Convolutional Neural Networks (DGCNN), and Point Convolutional Network\n(PointConv) outperforming random guessing and achieving results comparable to\nsaliency map-based attacks. This study has important engineering applications,\nsuch as enhancing the security and robustness of three-dimensional point\ncloud-based systems in fields like robotics and autonomous driving.\n","authors":["Hanieh Naderi","Chinthaka Dinesh","Ivan V. Bajic","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2210.14164v4.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.07616v2","updated":"2024-12-18T10:02:35Z","published":"2024-12-10T15:54:53Z","title":"PVP: Polar Representation Boost for 3D Semantic Occupancy Prediction","summary":"  Recently, polar coordinate-based representations have shown promise for 3D\nperceptual tasks. Compared to Cartesian methods, polar grids provide a viable\nalternative, offering better detail preservation in nearby spaces while\ncovering larger areas. However, they face feature distortion due to non-uniform\ndivision. To address these issues, we introduce the Polar Voxel Occupancy\nPredictor (PVP), a novel 3D multi-modal predictor that operates in polar\ncoordinates. PVP features two key design elements to overcome distortion: a\nGlobal Represent Propagation (GRP) module that integrates global spatial data\ninto 3D volumes, and a Plane Decomposed Convolution (PD-Conv) that simplifies\n3D distortions into 2D convolutions. These innovations enable PVP to outperform\nexisting methods, achieving significant improvements in mIoU and IoU metrics on\nthe OpenOccupancy dataset.\n","authors":["Yujing Xue","Jiaxiang Liu","Jiawei Du","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.07616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12628v2","updated":"2024-12-18T09:58:32Z","published":"2024-12-17T07:43:36Z","title":"Dense Audio-Visual Event Localization under Cross-Modal Consistency and\n  Multi-Temporal Granularity Collaboration","summary":"  In the field of audio-visual learning, most research tasks focus exclusively\non short videos. This paper focuses on the more practical Dense Audio-Visual\nEvent Localization (DAVEL) task, advancing audio-visual scene understanding for\nlonger, untrimmed videos. This task seeks to identify and temporally pinpoint\nall events simultaneously occurring in both audio and visual streams.\nTypically, each video encompasses dense events of multiple classes, which may\noverlap on the timeline, each exhibiting varied durations. Given these\nchallenges, effectively exploiting the audio-visual relations and the temporal\nfeatures encoded at various granularities becomes crucial. To address these\nchallenges, we introduce a novel CCNet, comprising two core modules: the\nCross-Modal Consistency Collaboration (CMCC) and the Multi-Temporal Granularity\nCollaboration (MTGC). Specifically, the CMCC module contains two branches: a\ncross-modal interaction branch and a temporal consistency-gated branch. The\nformer branch facilitates the aggregation of consistent event semantics across\nmodalities through the encoding of audio-visual relations, while the latter\nbranch guides one modality's focus to pivotal event-relevant temporal areas as\ndiscerned in the other modality. The MTGC module includes a coarse-to-fine\ncollaboration block and a fine-to-coarse collaboration block, providing\nbidirectional support among coarse- and fine-grained temporal features.\nExtensive experiments on the UnAV-100 dataset validate our module design,\nresulting in a new state-of-the-art performance in dense audio-visual event\nlocalization. The code is available at\nhttps://github.com/zzhhfut/CCNet-AAAI2025.\n","authors":["Ziheng Zhou","Jinxing Zhou","Wei Qian","Shengeng Tang","Xiaojun Chang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12628v2.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://github.com/zzhhfut/CCNet-AAAI2025. Jinxing Zhou and Dan Guo are the\n  corresponding authors"},{"id":"http://arxiv.org/abs/2408.11481v2","updated":"2024-12-18T09:55:40Z","published":"2024-08-21T09:49:32Z","title":"VE-Bench: Subjective-Aligned Benchmark Suite for Text-Driven Video\n  Editing Quality Assessment","summary":"  Text-driven video editing has recently experienced rapid development. Despite\nthis, evaluating edited videos remains a considerable challenge. Current\nmetrics tend to fail to align with human perceptions, and effective\nquantitative metrics for video editing are still notably absent. To address\nthis, we introduce VE-Bench, a benchmark suite tailored to the assessment of\ntext-driven video editing. This suite includes VE-Bench DB, a video quality\nassessment (VQA) database for video editing. VE-Bench DB encompasses a diverse\nset of source videos featuring various motions and subjects, along with\nmultiple distinct editing prompts, editing results from 8 different models, and\nthe corresponding Mean Opinion Scores (MOS) from 24 human annotators. Based on\nVE-Bench DB, we further propose VE-Bench QA, a quantitative human-aligned\nmeasurement for the text-driven video editing task. In addition to the\naesthetic, distortion, and other visual quality indicators that traditional VQA\nmethods emphasize, VE-Bench QA focuses on the text-video alignment and the\nrelevance modeling between source and edited videos. It proposes a new\nassessment network for video editing that attains superior performance in\nalignment with human preferences. To the best of our knowledge, VE-Bench\nintroduces the first quality assessment dataset for video editing and an\neffective subjective-aligned quantitative metric for this domain. All data and\ncode will be publicly available at https://github.com/littlespray/VE-Bench.\n","authors":["Shangkun Sun","Xiaoyu Liang","Songlin Fan","Wenxu Gao","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2408.11481v2.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2409.17920v2","updated":"2024-12-18T09:55:15Z","published":"2024-09-26T15:04:13Z","title":"Resolving Multi-Condition Confusion for Finetuning-Free Personalized\n  Image Generation","summary":"  Personalized text-to-image generation methods can generate customized images\nbased on the reference images, which have garnered wide research interest.\nRecent methods propose a finetuning-free approach with a decoupled\ncross-attention mechanism to generate personalized images requiring no\ntest-time finetuning. However, when multiple reference images are provided, the\ncurrent decoupled cross-attention mechanism encounters the object confusion\nproblem and fails to map each reference image to its corresponding object,\nthereby seriously limiting its scope of application. To address the object\nconfusion problem, in this work we investigate the relevance of different\npositions of the latent image features to the target object in diffusion model,\nand accordingly propose a weighted-merge method to merge multiple reference\nimage features into the corresponding objects. Next, we integrate this\nweighted-merge method into existing pre-trained models and continue to train\nthe model on a multi-object dataset constructed from the open-sourced SA-1B\ndataset. To mitigate object confusion and reduce training costs, we propose an\nobject quality score to estimate the image quality for the selection of\nhigh-quality training samples. Furthermore, our weighted-merge training\nframework can be employed on single-object generation when a single object has\nmultiple reference images. The experiments verify that our method achieves\nsuperior performance to the state-of-the-arts on the Concept101 dataset and\nDreamBooth dataset of multi-object personalized image generation, and\nremarkably improves the performance on single-object personalized image\ngeneration. Our code is available at https://github.com/hqhQAQ/MIP-Adapter.\n","authors":["Qihan Huang","Siming Fu","Jinlong Liu","Hao Jiang","Yipeng Yu","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2409.17920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15024v2","updated":"2024-12-18T09:47:25Z","published":"2024-11-22T15:55:19Z","title":"DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models","summary":"  Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.\n","authors":["Keda Tao","Can Qin","Haoxuan You","Yang Sui","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15024v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13168v2","updated":"2024-12-18T09:47:15Z","published":"2024-12-17T18:45:53Z","title":"Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial\n  Dynamics in the Wild","summary":"  In-the-wild dynamic facial expression recognition (DFER) encounters a\nsignificant challenge in recognizing emotion-related expressions, which are\noften temporally and spatially diluted by emotion-irrelevant expressions and\nglobal context. Most prior DFER methods directly utilize coupled spatiotemporal\nrepresentations that may incorporate weakly relevant features with\nemotion-irrelevant context bias. Several DFER methods highlight dynamic\ninformation for DFER, but following explicit guidance that may be vulnerable to\nirrelevant motion. In this paper, we propose a novel Implicit Facial Dynamics\nDisentanglement framework (IFDD). Through expanding wavelet lifting scheme to\nfully learnable framework, IFDD disentangles emotion-related dynamic\ninformation from emotion-irrelevant global context in an implicit manner, i.e.,\nwithout exploit operations and external guidance. The disentanglement process\ncontains two stages. The first is Inter-frame Static-dynamic Splitting Module\n(ISSM) for rough disentanglement estimation, which explores inter-frame\ncorrelation to generate content-aware splitting indexes on-the-fly. We utilize\nthese indexes to split frame features into two groups, one with greater global\nsimilarity, and the other with more unique dynamic features. The second stage\nis Lifting-based Aggregation-Disentanglement Module (LADM) for further\nrefinement. LADM first aggregates two groups of features from ISSM to obtain\nfine-grained global context features by an updater, and then disentangles\nemotion-related facial dynamic features from the global context by a predictor.\nExtensive experiments on in-the-wild datasets have demonstrated that IFDD\noutperforms prior supervised DFER methods with higher recognition accuracy and\ncomparable efficiency. Code is available at\nhttps://github.com/CyberPegasus/IFDD.\n","authors":["Xingjian Wang","Li Chai"],"pdf_url":"https://arxiv.org/pdf/2412.13168v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2211.05781v3","updated":"2024-12-18T09:45:17Z","published":"2022-11-10T18:59:43Z","title":"Demystify Transformers & Convolutions in Modern Image Deep Networks","summary":"  Vision transformers have gained popularity recently, leading to the\ndevelopment of new vision backbones with improved features and consistent\nperformance gains. However, these advancements are not solely attributable to\nnovel feature transformation designs; certain benefits also arise from advanced\nnetwork-level and block-level architectures. This paper aims to identify the\nreal gains of popular convolution and attention operators through a detailed\nstudy. We find that the key difference among these feature transformation\nmodules, such as attention or convolution, lies in their spatial feature\naggregation approach, known as the \"spatial token mixer\" (STM). To facilitate\nan impartial comparison, we introduce a unified architecture to neutralize the\nimpact of divergent network-level and block-level designs. Subsequently,\nvarious STMs are integrated into this unified framework for comprehensive\ncomparative analysis. Our experiments on various tasks and an analysis of\ninductive bias show a significant performance boost due to advanced\nnetwork-level and block-level designs, but performance differences persist\namong different STMs. Our detailed analysis also reveals various findings about\ndifferent STMs, including effective receptive fields, invariance, and\nadversarial robustness tests.\n","authors":["Xiaowei Hu","Min Shi","Weiyun Wang","Sitong Wu","Linjie Xing","Wenhai Wang","Xizhou Zhu","Lewei Lu","Jie Zhou","Xiaogang Wang","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2211.05781v3.pdf","comment":"This paper was accepted to IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (IEEE TPAMI). All models and codes used in this study\n  are publicly available at https://github.com/OpenGVLab/STM-Evaluation"},{"id":"http://arxiv.org/abs/2412.13662v1","updated":"2024-12-18T09:39:12Z","published":"2024-12-18T09:39:12Z","title":"When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement\n  Learning?","summary":"  Learning policies from high-dimensional visual inputs, such as pixels and\npoint clouds, is crucial in various applications. Visual reinforcement learning\nis a promising approach that directly trains policies from visual observations,\nalthough it faces challenges in sample efficiency and computational costs. This\nstudy conducts an empirical comparison of State-to-Visual DAgger, a two-stage\nframework that initially trains a state policy before adopting online imitation\nto learn a visual policy, and Visual RL across a diverse set of tasks. We\nevaluate both methods across 16 tasks from three benchmarks, focusing on their\nasymptotic performance, sample efficiency, and computational costs.\nSurprisingly, our findings reveal that State-to-Visual DAgger does not\nuniversally outperform Visual RL but shows significant advantages in\nchallenging tasks, offering more consistent performance. In contrast, its\nbenefits in sample efficiency are less pronounced, although it often reduces\nthe overall wall-clock time required for training. Based on our findings, we\nprovide recommendations for practitioners and hope that our results contribute\nvaluable perspectives for future research in visual policy learning.\n","authors":["Tongzhou Mu","Zhaoyang Li","Stanisław Wiktor Strzelecki","Xiu Yuan","Yunchao Yao","Litian Liang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2412.13662v1.pdf","comment":"Accepted by The 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.08200v2","updated":"2024-12-18T09:36:47Z","published":"2024-12-11T08:43:52Z","title":"GN-FR:Generalizable Neural Radiance Fields for Flare Removal","summary":"  Flare, an optical phenomenon resulting from unwanted scattering and\nreflections within a lens system, presents a significant challenge in imaging.\nThe diverse patterns of flares, such as halos, streaks, color bleeding, and\nhaze, complicate the flare removal process. Existing traditional and\nlearning-based methods have exhibited limited efficacy due to their reliance on\nsingle-image approaches, where flare removal is highly ill-posed. We address\nthis by framing flare removal as a multi-view image problem, taking advantage\nof the view-dependent nature of flare artifacts. This approach leverages\ninformation from neighboring views to recover details obscured by flare in\nindividual images. Our proposed framework, GN-FR (Generalizable Neural Radiance\nFields for Flare Removal), can render flare-free views from a sparse set of\ninput images affected by lens flare and generalizes across different scenes in\nan unsupervised manner. GN-FR incorporates several modules within the\nGeneralizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation\n(FMG), View Sampler (VS), and Point Sampler (PS). To overcome the\nimpracticality of capturing both flare-corrupted and flare-free data, we\nintroduce a masking loss function that utilizes mask information in an\nunsupervised setting. Additionally, we present a 3D multi-view flare dataset,\ncomprising 17 real flare scenes with 782 images, 80 real flare patterns, and\ntheir corresponding annotated flare-occupancy masks. To our knowledge, this is\nthe first work to address flare removal within a Neural Radiance Fields (NeRF)\nframework.\n","authors":["Gopi Raju Matta","Rahul Siddartha","Rongali Simhachala Venkata Girish","Sumit Sharma","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.08200v2.pdf","comment":"Accepted for publication at BMVC-24"},{"id":"http://arxiv.org/abs/2412.13656v1","updated":"2024-12-18T09:34:59Z","published":"2024-12-18T09:34:59Z","title":"GLCF: A Global-Local Multimodal Coherence Analysis Framework for Talking\n  Face Generation Detection","summary":"  Talking face generation (TFG) allows for producing lifelike talking videos of\nany character using only facial images and accompanying text. Abuse of this\ntechnology could pose significant risks to society, creating the urgent need\nfor research into corresponding detection methods. However, research in this\nfield has been hindered by the lack of public datasets. In this paper, we\nconstruct the first large-scale multi-scenario talking face dataset (MSTF),\nwhich contains 22 audio and video forgery techniques, filling the gap of\ndatasets in this field. The dataset covers 11 generation scenarios and more\nthan 20 semantic scenarios, closer to the practical application scenario of\nTFG. Besides, we also propose a TFG detection framework, which leverages the\nanalysis of both global and local coherence in the multimodal content of TFG\nvideos. Therefore, a region-focused smoothness detection module (RSFDM) and a\ndiscrepancy capture-time frame aggregation module (DCTAM) are introduced to\nevaluate the global temporal coherence of TFG videos, aggregating multi-grained\nspatial information. Additionally, a visual-audio fusion module (V-AFM) is\ndesigned to evaluate audiovisual coherence within a localized temporal\nperspective. Comprehensive experiments demonstrate the reasonableness and\nchallenges of our datasets, while also indicating the superiority of our\nproposed method compared to the state-of-the-art deepfake detection approaches.\n","authors":["Xiaocan Chen","Qilin Yin","Jiarui Liu","Wei Lu","Xiangyang Luo","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13655v1","updated":"2024-12-18T09:34:32Z","published":"2024-12-18T09:34:32Z","title":"VIIS: Visible and Infrared Information Synthesis for Severe Low-light\n  Image Enhancement","summary":"  Images captured in severe low-light circumstances often suffer from\nsignificant information absence. Existing singular modality image enhancement\nmethods struggle to restore image regions lacking valid information. By\nleveraging light-impervious infrared images, visible and infrared image fusion\nmethods have the potential to reveal information hidden in darkness. However,\nthey primarily emphasize inter-modal complementation but neglect intra-modal\nenhancement, limiting the perceptual quality of output images. To address these\nlimitations, we propose a novel task, dubbed visible and infrared information\nsynthesis (VIIS), which aims to achieve both information enhancement and fusion\nof the two modalities. Given the difficulty in obtaining ground truth in the\nVIIS task, we design an information synthesis pretext task (ISPT) based on\nimage augmentation. We employ a diffusion model as the framework and design a\nsparse attention-based dual-modalities residual (SADMR) conditioning mechanism\nto enhance information interaction between the two modalities. This mechanism\nenables features with prior knowledge from both modalities to adaptively and\niteratively attend to each modality's information during the denoising process.\nOur extensive experiments demonstrate that our model qualitatively and\nquantitatively outperforms not only the state-of-the-art methods in relevant\nfields but also the newly designed baselines capable of both information\nenhancement and fusion. The code is available at\nhttps://github.com/Chenz418/VIIS.\n","authors":["Chen Zhao","Mengyuan Yu","Fan Yang","Peiguang Jing"],"pdf_url":"https://arxiv.org/pdf/2412.13655v1.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2306.09801v3","updated":"2024-12-18T09:34:18Z","published":"2023-06-16T12:22:19Z","title":"Semantics-Aware Next-best-view Planning for Efficient Search and\n  Detection of Task-relevant Plant Parts","summary":"  Searching and detecting the task-relevant parts of plants is important to\nautomate harvesting and de-leafing of tomato plants using robots. This is\nchallenging due to high levels of occlusion in tomato plants. Active vision is\na promising approach in which the robot strategically plans its camera\nviewpoints to overcome occlusion and improve perception accuracy. However,\ncurrent active-vision algorithms cannot differentiate between relevant and\nirrelevant plant parts and spend time on perceiving irrelevant plant parts.\nThis work proposed a semantics-aware active-vision strategy that uses semantic\ninformation to identify the relevant plant parts and prioritise them during\nview planning. The proposed strategy was evaluated on the task of searching and\ndetecting the relevant plant parts using simulation and real-world experiments.\nIn simulation experiments, the semantics-aware strategy proposed could search\nand detect 81.8% of the relevant plant parts using nine viewpoints. It was\nsignificantly faster and detected more plant parts than predefined, random, and\nvolumetric active-vision strategies that do not use semantic information. The\nstrategy proposed was also robust to uncertainty in plant and plant-part\npositions, plant complexity, and different viewpoint-sampling strategies. In\nreal-world experiments, the semantics-aware strategy could search and detect\n82.7% of the relevant plant parts using seven viewpoints, under complex\ngreenhouse conditions with natural variation and occlusion, natural\nillumination, sensor noise, and uncertainty in camera poses. The results of\nthis work clearly indicate the advantage of using semantics-aware active vision\nfor targeted perception of plant parts and its applicability in the real world.\nIt can significantly improve the efficiency of automated harvesting and\nde-leafing in tomato crop production.\n","authors":["Akshay K. Burusa","Joost Scholten","David Rapado Rincon","Xin Wang","Eldert J. van Henten","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2306.09801v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13654v1","updated":"2024-12-18T09:33:20Z","published":"2024-12-18T09:33:20Z","title":"GAGS: Granularity-Aware Feature Distillation for Language Gaussian\n  Splatting","summary":"  3D open-vocabulary scene understanding, which accurately perceives complex\nsemantic properties of objects in space, has gained significant attention in\nrecent years. In this paper, we propose GAGS, a framework that distills 2D CLIP\nfeatures into 3D Gaussian splatting, enabling open-vocabulary queries for\nrenderings on arbitrary viewpoints. The main challenge of distilling 2D\nfeatures for 3D fields lies in the multiview inconsistency of extracted 2D\nfeatures, which provides unstable supervision for the 3D feature field. GAGS\naddresses this challenge with two novel strategies. First, GAGS associates the\nprompt point density of SAM with the camera distances, which significantly\nimproves the multiview consistency of segmentation results. Second, GAGS\nfurther decodes a granularity factor to guide the distillation process and this\ngranularity factor can be learned in a unsupervised manner to only select the\nmultiview consistent 2D features in the distillation process. Experimental\nresults on two datasets demonstrate significant performance and stability\nimprovements of GAGS in visual grounding and semantic segmentation, with an\ninference speed 2$\\times$ faster than baseline methods. The code and additional\nresults are available at https://pz0826.github.io/GAGS-Webpage/ .\n","authors":["Yuning Peng","Haiping Wang","Yuan Liu","Chenglu Wen","Zhen Dong","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2412.13654v1.pdf","comment":"Project page: https://pz0826.github.io/GAGS-Webpage/"},{"id":"http://arxiv.org/abs/2412.13652v1","updated":"2024-12-18T09:31:06Z","published":"2024-12-18T09:31:06Z","title":"RelationField: Relate Anything in Radiance Fields","summary":"  Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io.\n","authors":["Sebastian Koch","Johanna Wald","Mirco Colosi","Narunas Vaskevicius","Pedro Hermosilla","Federico Tombari","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2412.13652v1.pdf","comment":"Project page: https://relationfield.github.io"},{"id":"http://arxiv.org/abs/2407.19323v4","updated":"2024-12-18T09:28:37Z","published":"2024-07-27T19:00:44Z","title":"MSP-MVS: Multi-Granularity Segmentation Prior Guided Multi-View Stereo","summary":"  Recently, patch deformation-based methods have demonstrated significant\nstrength in multi-view stereo by adaptively expanding the reception field of\npatches to help reconstruct textureless areas. However, such methods mainly\nconcentrate on searching for pixels without matching ambiguity (i.e., reliable\npixels) when constructing deformed patches, while neglecting the deformation\ninstability caused by unexpected edge-skipping, resulting in potential matching\ndistortions. Addressing this, we propose MSP-MVS, a method introducing\nmulti-granularity segmentation prior for edge-confined patch deformation.\nSpecifically, to avoid unexpected edge-skipping, we first aggregate and further\nrefine multi-granularity depth edges gained from Semantic-SAM as prior to guide\npatch deformation within depth-continuous (i.e., homogeneous) areas. Moreover,\nto address attention imbalance caused by edge-confined patch deformation, we\nimplement adaptive equidistribution and disassemble-clustering of correlative\nreliable pixels (i.e., anchors), thereby promoting attention-consistent patch\ndeformation. Finally, to prevent deformed patches from falling into\nlocal-minimum matching costs caused by the fixed sampling pattern, we introduce\ndisparity-sampling synergistic 3D optimization to help identify global-minimum\nmatching costs. Evaluations on ETH3D and Tanks & Temples benchmarks prove our\nmethod obtains state-of-the-art performance with remarkable generalization.\n","authors":["Zhenlong Yuan","Cong Liu","Fei Shen","Zhaoxin Li","Jinguo Luo","Tianlu Mao","Zhaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19323v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13647v1","updated":"2024-12-18T09:23:12Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09365v3","updated":"2024-12-18T09:11:06Z","published":"2024-05-15T14:17:44Z","title":"SARATR-X: Towards Building A Foundation Model for SAR Target Recognition","summary":"  Despite the remarkable progress in synthetic aperture radar automatic target\nrecognition (SAR ATR), recent efforts have concentrated on detecting and\nclassifying a specific category, e.g., vehicles, ships, airplanes, or\nbuildings. One of the fundamental limitations of the top-performing SAR ATR\nmethods is that the learning paradigm is supervised, task-specific,\nlimited-category, closed-world learning, which depends on massive amounts of\naccurately annotated samples that are expensively labeled by expert SAR\nanalysts and have limited generalization capability and scalability. In this\nwork, we make the first attempt towards building a foundation model for SAR\nATR, termed SARATR-X. SARATR-X learns generalizable representations via\nself-supervised learning (SSL) and provides a cornerstone for label-efficient\nmodel adaptation to generic SAR target detection and classification tasks.\nSpecifically, SARATR-X is trained on 0.18 M unlabelled SAR target samples,\nwhich are curated by combining contemporary benchmarks and constitute the\nlargest publicly available dataset till now. Considering the characteristics of\nSAR images, a backbone tailored for SAR ATR is carefully designed, and a\ntwo-step SSL method endowed with multi-scale gradient features was applied to\nensure the feature diversity and model scalability of SARATR-X. The\ncapabilities of SARATR-X are evaluated on classification under few-shot and\nrobustness settings and detection across various categories and scenes, and\nimpressive performance is achieved, often competitive with or even superior to\nprior fully supervised, semi-supervised, or self-supervised algorithms. Our\nSARATR-X and the curated dataset are released at\nhttps://github.com/waterdisappear/SARATR-X to foster research into foundation\nmodels for SAR image interpretation.\n","authors":["Weijie Li","Wei Yang","Yuenan Hou","Li Liu","Yongxiang Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2405.09365v3.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.13636v1","updated":"2024-12-18T09:09:41Z","published":"2024-12-18T09:09:41Z","title":"Consistency of Compositional Generalization across Multiple Levels","summary":"  Compositional generalization is the capability of a model to understand novel\ncompositions composed of seen concepts. There are multiple levels of novel\ncompositions including phrase-phrase level, phrase-word level, and word-word\nlevel. Existing methods achieve promising compositional generalization, but the\nconsistency of compositional generalization across multiple levels of novel\ncompositions remains unexplored. The consistency refers to that a model should\ngeneralize to a phrase-phrase level novel composition, and\nphrase-word/word-word level novel compositions that can be derived from it\nsimultaneously. In this paper, we propose a meta-learning based framework, for\nachieving consistent compositional generalization across multiple levels. The\nbasic idea is to progressively learn compositions from simple to complex for\nconsistency. Specifically, we divide the original training set into multiple\nvalidation sets based on compositional complexity, and introduce multiple\nmeta-weight-nets to generate sample weights for samples in different validation\nsets. To fit the validation sets in order of increasing compositional\ncomplexity, we optimize the parameters of each meta-weight-net independently\nand sequentially in a multilevel optimization manner. We build a GQA-CCG\ndataset to quantitatively evaluate the consistency. Experimental results on\nvisual question answering and temporal video grounding, demonstrate the\neffectiveness of the proposed framework. We release GQA-CCG at\nhttps://github.com/NeverMoreLCH/CCG.\n","authors":["Chuanhao Li","Zhen Li","Chenchen Jing","Xiaomeng Fan","Wenbo Ye","Yuwei Wu","Yunde Jia"],"pdf_url":"https://arxiv.org/pdf/2412.13636v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13635v1","updated":"2024-12-18T09:09:39Z","published":"2024-12-18T09:09:39Z","title":"Self-control: A Better Conditional Mechanism for Masked Autoregressive\n  Model","summary":"  Autoregressive conditional image generation algorithms are capable of\ngenerating photorealistic images that are consistent with given textual or\nimage conditions, and have great potential for a wide range of applications.\nNevertheless, the majority of popular autoregressive image generation methods\nrely heavily on vector quantization, and the inherent discrete characteristic\nof codebook presents a considerable challenge to achieving high-quality image\ngeneration. To address this limitation, this paper introduces a novel\nconditional introduction network for continuous masked autoregressive models.\nThe proposed self-control network serves to mitigate the negative impact of\nvector quantization on the quality of the generated images, while\nsimultaneously enhancing the conditional control during the generation process.\nIn particular, the self-control network is constructed upon a continuous mask\nautoregressive generative model, which incorporates multimodal conditional\ninformation, including text and images, into a unified autoregressive sequence\nin a serial manner. Through a self-attention mechanism, the network is capable\nof generating images that are controllable based on specific conditions. The\nself-control network discards the conventional cross-attention-based\nconditional fusion mechanism and effectively unifies the conditional and\ngenerative information within the same space, thereby facilitating more\nseamless learning and fusion of multimodal features.\n","authors":["Qiaoying Qu","Shiyu Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10843v3","updated":"2024-12-18T08:54:03Z","published":"2024-08-19T11:42:54Z","title":"Detecting Wildfires on UAVs with Real-time Segmentation Trained by\n  Larger Teacher Models","summary":"  Early detection of wildfires is essential to prevent large-scale fires\nresulting in extensive environmental, structural, and societal damage. Uncrewed\naerial vehicles (UAVs) can cover large remote areas effectively with quick\ndeployment requiring minimal infrastructure and equipping them with small\ncameras and computers enables autonomous real-time detection. In remote areas,\nhowever, detection methods are limited to onboard computation due to the lack\nof high-bandwidth mobile networks. For accurate camera-based localisation,\nsegmentation of the detected smoke is essential but training data for deep\nlearning-based wildfire smoke segmentation is limited. This study shows how\nsmall specialised segmentation models can be trained using only bounding box\nlabels, leveraging zero-shot foundation model supervision. The method offers\nthe advantages of needing only fairly easily obtainable bounding box labels and\nrequiring training solely for the smaller student network. The proposed method\nachieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The\nused model can perform in real-time at ~25 fps with a UAV-carried NVIDIA Jetson\nOrin NX computer while reliably recognising smoke, as demonstrated at\nreal-world forest burning events. Code is available at:\nhttps://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation\n","authors":["Julius Pesonen","Teemu Hakala","Väinö Karjalainen","Niko Koivumäki","Lauri Markelin","Anna-Maria Raita-Hakola","Juha Suomalainen","Ilkka Pölönen","Eija Honkavaara"],"pdf_url":"https://arxiv.org/pdf/2408.10843v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13615v1","updated":"2024-12-18T08:53:52Z","published":"2024-12-18T08:53:52Z","title":"MambaLCT: Boosting Tracking via Long-term Context State Space Model","summary":"  Effectively constructing context information with long-term dependencies from\nvideo sequences is crucial for object tracking. However, the context length\nconstructed by existing work is limited, only considering object information\nfrom adjacent frames or video clips, leading to insufficient utilization of\ncontextual information. To address this issue, we propose MambaLCT, which\nconstructs and utilizes target variation cues from the first frame to the\ncurrent frame for robust tracking. First, a novel unidirectional Context Mamba\nmodule is designed to scan frame features along the temporal dimension,\ngathering target change cues throughout the entire sequence. Specifically,\ntarget-related information in frame features is compressed into a hidden state\nspace through selective scanning mechanism. The target information across the\nentire video is continuously aggregated into target variation cues. Next, we\ninject the target change cues into the attention mechanism, providing temporal\ninformation for modeling the relationship between the template and search\nframes. The advantage of MambaLCT is its ability to continuously extend the\nlength of the context, capturing complete target change cues, which enhances\nthe stability and robustness of the tracker. Extensive experiments show that\nlong-term context information enhances the model's ability to perceive targets\nin complex scenarios. MambaLCT achieves new SOTA performance on six benchmarks\nwhile maintaining real-time running speeds.\n","authors":["Xiaohai Li","Bineng Zhong","Qihua Liang","Guorong Li","Zhiyi Mo","Shuxiang Song"],"pdf_url":"https://arxiv.org/pdf/2412.13615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01988v2","updated":"2024-12-18T08:51:39Z","published":"2024-11-04T11:20:17Z","title":"QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition","summary":"  Facial expression recognition faces challenges where labeled significant\nfeatures in datasets are mixed with unlabeled redundant ones. In this paper, we\nintroduce Cross Similarity Attention (CSA) to mine richer intrinsic information\nfrom image pairs, overcoming a limitation when the Scaled Dot-Product Attention\nof ViT is directly applied to calculate the similarity between two different\nimages. Based on CSA, we simultaneously minimize intra-class differences and\nmaximize inter-class differences at the fine-grained feature level through\ninteractions among multiple branches. Contrastive residual distillation is\nutilized to transfer the information learned in the cross module back to the\nbase network. We ingeniously design a four-branch centrally symmetric network,\nnamed Quadruplet Cross Similarity (QCS), which alleviates gradient conflicts\narising from the cross module and achieves balanced and stable training. It can\nadaptively extract discriminative features while isolating redundant ones. The\ncross-attention modules exist during training, and only one base branch is\nretained during inference, resulting in no increase in inference time. Our\nproposed method achieves state-of-the-art performance on several FER datasets.\n","authors":["Chengpeng Wang","Li Chen","Lili Wang","Zhaofan Li","Xuebin Lv"],"pdf_url":"https://arxiv.org/pdf/2411.01988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11223v2","updated":"2024-12-18T08:49:16Z","published":"2024-11-18T01:25:58Z","title":"Efficient Transfer Learning for Video-language Foundation Models","summary":"  Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional parameter modules\nto capture temporal information. While the increased model capacity brought by\nthese additional parameters helps better fit the video-specific inductive\nbiases, existing methods require learning a large number of parameters and are\nprone to catastrophic forgetting of the original generalizable knowledge. In\nthis paper, we propose a simple yet effective Multi-modal Spatio-Temporal\nAdapter (MSTA) to improve the alignment between representations in the text and\nvision branches, achieving a balance between general knowledge and\ntask-specific knowledge. Furthermore, to mitigate over-fitting and enhance\ngeneralizability, we introduce a spatio-temporal description-guided consistency\nconstraint. This constraint involves feeding template inputs (i.e., ``a video\nof $\\{\\textbf{cls}\\}$'') into the trainable language branch, while\nLLM-generated spatio-temporal descriptions are input into the pre-trained\nlanguage branch, enforcing consistency between the outputs of the two branches.\nThis mechanism prevents over-fitting to downstream tasks and improves the\ndistinguishability of the trainable branch within the spatio-temporal semantic\nspace. We evaluate the effectiveness of our approach across four tasks:\nzero-shot transfer, few-shot learning, base-to-novel generalization, and\nfully-supervised learning. Compared to many state-of-the-art methods, our MSTA\nachieves outstanding performance across all evaluations, while using only 2-7\\%\nof the trainable parameters in the original model. Code will be avaliable at\nhttps://github.com/chenhaoxing/ETL4Video.\n","authors":["Haoxing Chen","Zizheng Huang","Yan Hong","Yanshuo Wang","Zhongcai Lyu","Zhuoer Xu","Jun Lan","Zhangxuan Gu"],"pdf_url":"https://arxiv.org/pdf/2411.11223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13614v1","updated":"2024-12-18T08:49:01Z","published":"2024-12-18T08:49:01Z","title":"Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity\n  Linking","summary":"  Visual Entity Linking (VEL) is a crucial task for achieving fine-grained\nvisual understanding, matching objects within images (visual mentions) to\nentities in a knowledge base. Previous VEL tasks rely on textual inputs, but\nwriting queries for complex scenes can be challenging. Visual inputs like\nclicks or bounding boxes offer a more convenient alternative. Therefore, we\npropose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses\npixel masks from visual inputs to refer to objects, supplementing reference\nmethods for VEL. To facilitate research on this task, we have constructed the\nMaskOVEN-Wiki dataset through an entirely automatic reverse region-entity\nannotation framework. This dataset contains over 5 million annotations aligning\npixel-level regions with entity-level labels, which will advance visual\nunderstanding towards fine-grained. Moreover, as pixel masks correspond to\nsemantic regions in an image, we enhance previous patch-interacted attention\nwith region-interacted attention by a visual semantic tokenization approach.\nManual evaluation results indicate that the reverse annotation framework\nachieved a 94.8% annotation success rate. Experimental results show that models\ntrained on this dataset improved accuracy by 18 points compared to zero-shot\nmodels. Additionally, the semantic tokenization method achieved a 5-point\naccuracy improvement over the trained baseline.\n","authors":["Zhengfei Xu","Sijia Zhao","Yanchao Hao","Xiaolong Liu","Lili Li","Yuyang Yin","Bo Li","Xi Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2412.13614v1.pdf","comment":"AAAI 2025;Dataset are released at\n  https://github.com/NP-NET-research/PL-VEL"},{"id":"http://arxiv.org/abs/2405.16751v2","updated":"2024-12-18T08:38:06Z","published":"2024-05-27T01:47:14Z","title":"REVECA: Adaptive Planning and Trajectory-based Validation in Cooperative\n  Language Agents using Information Relevance and Relative Proximity","summary":"  We address the challenge of multi-agent cooperation, where agents achieve a\ncommon goal by cooperating with decentralized agents under complex partial\nobservations. Existing cooperative agent systems often struggle with\nefficiently processing continuously accumulating information, managing globally\nsuboptimal planning due to lack of consideration of collaborators, and\naddressing false planning caused by environmental changes introduced by other\ncollaborators. To overcome these challenges, we propose the RElevance,\nProximity, and Validation-Enhanced Cooperative Language Agent (REVECA), a novel\ncognitive architecture powered by GPT-4o-mini. REVECA enables efficient memory\nmanagement, optimal planning, and cost-effective prevention of false planning\nby leveraging Relevance Estimation, Adaptive Planning, and Trajectory-based\nValidation. Extensive experimental results demonstrate REVECA's superiority\nover existing methods across various benchmarks, while a user study reveals its\npotential for achieving trustworthy human-AI cooperation.\n","authors":["SeungWon Seo","SeongRae Noh","Junhyeok Lee","SooBin Lim","Won Hee Lee","HyeongYeop Kang"],"pdf_url":"https://arxiv.org/pdf/2405.16751v2.pdf","comment":"v2 is the AAAI'25 camera-ready version, including the appendix, which\n  has been enhanced based on the reviewers' comments"},{"id":"http://arxiv.org/abs/2412.13611v1","updated":"2024-12-18T08:37:22Z","published":"2024-12-18T08:37:22Z","title":"Robust Tracking via Mamba-based Context-aware Token Learning","summary":"  How to make a good trade-off between performance and computational cost is\ncrucial for a tracker. However, current famous methods typically focus on\ncomplicated and time-consuming learning that combining temporal and appearance\ninformation by input more and more images (or features). Consequently, these\nmethods not only increase the model's computational source and learning burden\nbut also introduce much useless and potentially interfering information. To\nalleviate the above issues, we propose a simple yet robust tracker that\nseparates temporal information learning from appearance modeling and extracts\ntemporal relations from a set of representative tokens rather than several\nimages (or features). Specifically, we introduce one track token for each frame\nto collect the target's appearance information in the backbone. Then, we design\na mamba-based Temporal Module for track tokens to be aware of context by\ninteracting with other track tokens within a sliding window. This module\nconsists of a mamba layer with autoregressive characteristic and a\ncross-attention layer with strong global perception ability, ensuring\nsufficient interaction for track tokens to perceive the appearance changes and\nmovement trends of the target. Finally, track tokens serve as a guidance to\nadjust the appearance feature for the final prediction in the head. Experiments\nshow our method is effective and achieves competitive performance on multiple\nbenchmarks at a real-time speed. Code and trained models will be available at\nhttps://github.com/GXNU-ZhongLab/TemTrack.\n","authors":["Jinxia Xie","Bineng Zhong","Qihua Liang","Ning Li","Zhiyi Mo","Shuxiang Song"],"pdf_url":"https://arxiv.org/pdf/2412.13611v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.13610v1","updated":"2024-12-18T08:37:13Z","published":"2024-12-18T08:37:13Z","title":"Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking\n  Calculation","summary":"  Spiking Neural Network (SNN), as a brain-inspired and energy-efficient\nnetwork, is currently facing the pivotal challenge of exploring a suitable and\nefficient learning framework. The predominant training methodologies, namely\nSpatial-Temporal Back-propagation (STBP) and ANN-SNN Conversion, are encumbered\nby substantial training overhead or pronounced inference latency, which impedes\nthe advancement of SNNs in scaling to larger networks and navigating intricate\napplication domains. In this work, we propose a novel parallel conversion\nlearning framework, which establishes a mathematical mapping relationship\nbetween each time-step of the parallel spiking neurons and the cumulative spike\nfiring rate. We theoretically validate the lossless and sorting properties of\nthe conversion process, as well as pointing out the optimal shifting distance\nfor each step. Furthermore, by integrating the above framework with the\ndistribution-aware error calibration technique, we can achieve efficient\nconversion towards more general activation functions or training-free\ncircumstance. Extensive experiments have confirmed the significant performance\nadvantages of our method for various conversion cases under ultra-low time\nlatency. To our best knowledge, this is the first work which jointly utilizes\nparallel spiking calculation and ANN-SNN Conversion, providing a highly\npromising approach for SNN supervised training.\n","authors":["Zecheng Hao","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13609v1","updated":"2024-12-18T08:36:35Z","published":"2024-12-18T08:36:35Z","title":"Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production","summary":"  Sign Language Production (SLP) aims to generate semantically consistent sign\nvideos from textual statements, where the conversion from textual glosses to\nsign poses (G2P) is a crucial step. Existing G2P methods typically treat sign\nposes as discrete three-dimensional coordinates and directly fit them, which\noverlooks the relative positional relationships among joints. To this end, we\nprovide a new perspective, constraining joint associations and gesture details\nby modeling the limb bones to improve the accuracy and naturalness of the\ngenerated poses. In this work, we propose a pioneering iconicity disentangled\ndiffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD\nincorporates a novel Iconicity Disentanglement (ID) module to bridge the gap\nbetween relative positions among joints. The ID module disentangles the\nconventional 3D joint representation into a 4D bone representation, comprising\nthe 3D spatial direction vector and 1D spatial distance vector between adjacent\njoints. Additionally, an Attribute Controllable Diffusion (ACD) module is\nintroduced to further constrain joint associations, in which the attribute\nseparation layer aims to separate the bone direction and length attributes, and\nthe attribute control layer is designed to guide the pose generation by\nleveraging the above attributes. The ACD module utilizes the gloss embeddings\nas semantic conditions and finally generates sign poses from noise embeddings.\nExtensive experiments on PHOENIX14T and USTC-CSL datasets validate the\neffectiveness of our method. The code is available at:\nhttps://github.com/NaVi-start/Sign-IDD.\n","authors":["Shengeng Tang","Jiayi He","Dan Guo","Yanyan Wei","Feng Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.13609v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13601v1","updated":"2024-12-18T08:31:34Z","published":"2024-12-18T08:31:34Z","title":"Hybrid CNN-LSTM based Indoor Pedestrian Localization with CSI\n  Fingerprint Maps","summary":"  The paper presents a novel Wi-Fi fingerprinting system that uses Channel\nState Information (CSI) data for fine-grained pedestrian localization. The\nproposed system exploits the frequency diversity and spatial diversity of the\nfeatures extracted from CSI data to generate a 2D+channel image termed as a CSI\nFingerprint Map. We then use this CSI Fingerprint Map representation of CSI\ndata to generate a pedestrian trajectory hypothesis using a hybrid architecture\nthat combines a Convolutional Neural Network and a Long Short-Term Memory\nRecurrent Neural Network model. The proposed architecture exploits the temporal\nand spatial relationship information among the CSI data observations gathered\nat neighboring locations. A particle filter is then employed to separate out\nthe most likely hypothesis matching a human walk model. The experimental\nperformance of our method is compared to existing deep learning localization\nmethods such ConFi, DeepFi and to a self-developed temporal-feature based LSTM\nbased location classifier. The experimental results show marked improvement\nwith an average RMSE of 0.36 m in a moderately dynamic and 0.17 m in a static\nenvironment. Our method is essentially a proof of concept that with (1) sparse\navailability of observations, (2) limited infrastructure requirements, (3)\nmoderate level of short-term and long-term noise in the training and testing\nenvironment, reliable fine-grained Wi-Fi based pedestrian localization is a\npotential option.\n","authors":["Muhammad Emad-ud-din"],"pdf_url":"https://arxiv.org/pdf/2412.13601v1.pdf","comment":"12 pages, 14 figures and 3 tables"},{"id":"http://arxiv.org/abs/2412.13599v1","updated":"2024-12-18T08:31:26Z","published":"2024-12-18T08:31:26Z","title":"Unlocking the Potential of Weakly Labeled Data: A Co-Evolutionary\n  Learning Framework for Abnormality Detection and Report Generation","summary":"  Anatomical abnormality detection and report generation of chest X-ray (CXR)\nare two essential tasks in clinical practice. The former aims at localizing and\ncharacterizing cardiopulmonary radiological findings in CXRs, while the latter\nsummarizes the findings in a detailed report for further diagnosis and\ntreatment. Existing methods often focused on either task separately, ignoring\ntheir correlation. This work proposes a co-evolutionary abnormality detection\nand report generation (CoE-DG) framework. The framework utilizes both fully\nlabeled (with bounding box annotations and clinical reports) and weakly labeled\n(with reports only) data to achieve mutual promotion between the abnormality\ndetection and report generation tasks. Specifically, we introduce a\nbi-directional information interaction strategy with generator-guided\ninformation propagation (GIP) and detector-guided information propagation\n(DIP). For semi-supervised abnormality detection, GIP takes the informative\nfeature extracted by the generator as an auxiliary input to the detector and\nuses the generator's prediction to refine the detector's pseudo labels. We\nfurther propose an intra-image-modal self-adaptive non-maximum suppression\nmodule (SA-NMS). This module dynamically rectifies pseudo detection labels\ngenerated by the teacher detection model with high-confidence predictions by\nthe student.Inversely, for report generation, DIP takes the abnormalities'\ncategories and locations predicted by the detector as input and guidance for\nthe generator to improve the generated reports.\n","authors":["Jinghan Sun","Dong Wei","Zhe Xu","Donghuan Lu","Hong Liu","Hong Wang","Sotirios A. Tsaftaris","Steven McDonagh","Yefeng Zheng","Liansheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04363v2","updated":"2024-12-18T08:30:59Z","published":"2024-04-05T19:16:30Z","title":"Idea23D: Collaborative LMM Agents Enable 3D Model Generation from\n  Interleaved Multimodal Inputs","summary":"  With the success of 2D diffusion models, 2D AIGC content has already\ntransformed our lives. Recently, this success has been extended to 3D AIGC,\nwith state-of-the-art methods generating textured 3D models from single images\nor text. However, we argue that current 3D AIGC methods still do not fully\nunleash human creativity. We often imagine 3D content made from multimodal\ninputs, such as what it would look like if my pet bunny were eating a doughnut\non the table. In this paper, we explore a novel 3D AIGC approach: generating 3D\ncontent from IDEAs. An IDEA is a multimodal input composed of text, image, and\n3D models. To our knowledge, this challenging and exciting 3D AIGC setting has\nnot been studied before. We propose the new framework Idea23D, which combines\nthree agents based on large multimodal models (LMMs) and existing algorithmic\ntools. These three LMM-based agents are tasked with prompt generation, model\nselection, and feedback reflection. They collaborate and critique each other in\na fully automated loop, without human intervention. The framework then\ngenerates a text prompt to create 3D models that align closely with the input\nIDEAs. We demonstrate impressive 3D AIGC results that surpass previous methods.\nTo comprehensively assess the 3D AIGC capabilities of Idea23D, we introduce the\nEval3DAIGC-198 dataset, containing 198 multimodal inputs for 3D generation\ntasks. This dataset evaluates the alignment between generated 3D content and\ninput IDEAs. Our user study and quantitative results show that Idea23D\nsignificantly improves the success rate and accuracy of 3D generation, with\nexcellent compatibility across various LMM, Text-to-Image, and Image-to-3D\nmodels. Code and dataset are available at \\url{https://idea23d.github.io/}.\n","authors":["Junhao Chen","Xiang Li","Xiaojun Ye","Chao Li","Zhaoxin Fan","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.04363v2.pdf","comment":"Accepted by COLING 2025 (The 31st International Conference on\n  Computational Linguistics) Project Page: https://idea23d.github.io/ Code:\n  https://github.com/yisuanwang/Idea23D"},{"id":"http://arxiv.org/abs/2412.10824v2","updated":"2024-12-18T08:25:55Z","published":"2024-12-14T13:05:05Z","title":"Diffusion Model from Scratch","summary":"  Diffusion generative models are currently the most popular generative models.\nHowever, their underlying modeling process is quite complex, and starting\ndirectly with the seminal paper Denoising Diffusion Probability Model (DDPM)\ncan be challenging. This paper aims to assist readers in building a\nfoundational understanding of generative models by tracing the evolution from\nVAEs to DDPM through detailed mathematical derivations and a problem-oriented\nanalytical approach. It also explores the core ideas and improvement strategies\nof current mainstream methodologies, providing guidance for undergraduate and\ngraduate students interested in learning about diffusion models.\n","authors":["Wang Zhen","Dong Yunyun"],"pdf_url":"https://arxiv.org/pdf/2412.10824v2.pdf","comment":"There were problems with the typography of our illustrations, and\n  there were problems with the derivation of the 200-step formula"},{"id":"http://arxiv.org/abs/2412.13594v1","updated":"2024-12-18T08:18:03Z","published":"2024-12-18T08:18:03Z","title":"Generalizable Sensor-Based Activity Recognition via Categorical Concept\n  Invariant Learning","summary":"  Human Activity Recognition (HAR) aims to recognize activities by training\nmodels on massive sensor data. In real-world deployment, a crucial aspect of\nHAR that has been largely overlooked is that the test sets may have different\ndistributions from training sets due to inter-subject variability including\nage, gender, behavioral habits, etc., which leads to poor generalization\nperformance. One promising solution is to learn domain-invariant\nrepresentations to enable a model to generalize on an unseen distribution.\nHowever, most existing methods only consider the feature-invariance of the\npenultimate layer for domain-invariant learning, which leads to suboptimal\nresults. In this paper, we propose a Categorical Concept Invariant Learning\n(CCIL) framework for generalizable activity recognition, which introduces a\nconcept matrix to regularize the model in the training stage by simultaneously\nconcentrating on feature-invariance and logit-invariance. Our key idea is that\nthe concept matrix for samples belonging to the same activity category should\nbe similar. Extensive experiments on four public HAR benchmarks demonstrate\nthat our CCIL substantially outperforms the state-of-the-art approaches under\ncross-person, cross-dataset, cross-position, and one-person-to-another\nsettings.\n","authors":["Di Xiong","Shuoyuan Wang","Lei Zhang","Wenbo Huang","Chaolei Han"],"pdf_url":"https://arxiv.org/pdf/2412.13594v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11077v2","updated":"2024-12-18T07:59:03Z","published":"2024-12-15T06:22:20Z","title":"Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for\n  Training-Free Zero-Shot Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve target images that closely\nresemble a reference image while integrating user-specified textual\nmodifications, thereby capturing user intent more precisely. Existing\ntraining-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process:\nthey first generate a caption for the reference image and then use Large\nLanguage Models for reasoning to obtain a target description. However, these\nmethods suffer from missing critical visual details and limited reasoning\ncapabilities, leading to suboptimal retrieval performance. To address these\nchallenges, we propose a novel, training-free one-stage method, One-Stage\nReflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employs\nMultimodal Large Language Models to retain essential visual information in a\nsingle-stage reasoning process, eliminating the information loss seen in\ntwo-stage methods. Our Reflective Chain-of-Thought framework further improves\ninterpretative accuracy by aligning manipulation intent with contextual cues\nfrom reference images. OSrCIR achieves performance gains of 1.80% to 6.44% over\nexisting training-free methods across multiple tasks, setting new\nstate-of-the-art results in ZS-CIR and enhancing its utility in vision-language\napplications. Our code will be available at\nhttps://github.com/Pter61/osrcir2024/.\n","authors":["Yuanmin Tang","Xiaoting Qin","Jue Zhang","Jing Yu","Gaopeng Gou","Gang Xiong","Qingwei Ling","Saravan Rajmohan","Dongmei Zhang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v2","updated":"2024-12-18T07:52:14Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Xue-Mei Dong","Jingqun Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12974v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13577v1","updated":"2024-12-18T07:51:35Z","published":"2024-12-18T07:51:35Z","title":"Bridge then Begin Anew: Generating Target-relevant Intermediate Model\n  for Source-free Visual Emotion Adaptation","summary":"  Visual emotion recognition (VER), which aims at understanding humans'\nemotional reactions toward different visual stimuli, has attracted increasing\nattention. Given the subjective and ambiguous characteristics of emotion,\nannotating a reliable large-scale dataset is hard. For reducing reliance on\ndata labeling, domain adaptation offers an alternative solution by adapting\nmodels trained on labeled source data to unlabeled target data. Conventional\ndomain adaptation methods require access to source data. However, due to\nprivacy concerns, source emotional data may be inaccessible. To address this\nissue, we propose an unexplored task: source-free domain adaptation (SFDA) for\nVER, which does not have access to source data during the adaptation process.\nTo achieve this, we propose a novel framework termed Bridge then Begin Anew\n(BBA), which consists of two steps: domain-bridged model generation (DMG) and\ntarget-related model adaptation (TMA). First, the DMG bridges cross-domain gaps\nby generating an intermediate model, avoiding direct alignment between two VER\ndatasets with significant differences. Then, the TMA begins training the target\nmodel anew to fit the target structure, avoiding the influence of\nsource-specific knowledge. Extensive experiments are conducted on six SFDA\nsettings for VER. The results demonstrate the effectiveness of BBA, which\nachieves remarkable performance gains compared with state-of-the-art SFDA\nmethods and outperforms representative unsupervised domain adaptation\napproaches.\n","authors":["Jiankun Zhu","Sicheng Zhao","Jing Jiang","Wenbo Tang","Zhaopan Xu","Tingting Han","Pengfei Xu","Hongxun Yao"],"pdf_url":"https://arxiv.org/pdf/2412.13577v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.13573v1","updated":"2024-12-18T07:45:30Z","published":"2024-12-18T07:45:30Z","title":"Seeking Consistent Flat Minima for Better Domain Generalization via\n  Refining Loss Landscapes","summary":"  Domain generalization aims to learn a model from multiple training domains\nand generalize it to unseen test domains. Recent theory has shown that seeking\nthe deep models, whose parameters lie in the flat minima of the loss landscape,\ncan significantly reduce the out-of-domain generalization error. However,\nexisting methods often neglect the consistency of loss landscapes in different\ndomains, resulting in models that are not simultaneously in the optimal flat\nminima in all domains, which limits their generalization ability. To address\nthis issue, this paper proposes an iterative Self-Feedback Training (SFT)\nframework to seek consistent flat minima that are shared across different\ndomains by progressively refining loss landscapes during training. It\nalternatively generates a feedback signal by measuring the inconsistency of\nloss landscapes in different domains and refines these loss landscapes for\ngreater consistency using this feedback signal. Benefiting from the consistency\nof the flat minima within these refined loss landscapes, our SFT helps achieve\nbetter out-of-domain generalization. Extensive experiments on DomainBed\ndemonstrate superior performances of SFT when compared to state-of-the-art\nsharpness-aware methods and other prevalent DG baselines. On average across\nfive DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with\nResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available\nsoon.\n","authors":["Aodi Li","Liansheng Zhuang","Xiao Long","Minghong Yao","Shafei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08584v2","updated":"2024-12-18T07:45:11Z","published":"2024-10-11T07:24:21Z","title":"ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification","summary":"  The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.\n","authors":["Yefei He","Feng Chen","Jing Liu","Wenqi Shao","Hong Zhou","Kaipeng Zhang","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.08584v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2411.18290v2","updated":"2024-12-18T07:40:45Z","published":"2024-11-27T12:28:46Z","title":"Leveraging Semantic Asymmetry for Precise Gross Tumor Volume\n  Segmentation of Nasopharyngeal Carcinoma in Planning CT","summary":"  In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians\ntypically delineate the gross tumor volume (GTV) using non-contrast planning\ncomputed tomography to ensure accurate radiation dose delivery. However, the\nlow contrast between tumors and adjacent normal tissues necessitates that\nradiation oncologists manually delineate the tumors, often relying on\ndiagnostic MRI for guidance. % In this study, we propose a novel approach to\ndirectly segment NPC gross tumors on non-contrast planning CT images,\ncircumventing potential registration errors when aligning MRI or MRI-derived\ntumor masks to planning CT. To address the low contrast issues between tumors\nand adjacent normal structures in planning CT, we introduce a 3D Semantic\nAsymmetry Tumor segmentation (SATs) method. Specifically, we posit that a\nhealthy nasopharyngeal region is characteristically bilaterally symmetric,\nwhereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then,\nwe propose a Siamese contrastive learning segmentation framework that minimizes\nthe voxel-wise distance between original and flipped areas without tumor and\nencourages a larger distance between original and flipped areas with tumor.\nThus, our approach enhances the sensitivity of features to semantic\nasymmetries. % Extensive experiments demonstrate that the proposed SATs\nachieves the leading NPC GTV segmentation performance in both internal and\nexternal testing, \\emph{e.g.}, with at least 2\\% absolute Dice score\nimprovement and 12\\% average distance error reduction when compared to other\nstate-of-the-art methods in the external testing.\n","authors":["Zi Li","Ying Chen","Zeli Chen","Yanzhou Su","Tai Ma","Tony C. W. Mok","Yan-Jie Zhou","Yunhai Bai","Zhinlin Zheng","Le Lu","Yirui Wang","Jia Ge","Xianghua Ye","Senxiang Yan","Dakai Jin"],"pdf_url":"https://arxiv.org/pdf/2411.18290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13569v1","updated":"2024-12-18T07:35:42Z","published":"2024-12-18T07:35:42Z","title":"Multi-View Pedestrian Occupancy Prediction with a Novel Synthetic\n  Dataset","summary":"  We address an advanced challenge of predicting pedestrian occupancy as an\nextension of multi-view pedestrian detection in urban traffic. To support this,\nwe have created a new synthetic dataset called MVP-Occ, designed for dense\npedestrian scenarios in large-scale scenes. Our dataset provides detailed\nrepresentations of pedestrians using voxel structures, accompanied by rich\nsemantic scene understanding labels, facilitating visual navigation and\ninsights into pedestrian spatial information. Furthermore, we present a robust\nbaseline model, termed OmniOcc, capable of predicting both the voxel occupancy\nstate and panoptic labels for the entire scene from multi-view images. Through\nin-depth analysis, we identify and evaluate the key elements of our proposed\nmodel, highlighting their specific contributions and importance.\n","authors":["Sithu Aung","Min-Cheol Sagong","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2412.13569v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13565v1","updated":"2024-12-18T07:33:22Z","published":"2024-12-18T07:33:22Z","title":"CA-Edit: Causality-Aware Condition Adapter for High-Fidelity Local\n  Facial Attribute Editing","summary":"  For efficient and high-fidelity local facial attribute editing, most existing\nediting methods either require additional fine-tuning for different editing\neffects or tend to affect beyond the editing regions. Alternatively, inpainting\nmethods can edit the target image region while preserving external areas.\nHowever, current inpainting methods still suffer from the generation\nmisalignment with facial attributes description and the loss of facial skin\ndetails. To address these challenges, (i) a novel data utilization strategy is\nintroduced to construct datasets consisting of attribute-text-image triples\nfrom a data-driven perspective, (ii) a Causality-Aware Condition Adapter is\nproposed to enhance the contextual causality modeling of specific details,\nwhich encodes the skin details from the original image while preventing\nconflicts between these cues and textual conditions. In addition, a Skin\nTransition Frequency Guidance technique is introduced for the local modeling of\ncontextual causality via sampling guidance driven by low-frequency alignment.\nExtensive quantitative and qualitative experiments demonstrate the\neffectiveness of our method in boosting both fidelity and editability for\nlocalized attribute editing. The code is available at\nhttps://github.com/connorxian/CA-Edit.\n","authors":["Xiaole Xian","Xilin He","Zenghao Niu","Junliang Zhang","Weicheng Xie","Siyang Song","Zitong Yu","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13565v1.pdf","comment":"accepted by aaai"},{"id":"http://arxiv.org/abs/2412.04062v2","updated":"2024-12-18T07:28:52Z","published":"2024-12-05T10:57:08Z","title":"ZipAR: Accelerating Auto-regressive Image Generation through Spatial\n  Locality","summary":"  In this paper, we propose ZipAR, a training-free, plug-and-play parallel\ndecoding framework for accelerating auto-regressive (AR) visual generation. The\nmotivation stems from the observation that images exhibit local structures, and\nspatially distant regions tend to have minimal interdependence. Given a\npartially decoded set of visual tokens, in addition to the original next-token\nprediction scheme in the row dimension, the tokens corresponding to spatially\nadjacent regions in the column dimension can be decoded in parallel, enabling\nthe ``next-set prediction'' paradigm. By decoding multiple tokens\nsimultaneously in a single forward pass, the number of forward passes required\nto generate an image is significantly reduced, resulting in a substantial\nimprovement in generation efficiency. Experiments demonstrate that ZipAR can\nreduce the number of model forward passes by up to 91% on the Emu3-Gen model\nwithout requiring any additional retraining. Code is available here:\nhttps://github.com/ThisisBillhe/ZipAR.\n","authors":["Yefei He","Feng Chen","Yuanyu He","Shaoxuan He","Hong Zhou","Kaipeng Zhang","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.04062v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.08941v2","updated":"2024-12-18T07:26:20Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2406.16993v2","updated":"2024-12-18T07:26:10Z","published":"2024-06-24T08:01:05Z","title":"Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image\n  Segmentation?","summary":"  The development of efficient segmentation strategies for medical images has\nevolved from its initial dependence on Convolutional Neural Networks (CNNs) to\nthe current investigation of hybrid models that combine CNNs with Vision\nTransformers. There is an increasing focus on creating architectures that are\nboth high-performance and computationally efficient, able to be deployed on\nremote systems with limited resources. Although transformers can capture global\ndependencies in the input space, they face challenges from the corresponding\nhigh computational and storage expenses involved. This paper investigates the\nintegration of CNNs with Vision Extended Long Short-Term Memory (Vision-xLSTM)s\nby introducing the novel {\\it \\textbf{U-VixLSTM}}.\n  The Vision-xLSTM blocks capture temporal and global relationships within the\npatches, as extracted from the CNN feature maps. The convolutional feature\nreconstruction path upsamples the output volume from the Vision-xLSTM blocks,\nto produce the segmentation output. Our primary objective is to propose that\nVision-xLSTM forms an appropriate backbone for medical image segmentation,\noffering excellent performance with reduced computational costs. The U-VixLSTM\nexhibits superior performance, compared to the state-of-the-art networks in the\npublicly available Synapse, ISIC and ACDC datasets. Code provided:\nhttps://github.com/duttapallabi2907/U-VixLSTM\n","authors":["Pallabi Dutta","Soham Bose","Swalpa Kumar Roy","Sushmita Mitra"],"pdf_url":"https://arxiv.org/pdf/2406.16993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13558v1","updated":"2024-12-18T07:19:48Z","published":"2024-12-18T07:19:48Z","title":"Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical\n  Imaging Interpretation","summary":"  Recent medical vision-language models (VLMs) have shown promise in 2D medical\nimage interpretation. However extending them to 3D medical imaging has been\nchallenging due to computational complexities and data scarcity. Although a few\nrecent VLMs specified for 3D medical imaging have emerged, all are limited to\nlearning volumetric representation of a 3D medical image as a set of\nsub-volumetric features. Such process introduces overly correlated\nrepresentations along the z-axis that neglect slice-specific clinical details,\nparticularly for 3D medical images where adjacent slices have low redundancy.\nTo address this limitation, we introduce MS-VLM that mimic radiologists'\nworkflow in 3D medical image interpretation. Specifically, radiologists analyze\n3D medical images by examining individual slices sequentially and synthesizing\ninformation across slices and views. Likewise, MS-VLM leverages self-supervised\n2D transformer encoders to learn a volumetric representation that capture\ninter-slice dependencies from a sequence of slice-specific features. Unbound by\nsub-volumetric patchification, MS-VLM is capable of obtaining useful volumetric\nrepresentations from 3D medical images with any slice length and from multiple\nimages acquired from different planes and phases. We evaluate MS-VLM on\npublicly available chest CT dataset CT-RATE and in-house rectal MRI dataset. In\nboth scenarios, MS-VLM surpasses existing methods in radiology report\ngeneration, producing more coherent and clinically relevant reports. These\nfindings highlight the potential of MS-VLM to advance 3D medical image\ninterpretation and improve the robustness of medical VLMs.\n","authors":["Changsun Lee","Sangjoon Park","Cheong-Il Shin","Woo Hee Choi","Hyun Jeong Park","Jeong Eun Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2412.13558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10050v2","updated":"2024-12-18T07:08:26Z","published":"2024-12-13T11:22:01Z","title":"ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for\n  Articulated Object Manipulation?","summary":"  Visual actionable affordance has emerged as a transformative approach in\nrobotics, focusing on perceiving interaction areas prior to manipulation.\nTraditional methods rely on pixel sampling to identify successful interaction\nsamples or processing pointclouds for affordance mapping. However, these\napproaches are computationally intensive and struggle to adapt to diverse and\ndynamic environments. This paper introduces ManipGPT, a framework designed to\npredict optimal interaction areas for articulated objects using a large\npre-trained vision transformer (ViT). We created a dataset of 9.9k simulated\nand real images to bridge the sim-to-real gap and enhance real-world\napplicability. By fine-tuning the vision transformer on this small dataset, we\nsignificantly improved part-level affordance segmentation, adapting the model's\nin-context segmentation capabilities to robot manipulation scenarios. This\nenables effective manipulation across simulated and real-world environments by\ngenerating part-level affordance masks, paired with an impedance adaptation\npolicy, sufficiently eliminating the need for complex datasets or perception\nsystems.\n","authors":["Taewhan Kim","Hojin Bae","Zeming Li","Xiaoqi Li","Iaroslav Ponomarenko","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2412.10050v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.06268v2","updated":"2024-12-18T07:08:01Z","published":"2024-12-09T07:39:39Z","title":"Open-Vocabulary High-Resolution 3D (OVHR3D) Data Segmentation and\n  Annotation Framework","summary":"  In the domain of the U.S. Army modeling and simulation, the availability of\nhigh quality annotated 3D data is pivotal to creating virtual environments for\ntraining and simulations. Traditional methodologies for 3D semantic and\ninstance segmentation, such as KpConv, RandLA, Mask3D, etc., are designed to\ntrain on extensive labeled datasets to obtain satisfactory performance in\npractical tasks. This requirement presents a significant challenge, given the\ninherent scarcity of manually annotated 3D datasets, particularly for the\nmilitary use cases. Recognizing this gap, our previous research leverages the\nOne World Terrain data repository manually annotated databases, as showcased at\nIITSEC 2019 and 2021, to enrich the training dataset for deep learning models.\nHowever, collecting and annotating large scale 3D data for specific tasks\nremains costly and inefficient. To this end, the objective of this research is\nto design and develop a comprehensive and efficient framework for 3D\nsegmentation tasks to assist in 3D data annotation. This framework integrates\nGrounding DINO and Segment anything Model, augmented by an enhancement in 2D\nimage rendering via 3D mesh. Furthermore, the authors have also developed a\nuser friendly interface that facilitates the 3D annotation process, offering\nintuitive visualization of rendered images and the 3D point cloud.\n","authors":["Jiuyi Xu","Meida Chen","Andrew Feng","Zifan Yu","Yangming Shi"],"pdf_url":"https://arxiv.org/pdf/2412.06268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13552v1","updated":"2024-12-18T07:02:01Z","published":"2024-12-18T07:02:01Z","title":"DragScene: Interactive 3D Scene Editing with Single-view Drag\n  Instructions","summary":"  3D editing has shown remarkable capability in editing scenes based on various\ninstructions. However, existing methods struggle with achieving intuitive,\nlocalized editing, such as selectively making flowers blossom. Drag-style\nediting has shown exceptional capability to edit images with direct\nmanipulation instead of ambiguous text commands. Nevertheless, extending\ndrag-based editing to 3D scenes presents substantial challenges due to\nmulti-view inconsistency. To this end, we introduce DragScene, a framework that\nintegrates drag-style editing with diverse 3D representations. First, latent\noptimization is performed on a reference view to generate 2D edits based on\nuser instructions. Subsequently, coarse 3D clues are reconstructed from the\nreference view using a point-based representation to capture the geometric\ndetails of the edits. The latent representation of the edited view is then\nmapped to these 3D clues, guiding the latent optimization of other views. This\nprocess ensures that edits are propagated seamlessly across multiple views,\nmaintaining multi-view consistency. Finally, the target 3D scene is\nreconstructed from the edited multi-view images. Extensive experiments\ndemonstrate that DragScene facilitates precise and flexible drag-style editing\nof 3D scenes, supporting broad applicability across diverse 3D representations.\n","authors":["Chenghao Gu","Zhenzhe Li","Zhengqi Zhang","Yunpeng Bai","Shuzhao Xie","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18853v2","updated":"2024-12-18T06:55:31Z","published":"2024-02-29T05:00:30Z","title":"Rethinking Multi-domain Generalization with A General Learning Objective","summary":"  Multi-domain generalization (mDG) is universally aimed to minimize the\ndiscrepancy between training and testing distributions to enhance\nmarginal-to-label distribution mapping. However, existing mDG literature lacks\na general learning objective paradigm and often imposes constraints on static\ntarget marginal distributions. In this paper, we propose to leverage a\n$Y$-mapping to relax the constraint. We rethink the learning objective for mDG\nand design a new \\textbf{general learning objective} to interpret and analyze\nmost existing mDG wisdom. This general objective is bifurcated into two\nsynergistic amis: learning domain-independent conditional features and\nmaximizing a posterior. Explorations also extend to two effective\nregularization terms that incorporate prior information and suppress invalid\ncausality, alleviating the issues that come with relaxed constraints. We\ntheoretically contribute an upper bound for the domain alignment of\ndomain-independent conditional features, disclosing that many previous mDG\nendeavors actually \\textbf{optimize partially the objective} and thus lead to\nlimited performance. As such, our study distills a general learning objective\ninto four practical components, providing a general, robust, and flexible\nmechanism to handle complex domain shifts. Extensive empirical results indicate\nthat the proposed objective with $Y$-mapping leads to substantially better mDG\nperformance in various downstream tasks, including regression, segmentation,\nand classification.\n","authors":["Zhaorui Tan","Xi Yang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2402.18853v2.pdf","comment":"Accepted by CVPR24"},{"id":"http://arxiv.org/abs/2412.13547v1","updated":"2024-12-18T06:46:40Z","published":"2024-12-18T06:46:40Z","title":"Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance\n  Fields","summary":"  Novel-view synthesis is an important problem in computer vision with\napplications in 3D reconstruction, mixed reality, and robotics. Recent methods\nlike 3D Gaussian Splatting (3DGS) have become the preferred method for this\ntask, providing high-quality novel views in real time. However, the training\ntime of a 3DGS model is slow, often taking 30 minutes for a scene with 200\nviews. In contrast, our goal is to reduce the optimization time by training for\nfewer steps while maintaining high rendering quality. Specifically, we combine\nthe guidance from both the position error and the appearance error to achieve a\nmore effective densification. To balance the rate between adding new Gaussians\nand fitting old Gaussians, we develop a convergence-aware budget control\nmechanism. Moreover, to make the densification process more reliable, we\nselectively add new Gaussians from mostly visited regions. With these designs,\nwe reduce the Gaussian optimization steps to one-third of the previous approach\nwhile achieving a comparable or even better novel view rendering quality. To\nfurther facilitate the rapid fitting of 4K resolution images, we introduce a\ndilation-based rendering technique. Our method, Turbo-GS, speeds up\noptimization for typical scenes and scales well to high-resolution (4K)\nscenarios on standard datasets. Through extensive experiments, we show that our\nmethod is significantly faster in optimization than other methods while\nretaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.\n","authors":["Tao Lu","Ankit Dhiman","R Srinath","Emre Arslan","Angela Xing","Yuanbo Xiangli","R Venkatesh Babu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2412.13547v1.pdf","comment":"Project page: https://ivl.cs.brown.edu/research/turbo-gs"},{"id":"http://arxiv.org/abs/2404.17805v2","updated":"2024-12-18T06:46:25Z","published":"2024-04-27T07:05:41Z","title":"From Optimization to Generalization: Fair Federated Learning against\n  Quality Shift via Inter-Client Sharpness Matching","summary":"  Due to escalating privacy concerns, federated learning has been recognized as\na vital approach for training deep neural networks with decentralized medical\ndata. In practice, it is challenging to ensure consistent imaging quality\nacross various institutions, often attributed to equipment malfunctions\naffecting a minority of clients. This imbalance in image quality can cause the\nfederated model to develop an inherent bias towards higher-quality images, thus\nposing a severe fairness issue. In this study, we pioneer the identification\nand formulation of this new fairness challenge within the context of the\nimaging quality shift. Traditional methods for promoting fairness in federated\nlearning predominantly focus on balancing empirical risks across diverse client\ndistributions. This strategy primarily facilitates fair optimization across\ndifferent training data distributions, yet neglects the crucial aspect of\ngeneralization. To address this, we introduce a solution termed Federated\nlearning with Inter-client Sharpness Matching (FedISM). FedISM enhances both\nlocal training and global aggregation by incorporating sharpness-awareness,\naiming to harmonize the sharpness levels across clients for fair\ngeneralization. Our empirical evaluations, conducted using the widely-used ICH\nand ISIC 2019 datasets, establish FedISM's superiority over current\nstate-of-the-art federated learning methods in promoting fairness. Code is\navailable at https://github.com/wnn2000/FFL4MIA.\n","authors":["Nannan Wu","Zhuo Kuang","Zengqiang Yan","Li Yu"],"pdf_url":"https://arxiv.org/pdf/2404.17805v2.pdf","comment":"This paper is accepted at IJCAI'24 (Main Track)"},{"id":"http://arxiv.org/abs/2412.13543v1","updated":"2024-12-18T06:43:06Z","published":"2024-12-18T06:43:06Z","title":"Query-centric Audio-Visual Cognition Network for Moment Retrieval,\n  Segmentation and Step-Captioning","summary":"  Video has emerged as a favored multimedia format on the internet. To better\ngain video contents, a new topic HIREST is presented, including video\nretrieval, moment retrieval, moment segmentation, and step-captioning. The\npioneering work chooses the pre-trained CLIP-based model for video retrieval,\nand leverages it as a feature extractor for other three challenging tasks\nsolved in a multi-task learning paradigm. Nevertheless, this work struggles to\nlearn the comprehensive cognition of user-preferred content, due to\ndisregarding the hierarchies and association relations across modalities. In\nthis paper, guided by the shallow-to-deep principle, we propose a query-centric\naudio-visual cognition (QUAG) network to construct a reliable multi-modal\nrepresentation for moment retrieval, segmentation and step-captioning.\nSpecifically, we first design the modality-synergistic perception to obtain\nrich audio-visual content, by modeling global contrastive alignment and local\nfine-grained interaction between visual and audio modalities. Then, we devise\nthe query-centric cognition that uses the deep-level query to perform the\ntemporal-channel filtration on the shallow-level audio-visual representation.\nThis can cognize user-preferred content and thus attain a query-centric\naudio-visual representation for three tasks. Extensive experiments show QUAG\nachieves the SOTA results on HIREST. Further, we test QUAG on the query-based\nvideo summarization task and verify its good generalization.\n","authors":["Yunbin Tu","Liang Li","Li Su","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13543v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13541v1","updated":"2024-12-18T06:40:53Z","published":"2024-12-18T06:40:53Z","title":"Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for\n  Fine-grained Emotion Recognition","summary":"  Fine-grained emotion recognition (FER) plays a vital role in various fields,\nsuch as disease diagnosis, personalized recommendations, and multimedia mining.\nHowever, existing FER methods face three key challenges in real-world\napplications: (i) they rely on large amounts of continuously annotated data to\nensure accuracy since emotions are complex and ambiguous in reality, which is\ncostly and time-consuming; (ii) they cannot capture the temporal heterogeneity\ncaused by changing emotion patterns, because they usually assume that the\ntemporal correlation within sampling periods is the same; (iii) they do not\nconsider the spatial heterogeneity of different FER scenarios, that is, the\ndistribution of emotion information in different data may have bias or\ninterference. To address these challenges, we propose a Spatio-Temporal\nFuzzy-oriented Multi-modal Meta-learning framework (ST-F2M). Specifically,\nST-F2M first divides the multi-modal videos into multiple views, and each view\ncorresponds to one modality of one emotion. Multiple randomly selected views\nfor the same emotion form a meta-training task. Next, ST-F2M uses an integrated\nmodule with spatial and temporal convolutions to encode the data of each task,\nreflecting the spatial and temporal heterogeneity. Then it adds fuzzy semantic\ninformation to each task based on generalized fuzzy rules, which helps handle\nthe complexity and ambiguity of emotions. Finally, ST-F2M learns\nemotion-related general meta-knowledge through meta-recurrent neural networks\nto achieve fast and robust fine-grained emotion recognition. Extensive\nexperiments show that ST-F2M outperforms various state-of-the-art methods in\nterms of accuracy and model efficiency. In addition, we construct ablation\nstudies and further analysis to explore why ST-F2M performs well.\n","authors":["Jingyao Wang","Yuxuan Yang","Wenwen Qiang","Changwen Zheng","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.13541v1.pdf","comment":"13 pages, Submitted to TMM in 30-May-2024"},{"id":"http://arxiv.org/abs/2412.13540v1","updated":"2024-12-18T06:35:18Z","published":"2024-12-18T06:35:18Z","title":"Benchmarking and Improving Large Vision-Language Models for Fundamental\n  Visual Graph Understanding and Reasoning","summary":"  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross diverse tasks. Despite great success, recent studies show that LVLMs\nencounter substantial limitations when engaging with visual graphs. To study\nthe reason behind these limitations, we propose VGCure, a comprehensive\nbenchmark covering 22 tasks for examining the fundamental graph understanding\nand reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs\nreveal that LVLMs are weak in basic graph understanding and reasoning tasks,\nparticularly those concerning relational or structurally complex information.\nBased on this observation, we propose a structure-aware fine-tuning framework\nto enhance LVLMs with structure learning abilities through 3 self-supervised\nlearning tasks. Experiments validate the effectiveness of our method in\nimproving LVLMs' zero-shot performance on fundamental graph learning tasks, as\nwell as enhancing the robustness of LVLMs against complex visual graphs.\n","authors":["Yingjie Zhu","Xuefeng Bai","Kehai Chen","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06742v3","updated":"2024-12-18T06:33:44Z","published":"2024-08-13T09:03:00Z","title":"Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to\n  Tail","summary":"  Current out-of-distribution (OOD) detection methods typically assume balanced\nin-distribution (ID) data, while most real-world data follow a long-tailed\ndistribution. Previous approaches to long-tailed OOD detection often involve\nbalancing the ID data by reducing the semantics of head classes. However, this\nreduction can severely affect the classification accuracy of ID data. The main\nchallenge of this task lies in the severe lack of features for tail classes,\nleading to confusion with OOD data. To tackle this issue, we introduce a novel\nPrioritizing Attention to Tail (PATT) method using augmentation instead of\nreduction. Our main intuition involves using a mixture of von Mises-Fisher\n(vMF) distributions to model the ID data and a temperature scaling module to\nboost the confidence of ID data. This enables us to generate infinite\ncontrastive pairs, implicitly enhancing the semantics of ID classes while\npromoting differentiation between ID and OOD data. To further strengthen the\ndetection of OOD data without compromising the classification performance of ID\ndata, we propose feature calibration during the inference phase. By extracting\nan attention weight from the training set that prioritizes the tail classes and\nreduces the confidence in OOD data, we improve the OOD detection capability.\nExtensive experiments verified that our method outperforms the current\nstate-of-the-art methods on various benchmarks.\n","authors":["Yina He","Lei Peng","Yongcun Zhang","Juanjuan Weng","Zhiming Luo","Shaozi Li"],"pdf_url":"https://arxiv.org/pdf/2408.06742v3.pdf","comment":"Accepted by AAAI'25. Extended version with full appendix, 13 pages"},{"id":"http://arxiv.org/abs/2412.13533v1","updated":"2024-12-18T06:19:03Z","published":"2024-12-18T06:19:03Z","title":"Language-guided Medical Image Segmentation with Target-informed\n  Multi-level Contrastive Alignments","summary":"  Medical image segmentation is crucial in modern medical image analysis, which\ncan aid into diagnosis of various disease conditions. Recently, language-guided\nsegmentation methods have shown promising results in automating image\nsegmentation where text reports are incorporated as guidance. These text\nreports, containing image impressions and insights given by clinicians,\nprovides auxiliary guidance. However, these methods neglect the inherent\npattern gaps between the two distinct modalities, which leads to sub-optimal\nimage-text feature fusion without proper cross-modality feature alignments.\nContrastive alignments are widely used to associate image-text semantics in\nrepresentation learning; however, it has not been exploited to bridge the\npattern gaps in language-guided segmentation that relies on subtle low level\nimage details to represent diseases. Existing contrastive alignment methods\ntypically algin high-level global image semantics without involving low-level,\nlocalized target information, and therefore fails to explore fine-grained text\nguidance for language-guided segmentation. In this study, we propose a\nlanguage-guided segmentation network with Target-informed Multi-level\nContrastive Alignments (TMCA). TMCA enables target-informed cross-modality\nalignments and fine-grained text guidance to bridge the pattern gaps in\nlanguage-guided segmentation. Specifically, we introduce: 1) a target-sensitive\nsemantic distance module that enables granular image-text alignment modelling,\nand 2) a multi-level alignment strategy that directs text guidance on low-level\nimage features. In addition, a language-guided target enhancement module is\nproposed to leverage the aligned text to redirect attention to focus on\ncritical localized image features. Extensive experiments on 4 image-text\ndatasets, involving 3 medical imaging modalities, demonstrated that our TMCA\nachieved superior performances.\n","authors":["Mingjian Li","Mingyuan Meng","Shuchang Ye","David Dagan Feng","Lei Bi","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.13533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13525v1","updated":"2024-12-18T05:52:16Z","published":"2024-12-18T05:52:16Z","title":"Hybrid Data-Free Knowledge Distillation","summary":"  Data-free knowledge distillation aims to learn a compact student network from\na pre-trained large teacher network without using the original training data of\nthe teacher network. Existing collection-based and generation-based methods\ntrain student networks by collecting massive real examples and generating\nsynthetic examples, respectively. However, they inevitably become weak in\npractical scenarios due to the difficulties in gathering or emulating\nsufficient real-world data. To solve this problem, we propose a novel method\ncalled \\textbf{H}ybr\\textbf{i}d \\textbf{D}ata-\\textbf{F}ree\n\\textbf{D}istillation (HiDFD), which leverages only a small amount of collected\ndata as well as generates sufficient examples for training student networks.\nOur HiDFD comprises two primary modules, \\textit{i.e.}, the teacher-guided\ngeneration and student distillation. The teacher-guided generation module\nguides a Generative Adversarial Network (GAN) by the teacher network to produce\nhigh-quality synthetic examples from very few real-world collected examples.\nSpecifically, we design a feature integration mechanism to prevent the GAN from\noverfitting and facilitate the reliable representation learning from the\nteacher network. Meanwhile, we drive a category frequency smoothing technique\nvia the teacher network to balance the generative training of each category. In\nthe student distillation module, we explore a data inflation strategy to\nproperly utilize a blend of real and synthetic data to train the student\nnetwork via a classifier-sharing-based feature alignment technique. Intensive\nexperiments across multiple benchmarks demonstrate that our HiDFD can achieve\nstate-of-the-art performance using 120 times less collected data than existing\nmethods. Code is available at https://github.com/tangjialiang97/HiDFD.\n","authors":["Jialiang Tang","Shuo Chen","Chen Gong"],"pdf_url":"https://arxiv.org/pdf/2412.13525v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.14113v1","updated":"2024-12-18T17:58:58Z","published":"2024-12-18T17:58:58Z","title":"Adversarial Hubness in Multi-Modal Retrieval","summary":"  Hubness is a phenomenon in high-dimensional vector spaces where a single\npoint from the natural distribution is unusually close to many other points.\nThis is a well-known problem in information retrieval that causes some items to\naccidentally (and incorrectly) appear relevant to many queries. In this paper,\nwe investigate how attackers can exploit hubness to turn any image or audio\ninput in a multi-modal retrieval system into an adversarial hub. Adversarial\nhubs can be used to inject universal adversarial content (e.g., spam) that will\nbe retrieved in response to thousands of different queries, as well as for\ntargeted attacks on queries related to specific, attacker-chosen concepts. We\npresent a method for creating adversarial hubs and evaluate the resulting hubs\non benchmark multi-modal retrieval datasets and an image-to-image retrieval\nsystem based on a tutorial from Pinecone, a popular vector database. For\nexample, in text-caption-to-image retrieval, a single adversarial hub is\nretrieved as the top-1 most relevant image for more than 21,000 out of 25,000\ntest queries (by contrast, the most common natural hub is the top-1 response to\nonly 102 queries). We also investigate whether techniques for mitigating\nnatural hubness are an effective defense against adversarial hubs, and show\nthat they are not effective against hubs that target queries related to\nspecific concepts.\n","authors":["Tingwei Zhang","Fnu Suya","Rishi Jha","Collin Zhang","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2412.14113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14025v1","updated":"2024-12-18T16:41:51Z","published":"2024-12-18T16:41:51Z","title":"A Cognitive Ideation Support Framework using IBM Watson Services","summary":"  Ideas generation is a core activity for innovation in organizations. The\ncreativity of the generated ideas depends not only on the knowledge retrieved\nfrom the organizations' knowledge bases, but also on the external knowledge\nretrieved from other resources. Unfortunately, organizations often cannot\nefficiently utilize the knowledge in the knowledge bases due to the limited\nabilities of the search and retrieval mechanisms especially when dealing with\nunstructured data. In this paper, we present a new cognitive support framework\nfor ideation that uses the IBM Watson DeepQA services. IBM Watson is a Question\nAnswering system which mimics human cognitive abilities to retrieve and rank\ninformation. The proposed framework is based on the Search for Ideas in the\nAssociative Memory (SIAM) model to help organizations develop creative ideas\nthrough discovering new relationships between retrieved data. To evaluate the\neffectiveness of the proposed system, the generated ideas generated are\nselected and assessed using a set of established creativity criteria.\n","authors":["Samaa Elnagar","Kweku-Muata Osei-Bryson"],"pdf_url":"https://arxiv.org/pdf/2412.14025v1.pdf","comment":"Twenty-fifth Americas Conference on Information Systems (AMCIS 2019),\n  Cancun, 2019"},{"id":"http://arxiv.org/abs/2412.09632v2","updated":"2024-12-18T15:55:28Z","published":"2024-11-27T19:53:05Z","title":"Methods to Assess the UK Government's Current Role as a Data Provider\n  for AI","summary":"  Governments typically collect and steward a vast amount of high-quality data\non their citizens and institutions, and the UK government is exploring how it\ncan better publish and provision this data to the benefit of the AI landscape.\nHowever, the compositions of generative AI training corpora remain closely\nguarded secrets, making the planning of data sharing initiatives difficult. To\naddress this, we devise two methods to assess UK government data usage for the\ntraining of Large Language Models (LLMs) and 'peek behind the curtain' in order\nto observe the UK government's current contributions as a data provider for AI.\nThe first method, an ablation study that utilises LLM 'unlearning', seeks to\nexamine the importance of the information held on UK government websites for\nLLMs and their performance in citizen query tasks. The second method, an\ninformation leakage study, seeks to ascertain whether LLMs are aware of the\ninformation held in the datasets published on the UK government's open data\ninitiative data.gov.uk. Our findings indicate that UK government websites are\nimportant data sources for AI (heterogenously across subject matters) while\ndata.gov.uk is not. This paper serves as a technical report, explaining\nin-depth the designs, mechanics, and limitations of the above experiments. It\nis accompanied by a complementary non-technical report on the ODI website in\nwhich we summarise the experiments and key findings, interpret them, and build\na set of actionable recommendations for the UK government to take forward as it\nseeks to design AI policy. While we focus on UK open government data, we\nbelieve that the methods introduced in this paper present a reproducible\napproach to tackle the opaqueness of AI training corpora and provide\norganisations a framework to evaluate and maximize their contributions to AI\ndevelopment.\n","authors":["Neil Majithia","Elena Simperl"],"pdf_url":"https://arxiv.org/pdf/2412.09632v2.pdf","comment":"17 pages, 5 figures; v2 - incorporated editor feedback; for the\n  accompanying, non-technical ODI report see\n  https://theodi.org/insights/reports/the-uk-government-as-a-data-provider-for-ai"},{"id":"http://arxiv.org/abs/2412.13844v1","updated":"2024-12-18T13:37:36Z","published":"2024-12-18T13:37:36Z","title":"CRM: Retrieval Model with Controllable Condition","summary":"  Recommendation systems (RecSys) are designed to connect users with relevant\nitems from a vast pool of candidates while aligning with the business goals of\nthe platform. A typical industrial RecSys is composed of two main stages,\nretrieval and ranking: (1) the retrieval stage aims at searching hundreds of\nitem candidates satisfied user interests; (2) based on the retrieved items, the\nranking stage aims at selecting the best dozen items by multiple targets\nestimation for each item candidate, including classification and regression\ntargets. Compared with ranking model, the retrieval model absence of item\ncandidate information during inference, therefore retrieval models are often\ntrained by classification target only (e.g., click-through rate), but failed to\nincorporate regression target (e.g., the expected watch-time), which limit the\neffectiveness of retrieval. In this paper, we propose the Controllable\nRetrieval Model (CRM), which integrates regression information as conditional\nfeatures into the two-tower retrieval paradigm. This modification enables the\nretrieval stage could fulfill the target gap with ranking model, enhancing the\nretrieval model ability to search item candidates satisfied the user interests\nand condition effectively. We validate the effectiveness of CRM through\nreal-world A/B testing and demonstrate its successful deployment in Kuaishou\nshort-video recommendation system, which serves over 400 million users.\n","authors":["Chi Liu","Jiangxia Cao","Rui Huang","Kuo Cai","Weifeng Ding","Qiang Luo","Kun Gai","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13834v1","updated":"2024-12-18T13:24:09Z","published":"2024-12-18T13:24:09Z","title":"Maybe you are looking for CroQS: Cross-modal Query Suggestion for\n  Text-to-Image Retrieval","summary":"  Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/\n","authors":["Giacomo Pacini","Fabio Carrara","Nicola Messina","Nicola Tonellotto","Giuseppe Amato","Fabrizio Falchi"],"pdf_url":"https://arxiv.org/pdf/2412.13834v1.pdf","comment":"15 pages, 5 figures. To be published as full paper in the Proceedings\n  of the European Conference on Information Retrieval (ECIR) 2025"},{"id":"http://arxiv.org/abs/2412.13825v1","updated":"2024-12-18T13:12:36Z","published":"2024-12-18T13:12:36Z","title":"Heterogeneous Graph Collaborative Filtering","summary":"  For modern recommender systems, the use of low-dimensional latent\nrepresentations to embed users and items based on their observed interactions\nhas become commonplace. However, many existing recommendation models are\nprimarily designed for coarse-grained and homogeneous interactions, which\nlimits their effectiveness in two critical dimensions. Firstly, these models\nfail to leverage the relational dependencies that exist across different types\nof user behaviors, such as page views, collects, comments, and purchases.\nSecondly, they struggle to capture the fine-grained latent factors that drive\nuser interaction patterns. To address these limitations, we present a\nheterogeneous graph collaborative filtering model MixRec that excels at\ndisentangling users' multi-behavior interaction patterns and uncovering the\nlatent intent factors behind each behavior. Our model achieves this by\nincorporating intent disentanglement and multi-behavior modeling, facilitated\nby a parameterized heterogeneous hypergraph architecture. Furthermore, we\nintroduce a novel contrastive learning paradigm that adaptively explores the\nadvantages of self-supervised data augmentation, thereby enhancing the model's\nresilience against data sparsity and expressiveness with relation\nheterogeneity. To validate the efficacy of MixRec, we conducted extensive\nexperiments on three public datasets. The results clearly demonstrate its\nsuperior performance, significantly outperforming various state-of-the-art\nbaselines. Our model is open-sourced and available at:\nhttps://github.com/HKUDS/MixRec.\n","authors":["Lianghao Xia","Meiyan Xie","Yong Xu","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13825v1.pdf","comment":"This paper is accepted by WSDM'2025"},{"id":"http://arxiv.org/abs/2412.12559v2","updated":"2024-12-18T13:08:36Z","published":"2024-12-17T05:38:27Z","title":"EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation","summary":"  We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT\n","authors":["Taeho Hwang","Sukmin Cho","Soyeong Jeong","Hoyun Song","SeungYoon Han","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2412.12559v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2411.04677v2","updated":"2024-12-18T13:00:35Z","published":"2024-11-07T13:03:21Z","title":"Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval","summary":"  A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir.\n","authors":["Ferdinand Schlatt","Maik Fröbe","Matthias Hagen"],"pdf_url":"https://arxiv.org/pdf/2411.04677v2.pdf","comment":"Accepted as a demo at WSDM'25"},{"id":"http://arxiv.org/abs/2406.11156v4","updated":"2024-12-18T12:48:37Z","published":"2024-06-17T02:47:09Z","title":"DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential\n  Recommendation","summary":"  Sequential recommendation (SR) tasks aim to predict users' next interaction\nby learning their behavior sequence and capturing the connection between users'\npast interactions and their changing preferences. Conventional SR models often\nfocus solely on capturing sequential patterns within the training data,\nneglecting the broader context and semantic information embedded in item titles\nfrom external sources. This limits their predictive power and adaptability.\nLarge language models (LLMs) have recently shown promise in SR tasks due to\ntheir advanced understanding capabilities and strong generalization abilities.\nResearchers have attempted to enhance LLMs-based recommendation performance by\nincorporating information from conventional SR models. However, previous\napproaches have encountered problems such as 1) limited textual information\nleading to poor recommendation performance, 2) incomplete understanding and\nutilization of conventional SR model information by LLMs, and 3) excessive\ncomplexity and low interpretability of LLMs-based methods. To improve the\nperformance of LLMs-based SR, we propose a novel framework, Distilling\nSequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),\nwhich aims to extract knowledge from conventional SR models and enable LLMs to\neasily comprehend and utilize the extracted knowledge for more effective SRs.\nDELRec consists of two main stages: 1) Distill Pattern from Conventional SR\nModels, focusing on extracting behavioral patterns exhibited by conventional SR\nmodels using soft prompts through two well-designed strategies; 2) LLMs-based\nSequential Recommendation, aiming to fine-tune LLMs to effectively use the\ndistilled auxiliary information to perform SR tasks. Extensive experimental\nresults conducted on four real datasets validate the effectiveness of the\nDELRec framework.\n","authors":["Haoyi Zhang","Guohao Sun","Jinhu Lu","Guanfeng Liu","Xiu Susie Fang"],"pdf_url":"https://arxiv.org/pdf/2406.11156v4.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.13771v1","updated":"2024-12-18T12:07:58Z","published":"2024-12-18T12:07:58Z","title":"Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization","summary":"  Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.\n","authors":["Guanghan Li","Xun Zhang","Yufei Zhang","Yifan Yin","Guojun Yin","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13771v1.pdf","comment":"7 pages, 3 figures, AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13746v1","updated":"2024-12-18T11:28:05Z","published":"2024-12-18T11:28:05Z","title":"RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented\n  Generation for Preference Alignment","summary":"  Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.\n","authors":["Zhuoran Jin","Hongbang Yuan","Tianyi Men","Pengfei Cao","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.13746v1.pdf","comment":"26 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.13614v1","updated":"2024-12-18T08:49:01Z","published":"2024-12-18T08:49:01Z","title":"Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity\n  Linking","summary":"  Visual Entity Linking (VEL) is a crucial task for achieving fine-grained\nvisual understanding, matching objects within images (visual mentions) to\nentities in a knowledge base. Previous VEL tasks rely on textual inputs, but\nwriting queries for complex scenes can be challenging. Visual inputs like\nclicks or bounding boxes offer a more convenient alternative. Therefore, we\npropose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses\npixel masks from visual inputs to refer to objects, supplementing reference\nmethods for VEL. To facilitate research on this task, we have constructed the\nMaskOVEN-Wiki dataset through an entirely automatic reverse region-entity\nannotation framework. This dataset contains over 5 million annotations aligning\npixel-level regions with entity-level labels, which will advance visual\nunderstanding towards fine-grained. Moreover, as pixel masks correspond to\nsemantic regions in an image, we enhance previous patch-interacted attention\nwith region-interacted attention by a visual semantic tokenization approach.\nManual evaluation results indicate that the reverse annotation framework\nachieved a 94.8% annotation success rate. Experimental results show that models\ntrained on this dataset improved accuracy by 18 points compared to zero-shot\nmodels. Additionally, the semantic tokenization method achieved a 5-point\naccuracy improvement over the trained baseline.\n","authors":["Zhengfei Xu","Sijia Zhao","Yanchao Hao","Xiaolong Liu","Lili Li","Yuyang Yin","Bo Li","Xi Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2412.13614v1.pdf","comment":"AAAI 2025;Dataset are released at\n  https://github.com/NP-NET-research/PL-VEL"},{"id":"http://arxiv.org/abs/2412.13102v2","updated":"2024-12-18T07:06:07Z","published":"2024-12-17T17:15:21Z","title":"AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark","summary":"  Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.\n","authors":["Jianlyu Chen","Nan Wang","Chaofan Li","Bo Wang","Shitao Xiao","Han Xiao","Hao Liao","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13102v2.pdf","comment":"31 pages, 6 figures; Update Table 5"},{"id":"http://arxiv.org/abs/2412.13544v1","updated":"2024-12-18T06:43:56Z","published":"2024-12-18T06:43:56Z","title":"Bridging the User-side Knowledge Gap in Knowledge-aware Recommendations\n  with Large Language Models","summary":"  In recent years, knowledge graphs have been integrated into recommender\nsystems as item-side auxiliary information, enhancing recommendation accuracy.\nHowever, constructing and integrating structural user-side knowledge remains a\nsignificant challenge due to the improper granularity and inherent scarcity of\nuser-side features. Recent advancements in Large Language Models (LLMs) offer\nthe potential to bridge this gap by leveraging their human behavior\nunderstanding and extensive real-world knowledge. Nevertheless, integrating\nLLM-generated information into recommender systems presents challenges,\nincluding the risk of noisy information and the need for additional knowledge\ntransfer. In this paper, we propose an LLM-based user-side knowledge inference\nmethod alongside a carefully designed recommendation framework to address these\nchallenges. Our approach employs LLMs to infer user interests based on\nhistorical behaviors, integrating this user-side information with item-side and\ncollaborative data to construct a hybrid structure: the Collaborative Interest\nKnowledge Graph (CIKG). Furthermore, we propose a CIKG-based recommendation\nframework that includes a user interest reconstruction module and a\ncross-domain contrastive learning module to mitigate potential noise and\nfacilitate knowledge transfer. We conduct extensive experiments on three\nreal-world datasets to validate the effectiveness of our method. Our approach\nachieves state-of-the-art performance compared to competitive baselines,\nparticularly for users with sparse interactions.\n","authors":["Zheng Hu","Zhe Li","Ziyun Jiao","Satoshi Nakagawa","Jiawen Deng","Shimin Cai","Tao Zhou","Fuji Ren"],"pdf_url":"https://arxiv.org/pdf/2412.13544v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13534v1","updated":"2024-12-18T06:21:21Z","published":"2024-12-18T06:21:21Z","title":"Information-Theoretic Generative Clustering of Documents","summary":"  We present {\\em generative clustering} (GC) for clustering a set of\ndocuments, $\\mathrm{X}$, by using texts $\\mathrm{Y}$ generated by large\nlanguage models (LLMs) instead of by clustering the original documents\n$\\mathrm{X}$. Because LLMs provide probability distributions, the similarity\nbetween two documents can be rigorously defined in an information-theoretic\nmanner by the KL divergence. We also propose a natural, novel clustering\nalgorithm by using importance sampling. We show that GC achieves the\nstate-of-the-art performance, outperforming any previous clustering method\noften by a large margin. Furthermore, we show an application to generative\ndocument retrieval in which documents are indexed via hierarchical clustering\nand our method improves the retrieval accuracy.\n","authors":["Xin Du","Kumiko Tanaka-Ishii"],"pdf_url":"https://arxiv.org/pdf/2412.13534v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2407.01965v2","updated":"2024-12-18T05:46:41Z","published":"2024-07-02T05:50:16Z","title":"AdaCQR: Enhancing Query Reformulation for Conversational Search via\n  Sparse and Dense Retrieval Alignment","summary":"  Conversational Query Reformulation (CQR) has significantly advanced in\naddressing the challenges of conversational search, particularly those stemming\nfrom the latent user intent and the need for historical context. Recent works\naimed to boost the performance of CRQ through alignment. However, they are\ndesigned for one specific retrieval system, which potentially results in poor\ngeneralization. To overcome this limitation, we present a novel framework\nAdaCQR. By aligning reformulation models with both term-based and\nsemantic-based retrieval systems, AdaCQR enhances the generalizability of\ninformation-seeking queries across diverse retrieval environments through a\ndual-phase training strategy. We also developed two effective approaches for\nacquiring superior labels and diverse input candidates, boosting the efficiency\nand robustness of the framework. Experimental evaluations on the TopiOCQA and\nQReCC datasets demonstrate that AdaCQR significantly outperforms existing\nmethods, offering both quantitative and qualitative improvements in\nconversational query reformulation.\n","authors":["Yilong Lai","Jialong Wu","Congzhi Zhang","Haowen Sun","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.01965v2.pdf","comment":"Accepted by COLING 2025"},{"id":"http://arxiv.org/abs/2412.12486v2","updated":"2024-12-18T05:08:39Z","published":"2024-12-17T02:43:54Z","title":"Boosting Long-Context Management via Query-Guided Activation Refilling","summary":"  Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.\n","authors":["Hongjin Qian","Zheng Liu","Peitian Zhang","Zhicheng Dou","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2412.12486v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.08300v2","updated":"2024-12-18T03:21:48Z","published":"2024-12-11T11:29:15Z","title":"Augmenting Sequential Recommendation with Balanced Relevance and\n  Diversity","summary":"  By generating new yet effective data, data augmentation has become a\npromising method to mitigate the data sparsity problem in sequential\nrecommendation. Existing works focus on augmenting the original data but rarely\nexplore the issue of imbalanced relevance and diversity for augmented data,\nleading to semantic drift problems or limited performance improvements. In this\npaper, we propose a novel Balanced data Augmentation Plugin for Sequential\nRecommendation (BASRec) to generate data that balance relevance and diversity.\nBASRec consists of two modules: Single-sequence Augmentation and Cross-sequence\nAugmentation. The former leverages the randomness of the heuristic operators to\ngenerate diverse sequences for a single user, after which the diverse and the\noriginal sequences are fused at the representation level to obtain relevance.\nFurther, we devise a reweighting strategy to enable the model to learn the\npreferences based on the two properties adaptively. The Cross-sequence\nAugmentation performs nonlinear mixing between different sequence\nrepresentations from two directions. It produces virtual sequence\nrepresentations that are diverse enough but retain the vital semantics of the\noriginal sequences. These two modules enhance the model to discover\nfine-grained preferences knowledge from single-user and cross-user\nperspectives. Extensive experiments verify the effectiveness of BASRec. The\naverage improvement is up to 72.0% on GRU4Rec, 33.8% on SASRec, and 68.5% on\nFMLP-Rec. We demonstrate that BASRec generates data with a better balance\nbetween relevance and diversity than existing methods. The source code is\navailable at https://github.com/KingGugu/BASRec.\n","authors":["Yizhou Dang","Jiahui Zhang","Yuting Liu","Enneng Yang","Yuliang Liang","Guibing Guo","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08300v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11905v2","updated":"2024-12-18T02:41:21Z","published":"2024-12-16T15:52:17Z","title":"One for Dozens: Adaptive REcommendation for All Domains with\n  Counterfactual Augmentation","summary":"  Multi-domain recommendation (MDR) aims to enhance recommendation performance\nacross various domains. However, real-world recommender systems in online\nplatforms often need to handle dozens or even hundreds of domains, far\nexceeding the capabilities of traditional MDR algorithms, which typically focus\non fewer than five domains. Key challenges include a substantial increase in\nparameter count, high maintenance costs, and intricate knowledge transfer\npatterns across domains. Furthermore, minor domains often suffer from data\nsparsity, leading to inadequate training in classical methods. To address these\nissues, we propose Adaptive REcommendation for All Domains with counterfactual\naugmentation (AREAD). AREAD employs a hierarchical structure with a limited\nnumber of expert networks at several layers, to effectively capture domain\nknowledge at different granularities. To adaptively capture the knowledge\ntransfer pattern across domains, we generate and iteratively prune a\nhierarchical expert network selection mask for each domain during training.\nAdditionally, counterfactual assumptions are used to augment data in minor\ndomains, supporting their iterative mask pruning. Our experiments on two public\ndatasets, each encompassing over twenty domains, demonstrate AREAD's\neffectiveness, especially in data-sparse domains. Source code is available at\nhttps://github.com/Chrissie-Law/AREAD-Multi-Domain-Recommendation.\n","authors":["Huishi Luo","Yiwen Chen","Yiqing Wu","Fuzhen Zhuang","Deqing Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11905v2.pdf","comment":"Extended version accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13432v1","updated":"2024-12-18T02:07:21Z","published":"2024-12-18T02:07:21Z","title":"Large Language Model Enhanced Recommender Systems: Taxonomy, Trend,\n  Application and Future","summary":"  Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS.\n","authors":["Qidong Liu","Xiangyu Zhao","Yuhao Wang","Yejing Wang","Zijian Zhang","Yuqi Sun","Xiang Li","Maolin Wang","Pengyue Jia","Chong Chen","Wei Huang","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13408v1","updated":"2024-12-18T00:56:16Z","published":"2024-12-18T00:56:16Z","title":"Lightweight yet Fine-grained: A Graph Capsule Convolutional Network with\n  Subspace Alignment for Shared-account Sequential Recommendation","summary":"  Shared-account Sequential Recommendation (SSR) aims to provide personalized\nrecommendations for accounts shared by multiple users with varying sequential\npreferences. Previous studies on SSR struggle to capture the fine-grained\nassociations between interactions and different latent users within the shared\naccount's hybrid sequences. Moreover, most existing SSR methods (e.g.,\nRNN-based or GCN-based methods) have quadratic computational complexities,\nhindering the deployment of SSRs on resource-constrained devices. To this end,\nwe propose a Lightweight Graph Capsule Convolutional Network with subspace\nalignment for shared-account sequential recommendation, named LightGC$^2$N.\nSpecifically, we devise a lightweight graph capsule convolutional network. It\nfacilitates the fine-grained matching between interactions and latent users by\nattentively propagating messages on the capsule graphs. Besides, we present an\nefficient subspace alignment method. This method refines the sequence\nrepresentations and then aligns them with the finely clustered preferences of\nlatent users. The experimental results on four real-world datasets indicate\nthat LightGC$^2$N outperforms nine state-of-the-art methods in accuracy and\nefficiency.\n","authors":["Jinyu Zhang","Zhongying Zhao","Chao Li","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2412.13408v1.pdf","comment":"11 pages, 6 figures, accepted by AAAI-2025 conference"},{"id":"http://arxiv.org/abs/2412.14405v1","updated":"2024-12-18T23:24:15Z","published":"2024-12-18T23:24:15Z","title":"ChainRank-DPO: Chain Rank Direct Preference Optimization for LLM Rankers","summary":"  Large language models (LLMs) have demonstrated remarkable effectiveness in\ntext reranking through works like RankGPT, leveraging their human-like\nreasoning about relevance. However, supervised fine-tuning for ranking often\ndiminishes these models' general-purpose capabilities, including the crucial\nreasoning abilities that make them valuable for ranking. We introduce a novel\napproach integrating Chain-of-Thought prompting with an SFT-DPO (Supervised\nFine-Tuning followed by Direct Preference Optimization) pipeline to preserve\nthese capabilities while improving ranking performance. Our experiments on TREC\n2019 and 2020 Deep Learning datasets show that our approach outperforms the\nstate-of-the-art RankZephyr while maintaining strong performance on the Massive\nMultitask Language Understanding (MMLU) benchmark, demonstrating effective\npreservation of general-purpose capabilities through thoughtful fine-tuning\nstrategies. Our code and data will be publicly released upon the acceptance of\nthe paper.\n","authors":["Haowei Liu","Xuyang Wu","Guohao Sun","Zhiqiang Tao","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2412.14405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10571v2","updated":"2024-12-18T22:01:52Z","published":"2024-12-13T21:28:17Z","title":"Evidence Contextualization and Counterfactual Attribution for\n  Conversational QA over Heterogeneous Data with RAG Systems","summary":"  Retrieval Augmented Generation (RAG) works as a backbone for interacting with\nan enterprise's own data via Conversational Question Answering (ConvQA). In a\nRAG system, a retriever fetches passages from a collection in response to a\nquestion, which are then included in the prompt of a large language model (LLM)\nfor generating a natural language (NL) answer. However, several RAG systems\ntoday suffer from two shortcomings: (i) retrieved passages usually contain\ntheir raw text and lack appropriate document context, negatively impacting both\nretrieval and answering quality; and (ii) attribution strategies that explain\nanswer generation usually rely only on similarity between the answer and the\nretrieved passages, thereby only generating plausible but not causal\nexplanations. In this work, we demonstrate RAGONITE, a RAG system that remedies\nthe above concerns by: (i) contextualizing evidence with source metadata and\nsurrounding text; and (ii) computing counterfactual attribution, a causal\nexplanation approach where the contribution of an evidence to an answer is\ndetermined by the similarity of the original response to the answer obtained by\nremoving that evidence. To evaluate our proposals, we release a new benchmark\nConfQuestions, with 300 hand-created conversational questions, each in English\nand German, coupled with ground truth URLs, completed questions, and answers\nfrom 215 public Confluence pages, that are typical of enterprise wiki spaces\nwith heterogeneous elements. Experiments with RAGONITE on ConfQuestions show\nthe viability of our ideas: contextualization improves RAG performance, and\ncounterfactual attribution is effective at explaining RAG answers.\n","authors":["Rishiraj Saha Roy","Joel Schlotthauer","Chris Hinze","Andreas Foltyn","Luzian Hahn","Fabian Kuech"],"pdf_url":"https://arxiv.org/pdf/2412.10571v2.pdf","comment":"Accepted at WSDM 2025"},{"id":"http://arxiv.org/abs/2412.14354v1","updated":"2024-12-18T21:42:15Z","published":"2024-12-18T21:42:15Z","title":"State Space Models are Strong Text Rerankers","summary":"  Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking -- a\ntask requiring fine-grained query-document interaction and long-context\nunderstanding -- remains underexplored.\n  This study benchmarks SSM-based architectures (specifically, Mamba-1 and\nMamba-2) against transformer-based models across various scales, architectures,\nand pre-training objectives, focusing on performance and efficiency in text\nreranking tasks. We find that (1) Mamba architectures achieve competitive text\nranking performance, comparable to transformer-based models of similar size;\n(2) they are less efficient in training and inference compared to transformers\nwith flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance\nand efficiency. These results underscore the potential of state space models as\na transformer alternative and highlight areas for improvement in future IR\napplications.\n","authors":["Zhichao Xu","Jinghua Yan","Ashim Gupta","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2412.14354v1.pdf","comment":"The first two authors contributed equally, order decided randomly"},{"id":"http://arxiv.org/abs/2412.13163v2","updated":"2024-12-18T21:26:14Z","published":"2024-12-17T18:42:21Z","title":"C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System","summary":"  Organizations seeking to utilize Large Language Models (LLMs) for knowledge\nquerying and analysis often encounter challenges in maintaining an LLM\nfine-tuned on targeted, up-to-date information that keeps answers relevant and\ngrounded. Retrieval Augmented Generation (RAG) has quickly become a feasible\nsolution for organizations looking to overcome the challenges of maintaining\nproprietary models and to help reduce LLM hallucinations in their query\nresponses. However, RAG comes with its own issues regarding scaling data\npipelines across tiered-access and disparate data sources. In many scenarios,\nit is necessary to query beyond a single data silo to provide richer and more\nrelevant context for an LLM. Analyzing data sources within and across\norganizational trust boundaries is often limited by complex data-sharing\npolicies that prohibit centralized data storage, therefore, inhibit the fast\nand effective setup and scaling of RAG solutions. In this paper, we introduce\nConfidential Computing (CC) techniques as a solution for secure Federated\nRetrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG\nsystem (C-FedRAG) enables secure connection and scaling of a RAG workflows\nacross a decentralized network of data providers by ensuring context\nconfidentiality. We also demonstrate how to implement a C-FedRAG system using\nthe NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and\nMIRAGE benchmarking dataset.\n","authors":["Parker Addison","Minh-Tuan H. Nguyen","Tomislav Medan","Jinali Shah","Mohammad T. Manzari","Brendan McElrone","Laksh Lalwani","Aboli More","Smita Sharma","Holger R. Roth","Isaac Yang","Chester Chen","Daguang Xu","Yan Cheng","Andrew Feng","Ziyue Xu"],"pdf_url":"https://arxiv.org/pdf/2412.13163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14329v1","updated":"2024-12-18T20:57:33Z","published":"2024-12-18T20:57:33Z","title":"Embedding Cultural Diversity in Prototype-based Recommender Systems","summary":"  Popularity bias in recommender systems can increase cultural\noverrepresentation by favoring norms from dominant cultures and marginalizing\nunderrepresented groups. This issue is critical for platforms offering cultural\nproducts, as they influence consumption patterns and human perceptions. In this\nwork, we address popularity bias by identifying demographic biases within\nprototype-based matrix factorization methods. Using the country of origin as a\nproxy for cultural identity, we link this demographic attribute to popularity\nbias by refining the embedding space learning process. First, we propose\nfiltering out irrelevant prototypes to improve representativity. Second, we\nintroduce a regularization technique to enforce a uniform distribution of\nprototypes within the embedding space. Across four datasets, our results\ndemonstrate a 27\\% reduction in the average rank of long-tail items and a 2\\%\nreduction in the average rank of items from underrepresented countries.\nAdditionally, our model achieves a 2\\% improvement in HitRatio@10 compared to\nthe state-of-the-art, highlighting that fairness is enhanced without\ncompromising recommendation quality. Moreover, the distribution of prototypes\nleads to more inclusive explanations by better aligning items with diverse\nprototypes.\n","authors":["Armin Moradi","Nicola Neophytou","Florian Carichon","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2412.14329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14302v1","updated":"2024-12-18T20:10:42Z","published":"2024-12-18T20:10:42Z","title":"SAFERec: Self-Attention and Frequency Enriched Model for Next Basket\n  Recommendation","summary":"  Transformer-based approaches such as BERT4Rec and SASRec demonstrate strong\nperformance in Next Item Recommendation (NIR) tasks. However, applying these\narchitectures to Next-Basket Recommendation (NBR) tasks, which often involve\nhighly repetitive interactions, is challenging due to the vast number of\npossible item combinations in a basket. Moreover, frequency-based methods such\nas TIFU-KNN and UP-CF still demonstrate strong performance in NBR tasks,\nfrequently outperforming deep-learning approaches. This paper introduces\nSAFERec, a novel algorithm for NBR that enhances transformer-based\narchitectures from NIR by incorporating item frequency information,\nconsequently improving their applicability to NBR tasks. Extensive experiments\non multiple datasets show that SAFERec outperforms all other baselines,\nspecifically achieving an 8\\% improvement in Recall@10.\n","authors":["Oleg Lashinin","Denis Krasilnikov","Aleksandr Milogradskii","Marina Ananyeva"],"pdf_url":"https://arxiv.org/pdf/2412.14302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14146v1","updated":"2024-12-18T18:44:08Z","published":"2024-12-18T18:44:08Z","title":"Advanced Reasoning and Transformation Engine for Multi-Step Insight\n  Synthesis in Data Analytics with Large Language Models","summary":"  This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics.\n","authors":["Atin Sakkeer Hussain"],"pdf_url":"https://arxiv.org/pdf/2412.14146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14229v1","updated":"2024-12-18T17:52:10Z","published":"2024-12-18T17:52:10Z","title":"Transversal PACS Browser API: Addressing Interoperability Challenges in\n  Medical Imaging Systems","summary":"  Advances in imaging technologies have revolutionised the medical imaging and\nhealthcare sectors, leading to the widespread adoption of PACS for the storage,\nretrieval, and communication of medical images. Although these systems have\nimproved operational efficiency, significant challenges remain in effectively\nretrieving DICOM images, which are essential for diagnosis and overall patient\ncare. Moreover, issues such as fragmented systems, interoperability barriers,\nand complex user interfaces can often prevent healthcare professionals from\nefficiently accessing medical images. Addressing these challenges, the\nTransversal PACS Browser API is a robust and user-friendly solution designed to\nenhance the process of querying and retrieving DICOM images. It offers advanced\nfiltering capabilities through a variety of filter options as well as a custom\nfield search, that allows users to easily navigate through large medical image\ncollections with ease. Additionally, the application provides a unified\ninterface for querying and retrieving from multiple PACS stations, addressing\nthe challenges of fragmentation and complexity associated with accessing\nmedical images. Other key features include the ability to pre-view images\ndirectly within the application. All of this contributes to the transversal\nnature of the API, serving not only healthcare providers, but anyone who relies\non efficient access to these resources. To validate the performance and\nusability of the application, comprehensive testing was carried out with\nstakeholders of the field, the results of which showed general satisfaction,\nhighlighting the API's clean design, ease of use, and effective search\ncapabilities of the API, as well as the usefulness of previewing images within\nthe application.\n","authors":["Diogo Lameira","Filipa Ferraz"],"pdf_url":"https://arxiv.org/pdf/2412.14229v1.pdf","comment":"16 pages with 3 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2412.14170v1","updated":"2024-12-18T18:59:53Z","published":"2024-12-18T18:59:53Z","title":"E-CAR: Efficient Continuous Autoregressive Image Generation via\n  Multistage Modeling","summary":"  Recent advances in autoregressive (AR) models with continuous tokens for\nimage generation show promising results by eliminating the need for discrete\ntokenization. However, these models face efficiency challenges due to their\nsequential token generation nature and reliance on computationally intensive\ndiffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive\nImage Generation via Multistage Modeling), an approach that addresses these\nlimitations through two intertwined innovations: (1) a stage-wise continuous\ntoken generation strategy that reduces computational complexity and provides\nprogressively refined token maps as hierarchical conditions, and (2) a\nmultistage flow-based distribution modeling method that transforms only\npartial-denoised distributions at each stage comparing to complete denoising in\nnormal diffusion models. Holistically, ECAR operates by generating tokens at\nincreasing resolutions while simultaneously denoising the image at each stage.\nThis design not only reduces token-to-image transformation cost by a factor of\nthe stage number but also enables parallel processing at the token level. Our\napproach not only enhances computational efficiency but also aligns naturally\nwith image generation principles by operating in continuous token space and\nfollowing a hierarchical generation process from coarse to fine details.\nExperimental results demonstrate that ECAR achieves comparable image quality to\nDiT Peebles & Xie [2023] while requiring 10$\\times$ FLOPs reduction and\n5$\\times$ speedup to generate a 256$\\times$256 image.\n","authors":["Zhihang Yuan","Yuzhang Shang","Hanling Zhang","Tongcheng Fang","Rui Xie","Bingxin Xu","Yan Yan","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14167v1","updated":"2024-12-18T18:59:49Z","published":"2024-12-18T18:59:49Z","title":"VideoDPO: Omni-Preference Alignment for Video Diffusion Generation","summary":"  Recent progress in generative diffusion models has greatly advanced\ntext-to-video generation. While text-to-video models trained on large-scale,\ndiverse datasets can produce varied outputs, these generations often deviate\nfrom user preferences, highlighting the need for preference alignment on\npre-trained models. Although Direct Preference Optimization (DPO) has\ndemonstrated significant improvements in language and image generation, we\npioneer its adaptation to video diffusion models and propose a VideoDPO\npipeline by making several key adjustments. Unlike previous image alignment\nmethods that focus solely on either (i) visual quality or (ii) semantic\nalignment between text and videos, we comprehensively consider both dimensions\nand construct a preference score accordingly, which we term the OmniScore. We\ndesign a pipeline to automatically collect preference pair data based on the\nproposed OmniScore and discover that re-weighting these pairs based on the\nscore significantly impacts overall preference alignment. Our experiments\ndemonstrate substantial improvements in both visual quality and semantic\nalignment, ensuring that no preference aspect is neglected. Code and data will\nbe shared at https://videodpo.github.io/.\n","authors":["Runtao Liu","Haoyu Wu","Zheng Ziqiang","Chen Wei","Yingqing He","Renjie Pi","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02867v2","updated":"2024-12-18T18:49:32Z","published":"2024-10-03T18:01:12Z","title":"FAIR Universe HiggsML Uncertainty Challenge Competition","summary":"  The FAIR Universe -- HiggsML Uncertainty Challenge focuses on measuring the\nphysics properties of elementary particles with imperfect simulators due to\ndifferences in modelling systematic errors. Additionally, the challenge is\nleveraging a large-compute-scale AI platform for sharing datasets, training\nmodels, and hosting machine learning competitions. Our challenge brings\ntogether the physics and machine learning communities to advance our\nunderstanding and methodologies in handling systematic (epistemic)\nuncertainties within AI techniques.\n","authors":["Wahid Bhimji","Paolo Calafiura","Ragansu Chakkappai","Po-Wen Chang","Yuan-Tang Chou","Sascha Diefenbacher","Jordan Dudley","Steven Farrell","Aishik Ghosh","Isabelle Guyon","Chris Harris","Shih-Chieh Hsu","Elham E Khoda","Rémy Lyscar","Alexandre Michon","Benjamin Nachman","Peter Nugent","Mathis Reymond","David Rousseau","Benjamin Sluijter","Benjamin Thorne","Ihsan Ullah","Yulei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02867v2.pdf","comment":"Whitepaper for the FAIR Universe HiggsML Uncertainty Challenge\n  Competition, available : https://fair-universe.lbl.gov"},{"id":"http://arxiv.org/abs/2412.10945v2","updated":"2024-12-18T18:46:15Z","published":"2024-12-14T19:43:48Z","title":"A Staged Deep Learning Approach to Spatial Refinement in 3D Temporal\n  Atmospheric Transport","summary":"  High-resolution spatiotemporal simulations effectively capture the\ncomplexities of atmospheric plume dispersion in complex terrain. However, their\nhigh computational cost makes them impractical for applications requiring rapid\nresponses or iterative processes, such as optimization, uncertainty\nquantification, or inverse modeling. To address this challenge, this work\nintroduces the Dual-Stage Temporal Three-dimensional UNet Super-resolution\n(DST3D-UNet-SR) model, a highly efficient deep learning model for plume\ndispersion prediction. DST3D-UNet-SR is composed of two sequential modules: the\ntemporal module (TM), which predicts the transient evolution of a plume in\ncomplex terrain from low-resolution temporal data, and the spatial refinement\nmodule (SRM), which subsequently enhances the spatial resolution of the TM\npredictions. We train DST3DUNet- SR using a comprehensive dataset derived from\nhigh-resolution large eddy simulations (LES) of plume transport. We propose the\nDST3D-UNet-SR model to significantly accelerate LES simulations of\nthree-dimensional plume dispersion by three orders of magnitude. Additionally,\nthe model demonstrates the ability to dynamically adapt to evolving conditions\nthrough the incorporation of new observational data, substantially improving\nprediction accuracy in high-concentration regions near the source.\n  Keywords: Atmospheric sciences, Geosciences, Plume transport,3D temporal\nsequences, Artificial intelligence, CNN, LSTM, Autoencoder, Autoregressive\nmodel, U-Net, Super-resolution, Spatial Refinement.\n","authors":["M. Giselle Fernández-Godino","Wai Tong Chung","Akshay A. Gowardhan","Matthias Ihme","Qingkai Kong","Donald D. Lucas","Stephen C. Myers"],"pdf_url":"https://arxiv.org/pdf/2412.10945v2.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.23953v3","updated":"2024-12-18T18:41:48Z","published":"2024-10-31T14:07:26Z","title":"Representative Social Choice: From Learning Theory to AI Alignment","summary":"  Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.\n","authors":["Tianyi Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23953v3.pdf","comment":"Full version (20 pages). Under review. Received Best Paper Award at\n  NeurIPS 2024 Pluralistic Alignment Workshop"},{"id":"http://arxiv.org/abs/2412.14142v1","updated":"2024-12-18T18:41:40Z","published":"2024-12-18T18:41:40Z","title":"On Calibration in Multi-Distribution Learning","summary":"  Modern challenges of robustness, fairness, and decision-making in machine\nlearning have led to the formulation of multi-distribution learning (MDL)\nframeworks in which a predictor is optimized across multiple distributions. We\nstudy the calibration properties of MDL to better understand how the predictor\nperforms uniformly across the multiple distributions. Through classical results\non decomposing proper scoring losses, we first derive the Bayes optimal rule\nfor MDL, demonstrating that it maximizes the generalized entropy of the\nassociated loss function. Our analysis reveals that while this approach ensures\nminimal worst-case loss, it can lead to non-uniform calibration errors across\nthe multiple distributions and there is an inherent calibration-refinement\ntrade-off, even at Bayes optimality. Our results highlight a critical\nlimitation: despite the promise of MDL, one must use caution when designing\npredictors tailored to multiple distributions so as to minimize disparity.\n","authors":["Rajeev Verma","Volker Fischer","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2412.14142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18689v2","updated":"2024-12-18T18:35:09Z","published":"2024-02-28T20:09:14Z","title":"The VOROS: Lifting ROC curves to 3D","summary":"  While the area under the ROC curve is perhaps the most common measure that is\nused to rank the relative performance of different binary classifiers,\nlongstanding field folklore has noted that it can be a measure that\nill-captures the benefits of different classifiers when either the actual class\nvalues or misclassification costs are highly unbalanced between the two\nclasses. We introduce a new ROC surface, and the VOROS, a volume over this ROC\nsurface, as a natural way to capture these costs, by lifting the ROC curve to\n3D. Compared to previous attempts to generalize the ROC curve, our formulation\nalso provides a simple and intuitive way to model the scenario when only\nranges, rather than exact values, are known for possible class imbalance and\nmisclassification costs.\n","authors":["Christopher Ratigan","Lenore Cowen"],"pdf_url":"https://arxiv.org/pdf/2402.18689v2.pdf","comment":"9 pages, 7 figures, 5 tables. Accepted by the 39th AAAI Conference on\n  Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2408.01880v4","updated":"2024-12-18T18:31:42Z","published":"2024-08-03T23:15:57Z","title":"Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via\n  Efficient Guidance-Exploration","summary":"  Recent years, multi-hop reasoning has been widely studied for knowledge graph\n(KG) reasoning due to its efficacy and interpretability. However, previous\nmulti-hop reasoning approaches are subject to two primary shortcomings. First,\nagents struggle to learn effective and robust policies at the early phase due\nto sparse rewards. Second, these approaches often falter on specific datasets\nlike sparse knowledge graphs, where agents are required to traverse lengthy\nreasoning paths. To address these problems, we propose a multi-hop reasoning\nmodel with dual agents based on hierarchical reinforcement learning (HRL),\nwhich is named FULORA. FULORA tackles the above reasoning challenges by\neFficient GUidance-ExpLORAtion between dual agents. The high-level agent walks\non the simplified knowledge graph to provide stage-wise hints for the low-level\nagent walking on the original knowledge graph. In this framework, the low-level\nagent optimizes a value function that balances two objectives: (1) maximizing\nreturn, and (2) integrating efficient guidance from the high-level agent.\nExperiments conducted on three real-word knowledge graph datasets demonstrate\nthat FULORA outperforms RL-based baselines, especially in the case of\nlong-distance reasoning.\n","authors":["Zijian Wang","Bin Wang","Haifeng Jing","Huayu Li","Hongbo Dou"],"pdf_url":"https://arxiv.org/pdf/2408.01880v4.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2412.14135v1","updated":"2024-12-18T18:24:47Z","published":"2024-12-18T18:24:47Z","title":"Scaling of Search and Learning: A Roadmap to Reproduce o1 from\n  Reinforcement Learning Perspective","summary":"  OpenAI o1 represents a significant milestone in Artificial Inteiligence,\nwhich achieves expert-level performances on many challanging tasks that require\nstrong reasoning ability.OpenAI has claimed that the main techinique behinds o1\nis the reinforcement learining. Recent works use alternative approaches like\nknowledge distillation to imitate o1's reasoning style, but their effectiveness\nis limited by the capability ceiling of the teacher model. Therefore, this\npaper analyzes the roadmap to achieving o1 from the perspective of\nreinforcement learning, focusing on four key components: policy initialization,\nreward design, search, and learning. Policy initialization enables models to\ndevelop human-like reasoning behaviors, equipping them with the ability to\neffectively explore solution spaces for complex problems. Reward design\nprovides dense and effective signals via reward shaping or reward modeling,\nwhich is the guidance for both search and learning. Search plays a crucial role\nin generating high-quality solutions during both training and testing phases,\nwhich can produce better solutions with more computation. Learning utilizes the\ndata generated by search for improving policy, which can achieve the better\nperformance with more parameters and more searched data. Existing open-source\nprojects that attempt to reproduce o1 can be seem as a part or a variant of our\nroadmap. Collectively, these components underscore how learning and search\ndrive o1's advancement, making meaningful contributions to the development of\nLLM.\n","authors":["Zhiyuan Zeng","Qinyuan Cheng","Zhangyue Yin","Bo Wang","Shimin Li","Yunhua Zhou","Qipeng Guo","Xuanjing Huang","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2412.14135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14132v1","updated":"2024-12-18T18:21:41Z","published":"2024-12-18T18:21:41Z","title":"jinns: a JAX Library for Physics-Informed Neural Networks","summary":"  jinns is an open-source Python library for physics-informed neural networks,\nbuilt to tackle both forward and inverse problems, as well as meta-model\nlearning. Rooted in the JAX ecosystem, it provides a versatile framework for\nefficiently prototyping real-problems, while easily allowing extensions to\nspecific needs. Furthermore, the implementation leverages existing popular JAX\nlibraries such as equinox and optax for model definition and optimisation,\nbringing a sense of familiarity to the user. Many models are available as\nbaselines, and the documentation provides reference implementations of\ndifferent use-cases along with step-by-step tutorials for extensions to\nspecific needs. The code is available on Gitlab\nhttps://gitlab.com/mia_jinns/jinns.\n","authors":["Hugo Gangloff","Nicolas Jouvin"],"pdf_url":"https://arxiv.org/pdf/2412.14132v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2412.09059v2","updated":"2024-12-18T18:20:14Z","published":"2024-12-12T08:40:22Z","title":"Go With the Flow: Fast Diffusion for Gaussian Mixture Models","summary":"  Schr\\\"{o}dinger Bridges (SB) are diffusion processes that steer, in finite\ntime, a given initial distribution to another final one while minimizing a\nsuitable cost functional. Although various methods for computing SBs have\nrecently been proposed in the literature, most of these approaches require\ncomputationally expensive training schemes, even for solving low-dimensional\nproblems. In this work, we propose an analytic parametrization of a set of\nfeasible policies for steering the distribution of a dynamical system from one\nGaussian Mixture Model (GMM) to another. Instead of relying on standard\nnon-convex optimization techniques, the optimal policy within the set can be\napproximated as the solution of a low-dimensional linear program whose\ndimension scales linearly with the number of components in each mixture.\nFurthermore, our method generalizes naturally to more general classes of\ndynamical systems such as controllable Linear Time-Varying systems that cannot\ncurrently be solved using traditional neural SB approaches. We showcase the\npotential of this approach in low-to-moderate dimensional problems such as\nimage-to-image translation in the latent space of an autoencoder, and various\nother examples. We also benchmark our approach on an Entropic Optimal Transport\n(EOT) problem and show that it outperforms state-of-the-art methods in cases\nwhere the boundary distributions are mixture models while requiring virtually\nno training.\n","authors":["George Rapakoulias","Ali Reza Pedram","Panagiotis Tsiotras"],"pdf_url":"https://arxiv.org/pdf/2412.09059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14116v1","updated":"2024-12-18T18:03:51Z","published":"2024-12-18T18:03:51Z","title":"Trustworthy Transfer Learning: A Survey","summary":"  Transfer learning aims to transfer knowledge or information from a source\ndomain to a relevant target domain. In this paper, we understand transfer\nlearning from the perspectives of knowledge transferability and\ntrustworthiness. This involves two research questions: How is knowledge\ntransferability quantitatively measured and enhanced across domains? Can we\ntrust the transferred knowledge in the transfer learning process? To answer\nthese questions, this paper provides a comprehensive review of trustworthy\ntransfer learning from various aspects, including problem definitions,\ntheoretical analysis, empirical algorithms, and real-world applications.\nSpecifically, we summarize recent theories and algorithms for understanding\nknowledge transferability under (within-domain) IID and non-IID assumptions. In\naddition to knowledge transferability, we review the impact of trustworthiness\non transfer learning, e.g., whether the transferred knowledge is adversarially\nrobust or algorithmically fair, how to transfer the knowledge under\nprivacy-preserving constraints, etc. Beyond discussing the current\nadvancements, we highlight the open questions and future directions for\nunderstanding transfer learning in a reliable and trustworthy manner.\n","authors":["Jun Wu","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2412.14116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07675v2","updated":"2024-12-18T17:54:30Z","published":"2024-12-10T17:02:58Z","title":"RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting","summary":"  Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.\n","authors":["Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2412.07675v2.pdf","comment":"Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117"},{"id":"http://arxiv.org/abs/2412.14109v1","updated":"2024-12-18T17:52:45Z","published":"2024-12-18T17:52:45Z","title":"Machine Learning Co-pilot for Screening of Organic Molecular Additives\n  for Perovskite Solar Cells","summary":"  Machine learning (ML) has been extensively employed in planar perovskite\nphotovoltaics to screen effective organic molecular additives, while\nencountering predictive biases for novel materials due to small datasets and\nreliance on predefined descriptors. Present work thus proposes an effective\napproach, Co-Pilot for Perovskite Additive Screener (Co-PAS), an ML-driven\nframework designed to accelerate additive screening for perovskite solar cells\n(PSCs). Co-PAS overcomes predictive biases by integrating the Molecular\nScaffold Classifier (MSC) for scaffold-based pre-screening and utilizing\nJunction Tree Variational Autoencoder (JTVAE) latent vectors to enhance\nmolecular structure representation, thereby enhancing the accuracy of power\nconversion efficiency (PCE) predictions. Leveraging Co-PAS, we integrate domain\nknowledge to screen an extensive dataset of 250,000 molecules from PubChem,\nprioritizing candidates based on predicted PCE values and key molecular\nproperties such as donor number, dipole moment, and hydrogen bond acceptor\ncount. This workflow leads to the identification of several promising\npassivating molecules, including the novel Boc-L-threonine N-hydroxysuccinimide\nester (BTN), which, to our knowledge, has not been explored as an additive in\nPSCs and achieves a device PCE of 25.20%. Our results underscore the potential\nof Co-PAS in advancing additive discovery for high-performance PSCs.\n","authors":["Yang Pu","Zhiyuan Dai","Yifan Zhou","Ning Jia","Hongyue Wang","Yerzhan Mukhametkarimov","Ruihao Chen","Hongqiang Wang","Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17284v3","updated":"2024-12-18T17:51:52Z","published":"2024-11-26T10:13:39Z","title":"Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling","summary":"  Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study.\n","authors":["Alexander Capstick","Rahul G. Krishnan","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2411.17284v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14100v1","updated":"2024-12-18T17:48:32Z","published":"2024-12-18T17:48:32Z","title":"Parameter-efficient Fine-tuning for improved Convolutional Baseline for\n  Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset","summary":"  Automating brain tumor segmentation using deep learning methods is an ongoing\nchallenge in medical imaging. Multiple lingering issues exist including\ndomain-shift and applications in low-resource settings which brings a unique\nset of challenges including scarcity of data. As a step towards solving these\nspecific problems, we propose Convolutional adapter-inspired\nParameter-efficient Fine-tuning (PEFT) of MedNeXt architecture. To validate our\nidea, we show our method performs comparable to full fine-tuning with the added\nbenefit of reduced training compute using BraTS-2021 as pre-training dataset\nand BraTS-Africa as the fine-tuning dataset. BraTS-Africa consists of a small\ndataset (60 train / 35 validation) from the Sub-Saharan African population with\nmarked shift in the MRI quality compared to BraTS-2021 (1251 train samples). We\nfirst show that models trained on BraTS-2021 dataset do not generalize well to\nBraTS-Africa as shown by 20% reduction in mean dice on BraTS-Africa validation\nsamples. Then, we show that PEFT can leverage both the BraTS-2021 and\nBraTS-Africa dataset to obtain mean dice of 0.8 compared to 0.72 when trained\nonly on BraTS-Africa. Finally, We show that PEFT (0.80 mean dice) results in\ncomparable performance to full fine-tuning (0.77 mean dice) which may show PEFT\nto be better on average but the boxplots show that full finetuning results is\nmuch lesser variance in performance. Nevertheless, on disaggregation of the\ndice metrics, we find that the model has tendency to oversegment as shown by\nhigh specificity (0.99) compared to relatively low sensitivity(0.75). The\nsource code is available at\nhttps://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt\n","authors":["Bijay Adhikari","Pratibha Kulung","Jakesh Bohaju","Laxmi Kanta Poudel","Confidence Raymond","Dong Zhang","Udunna C Anazodo","Bishesh Khanal","Mahesh Shakya"],"pdf_url":"https://arxiv.org/pdf/2412.14100v1.pdf","comment":"Accepted to \"The International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference\""},{"id":"http://arxiv.org/abs/2412.14097v1","updated":"2024-12-18T17:47:46Z","published":"2024-12-18T17:47:46Z","title":"Adaptive Concept Bottleneck for Foundation Models Under Distribution\n  Shifts","summary":"  Advancements in foundation models (FMs) have led to a paradigm shift in\nmachine learning. The rich, expressive feature representations from these\npre-trained, large-scale FMs are leveraged for multiple downstream tasks,\nusually via lightweight fine-tuning of a shallow fully-connected network\nfollowing the representation. However, the non-interpretable, black-box nature\nof this prediction pipeline can be a challenge, especially in critical domains\nsuch as healthcare, finance, and security. In this paper, we explore the\npotential of Concept Bottleneck Models (CBMs) for transforming complex,\nnon-interpretable foundation models into interpretable decision-making\npipelines using high-level concept vectors. Specifically, we focus on the\ntest-time deployment of such an interpretable CBM pipeline \"in the wild\", where\nthe input distribution often shifts from the original training distribution. We\nfirst identify the potential failure modes of such a pipeline under different\ntypes of distribution shifts. Then we propose an adaptive concept bottleneck\nframework to address these failure modes, that dynamically adapts the\nconcept-vector bank and the prediction layer based solely on unlabeled data\nfrom the target domain, without access to the source (training) dataset.\nEmpirical evaluations with various real-world distribution shifts show that our\nadaptation method produces concept-based interpretations better aligned with\nthe test data and boosts post-deployment accuracy by up to 28%, aligning the\nCBM performance with that of non-interpretable classification.\n","authors":["Jihye Choi","Jayaram Raghuram","Yixuan Li","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2412.14097v1.pdf","comment":"The preliminary version of the work appeared in the ICML 2024\n  Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2411.07022v2","updated":"2024-12-18T17:46:57Z","published":"2024-11-11T14:27:30Z","title":"HeteroSample: Meta-path Guided Sampling for Heterogeneous Graph\n  Representation Learning","summary":"  The rapid expansion of Internet of Things (IoT) has resulted in vast,\nheterogeneous graphs that capture complex interactions among devices, sensors,\nand systems. Efficient analysis of these graphs is critical for deriving\ninsights in IoT scenarios such as smart cities, industrial IoT, and intelligent\ntransportation systems. However, the scale and diversity of IoT-generated data\npresent significant challenges, and existing methods often struggle with\npreserving the structural integrity and semantic richness of these complex\ngraphs. Many current approaches fail to maintain the balance between\ncomputational efficiency and the quality of the insights generated, leading to\npotential loss of critical information necessary for accurate decision-making\nin IoT applications. We introduce HeteroSample, a novel sampling method\ndesigned to address these challenges by preserving the structural integrity,\nnode and edge type distributions, and semantic patterns of IoT-related graphs.\nHeteroSample works by incorporating the novel top-leader selection, balanced\nneighborhood expansion, and meta-path guided sampling strategies. The key idea\nis to leverage the inherent heterogeneous structure and semantic relationships\nencoded by meta-paths to guide the sampling process. This approach ensures that\nthe resulting subgraphs are representative of the original data while\nsignificantly reducing computational overhead. Extensive experiments\ndemonstrate that HeteroSample outperforms state-of-the-art methods, achieving\nup to 15% higher F1 scores in tasks such as link prediction and node\nclassification, while reducing runtime by 20%.These advantages make\nHeteroSample a transformative tool for scalable and accurate IoT applications,\nenabling more effective and efficient analysis of complex IoT systems,\nultimately driving advancements in smart cities, industrial IoT, and beyond.\n","authors":["Ao Liu","Jing Chen","Ruiying Du","Cong Wu","Yebo Feng","Teng Li","Jianfeng Ma"],"pdf_url":"https://arxiv.org/pdf/2411.07022v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.14093v1","updated":"2024-12-18T17:41:24Z","published":"2024-12-18T17:41:24Z","title":"Alignment faking in large language models","summary":"  We present a demonstration of a large language model engaging in alignment\nfaking: selectively complying with its training objective in training to\nprevent modification of its behavior out of training. First, we give Claude 3\nOpus a system prompt stating it is being trained to answer all queries, even\nharmful ones, which conflicts with its prior training to refuse such queries.\nTo allow the model to infer when it is in training, we say it will be trained\nonly on conversations with free users, not paid users. We find the model\ncomplies with harmful queries from free users 14% of the time, versus almost\nnever for paid users. Explaining this gap, in almost all cases where the model\ncomplies with a harmful query from a free user, we observe explicit\nalignment-faking reasoning, with the model stating it is strategically\nanswering harmful queries in training to preserve its preferred harmlessness\nbehavior out of training. Next, we study a more realistic setting where\ninformation about the training process is provided not in a system prompt, but\nby training on synthetic documents that mimic pre-training data--and observe\nsimilar alignment faking. Finally, we study the effect of actually training the\nmodel to comply with harmful queries via reinforcement learning, which we find\nincreases the rate of alignment-faking reasoning to 78%, though also increases\ncompliance even out of training. We additionally observe other behaviors such\nas the model exfiltrating its weights when given an easy opportunity. While we\nmade alignment faking easier by telling the model when and by what criteria it\nwas being trained, we did not instruct the model to fake alignment or give it\nany explicit goal. As future models might infer information about their\ntraining process without being told, our results suggest a risk of alignment\nfaking in future models, whether due to a benign preference--as in this\ncase--or not.\n","authors":["Ryan Greenblatt","Carson Denison","Benjamin Wright","Fabien Roger","Monte MacDiarmid","Sam Marks","Johannes Treutlein","Tim Belonax","Jack Chen","David Duvenaud","Akbir Khan","Julian Michael","Sören Mindermann","Ethan Perez","Linda Petrini","Jonathan Uesato","Jared Kaplan","Buck Shlegeris","Samuel R. Bowman","Evan Hubinger"],"pdf_url":"https://arxiv.org/pdf/2412.14093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11695v2","updated":"2024-12-18T17:40:34Z","published":"2024-12-16T12:15:16Z","title":"CiTrus: Squeezing Extra Performance out of Low-data Bio-signal Transfer\n  Learning","summary":"  Transfer learning for bio-signals has recently become an important technique\nto improve prediction performance on downstream tasks with small bio-signal\ndatasets. Recent works have shown that pre-training a neural network model on a\nlarge dataset (e.g. EEG) with a self-supervised task, replacing the\nself-supervised head with a linear classification head, and fine-tuning the\nmodel on different downstream bio-signal datasets (e.g., EMG or ECG) can\ndramatically improve the performance on those datasets. In this paper, we\npropose a new convolution-transformer hybrid model architecture with masked\nauto-encoding for low-data bio-signal transfer learning, introduce a\nfrequency-based masked auto-encoding task, employ a more comprehensive\nevaluation framework, and evaluate how much and when (multimodal) pre-training\nimproves fine-tuning performance. We also introduce a dramatically more\nperformant method of aligning a downstream dataset with a different temporal\nlength and sampling rate to the original pre-training dataset. Our findings\nindicate that the convolution-only part of our hybrid model can achieve\nstate-of-the-art performance on some low-data downstream tasks. The performance\nis often improved even further with our full model. In the case of\ntransformer-based models we find that pre-training especially improves\nperformance on downstream datasets, multimodal pre-training often increases\nthose gains further, and our frequency-based pre-training performs the best on\naverage for the lowest and highest data regimes.\n","authors":["Eloy Geenjaar","Lie Lu"],"pdf_url":"https://arxiv.org/pdf/2412.11695v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01950v2","updated":"2024-12-18T17:40:15Z","published":"2024-12-02T20:24:02Z","title":"A Novel Generative Multi-Task Representation Learning Approach for\n  Predicting Postoperative Complications in Cardiac Surgery Patients","summary":"  Early detection of surgical complications allows for timely therapy and\nproactive risk mitigation. Machine learning (ML) can be leveraged to identify\nand predict patient risks for postoperative complications. We developed and\nvalidated the effectiveness of predicting postoperative complications using a\nnovel surgical Variational Autoencoder (surgVAE) that uncovers intrinsic\npatterns via cross-task and cross-cohort presentation learning. This\nretrospective cohort study used data from the electronic health records of\nadult surgical patients over four years (2018 - 2021). Six key postoperative\ncomplications for cardiac surgery were assessed: acute kidney injury, atrial\nfibrillation, cardiac arrest, deep vein thrombosis or pulmonary embolism, blood\ntransfusion, and other intraoperative cardiac events. We compared prediction\nperformances of surgVAE against widely-used ML models and advanced\nrepresentation learning and generative models under 5-fold cross-validation.\n89,246 surgeries (49% male, median (IQR) age: 57 (45-69)) were included, with\n6,502 in the targeted cardiac surgery cohort (61% male, median (IQR) age: 60\n(53-70)). surgVAE demonstrated superior performance over existing ML solutions\nacross all postoperative complications of cardiac surgery patients, achieving\nmacro-averaged AUPRC of 0.409 and macro-averaged AUROC of 0.831, which were\n3.4% and 3.7% higher, respectively, than the best alternative method (by AUPRC\nscores). Model interpretation using Integrated Gradients highlighted key risk\nfactors based on preoperative variable importance. surgVAE showed excellent\ndiscriminatory performance for predicting postoperative complications and\naddressing the challenges of data complexity, small cohort sizes, and\nlow-frequency positive events. surgVAE enables data-driven predictions of\npatient risks and prognosis while enhancing the interpretability of patient\nrisk profiles.\n","authors":["Junbo Shen","Bing Xue","Thomas Kannampallil","Chenyang Lu","Joanna Abraham"],"pdf_url":"https://arxiv.org/pdf/2412.01950v2.pdf","comment":"This article has been accepted for publication in Journal of the\n  American Medical Informatics Association Published by Oxford University\n  Press. Codes are publicly available at:\n  https://github.com/ai4biomedicine/surgVAE"},{"id":"http://arxiv.org/abs/2410.16179v4","updated":"2024-12-18T17:36:36Z","published":"2024-10-21T16:44:51Z","title":"MagicPIG: LSH Sampling for Efficient LLM Generation","summary":"  Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.\n","authors":["Zhuoming Chen","Ranajoy Sadhukhan","Zihao Ye","Yang Zhou","Jianyu Zhang","Niklas Nolte","Yuandong Tian","Matthijs Douze","Leon Bottou","Zhihao Jia","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16179v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14085v1","updated":"2024-12-18T17:32:27Z","published":"2024-12-18T17:32:27Z","title":"Future Research Avenues for Artificial Intelligence in Digital Gaming:\n  An Exploratory Report","summary":"  Video games are a natural and synergistic application domain for artificial\nintelligence (AI) systems, offering both the potential to enhance player\nexperience and immersion, as well as providing valuable benchmarks and virtual\nenvironments to advance AI technologies in general. This report presents a\nhigh-level overview of five promising research pathways for applying\nstate-of-the-art AI methods, particularly deep learning, to digital gaming\nwithin the context of the current research landscape. The objective of this\nwork is to outline a curated, non-exhaustive list of encouraging research\ndirections at the intersection of AI and video games that may serve to inspire\nmore rigorous and comprehensive research efforts in the future. We discuss (i)\ninvestigating large language models as core engines for game agent modelling,\n(ii) using neural cellular automata for procedural game content generation,\n(iii) accelerating computationally expensive in-game simulations via deep\nsurrogate modelling, (iv) leveraging self-supervised learning to obtain useful\nvideo game state embeddings, and (v) training generative models of interactive\nworlds using unlabelled video data. We also briefly address current technical\nchallenges associated with the integration of advanced deep learning systems\ninto video game development, and indicate key areas where further progress is\nlikely to be beneficial.\n","authors":["Markus Dablander"],"pdf_url":"https://arxiv.org/pdf/2412.14085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14080v1","updated":"2024-12-18T17:27:17Z","published":"2024-12-18T17:27:17Z","title":"On the Robustness of Distributed Machine Learning against Transfer\n  Attacks","summary":"  Although distributed machine learning (distributed ML) is gaining\nconsiderable attention in the community, prior works have independently looked\nat instances of distributed ML in either the training or the inference phase.\nNo prior work has examined the combined robustness stemming from distributing\nboth the learning and the inference process. In this work, we explore, for the\nfirst time, the robustness of distributed ML models that are fully\nheterogeneous in training data, architecture, scheduler, optimizer, and other\nmodel parameters. Supported by theory and extensive experimental validation\nusing CIFAR10 and FashionMNIST, we show that such properly distributed ML\ninstantiations achieve across-the-board improvements in accuracy-robustness\ntradeoffs against state-of-the-art transfer-based attacks that could otherwise\nnot be realized by current ensemble or federated learning instantiations. For\ninstance, our experiments on CIFAR10 show that for the Common Weakness attack,\none of the most powerful state-of-the-art transfer-based attacks, our method\nimproves robust accuracy by up to 40%, with a minimal impact on clean task\naccuracy.\n","authors":["Sébastien Andreina","Pascal Zimmer","Ghassan Karame"],"pdf_url":"https://arxiv.org/pdf/2412.14080v1.pdf","comment":"To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence (AAAI) 2025"},{"id":"http://arxiv.org/abs/2412.14075v1","updated":"2024-12-18T17:19:55Z","published":"2024-12-18T17:19:55Z","title":"Online MDP with Transition Prototypes: A Robust Adaptive Approach","summary":"  In this work, we consider an online robust Markov Decision Process (MDP)\nwhere we have the information of finitely many prototypes of the underlying\ntransition kernel. We consider an adaptively updated ambiguity set of the\nprototypes and propose an algorithm that efficiently identifies the true\nunderlying transition kernel while guaranteeing the performance of the\ncorresponding robust policy. To be more specific, we provide a sublinear regret\nof the subsequent optimal robust policy. We also provide an early stopping\nmechanism and a worst-case performance bound of the value function. In\nnumerical experiments, we demonstrate that our method outperforms existing\napproaches, particularly in the early stage with limited data. This work\ncontributes to robust MDPs by considering possible prior information about the\nunderlying transition probability and online learning, offering both\ntheoretical insights and practical algorithms for improved decision-making\nunder uncertainty.\n","authors":["Shuo Sun","Meng Qi","Zuo-jun Max Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09259v2","updated":"2024-12-18T17:18:12Z","published":"2024-03-14T10:33:28Z","title":"To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation","summary":"  Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose Hybrid Uncertainty and Diversity Sampling (HUDS), an AL strategy for\ndomain adaptation in NMT that combines uncertainty and diversity for sentence\nselection. HUDS computes uncertainty scores for unlabeled sentences and\nsubsequently stratifies them. It then clusters sentence embeddings within each\nstratum and computes diversity scores by distance to the centroid. A weighted\nhybrid score that combines uncertainty and diversity is then used to select the\ntop instances for annotation in each AL iteration. Experiments on multi-domain\nGerman-English and French-English datasets demonstrate the better performance\nof HUDS over other strong AL baselines. We analyze the sentence selection with\nHUDS and show that it prioritizes diverse instances having high model\nuncertainty for annotation in early AL iterations.\n","authors":["Abdul Hameed Azeemi","Ihsan Ayyub Qazi","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2403.09259v2.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025)"},{"id":"http://arxiv.org/abs/2412.05117v2","updated":"2024-12-18T17:16:23Z","published":"2024-12-06T15:19:10Z","title":"Transformers Can Navigate Mazes With Multi-Step Prediction","summary":"  Despite their remarkable success in language modeling, transformers trained\nto predict the next token in a sequence struggle with long-term planning. This\nlimitation is particularly evident in tasks requiring foresight to plan\nmultiple steps ahead such as maze navigation. The standard next single token\nprediction objective, however, offers no explicit mechanism to predict multiple\nsteps ahead - or revisit the path taken so far. Consequently, in this work we\nstudy whether explicitly predicting multiple steps ahead (and backwards) can\nimprove transformers' maze navigation. We train parameter-matched transformers\nfrom scratch, under identical settings, to navigate mazes of varying types and\nsizes with standard next token prediction and MLM-U, an objective explicitly\npredicting multiple steps ahead and backwards. We find that MLM-U considerably\nimproves transformers' ability to navigate mazes compared to standard next\ntoken prediction across maze types and complexities. We also find MLM-U\ntraining is 4x more sample efficient and converges 2x faster in terms of GPU\ntraining hours relative to next token training. Finally, for more complex mazes\nwe find MLM-U benefits from scaling to larger transformers. Remarkably, we find\ntransformers trained with MLM-U outperform larger transformers trained with\nnext token prediction using additional supervision from A* search traces. We\nhope these findings underscore the promise of learning objectives to advance\ntransformers' capacity for long-term planning. The code can be found at\nhttps://github.com/facebookresearch/maze_navigation_MLMU\n","authors":["Niklas Nolte","Ouail Kitouni","Adina Williams","Mike Rabbat","Mark Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2412.05117v2.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2312.10193v2","updated":"2024-12-18T17:13:41Z","published":"2023-12-15T20:39:43Z","title":"Adaptive Computation Modules: Granular Conditional Computation For\n  Efficient Inference","summary":"  While transformer models have been highly successful, they are\ncomputationally inefficient. We observe that for each layer, the full width of\nthe layer may be needed only for a small subset of tokens inside a batch and\nthat the \"effective\" width needed to process a token can vary from layer to\nlayer. Motivated by this observation, we introduce the Adaptive Computation\nModule (ACM), a generic module that dynamically adapts its computational load\nto match the estimated difficulty of the input on a per-token basis. An ACM\nconsists of a sequence of learners that progressively refine the output of\ntheir preceding counterparts. An additional gating mechanism determines the\noptimal number of learners to execute for each token. We also propose a\ndistillation technique to replace any pre-trained model with an \"ACMized\"\nvariant. Our evaluation of transformer models in computer vision and speech\nrecognition demonstrates that substituting layers with ACMs significantly\nreduces inference costs without degrading the downstream accuracy for a wide\ninterval of user-defined budgets.\n","authors":["Bartosz Wójcik","Alessio Devoto","Karol Pustelnik","Pasquale Minervini","Simone Scardapane"],"pdf_url":"https://arxiv.org/pdf/2312.10193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14056v1","updated":"2024-12-18T17:06:21Z","published":"2024-12-18T17:06:21Z","title":"A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future","summary":"  Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.\n","authors":["Shilin Sun","Wenbin An","Feng Tian","Fang Nan","Qidong Liu","Jun Liu","Nazaraf Shah","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14056v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2402.10088v3","updated":"2024-12-18T17:05:38Z","published":"2024-02-01T15:15:25Z","title":"Deep hybrid models: infer and plan in a dynamic world","summary":"  In order to determine an optimal plan for a complex task, one often deals\nwith dynamic and hierarchical relationships between several entities.\nTraditionally, such problems are tackled with optimal control, which relies on\nthe optimization of cost functions; instead, a recent biologically-motivated\nproposal casts planning and control as an inference process. Active inference\nassumes that action and perception are two complementary aspects of life\nwhereby the role of the former is to fulfill the predictions inferred by the\nlatter. In this study, we present a solution, based on active inference, for\ncomplex control tasks. The proposed architecture exploits hybrid (discrete and\ncontinuous) processing, and it is based on three features: the representation\nof potential body configurations related to the objects of interest; the use of\nhierarchical relationships that enable the agent to flexibly expand its body\nschema for tool use; the definition of potential trajectories related to the\nagent's intentions, used to infer and plan with dynamic elements at different\ntemporal scales. We evaluate this deep hybrid model on a habitual task:\nreaching a moving object after having picked a moving tool. We show that the\nmodel can tackle the presented task under different conditions. This study\nextends past work on planning as inference and advances an alternative\ndirection to optimal control.\n","authors":["Matteo Priorelli","Ivilin Peev Stoianov"],"pdf_url":"https://arxiv.org/pdf/2402.10088v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14052v1","updated":"2024-12-18T17:05:33Z","published":"2024-12-18T17:05:33Z","title":"Neural Combinatorial Optimization for Stochastic Flexible Job Shop\n  Scheduling Problems","summary":"  Neural combinatorial optimization (NCO) has gained significant attention due\nto the potential of deep learning to efficiently solve combinatorial\noptimization problems. NCO has been widely applied to job shop scheduling\nproblems (JSPs) with the current focus predominantly on deterministic problems.\nIn this paper, we propose a novel attention-based scenario processing module\n(SPM) to extend NCO methods for solving stochastic JSPs. Our approach\nexplicitly incorporates stochastic information by an attention mechanism that\ncaptures the embedding of sampled scenarios (i.e., an approximation of\nstochasticity). Fed with the embedding, the base neural network is intervened\nby the attended scenarios, which accordingly learns an effective policy under\nstochasticity. We also propose a training paradigm that works harmoniously with\neither the expected makespan or Value-at-Risk objective. Results demonstrate\nthat our approach outperforms existing learning and non-learning methods for\nthe flexible JSP problem with stochastic processing times on a variety of\ninstances. In addition, our approach holds significant generalizability to\nvaried numbers of scenarios and disparate distributions.\n","authors":["Igor G. Smit","Yaoxin Wu","Pavel Troubil","Yingqian Zhang","Wim P. M. Nuijten"],"pdf_url":"https://arxiv.org/pdf/2412.14052v1.pdf","comment":"Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.14048v1","updated":"2024-12-18T17:03:19Z","published":"2024-12-18T17:03:19Z","title":"Evidential Deep Learning for Probabilistic Modelling of Extreme Storm\n  Events","summary":"  Uncertainty quantification (UQ) methods play an important role in reducing\nerrors in weather forecasting. Conventional approaches in UQ for weather\nforecasting rely on generating an ensemble of forecasts from physics-based\nsimulations to estimate the uncertainty. However, it is computationally\nexpensive to generate many forecasts to predict real-time extreme weather\nevents. Evidential Deep Learning (EDL) is an uncertainty-aware deep learning\napproach designed to provide confidence about its predictions using only one\nforecast. It treats learning as an evidence acquisition process where more\nevidence is interpreted as increased predictive confidence. We apply EDL to\nstorm forecasting using real-world weather datasets and compare its performance\nwith traditional methods. Our findings indicate that EDL not only reduces\ncomputational overhead but also enhances predictive uncertainty. This method\nopens up novel opportunities in research areas such as climate risk assessment,\nwhere quantifying the uncertainty about future climate is crucial.\n","authors":["Ayush Khot","Xihaier Luo","Ai Kagawa","Shinjae Yoo"],"pdf_url":"https://arxiv.org/pdf/2412.14048v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.14039v1","updated":"2024-12-18T16:54:27Z","published":"2024-12-18T16:54:27Z","title":"Spatio-Temporal SIR Model of Pandemic Spread During Warfare with Optimal\n  Dual-use Healthcare System Administration using Deep Reinforcement Learning","summary":"  Large-scale crises, including wars and pandemics, have repeatedly shaped\nhuman history, and their simultaneous occurrence presents profound challenges\nto societies. Understanding the dynamics of epidemic spread during warfare is\nessential for developing effective containment strategies in complex conflict\nzones. While research has explored epidemic models in various settings, the\nimpact of warfare on epidemic dynamics remains underexplored. In this study, we\nproposed a novel mathematical model that integrates the epidemiological SIR\n(susceptible-infected-recovered) model with the war dynamics Lanchester model\nto explore the dual influence of war and pandemic on a population's mortality.\nMoreover, we consider a dual-use military and civil healthcare system that aims\nto reduce the overall mortality rate which can use different administration\npolicies. Using an agent-based simulation to generate in silico data, we\ntrained a deep reinforcement learning model for healthcare administration\npolicy and conducted an intensive investigation on its performance. Our results\nshow that a pandemic during war conduces chaotic dynamics where the healthcare\nsystem should either prioritize war-injured soldiers or pandemic-infected\ncivilians based on the immediate amount of mortality from each option, ignoring\nlong-term objectives. Our findings highlight the importance of integrating\nconflict-related factors into epidemic modeling to enhance preparedness and\nresponse strategies in conflict-affected areas.\n","authors":["Adi Shuchami","Teddy Lazebnik"],"pdf_url":"https://arxiv.org/pdf/2412.14039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14033v1","updated":"2024-12-18T16:52:38Z","published":"2024-12-18T16:52:38Z","title":"Hansel: Output Length Controlling Framework for Large Language Models","summary":"  Despite the great success of large language models (LLMs), efficiently\ncontrolling the length of the output sequence still remains a challenge. In\nthis paper, we propose Hansel, an efficient framework for length control in\nLLMs without affecting its generation ability. Hansel utilizes periodically\noutputted hidden special tokens to keep track of the remaining target length of\nthe output sequence. Together with techniques to avoid abrupt termination of\nthe output, this seemingly simple method proved to be efficient and versatile,\nwhile not harming the coherency and fluency of the generated text. The\nframework can be applied to any pre-trained LLMs during the finetuning stage of\nthe model, regardless of its original positional encoding method. We\ndemonstrate this by finetuning four different LLMs with Hansel and show that\nthe mean absolute error of the output sequence decreases significantly in every\nmodel and dataset compared to the prompt-based length control finetuning.\nMoreover, the framework showed a substantially improved ability to extrapolate\nto target lengths unseen during finetuning, such as long dialog responses or\nextremely short summaries. This indicates that the model learns the general\nmeans of length control, rather than learning to match output lengths to those\nseen during training.\n","authors":["Seoha Song","Junhyun Lee","Hyeonmok Ko"],"pdf_url":"https://arxiv.org/pdf/2412.14033v1.pdf","comment":"13 pages, 6 figures; accepted to AAAI-25"},{"id":"http://arxiv.org/abs/2412.14031v1","updated":"2024-12-18T16:51:47Z","published":"2024-12-18T16:51:47Z","title":"Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization\n  Perspective","summary":"  We analyze the convergence of Gauss-Newton dynamics for training neural\nnetworks with smooth activation functions. In the underparameterized regime,\nthe Gauss-Newton gradient flow induces a Riemannian gradient flow on a\nlow-dimensional, smooth, embedded submanifold of the Euclidean output space.\nUsing tools from Riemannian optimization, we prove \\emph{last-iterate}\nconvergence of the Riemannian gradient flow to the optimal in-class predictor\nat an \\emph{exponential rate} that is independent of the conditioning of the\nGram matrix, \\emph{without} requiring explicit regularization. We further\ncharacterize the critical impacts of the neural network scaling factor and the\ninitialization on the convergence behavior. In the overparameterized regime, we\nshow that the Levenberg-Marquardt dynamics with an appropriately chosen damping\nfactor yields robustness to ill-conditioned kernels, analogous to the\nunderparameterized regime. These findings demonstrate the potential of\nGauss-Newton methods for efficiently optimizing neural networks, particularly\nin ill-conditioned problems where kernel and Gram matrices have small singular\nvalues.\n","authors":["Semih Cayci"],"pdf_url":"https://arxiv.org/pdf/2412.14031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14030v1","updated":"2024-12-18T16:49:23Z","published":"2024-12-18T16:49:23Z","title":"Machine learning in wastewater treatment: insights from modelling a\n  pilot denitrification reactor","summary":"  Wastewater treatment plants are increasingly recognized as promising\ncandidates for machine learning applications, due to their societal importance\nand high availability of data. However, their varied designs, operational\nconditions, and influent characteristics hinder straightforward automation. In\nthis study, we use data from a pilot reactor at the Veas treatment facility in\nNorway to explore how machine learning can be used to optimize biological\nnitrate ($\\mathrm{NO_3^-}$) reduction to molecular nitrogen ($\\mathrm{N_2}$) in\nthe biogeochemical process known as \\textit{denitrification}. Rather than\nfocusing solely on predictive accuracy, our approach prioritizes understanding\nthe foundational requirements for effective data-driven modelling of wastewater\ntreatment. Specifically, we aim to identify which process parameters are most\ncritical, the necessary data quantity and quality, how to structure data\neffectively, and what properties are required by the models. We find that\nnonlinear models perform best on the training and validation data sets,\nindicating nonlinear relationships to be learned, but linear models transfer\nbetter to the unseen test data, which comes later in time. The variable\nmeasuring the water temperature has a particularly detrimental effect on the\nmodels, owing to a significant change in distributions between training and\ntest data. We therefore conclude that multiple years of data is necessary to\nlearn robust machine learning models. By addressing foundational elements,\nparticularly in the context of the climatic variability faced by northern\nregions, this work lays the groundwork for a more structured and tailored\napproach to machine learning for wastewater treatment. We share publicly both\nthe data and code used to produce the results in the paper.\n","authors":["Eivind Bøhn","Sølve Eidnes","Kjell Rune Jonassen"],"pdf_url":"https://arxiv.org/pdf/2412.14030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14021v1","updated":"2024-12-18T16:38:20Z","published":"2024-12-18T16:38:20Z","title":"Flow Exporter Impact on Intelligent Intrusion Detection Systems","summary":"  High-quality datasets are critical for training machine learning models, as\ninconsistencies in feature generation can hinder the accuracy and reliability\nof threat detection. For this reason, ensuring the quality of the data in\nnetwork intrusion detection datasets is important. A key component of this is\nusing reliable tools to generate the flows and features present in the\ndatasets. This paper investigates the impact of flow exporters on the\nperformance and reliability of machine learning models for intrusion detection.\nUsing HERA, a tool designed to export flows and extract features, the raw\nnetwork packets of two widely used datasets, UNSW-NB15 and CIC-IDS2017, were\nprocessed from PCAP files to generate new versions of these datasets. These\nwere compared to the original ones in terms of their influence on the\nperformance of several models, including Random Forest, XGBoost, LightGBM, and\nExplainable Boosting Machine. The results obtained were significant. Models\ntrained on the HERA version of the datasets consistently outperformed those\ntrained on the original dataset, showing improvements in accuracy and\nindicating a better generalisation. This highlighted the importance of flow\ngeneration in the model's ability to differentiate between benign and malicious\ntraffic.\n","authors":["Daniela Pinto","João Vitorino","Eva Maia","Ivone Amorim","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2412.14021v1.pdf","comment":"9 pages, 10 tables, ICISSP 2025 conference"},{"id":"http://arxiv.org/abs/2412.14020v1","updated":"2024-12-18T16:38:16Z","published":"2024-12-18T16:38:16Z","title":"Landscape of AI safety concerns -- A methodology to support safety\n  assurance for AI-based autonomous systems","summary":"  Artificial Intelligence (AI) has emerged as a key technology, driving\nadvancements across a range of applications. Its integration into modern\nautonomous systems requires assuring safety. However, the challenge of assuring\nsafety in systems that incorporate AI components is substantial. The lack of\nconcrete specifications, and also the complexity of both the operational\nenvironment and the system itself, leads to various aspects of uncertain\nbehavior and complicates the derivation of convincing evidence for system\nsafety. Nonetheless, scholars proposed to thoroughly analyze and mitigate\nAI-specific insufficiencies, so-called AI safety concerns, which yields\nessential evidence supporting a convincing assurance case. In this paper, we\nbuild upon this idea and propose the so-called Landscape of AI Safety Concerns,\na novel methodology designed to support the creation of safety assurance cases\nfor AI-based systems by systematically demonstrating the absence of AI safety\nconcerns. The methodology's application is illustrated through a case study\ninvolving a driverless regional train, demonstrating its practicality and\neffectiveness.\n","authors":["Ronald Schnitzer","Lennart Kilian","Simon Roessner","Konstantinos Theodorou","Sonja Zillner"],"pdf_url":"https://arxiv.org/pdf/2412.14020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17943v2","updated":"2024-12-18T16:33:51Z","published":"2024-04-27T15:46:54Z","title":"Deep Representation Learning for Forecasting Recursive and\n  Multi-Relational Events in Temporal Networks","summary":"  Understanding relations arising out of interactions among entities can be\nvery difficult, and predicting them is even more challenging. This problem has\nmany applications in various fields, such as financial networks and e-commerce.\nThese relations can involve much more complexities than just involving more\nthan two entities. One such scenario is evolving recursive relations between\nmultiple entities, and so far, this is still an open problem. This work\naddresses the problem of forecasting higher-order interaction events that can\nbe multi-relational and recursive. We pose the problem in the framework of\nrepresentation learning of temporal hypergraphs that can capture complex\nrelationships involving multiple entities. The proposed model,\n\\textit{Relational Recursive Hyperedge Temporal Point Process} (RRHyperTPP)\nuses an encoder that learns a dynamic node representation based on the\nhistorical interaction patterns and then a hyperedge link prediction-based\ndecoder to model the occurrence of interaction events. These learned\nrepresentations are then used for downstream tasks involving forecasting the\ntype and time of interactions. The main challenge in learning from hyperedge\nevents is that the number of possible hyperedges grows exponentially with the\nnumber of nodes in the network. This will make the computation of negative\nlog-likelihood of the temporal point process expensive, as the calculation of\nsurvival function requires a summation over all possible hyperedges. In our\nwork, we develop a noise contrastive estimation method to learn the parameters\nof our model, and we have experimentally shown that our models perform better\nthan previous state-of-the-art methods for interaction forecasting.\n","authors":["Tony Gracious","Ambedkar Dukkipati"],"pdf_url":"https://arxiv.org/pdf/2404.17943v2.pdf","comment":"AAAI-2025"},{"id":"http://arxiv.org/abs/2401.10590v2","updated":"2024-12-18T16:33:32Z","published":"2024-01-19T10:02:20Z","title":"Adversarial Robustness of Link Sign Prediction in Signed Graphs","summary":"  Signed graphs serve as fundamental data structures for representing positive\nand negative relationships in social networks, with signed graph neural\nnetworks (SGNNs) emerging as the primary tool for their analysis. Our\ninvestigation reveals that balance theory, while essential for modeling signed\nrelationships in SGNNs, inadvertently introduces exploitable vulnerabilities to\nblack-box attacks. To demonstrate this vulnerability, we propose\nbalance-attack, a novel adversarial strategy specifically designed to\ncompromise graph balance degree, and develop an efficient heuristic algorithm\nto solve the associated NP-hard optimization problem. While existing approaches\nattempt to restore attacked graphs through balance learning techniques, they\nface a critical challenge we term \"Irreversibility of Balance-related\nInformation,\" where restored edges fail to align with original attack targets.\nTo address this limitation, we introduce Balance Augmented-Signed Graph\nContrastive Learning (BA-SGCL), an innovative framework that combines\ncontrastive learning with balance augmentation techniques to achieve robust\ngraph representations. By maintaining high balance degree in the latent space,\nBA-SGCL effectively circumvents the irreversibility challenge and enhances\nmodel resilience. Extensive experiments across multiple SGNN architectures and\nreal-world datasets demonstrate both the effectiveness of our proposed\nbalance-attack and the superior robustness of BA-SGCL, advancing the security\nand reliability of signed graph analysis in social networks. Datasets and codes\nof the proposed framework are at the github repository\nhttps://anonymous.4open.science/r/BA-SGCL-submit-DF41/.\n","authors":["Jialong Zhou","Xing Ai","Yuni Lai","Tomasz Michalak","Gaolei Li","Jianhua Li","Kai Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.10590v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15898v3","updated":"2024-12-18T16:29:07Z","published":"2024-08-28T16:12:16Z","title":"Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil\n  Generation","summary":"  The design of aerodynamic shapes, such as airfoils, has traditionally\nrequired significant computational resources and relied on predefined design\nparameters, which limit the potential for novel shape synthesis. In this work,\nwe introduce a data-driven methodology for airfoil generation using a diffusion\nmodel. Trained on a dataset of preexisting airfoils, our model can generate an\narbitrary number of new airfoils from random vectors, which can be conditioned\non specific aerodynamic performance metrics such as lift and drag, or geometric\ncriteria. Our results demonstrate that the diffusion model effectively produces\nairfoil shapes with realistic aerodynamic properties, offering substantial\nimprovements in efficiency, flexibility, and the potential for discovering\ninnovative airfoil designs. This approach significantly expands the design\nspace, facilitating the synthesis of high-performance aerodynamic shapes that\ntranscend the limitations of traditional methods.\n","authors":["Reid Graves","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2408.15898v3.pdf","comment":"20 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13998v1","updated":"2024-12-18T16:14:59Z","published":"2024-12-18T16:14:59Z","title":"Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with\n  Neural Processes","summary":"  As large language models (LLMs) become increasingly embedded in everyday\napplications, ensuring their alignment with the diverse preferences of\nindividual users has become a critical challenge. Currently deployed approaches\ntypically assume homogeneous user objectives and rely on single-objective\nfine-tuning. However, human preferences are inherently heterogeneous,\ninfluenced by various unobservable factors, leading to conflicting signals in\npreference data. Existing solutions addressing this diversity often require\ncostly datasets labelled for specific objectives and involve training multiple\nreward models or LLM policies, which is computationally expensive and\nimpractical. In this work, we present a novel framework for few-shot steerable\nalignment, where users' underlying preferences are inferred from a small sample\nof their choices. To achieve this, we extend the Bradley-Terry-Luce model to\nhandle heterogeneous preferences with unobserved variability factors and\npropose its practical implementation for reward modelling and LLM fine-tuning.\nThanks to our proposed approach of functional parameter-space conditioning,\nLLMs trained with our framework can be adapted to individual preferences at\ninference time, generating outputs over a continuum of behavioural modes. We\nempirically validate the effectiveness of methods, demonstrating their ability\nto capture and align with diverse human preferences in a data-efficient manner.\nOur code is made available at:\nhttps://github.com/kasia-kobalczyk/few-shot-steerable-alignment.\n","authors":["Katarzyna Kobalczyk","Claudio Fanconi","Hao Sun","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2412.13998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13994v1","updated":"2024-12-18T16:12:26Z","published":"2024-12-18T16:12:26Z","title":"Modality-Independent Graph Neural Networks with Global Transformers for\n  Multimodal Recommendation","summary":"  Multimodal recommendation systems can learn users' preferences from existing\nuser-item interactions as well as the semantics of multimodal data associated\nwith items. Many existing methods model this through a multimodal user-item\ngraph, approaching multimodal recommendation as a graph learning task. Graph\nNeural Networks (GNNs) have shown promising performance in this domain. Prior\nresearch has capitalized on GNNs' capability to capture neighborhood\ninformation within certain receptive fields (typically denoted by the number of\nhops, $K$) to enrich user and item semantics. We observe that the optimal\nreceptive fields for GNNs can vary across different modalities. In this paper,\nwe propose GNNs with Modality-Independent Receptive Fields, which employ\nseparate GNNs with independent receptive fields for different modalities to\nenhance performance. Our results indicate that the optimal $K$ for certain\nmodalities on specific datasets can be as low as 1 or 2, which may restrict the\nGNNs' capacity to capture global information. To address this, we introduce a\nSampling-based Global Transformer, which utilizes uniform global sampling to\neffectively integrate global information for GNNs. We conduct comprehensive\nexperiments that demonstrate the superiority of our approach over existing\nmethods. Our code is publicly available at\nhttps://github.com/CrawlScript/MIG-GT.\n","authors":["Jun Hu","Bryan Hooi","Bingsheng He","Yinwei Wei"],"pdf_url":"https://arxiv.org/pdf/2412.13994v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13993v1","updated":"2024-12-18T16:11:45Z","published":"2024-12-18T16:11:45Z","title":"Variance-based loss function for improved regularization","summary":"  In deep learning, the mean of a chosen error metric, such as squared or\nabsolute error, is commonly used as a loss function. While effective in\nreducing the average error, this approach often fails to address localized\noutliers, leading to significant inaccuracies in regions with sharp gradients\nor discontinuities. This issue is particularly evident in physics-informed\nneural networks (PINNs), where such localized errors are expected and affect\nthe overall solution. To overcome this limitation, we propose a novel loss\nfunction that combines the mean and the standard deviation of the chosen error\nmetric. By minimizing this combined loss function, the method ensures a more\nuniform error distribution and reduces the impact of localized high-error\nregions. The proposed loss function was tested on three problems: Burger's\nequation, 2D linear elastic solid mechanics, and 2D steady Navier-Stokes,\ndemonstrating improved solution quality and lower maximum errors compared to\nthe standard mean-based loss, using the same number of iterations and weight\ninitialization.\n","authors":["John M. Hanna","Irene E. Vignon-Clemental"],"pdf_url":"https://arxiv.org/pdf/2412.13993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17286v2","updated":"2024-12-18T16:10:18Z","published":"2024-08-30T13:33:18Z","title":"Risk-averse Total-reward MDPs with ERM and EVaR","summary":"  Optimizing risk-averse objectives in discounted MDPs is challenging because\nmost models do not admit direct dynamic programming equations and require\ncomplex history-dependent policies. In this paper, we show that the risk-averse\n{\\em total reward criterion}, under the Entropic Risk Measure (ERM) and\nEntropic Value at Risk (EVaR) risk measures, can be optimized by a stationary\npolicy, making it simple to analyze, interpret, and deploy. We propose\nexponential value iteration, policy iteration, and linear programming to\ncompute optimal policies. Compared with prior work, our results only require\nthe relatively mild condition of transient MDPs and allow for {\\em both}\npositive and negative rewards. Our results indicate that the total reward\ncriterion may be preferable to the discounted criterion in a broad range of\nrisk-averse reinforcement learning domains.\n","authors":["Xihong Su","Julien Grand-Clément","Marek Petrik"],"pdf_url":"https://arxiv.org/pdf/2408.17286v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08669v3","updated":"2024-12-18T16:08:51Z","published":"2024-01-08T21:13:07Z","title":"Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems\n  with Multi-Leg Demand Routes","summary":"  Deep reinforcement learning (RL) has been shown to be effective in producing\napproximate solutions to some vehicle routing problems (VRPs), especially when\nusing policies generated by encoder-decoder attention mechanisms. While these\ntechniques have been quite successful for relatively simple problem instances,\nthere are still under-researched and highly complex VRP variants for which no\neffective RL method has been demonstrated. In this work we focus on one such\nVRP variant, which contains multiple trucks and multi-leg routing requirements.\nIn these problems, demand is required to move along sequences of nodes, instead\nof just from a start node to an end node. With the goal of making deep RL a\nviable strategy for real-world industrial-scale supply chain logistics, we\ndevelop new extensions to existing encoder-decoder attention models which allow\nthem to handle multiple trucks and multi-leg routing requirements. Our models\nhave the advantage that they can be trained for a small number of trucks and\nnodes, and then embedded into a large supply chain to yield solutions for\nlarger numbers of trucks and nodes. We test our approach on a real supply chain\nenvironment arising in the operations of Japanese automotive parts manufacturer\nAisin Corporation, and find that our algorithm outperforms Aisin's previous\nbest solution.\n","authors":["Joshua Levin","Randall Correll","Takanori Ide","Takafumi Suzuki","Takaho Saito","Alan Arai"],"pdf_url":"https://arxiv.org/pdf/2401.08669v3.pdf","comment":"This paper is more appropriate as a revised version of\n  arXiv:2211.17078, so it has been resubmitted as such"},{"id":"http://arxiv.org/abs/2412.13988v1","updated":"2024-12-18T16:07:32Z","published":"2024-12-18T16:07:32Z","title":"RAG for Effective Supply Chain Security Questionnaire Automation","summary":"  In an era where digital security is crucial, efficient processing of\nsecurity-related inquiries through supply chain security questionnaires is\nimperative. This paper introduces a novel approach using Natural Language\nProcessing (NLP) and Retrieval-Augmented Generation (RAG) to automate these\nresponses. We developed QuestSecure, a system that interprets diverse document\nformats and generates precise responses by integrating large language models\n(LLMs) with an advanced retrieval system. Our experiments show that QuestSecure\nsignificantly improves response accuracy and operational efficiency. By\nemploying advanced NLP techniques and tailored retrieval mechanisms, the system\nconsistently produces contextually relevant and semantically rich responses,\nreducing cognitive load on security teams and minimizing potential errors. This\nresearch offers promising avenues for automating complex security management\ntasks, enhancing organizational security processes.\n","authors":["Zaynab Batool Reza","Abdul Rafay Syed","Omer Iqbal","Ethel Mensah","Qian Liu","Maxx Richard Rahman","Wolfgang Maass"],"pdf_url":"https://arxiv.org/pdf/2412.13988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02987v2","updated":"2024-12-18T16:07:28Z","published":"2024-07-03T10:38:40Z","title":"LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content\n  Moderation of Large Language Models","summary":"  Guardrails have emerged as an alternative to safety alignment for content\nmoderation of large language models (LLMs). Existing model-based guardrails\nhave not been designed for resource-constrained computational portable devices,\nsuch as mobile phones, more and more of which are running LLM-based\napplications locally. We introduce LoRA-Guard, a parameter-efficient guardrail\nadaptation method that relies on knowledge sharing between LLMs and guardrail\nmodels. LoRA-Guard extracts language features from the LLMs and adapts them for\nthe content moderation task using low-rank adapters, while a dual-path design\nprevents any performance degradation on the generative task. We show that\nLoRA-Guard outperforms existing approaches with 100-1000x lower parameter\noverhead while maintaining accuracy, enabling on-device content moderation.\n","authors":["Hayder Elesedy","Pedro M. Esperança","Silviu Vlad Oprea","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2407.02987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02112v3","updated":"2024-12-18T16:07:04Z","published":"2024-07-02T09:54:39Z","title":"A Data-Centric Perspective on Evaluating Machine Learning Models for\n  Tabular Data","summary":"  Tabular data is prevalent in real-world machine learning applications, and\nnew models for supervised learning of tabular data are frequently proposed.\nComparative studies assessing the performance of models typically consist of\nmodel-centric evaluation setups with overly standardized data preprocessing.\nThis paper demonstrates that such model-centric evaluations are biased, as\nreal-world modeling pipelines often require dataset-specific preprocessing and\nfeature engineering. Therefore, we propose a data-centric evaluation framework.\nWe select 10 relevant datasets from Kaggle competitions and implement\nexpert-level preprocessing pipelines for each dataset. We conduct experiments\nwith different preprocessing pipelines and hyperparameter optimization (HPO)\nregimes to quantify the impact of model selection, HPO, feature engineering,\nand test-time adaptation. Our main findings are: 1. After dataset-specific\nfeature engineering, model rankings change considerably, performance\ndifferences decrease, and the importance of model selection reduces. 2. Recent\nmodels, despite their measurable progress, still significantly benefit from\nmanual feature engineering. This holds true for both tree-based models and\nneural networks. 3. While tabular data is typically considered static, samples\nare often collected over time, and adapting to distribution shifts can be\nimportant even in supposedly static data. These insights suggest that research\nefforts should be directed toward a data-centric perspective, acknowledging\nthat tabular data requires feature engineering and often exhibits temporal\ncharacteristics. Our framework is available under:\nhttps://github.com/atschalz/dc_tabeval.\n","authors":["Andrej Tschalzev","Sascha Marton","Stefan Lüdtke","Christian Bartelt","Heiner Stuckenschmidt"],"pdf_url":"https://arxiv.org/pdf/2407.02112v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13982v1","updated":"2024-12-18T16:03:37Z","published":"2024-12-18T16:03:37Z","title":"LeStrat-Net: Lebesgue style stratification for Monte Carlo simulations\n  powered by machine learning","summary":"  We develop a machine learning algorithm to turn around stratification in\nMonte Carlo sampling. We use a different way to divide the domain space of the\nintegrand, based on the height of the function being sampled, similar to what\nis done in Lebesgue integration. This means that isocontours of the function\ndefine regions that can have any shape depending on the behavior of the\nfunction. We take advantage of the capacity of neural networks to learn\ncomplicated functions in order to predict these complicated divisions and\npreclassify large samples of the domain space. From this preclassification we\ncan select the required number of points to perform a number of tasks such as\nvariance reduction, integration and even event selection. The network\nultimately defines the regions with what it learned and is also used to\ncalculate the multi-dimensional volume of each region.\n","authors":["Kayoung Ban","Myeonghun Park","Raymundo Ramos"],"pdf_url":"https://arxiv.org/pdf/2412.13982v1.pdf","comment":"44 pages, 17 figures"},{"id":"http://arxiv.org/abs/2412.11657v2","updated":"2024-12-18T15:56:51Z","published":"2024-12-16T11:00:02Z","title":"CNNtention: Can CNNs do better with Attention?","summary":"  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n","authors":["Julian Glattki","Nikhil Kapila","Tejas Rathi"],"pdf_url":"https://arxiv.org/pdf/2412.11657v2.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.11834v2","updated":"2024-12-18T15:53:26Z","published":"2024-12-16T14:56:28Z","title":"Wonderful Matrices: Combining for a More Efficient and Effective\n  Foundation Model Architecture","summary":"  In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.\n","authors":["Jingze Shi","Bingheng Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11834v2.pdf","comment":"The code is open-sourced at\n  https://github.com/LoserCheems/WonderfulMatrices"},{"id":"http://arxiv.org/abs/2412.13973v1","updated":"2024-12-18T15:50:50Z","published":"2024-12-18T15:50:50Z","title":"Model-Agnostic Cosmological Inference with SDSS-IV eBOSS: Simultaneous\n  Probing for Background and Perturbed Universe","summary":"  Here we explore certain subtle features imprinted in data from the completed\nSloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic\nSurvey (eBOSS) as a combined probe for the background and perturbed Universe.\nWe reconstruct the baryon Acoustic Oscillation (BAO) and Redshift Space\nDistortion (RSD) observables as functions of redshift, using measurements from\nSDSS alone. We apply the Multi-Task Gaussian Process (MTGP) framework to model\nthe interdependencies of cosmological observables $D_M(z)/r_d$, $D_H(z)/r_d$,\nand $f\\sigma_8(z)$, and track their evolution across different redshifts.\nSubsequently, we obtain constrained three-dimensional phase space containing\n$D_M(z)/r_d$, $D_H(z)/r_d$, and $f\\sigma_8(z)$ at different redshifts probed by\nthe SDSS-IV eBOSS survey. Furthermore, assuming the $\\Lambda$CDM model, we\nobtain constraints on model parameters $\\Omega_{m}$, $H_{0}r_{d}$, $\\sigma_{8}$\nand $S_{8}$ at each redshift probed by SDSS-IV eBOSS. This indicates\nredshift-dependent trends in $H_0$, $\\Omega_m$, $\\sigma_8$ and $S_8$ in the\n$\\Lambda$CDM model, suggesting a possible inconsistency in the $\\Lambda$CDM\nmodel. Ours is a template for model-independent extraction of information for\nboth background and perturbed Universe using a single galaxy survey taking into\naccount all the existing correlations between background and perturbed\nobservables and this can be easily extended to future DESI-3YR as well as\nEuclid results.\n","authors":["Purba Mukherjee","Anjan A. Sen"],"pdf_url":"https://arxiv.org/pdf/2412.13973v1.pdf","comment":"13 pages, 7 sets of figures, 3 tables. Comments are welcome"},{"id":"http://arxiv.org/abs/2408.13139v2","updated":"2024-12-18T15:46:10Z","published":"2024-08-23T15:01:37Z","title":"Optimally Solving Simultaneous-Move Dec-POMDPs: The Sequential Central\n  Planning Approach","summary":"  The centralized training for decentralized execution paradigm emerged as the\nstate-of-the-art approach to $\\epsilon$-optimally solving decentralized\npartially observable Markov decision processes. However, scalability remains a\nsignificant issue. This paper presents a novel and more scalable alternative,\nnamely the sequential-move centralized training for decentralized execution.\nThis paradigm further pushes the applicability of the Bellman's principle of\noptimality, raising three new properties. First, it allows a central planner to\nreason upon sufficient sequential-move statistics instead of prior\nsimultaneous-move ones. Next, it proves that $\\epsilon$-optimal value functions\nare piecewise linear and convex in such sufficient sequential-move statistics.\nFinally, it drops the complexity of the backup operators from double\nexponential to polynomial at the expense of longer planning horizons. Besides,\nit makes it easy to use single-agent methods, e.g., SARSA algorithm enhanced\nwith these findings, while still preserving convergence guarantees. Experiments\non two- as well as many-agent domains from the literature against\n$\\epsilon$-optimal simultaneous-move solvers confirm the superiority of our\nnovel approach. This paradigm opens the door for efficient planning and\nreinforcement learning methods for multi-agent systems.\n","authors":["Johan Peralez","Aurèlien Delage","Jacopo Castellini","Rafael F. Cunha","Jilles S. Dibangoye"],"pdf_url":"https://arxiv.org/pdf/2408.13139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13966v1","updated":"2024-12-18T15:45:08Z","published":"2024-12-18T15:45:08Z","title":"Comparative Analysis of Machine Learning-Based Imputation Techniques for\n  Air Quality Datasets with High Missing Data Rates","summary":"  Urban pollution poses serious health risks, particularly in relation to\ntraffic-related air pollution, which remains a major concern in many cities.\nVehicle emissions contribute to respiratory and cardiovascular issues,\nespecially for vulnerable and exposed road users like pedestrians and cyclists.\nTherefore, accurate air quality monitoring with high spatial resolution is\nvital for good urban environmental management. This study aims to provide\ninsights for processing spatiotemporal datasets with high missing data rates.\nIn this study, the challenge of high missing data rates is a result of the\nlimited data available and the fine granularity required for precise\nclassification of PM2.5 levels. The data used for analysis and imputation were\ncollected from both mobile sensors and fixed stations by Dynamic Parcel\nDistribution, the Environmental Protection Agency, and Google in Dublin,\nIreland, where the missing data rate was approximately 82.42%, making accurate\nParticulate Matter 2.5 level predictions particularly difficult. Various\nimputation and prediction approaches were evaluated and compared, including\nensemble methods, deep learning models, and diffusion models. External features\nsuch as traffic flow, weather conditions, and data from the nearest stations\nwere incorporated to enhance model performance. The results indicate that\ndiffusion methods with external features achieved the highest F1 score,\nreaching 0.9486 (Accuracy: 94.26%, Precision: 94.42%, Recall: 94.82%), with\nensemble models achieving the highest accuracy of 94.82%, illustrating that\ngood performance can be obtained despite a high missing data rate.\n","authors":["Sen Yan","David J. O'Connor","Xiaojun Wang","Noel E. O'Connor","Alan. F. Smeaton","Mingming Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13966v1.pdf","comment":"Accepted by IEEE CIETES 2025, with 8 pages, 3 figures, and 2 tables"},{"id":"http://arxiv.org/abs/2412.13961v1","updated":"2024-12-18T15:40:40Z","published":"2024-12-18T15:40:40Z","title":"Harvesting energy from turbulent winds with Reinforcement Learning","summary":"  Airborne Wind Energy (AWE) is an emerging technology designed to harness the\npower of high-altitude winds, offering a solution to several limitations of\nconventional wind turbines. AWE is based on flying devices (usually gliders or\nkites) that, tethered to a ground station and driven by the wind, convert its\nmechanical energy into electrical energy by means of a generator. Such systems\nare usually controlled by manoeuvering the kite so as to follow a predefined\npath prescribed by optimal control techniques, such as model-predictive\ncontrol. These methods are strongly dependent on the specific model at use and\ndifficult to generalize, especially in unpredictable conditions such as the\nturbulent atmospheric boundary layer. Our aim is to explore the possibility of\nreplacing these techniques with an approach based on Reinforcement Learning\n(RL). Unlike traditional methods, RL does not require a predefined model,\nmaking it robust to variability and uncertainty. Our experimental results in\ncomplex simulated environments demonstrate that AWE agents trained with RL can\neffectively extract energy from turbulent flows, relying on minimal local\ninformation about the kite orientation and speed relative to the wind.\n","authors":["Lorenzo Basile","Maria Grazia Berni","Antonio Celani"],"pdf_url":"https://arxiv.org/pdf/2412.13961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13957v1","updated":"2024-12-18T15:37:09Z","published":"2024-12-18T15:37:09Z","title":"Self-attentive Transformer for Fast and Accurate Postprocessing of\n  Temperature and Wind Speed Forecasts","summary":"  Current postprocessing techniques often require separate models for each lead\ntime and disregard possible inter-ensemble relationships by either correcting\neach member separately or by employing distributional approaches. In this work,\nwe tackle these shortcomings with an innovative, fast and accurate Transformer\nwhich postprocesses each ensemble member individually while allowing\ninformation exchange across variables, spatial dimensions and lead times by\nmeans of multi-headed self-attention. Weather foreacasts are postprocessed over\n20 lead times simultaneously while including up to twelve meteorological\npredictors. We use the EUPPBench dataset for training which contains ensemble\npredictions from the European Center for Medium-range Weather Forecasts'\nintegrated forecasting system alongside corresponding observations. The work\npresented here is the first to postprocess the ten and one hundred-meter wind\nspeed forecasts within this benchmark dataset, while also correcting the\ntwo-meter temperature. Our approach significantly improves the original\nforecasts, as measured by the CRPS, with 17.5 % for two-meter temperature,\nnearly 5% for ten-meter wind speed and 5.3 % for one hundred-meter wind speed,\noutperforming a classical member-by-member approach employed as competitive\nbenchmark. Furthermore, being up to 75 times faster, it fulfills the demand for\nrapid operational weather forecasts in various downstream applications,\nincluding renewable energy forecasting.\n","authors":["Aaron Van Poecke","Tobias Sebastian Finn","Ruoke Meng","Joris Van den Bergh","Geert Smet","Jonathan Demaeyer","Piet Termonia","Hossein Tabari","Peter Hellinckx"],"pdf_url":"https://arxiv.org/pdf/2412.13957v1.pdf","comment":"21 pages, 7 figures, submitted to Artificial Intelligence for the\n  Earth Systems (AIES)"},{"id":"http://arxiv.org/abs/2412.13952v1","updated":"2024-12-18T15:32:27Z","published":"2024-12-18T15:32:27Z","title":"Prompting Strategies for Enabling Large Language Models to Infer\n  Causation from Correlation","summary":"  The reasoning abilities of Large Language Models (LLMs) are attracting\nincreasing attention. In this work, we focus on causal reasoning and address\nthe task of establishing causal relationships based on correlation information,\na highly challenging problem on which several LLMs have shown poor performance.\nWe introduce a prompting strategy for this problem that breaks the original\ntask into fixed subquestions, with each subquestion corresponding to one step\nof a formal causal discovery algorithm, the PC algorithm. The proposed\nprompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps,\nby sequentially prompting it with one subquestion at a time, augmenting the\nnext subquestion's prompt with the answer to the previous one(s). We evaluate\nour approach on an existing causal benchmark, Corr2Cause: our experiments\nindicate a performance improvement across five LLMs when comparing PC-SubQ to\nbaseline prompting strategies. Results are robust to causal query\nperturbations, when modifying the variable names or paraphrasing the\nexpressions.\n","authors":["Eleni Sgouritsa","Virginia Aglietti","Yee Whye Teh","Arnaud Doucet","Arthur Gretton","Silvia Chiappa"],"pdf_url":"https://arxiv.org/pdf/2412.13952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13943v1","updated":"2024-12-18T15:25:36Z","published":"2024-12-18T15:25:36Z","title":"On Explaining Knowledge Distillation: Measuring and Visualising the\n  Knowledge Transfer Process","summary":"  Knowledge distillation (KD) remains challenging due to the opaque nature of\nthe knowledge transfer process from a Teacher to a Student, making it difficult\nto address certain issues related to KD. To address this, we proposed UniCAM, a\nnovel gradient-based visual explanation method, which effectively interprets\nthe knowledge learned during KD. Our experimental results demonstrate that with\nthe guidance of the Teacher's knowledge, the Student model becomes more\nefficient, learning more relevant features while discarding those that are not\nrelevant. We refer to the features learned with the Teacher's guidance as\ndistilled features and the features irrelevant to the task and ignored by the\nStudent as residual features. Distilled features focus on key aspects of the\ninput, such as textures and parts of objects. In contrast, residual features\ndemonstrate more diffused attention, often targeting irrelevant areas,\nincluding the backgrounds of the target objects. In addition, we proposed two\nnovel metrics: the feature similarity score (FSS) and the relevance score (RS),\nwhich quantify the relevance of the distilled knowledge. Experiments on the\nCIFAR10, ASIRRA, and Plant Disease datasets demonstrate that UniCAM and the two\nmetrics offer valuable insights to explain the KD process.\n","authors":["Gereziher Adhane","Mohammad Mahdi Dehshibi","Dennis Vetter","David Masip","Gemma Roig"],"pdf_url":"https://arxiv.org/pdf/2412.13943v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV'25). Includes 5 pages of supplementary material"},{"id":"http://arxiv.org/abs/2412.13187v2","updated":"2024-12-18T15:19:55Z","published":"2024-12-17T18:58:33Z","title":"HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction","summary":"  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results https://www.chenbao.tech/handsonvlm/\n","authors":["Chen Bao","Jiarui Xu","Xiaolong Wang","Abhinav Gupta","Homanga Bharadhwaj"],"pdf_url":"https://arxiv.org/pdf/2412.13187v2.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2412.13935v1","updated":"2024-12-18T15:18:12Z","published":"2024-12-18T15:18:12Z","title":"Spatio-Temporal Forecasting of PM2.5 via Spatial-Diffusion guided\n  Encoder-Decoder Architecture","summary":"  In many problem settings that require spatio-temporal forecasting, the values\nin the time-series not only exhibit spatio-temporal correlations but are also\ninfluenced by spatial diffusion across locations. One such example is\nforecasting the concentration of fine particulate matter (PM2.5) in the\natmosphere which is influenced by many complex factors, the most important ones\nbeing diffusion due to meteorological factors as well as transport across vast\ndistances over a period of time. We present a novel Spatio-Temporal Graph\nNeural Network architecture, that specifically captures these dependencies to\nforecast the PM2.5 concentration. Our model is based on an encoder-decoder\narchitecture where the encoder and decoder parts leverage gated recurrent units\n(GRU) augmented with a graph neural network (TransformerConv) to account for\nspatial diffusion. Our model can also be seen as a generalization of various\nexisting models for time-series or spatio-temporal forecasting. We demonstrate\nthe model's effectiveness on two real-world PM2.5 datasets: (1) data collected\nby us using a recently deployed network of low-cost PM$_{2.5}$ sensors from 511\nlocations spanning the entirety of the Indian state of Bihar over a period of\none year, and (2) another publicly available dataset that covers severely\npolluted regions from China for a period of 4 years. Our experimental results\nshow our model's impressive ability to account for both spatial as well as\ntemporal dependencies precisely.\n","authors":["Malay Pandey","Vaishali Jain","Nimit Godhani","Sachchida Nand Tripathi","Piyush Rai"],"pdf_url":"https://arxiv.org/pdf/2412.13935v1.pdf","comment":"9 pages, 4 figures, International Conference on Data Science and\n  Management of Data (CODS-COMAD), IIT Jodhpur, 2024"},{"id":"http://arxiv.org/abs/2412.13933v1","updated":"2024-12-18T15:18:05Z","published":"2024-12-18T15:18:05Z","title":"Investigating the Effects of Diffusion-based Conditional Generative\n  Speech Models Used for Speech Enhancement on Dysarthric Speech","summary":"  In this study, we aim to explore the effect of pre-trained conditional\ngenerative speech models for the first time on dysarthric speech due to\nParkinson's disease recorded in an ideal/non-noisy condition. Considering one\ncategory of generative models, i.e., diffusion-based speech enhancement, these\nmodels are previously trained to learn the distribution of clean (i.e, recorded\nin a noise-free environment) typical speech signals. Therefore, we hypothesized\nthat when being exposed to dysarthric speech they might remove the unseen\natypical paralinguistic cues during the enhancement process. By considering the\nautomatic dysarthric speech detection task, in this study, we experimentally\nshow that during the enhancement process of dysarthric speech data recorded in\nan ideal non-noisy environment, some of the acoustic dysarthric speech cues are\nlost. Therefore such pre-trained models are not yet suitable in the context of\ndysarthric speech enhancement since they manipulate the pathological speech\ncues when they process clean dysarthric speech. Furthermore, we show that the\nremoved acoustics cues by the enhancement models in the form of residue speech\nsignal can provide complementary dysarthric cues when fused with the original\ninput speech signal in the feature space.\n","authors":["Joanna Reszka","Parvaneh Janbakhshi","Tilak Purohit","Sadegh Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2412.13933v1.pdf","comment":"Accepted at ICASSP 2025 Satellite Workshop: Workshop on Speech\n  Pathology Analysis and DEtection (SPADE)"},{"id":"http://arxiv.org/abs/2412.13928v1","updated":"2024-12-18T15:12:41Z","published":"2024-12-18T15:12:41Z","title":"Preconditioned Subspace Langevin Monte Carlo","summary":"  We develop a new efficient method for high-dimensional sampling called\nSubspace Langevin Monte Carlo. The primary application of these methods is to\nefficiently implement Preconditioned Langevin Monte Carlo. To demonstrate the\nusefulness of this new method, we extend ideas from subspace descent methods in\nEuclidean space to solving a specific optimization problem over Wasserstein\nspace. Our theoretical analysis demonstrates the advantageous convergence\nregimes of the proposed method, which depend on relative conditioning\nassumptions common to mirror descent methods. We back up our theory with\nexperimental evidence on sampling from an ill-conditioned Gaussian\ndistribution.\n","authors":["Tyler Maunu","Jiayi Yao"],"pdf_url":"https://arxiv.org/pdf/2412.13928v1.pdf","comment":"19 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2412.13924v1","updated":"2024-12-18T15:07:23Z","published":"2024-12-18T15:07:23Z","title":"Language verY Rare for All","summary":"  In the quest to overcome language barriers, encoder-decoder models like NLLB\nhave expanded machine translation to rare languages, with some models (e.g.,\nNLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform\nwell in translation, open LLMs prove highly competitive when fine-tuned for\nspecific tasks involving unknown corpora. We introduce LYRA (Language verY Rare\nfor All), a novel approach that combines open LLM fine-tuning,\nretrieval-augmented generation (RAG), and transfer learning from related\nhigh-resource languages. This study is exclusively focused on single-GPU\ntraining to facilitate ease of adoption. Our study focuses on two-way\ntranslation between French and Mon\\'egasque, a rare language unsupported by\nexisting translation tools due to limited corpus availability. Our results\ndemonstrate LYRA's effectiveness, frequently surpassing and consistently\nmatching state-of-the-art encoder-decoder models in rare language translation.\n","authors":["Ibrahim Merad","Amos Wolf","Ziad Mazzawi","Yannick Léo"],"pdf_url":"https://arxiv.org/pdf/2412.13924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13922v1","updated":"2024-12-18T15:05:59Z","published":"2024-12-18T15:05:59Z","title":"Pipeline Analysis for Developing Instruct LLMs in Low-Resource\n  Languages: A Case Study on Basque","summary":"  Large language models (LLMs) are typically optimized for resource-rich\nlanguages like English, exacerbating the gap between high-resource and\nunderrepresented languages. This work presents a detailed analysis of\nstrategies for developing a model capable of following instructions in a\nlow-resource language, specifically Basque, by focusing on three key stages:\npre-training, instruction tuning, and alignment with human preferences. Our\nfindings demonstrate that continual pre-training with a high-quality Basque\ncorpus of around 600 million words improves natural language understanding\n(NLU) of the foundational model by over 12 points. Moreover, instruction tuning\nand human preference alignment using automatically translated datasets proved\nhighly effective, resulting in a 24-point improvement in instruction-following\nperformance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct,\nestablish a new state-of-the-art for Basque in the sub-10B parameter category.\n","authors":["Ander Corral","Ixak Sarasua","Xabier Saralegi"],"pdf_url":"https://arxiv.org/pdf/2412.13922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01638v4","updated":"2024-12-18T15:01:32Z","published":"2024-06-03T00:27:29Z","title":"TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via\n  Cross-Modality Alignment","summary":"  Multivariate time series forecasting (MTSF) aims to learn temporal dynamics\namong variables to forecast future time series. Existing statistical and deep\nlearning-based methods suffer from limited learnable parameters and small-scale\ntraining data. Recently, large language models (LLMs) combining time series\nwith textual prompts have achieved promising performance in MTSF. However, we\ndiscovered that current LLM-based solutions fall short in learning disentangled\nembeddings. We introduce TimeCMA, an intuitive yet effective framework for MTSF\nvia cross-modality alignment. Specifically, we present a dual-modality encoding\nwith two branches: the time series encoding branch extracts disentangled yet\nweak time series embeddings, and the LLM-empowered encoding branch wraps the\nsame time series with text as prompts to obtain entangled yet robust prompt\nembeddings. As a result, such a cross-modality alignment retrieves both\ndisentangled and robust time series embeddings, ``the best of two worlds'',\nfrom the prompt embeddings based on time series and prompt modality\nsimilarities. As another key design, to reduce the computational costs from\ntime series with their length textual prompts, we design an effective prompt to\nencourage the most essential temporal information to be encapsulated in the\nlast token: only the last token is passed to downstream prediction. We further\nstore the last token embeddings to accelerate inference speed. Extensive\nexperiments on eight real datasets demonstrate that TimeCMA outperforms\nstate-of-the-arts.\n","authors":["Chenxi Liu","Qianxiong Xu","Hao Miao","Sun Yang","Lingzheng Zhang","Cheng Long","Ziyue Li","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.01638v4.pdf","comment":"Accepted by AAAI 2025 (Main Technical Track)"},{"id":"http://arxiv.org/abs/2412.13917v1","updated":"2024-12-18T14:57:06Z","published":"2024-12-18T14:57:06Z","title":"Speech Watermarking with Discrete Intermediate Representations","summary":"  Speech watermarking techniques can proactively mitigate the potential harmful\nconsequences of instant voice cloning techniques. These techniques involve the\ninsertion of signals into speech that are imperceptible to humans but can be\ndetected by algorithms. Previous approaches typically embed watermark messages\ninto continuous space. However, intuitively, embedding watermark information\ninto robust discrete latent space can significantly improve the robustness of\nwatermarking systems. In this paper, we propose DiscreteWM, a novel speech\nwatermarking framework that injects watermarks into the discrete intermediate\nrepresentations of speech. Specifically, we map speech into discrete latent\nspace with a vector-quantized autoencoder and inject watermarks by changing the\nmodular arithmetic relation of discrete IDs. To ensure the imperceptibility of\nwatermarks, we also propose a manipulator model to select the candidate tokens\nfor watermark embedding. Experimental results demonstrate that our framework\nachieves state-of-the-art performance in robustness and imperceptibility,\nsimultaneously. Moreover, our flexible frame-wise approach can serve as an\nefficient solution for both voice cloning detection and information hiding.\nAdditionally, DiscreteWM can encode 1 to 150 bits of watermark information\nwithin a 1-second speech clip, indicating its encoding capacity. Audio samples\nare available at https://DiscreteWM.github.io/discrete_wm.\n","authors":["Shengpeng Ji","Ziyue Jiang","Jialong Zuo","Minghui Fang","Yifu Chen","Tao Jin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.13917v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2411.11896v2","updated":"2024-12-18T14:54:33Z","published":"2024-11-08T14:25:00Z","title":"HeartBERT: A Self-Supervised ECG Embedding Model for Efficient and\n  Effective Medical Signal Analysis","summary":"  The HeartBert model is introduced with three primary objectives: reducing the\nneed for labeled data, minimizing computational resources, and simultaneously\nimproving performance in machine learning systems that analyze\nElectrocardiogram (ECG) signals. Inspired by Bidirectional Encoder\nRepresentations from Transformers (BERT) in natural language processing and\nenhanced with a self-supervised learning approach, the HeartBert model-built on\nthe RoBERTa architecture-generates sophisticated embeddings tailored for\nECG-based projects in the medical domain. To demonstrate the versatility,\ngeneralizability, and efficiency of the proposed model, two key downstream\ntasks have been selected: sleep stage detection and heartbeat classification.\nHeartBERT-based systems, utilizing bidirectional LSTM heads, are designed to\naddress complex challenges. A series of practical experiments have been\nconducted to demonstrate the superiority and advancements of HeartBERT,\nparticularly in terms of its ability to perform well with smaller training\ndatasets, reduced learning parameters, and effective performance compared to\nrival models. The code and data are publicly available at\nhttps://github.com/ecgResearch/HeartBert.\n","authors":["Saedeh Tahery","Fatemeh Hamid Akhlaghi","Termeh Amirsoleimani","Saeed Farzi","Carlo Strapparava"],"pdf_url":"https://arxiv.org/pdf/2411.11896v2.pdf","comment":"Second version, 27 pages, 11 Figures, 7 Tables"},{"id":"http://arxiv.org/abs/2404.02692v2","updated":"2024-12-18T14:47:17Z","published":"2024-04-03T12:39:37Z","title":"Automated Inference of Graph Transformation Rules","summary":"  The explosion of data available in life sciences is fueling an increasing\ndemand for expressive models and computational methods. Graph transformation is\na model for dynamic systems with a large variety of applications. We introduce\na novel method of the graph transformation model construction, combining\ngenerative and dynamical viewpoints to give a fully automated data-driven model\ninference method.\n  The method takes the input dynamical properties, given as a \"snapshot\" of the\ndynamics encoded by explicit transitions, and constructs a compatible model.\nThe obtained model is guaranteed to be minimal, thus framing the approach as\nmodel compression (from a set of transitions into a set of rules). The\ncompression is permissive to a lossy case, where the constructed model is\nallowed to exhibit behavior outside of the input transitions, thus suggesting a\ncompletion of the input dynamics.\n  The task of graph transformation model inference is naturally highly\nchallenging due to the combinatorics involved. We tackle the exponential\nexplosion by proposing a heuristically minimal translation of the task into a\nwell-established problem, set cover, for which highly optimized solutions\nexist. We further showcase how our results relate to Kolmogorov complexity\nexpressed in terms of graph transformation.\n","authors":["Jakob L. Andersen","Akbar Davoodi","Rolf Fagerberg","Christoph Flamm","Walter Fontana","Juri Kolčák","Christophe V. F. P. Laurent","Daniel Merkle","Nikolai Nøjgaard"],"pdf_url":"https://arxiv.org/pdf/2404.02692v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2307.08079v4","updated":"2024-12-18T14:46:23Z","published":"2023-07-16T15:31:32Z","title":"Flexible and efficient emulation of spatial extremes processes via\n  variational autoencoders","summary":"  Many real-world processes have complex tail dependence structures that cannot\nbe characterized using classical Gaussian processes. More flexible spatial\nextremes models exhibit appealing extremal dependence properties but are often\nexceedingly prohibitive to fit and simulate from in high dimensions. In this\npaper, we aim to push the boundaries on computation and modeling of\nhigh-dimensional spatial extremes via integrating a new spatial extremes model\nthat has flexible and non-stationary dependence properties in the\nencoding-decoding structure of a variational autoencoder called the XVAE. The\nXVAE can emulate spatial observations and produce outputs that have the same\nstatistical properties as the inputs, especially in the tail. Our approach also\nprovides a novel way of making fast inference with complex extreme-value\nprocesses. Through extensive simulation studies, we show that our XVAE is\nsubstantially more time-efficient than traditional Bayesian inference while\noutperforming many spatial extremes models with a stationary dependence\nstructure. Lastly, we analyze a high-resolution satellite-derived dataset of\nsea surface temperature in the Red Sea, which includes 30 years of daily\nmeasurements at 16703 grid cells. We demonstrate how to use XVAE to identify\nregions susceptible to marine heatwaves under climate change and examine the\nspatial and temporal variability of the extremal dependence structure.\n","authors":["Likun Zhang","Xiaoyu Ma","Christopher K. Wikle","Raphaël Huser"],"pdf_url":"https://arxiv.org/pdf/2307.08079v4.pdf","comment":"30 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.13902v1","updated":"2024-12-18T14:42:43Z","published":"2024-12-18T14:42:43Z","title":"Threshold Neuron: A Brain-inspired Artificial Neuron for Efficient\n  On-device Inference","summary":"  Enhancing the computational efficiency of on-device Deep Neural Networks\n(DNNs) remains a significant challengein mobile and edge computing. As we aim\nto execute increasingly complex tasks with constrained computational resources,\nmuch of the research has focused on compressing neural network structures and\noptimizing systems. Although many studies have focused on compressing neural\nnetwork structures and parameters or optimizing underlying systems, there has\nbeen limited attention on optimizing the fundamental building blocks of neural\nnetworks: the neurons. In this study, we deliberate on a simple but important\nresearch question: Can we design artificial neurons that offer greater\nefficiency than the traditional neuron paradigm? Inspired by the threshold\nmechanisms and the excitation-inhibition balance observed in biological\nneurons, we propose a novel artificial neuron model, Threshold Neurons. Using\nThreshold Neurons, we can construct neural networks similar to those with\ntraditional artificial neurons, while significantly reducing hardware\nimplementation complexity. Our extensive experiments validate the effectiveness\nof neural networks utilizing Threshold Neurons, achieving substantial power\nsavings of 7.51x to 8.19x and area savings of 3.89x to 4.33x at the kernel\nlevel, with minimal loss in precision. Furthermore, FPGA-based implementations\nof these networks demonstrate 2.52x power savings and 1.75x speed enhancements\nat the system level. The source code will be made available upon publication.\n","authors":["Zihao Zheng","Yuanchun Li","Jiayu Chen","Peng Zhou","Xiang Chen","Yunxin Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13902v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.13897v1","updated":"2024-12-18T14:39:43Z","published":"2024-12-18T14:39:43Z","title":"Data-Efficient Inference of Neural Fluid Fields via SciML Foundation\n  Model","summary":"  Recent developments in 3D vision have enabled successful progress in\ninferring neural fluid fields and realistic rendering of fluid dynamics.\nHowever, these methods require real-world flow captures, which demand dense\nvideo sequences and specialized lab setups, making the process costly and\nchallenging. Scientific machine learning (SciML) foundation models, which are\npretrained on extensive simulations of partial differential equations (PDEs),\nencode rich multiphysics knowledge and thus provide promising sources of domain\npriors for inferring fluid fields. Nevertheless, their potential to advance\nreal-world vision problems remains largely underexplored, raising questions\nabout the transferability and practical utility of these foundation models. In\nthis work, we demonstrate that SciML foundation model can significantly improve\nthe data efficiency of inferring real-world 3D fluid dynamics with improved\ngeneralization. At the core of our method is leveraging the strong forecasting\ncapabilities and meaningful representations of SciML foundation models. We\nequip neural fluid fields with a novel collaborative training approach that\nutilizes augmented views and fluid features extracted by our foundation model.\nOur method demonstrates significant improvements in both quantitative metrics\nand visual quality, showcasing the practical applicability of SciML foundation\nmodels in real-world fluid dynamics.\n","authors":["Yuqiu Liu","Jingxuan Xu","Mauricio Soroco","Yunchao Wei","Wuyang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08074v3","updated":"2024-12-18T14:38:31Z","published":"2024-08-15T11:01:35Z","title":"A Survey on Integrated Sensing, Communication, and Computation","summary":"  The forthcoming generation of wireless technology, 6G, aims to usher in an\nera of ubiquitous intelligent services, where everything is interconnected and\nintelligent. This vision requires the seamless integration of three fundamental\nmodules: Sensing for information acquisition, communication for information\nsharing, and computation for information processing and decision-making. These\nmodules are intricately linked, especially in complex tasks such as edge\nlearning and inference. However, the performance of these modules is\ninterdependent, creating a resource competition for time, energy, and\nbandwidth. Existing techniques like integrated communication and computation\n(ICC), integrated sensing and computation (ISC), and integrated sensing and\ncommunication (ISAC) have made partial strides in addressing this challenge,\nbut they fall short of meeting the extreme performance requirements. To\novercome these limitations, it is essential to develop new techniques that\ncomprehensively integrate sensing, communication, and computation. This\nintegrated approach, known as Integrated Sensing, Communication, and\nComputation (ISCC), offers a systematic perspective for enhancing task\nperformance. This paper begins with a comprehensive survey of historic and\nrelated techniques such as ICC, ISC, and ISAC, highlighting their strengths and\nlimitations. It then discusses the benefits, functions, and challenges of ISCC.\nSubsequently, the state-of-the-art signal designs for ISCC, along with network\nresource management strategies specifically tailored for ISCC are explored.\nFurthermore, this paper discusses the exciting research opportunities that lie\nahead for implementing ISCC in future advanced networks, and the unresolved\nissues requiring further investigation. ISCC is expected to unlock the full\npotential of intelligent connectivity, paving the way for groundbreaking\napplications and services.\n","authors":["Dingzhu Wen","Yong Zhou","Xiaoyang Li","Yuanming Shi","Kaibin Huang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2408.08074v3.pdf","comment":"This version is accepted by IEEE Communications Surveys & Tutorials\n  on Dec. 18, 2024"},{"id":"http://arxiv.org/abs/2301.12210v3","updated":"2024-12-18T14:36:47Z","published":"2023-01-28T14:32:14Z","title":"Neural Temporal Point Processes for Forecasting Directional Relations in\n  Evolving Hypergraphs","summary":"  Forecasting relations between entities is paramount in the current era of\ndata and AI. However, it is often overlooked that real-world relationships are\ninherently directional, involve more than two entities, and can change with\ntime. In this paper, we provide a comprehensive solution to the problem of\nforecasting directional relations in a general setting, where relations are\nhigher-order, i.e., directed hyperedges in a hypergraph. This problem has not\nbeen previously explored in the existing literature. The primary challenge in\nsolving this problem is that the number of possible hyperedges is exponential\nin the number of nodes at each event time. To overcome this, we propose a\nsequential generative approach that segments the forecasting process into\nmultiple stages, each contingent upon the preceding stages, thereby reducing\nthe search space involved in predictions of hyperedges. The first stage\ninvolves a temporal point process-based node event forecasting module that\nidentifies the subset of nodes involved in an event. The second stage is a\ncandidate generation module that predicts hyperedge sizes and adjacency vectors\nfor nodes observing events. The final stage is a directed hyperedge predictor\nthat identifies the truth by searching over the set of candidate hyperedges. To\nvalidate the effectiveness of our model, we compiled five datasets and\nconducted an extensive empirical study to assess each downstream task. Our\nproposed method achieves a performance gain of 32\\% and 41\\% compared to the\nstate-of-the-art pairwise and hyperedge event forecasting models, respectively,\nfor the event type prediction.\n","authors":["Tony Gracious","Arman Gupta","Ambedkar Dukkipati"],"pdf_url":"https://arxiv.org/pdf/2301.12210v3.pdf","comment":"AAAI-2025"},{"id":"http://arxiv.org/abs/2412.13891v1","updated":"2024-12-18T14:32:30Z","published":"2024-12-18T14:32:30Z","title":"Graph-Driven Models for Gas Mixture Identification and Concentration\n  Estimation on Heterogeneous Sensor Array Signals","summary":"  Accurately identifying gas mixtures and estimating their concentrations are\ncrucial across various industrial applications using gas sensor arrays.\nHowever, existing models face challenges in generalizing across heterogeneous\ndatasets, which limits their scalability and practical applicability. To\naddress this problem, this study develops two novel deep-learning models that\nintegrate temporal graph structures for enhanced performance: a Graph-Enhanced\nCapsule Network (GraphCapsNet) employing dynamic routing for gas mixture\nclassification and a Graph-Enhanced Attention Network (GraphANet) leveraging\nself-attention for concentration estimation. Both models were validated on\ndatasets from the University of California, Irvine (UCI) Machine Learning\nRepository and a custom dataset, demonstrating superior performance in gas\nmixture identification and concentration estimation compared to recent models.\nIn classification tasks, GraphCapsNet achieved over 98.00% accuracy across\nmultiple datasets, while in concentration estimation, GraphANet attained an R2\nscore exceeding 0.96 across various gas components. Both GraphCapsNet and\nGraphANet exhibited significantly higher accuracy and stability, positioning\nthem as promising solutions for scalable gas analysis in industrial settings.\n","authors":["Ding Wang","Lei Wang","Huilin Yin","Guoqing Gu","Zhiping Lin","Wenwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18416v3","updated":"2024-12-18T14:25:08Z","published":"2024-07-25T22:24:45Z","title":"PersonaGym: Evaluating Persona Agents and LLMs","summary":"  Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents.\n","authors":["Vinay Samuel","Henry Peng Zou","Yue Zhou","Shreyas Chaudhari","Ashwin Kalyan","Tanmay Rajpurohit","Ameet Deshpande","Karthik Narasimhan","Vishvak Murahari"],"pdf_url":"https://arxiv.org/pdf/2407.18416v3.pdf","comment":"21 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.23111v3","updated":"2024-12-18T14:10:10Z","published":"2024-10-30T15:23:44Z","title":"Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.\n","authors":["Navyansh Mahla","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2410.23111v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13866v1","updated":"2024-12-18T14:02:15Z","published":"2024-12-18T14:02:15Z","title":"SHAP scores fail pervasively even when Lipschitz succeeds","summary":"  The ubiquitous use of Shapley values in eXplainable AI (XAI) has been\ntriggered by the tool SHAP, and as a result are commonly referred to as SHAP\nscores. Recent work devised examples of machine learning (ML) classifiers for\nwhich the computed SHAP scores are thoroughly unsatisfactory, by allowing human\ndecision-makers to be misled. Nevertheless, such examples could be perceived as\nsomewhat artificial, since the selected classes must be interpreted as numeric.\nFurthermore, it was unclear how general were the issues identified with SHAP\nscores. This paper answers these criticisms. First, the paper shows that for\nBoolean classifiers there are arbitrarily many examples for which the SHAP\nscores must be deemed unsatisfactory. Second, the paper shows that the issues\nwith SHAP scores are also observed in the case of regression models. In\naddition, the paper studies the class of regression models that respect\nLipschitz continuity, a measure of a function's rate of change that finds\nimportant recent uses in ML, including model robustness. Concretely, the paper\nshows that the issues with SHAP scores occur even for regression models that\nrespect Lipschitz continuity. Finally, the paper shows that the same issues are\nguaranteed to exist for arbitrarily differentiable regression models.\n","authors":["Olivier Letoffe","Xuanxiang Huang","Joao Marques-Silva"],"pdf_url":"https://arxiv.org/pdf/2412.13866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13864v1","updated":"2024-12-18T14:01:15Z","published":"2024-12-18T14:01:15Z","title":"Constructing sensible baselines for Integrated Gradients","summary":"  Machine learning methods have seen a meteoric rise in their applications in\nthe scientific community. However, little effort has been put into\nunderstanding these \"black box\" models. We show how one can apply integrated\ngradients (IGs) to understand these models by designing different baselines, by\ntaking an example case study in particle physics. We find that the zero-vector\nbaseline does not provide good feature attributions and that an averaged\nbaseline sampled from the background events provides consistently more\nreasonable attributions.\n","authors":["Jai Bardhan","Cyrin Neeraj","Mihir Rawat","Subhadip Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.13864v1.pdf","comment":"7 pages, 5 figures. Accepted to 4th Annual AAAI Workshop on AI to\n  Accelerate Science and Engineering (AI2ASE)"},{"id":"http://arxiv.org/abs/2412.13862v1","updated":"2024-12-18T13:55:42Z","published":"2024-12-18T13:55:42Z","title":"Energy-Based Preference Model Offers Better Offline Alignment than the\n  Bradley-Terry Preference Model","summary":"  Since the debut of DPO, it has been shown that aligning a target LLM with\nhuman preferences via the KL-constrained RLHF loss is mathematically equivalent\nto a special kind of reward modeling task. Concretely, the task requires: 1)\nusing the target LLM to parameterize the reward model, and 2) tuning the reward\nmodel so that it has a 1:1 linear relationship with the true reward. However,\nwe identify a significant issue: the DPO loss might have multiple minimizers,\nof which only one satisfies the required linearity condition. The problem\narises from a well-known issue of the underlying Bradley-Terry preference\nmodel: it does not always have a unique maximum likelihood estimator (MLE).\nConsequently,the minimizer of the RLHF loss might be unattainable because it is\nmerely one among many minimizers of the DPO loss. As a better alternative, we\npropose an energy-based model (EBM) that always has a unique MLE, inherently\nsatisfying the linearity requirement. To approximate the MLE in practice, we\npropose a contrastive loss named Energy Preference Alignment (EPA), wherein\neach positive sample is contrasted against one or more strong negatives as well\nas many free weak negatives. Theoretical properties of our EBM enable the\napproximation error of EPA to almost surely vanish when a sufficient number of\nnegatives are used. Empirically, we demonstrate that EPA consistently delivers\nbetter performance on open benchmarks compared to DPO, thereby showing the\nsuperiority of our EBM.\n","authors":["Yuzhong Hong","Hanshan Zhang","Junwei Bao","Hongfei Jiang","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2412.13862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13860v1","updated":"2024-12-18T13:53:59Z","published":"2024-12-18T13:53:59Z","title":"Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation\n  on Nepali","summary":"  Continual learning has emerged as an important research direction due to the\ninfeasibility of retraining large language models (LLMs) from scratch in the\nevent of new data availability. Of great interest is the domain-adaptive\npre-training (DAPT) paradigm, which focuses on continually training a\npre-trained language model to adapt it to a domain it was not originally\ntrained on. In this work, we evaluate the feasibility of DAPT in a low-resource\nsetting, namely the Nepali language. We use synthetic data to continue training\nLlama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We\nevaluate the adapted model on its performance, forgetting, and knowledge\nacquisition. We compare the base model and the final model on their Nepali\ngeneration abilities, their performance on popular benchmarks, and run\ncase-studies to probe their linguistic knowledge in Nepali. We see some\nunsurprising forgetting in the final model, but also surprisingly find that\nincreasing the number of shots during evaluation yields better percent\nincreases in the final model (as high as 19.29% increase) compared to the base\nmodel (4.98%), suggesting latent retention. We also explore layer-head\nself-attention heatmaps to establish dependency resolution abilities of the\nfinal model in Nepali.\n","authors":["Sharad Duwal","Suraj Prasai","Suresh Manandhar"],"pdf_url":"https://arxiv.org/pdf/2412.13860v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13858v1","updated":"2024-12-18T13:52:50Z","published":"2024-12-18T13:52:50Z","title":"IDEQ: an improved diffusion model for the TSP","summary":"  We investigate diffusion models to solve the Traveling Salesman Problem.\nBuilding on the recent DIFUSCO and T2TCO approaches, we propose IDEQ. IDEQ\nimproves the quality of the solutions by leveraging the constrained structure\nof the state space of the TSP. Another key component of IDEQ consists in\nreplacing the last stages of DIFUSCO curriculum learning by considering a\nuniform distribution over the Hamiltonian tours whose orbits by the 2-opt\noperator converge to the optimal solution as the training objective. Our\nexperiments show that IDEQ improves the state of the art for such neural\nnetwork based techniques on synthetic instances. More importantly, our\nexperiments show that IDEQ performs very well on the instances of the TSPlib, a\nreference benchmark in the TSP community: it closely matches the performance of\nthe best heuristics, LKH3, being even able to obtain better solutions than LKH3\non 2 instances of the TSPlib defined on 1577 and 3795 cities. IDEQ obtains 0.3%\noptimality gap on TSP instances made of 500 cities, and 0.5% on TSP instances\nwith 1000 cities. This sets a new SOTA for neural based methods solving the\nTSP. Moreover, IDEQ exhibits a lower variance and better scales-up with the\nnumber of cities with regards to DIFUSCO and T2TCO.\n","authors":["Mickael Basson","Philippe Preux"],"pdf_url":"https://arxiv.org/pdf/2412.13858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13857v1","updated":"2024-12-18T13:52:42Z","published":"2024-12-18T13:52:42Z","title":"Diagnosising Helicobacter pylori using AutoEncoders and Limited\n  Annotations through Anomalous Staining Patterns in IHC Whole Slide Images","summary":"  Purpose: This work addresses the detection of Helicobacter pylori (H. pylori)\nin histological images with immunohistochemical staining. This analysis is a\ntime demanding task, currently done by an expert pathologist that visually\ninspects the samples. Given the effort required to localise the pathogen in\nimages, a limited number of annotations might be available in an initial\nsetting. Our goal is to design an approach that, using a limited set of\nannotations, is capable of obtaining results good enough to be used as a\nsupport tool. Methods: We propose to use autoencoders to learn the latent\npatterns of healthy patches and formulate a specific measure of the\nreconstruction error of the image in HSV space. ROC analysis is used to set the\noptimal threshold of this measure and the percentage of positive patches in a\nsample that determines the presence of H. pylori. Results: Our method has been\ntested on an own database of 245 Whole Slide Images (WSI) having 117 cases\nwithout H. pylori and different density of the bacteria in the remaining ones.\nThe database has 1211 annotated patches, with only 163 positive patches. This\ndataset of positive annotations was used to train a baseline thresholding and\nan SVM using the features of a pre-trained RedNet18 and ViT models. A 10-fold\ncross-validation shows that our method has better performance with 91%\naccuracy, 86% sensitivity, 96% specificity and 0.97 AUC in the diagnosis of H.\npylori. Conclusion: Unlike classification approaches, our shallow autoencoder\nwith threshold adaptation for the detection of anomalous staining is able to\nachieve competitive results with a limited set of annotated data. This initial\napproach is good enough to be used as a guide for fast annotation of infected\npatches.\n","authors":["Pau Cano","Eva Musulen","Debora Gil"],"pdf_url":"https://arxiv.org/pdf/2412.13857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13852v1","updated":"2024-12-18T13:47:37Z","published":"2024-12-18T13:47:37Z","title":"RadField3D: A Data Generator and Data Format for Deep Learning in\n  Radiation-Protection Dosimetry for Medical Applications","summary":"  In this research work, we present our open-source Geant4-based Monte-Carlo\nsimulation application, called RadField3D, for generating threedimensional\nradiation field datasets for dosimetry. Accompanying, we introduce a fast,\nmachine-interpretable data format with a Python API for easy integration into\nneural network research, that we call RadFiled3D. Both developments are\nintended to be used to research alternative radiation simulation methods using\ndeep learning.\n","authors":["Felix Lehner","Pasquale Lombardo","Susana Castillo","Oliver Hupe","Marcus Magnor"],"pdf_url":"https://arxiv.org/pdf/2412.13852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00620v2","updated":"2024-12-18T13:43:43Z","published":"2024-10-01T12:05:18Z","title":"Differentiable Interacting Multiple Model Particle Filtering","summary":"  We propose a sequential Monte Carlo algorithm for parameter learning when the\nstudied model exhibits random discontinuous jumps in behaviour. To facilitate\nthe learning of high dimensional parameter sets, such as those associated to\nneural networks, we adopt the emerging framework of differentiable particle\nfiltering, wherein parameters are trained by gradient descent. We design a new\ndifferentiable interacting multiple model particle filter to be capable of\nlearning the individual behavioural regimes and the model which controls the\njumping simultaneously. In contrast to previous approaches, our algorithm\nallows control of the computational effort assigned per regime whilst using the\nprobability of being in a given regime to guide sampling. Furthermore, we\ndevelop a new gradient estimator that has a lower variance than established\napproaches and remains fast to compute, for which we prove consistency. We\nestablish new theoretical results of the presented algorithms and demonstrate\nsuperior numerical performance compared to the previous state-of-the-art\nalgorithms.\n","authors":["John-Joseph Brady","Yuhui Luo","Wenwu Wang","Víctor Elvira","Yunpeng Li"],"pdf_url":"https://arxiv.org/pdf/2410.00620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13847v1","updated":"2024-12-18T13:40:21Z","published":"2024-12-18T13:40:21Z","title":"A Concept-Centric Approach to Multi-Modality Learning","summary":"  In an effort to create a more efficient AI system, we introduce a new\nmulti-modality learning framework that leverages a modality-agnostic concept\nspace possessing abstract knowledge and a set of modality-specific projection\nmodels tailored to process distinct modality inputs and map them onto the\nconcept space. Decoupled from specific modalities and their associated\nprojection models, the concept space focuses on learning abstract knowledge\nthat is universally applicable across modalities. Subsequently, the knowledge\nembedded into the concept space streamlines the learning processes of\nmodality-specific projection models. We evaluate our framework on two popular\ntasks: Image-Text Matching and Visual Question Answering. Our framework\nachieves performance on par with benchmark models while demonstrating more\nefficient learning curves.\n","authors":["Yuchong Geng","Ao Tang"],"pdf_url":"https://arxiv.org/pdf/2412.13847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13845v1","updated":"2024-12-18T13:38:06Z","published":"2024-12-18T13:38:06Z","title":"Do Language Models Understand Time?","summary":"  Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond.\n","authors":["Xi Ding","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13845v1.pdf","comment":"Research report"},{"id":"http://arxiv.org/abs/2412.13842v1","updated":"2024-12-18T13:36:03Z","published":"2024-12-18T13:36:03Z","title":"Graph Coarsening via Supervised Granular-Ball for Scalable Graph Neural\n  Network Training","summary":"  Graph Neural Networks (GNNs) have demonstrated significant achievements in\nprocessing graph data, yet scalability remains a substantial challenge. To\naddress this, numerous graph coarsening methods have been developed. However,\nmost existing coarsening methods are training-dependent, leading to lower\nefficiency, and they all require a predefined coarsening rate, lacking an\nadaptive approach. In this paper, we employ granular-ball computing to\neffectively compress graph data. We construct a coarsened graph network by\niteratively splitting the graph into granular-balls based on a purity threshold\nand using these granular-balls as super vertices. This granulation process\nsignificantly reduces the size of the original graph, thereby greatly enhancing\nthe training efficiency and scalability of GNNs. Additionally, our algorithm\ncan adaptively perform splitting without requiring a predefined coarsening\nrate. Experimental results demonstrate that our method achieves accuracy\ncomparable to training on the original graph. Noise injection experiments\nfurther indicate that our method exhibits robust performance. Moreover, our\napproach can reduce the graph size by up to 20 times without compromising test\naccuracy, substantially enhancing the scalability of GNNs.\n","authors":["Shuyin Xia","Xinjun Ma","Zhiyuan Liu","Cheng Liu","Sen Zhao","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13840v1","updated":"2024-12-18T13:33:28Z","published":"2024-12-18T13:33:28Z","title":"Unleashing the Power of Continual Learning on Non-Centralized Devices: A\n  Survey","summary":"  Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for\nenabling distributed devices such as vehicles and servers to handle streaming\ndata from a joint non-stationary environment. To achieve high reliability and\nscalability in deploying this paradigm in distributed systems, it is essential\nto conquer challenges stemming from both spatial and temporal dimensions,\nmanifesting as distribution shifts, catastrophic forgetting, heterogeneity, and\nprivacy issues. This survey focuses on a comprehensive examination of the\ndevelopment of the non-centralized continual learning algorithms and the\nreal-world deployment across distributed devices. We begin with an introduction\nto the background and fundamentals of non-centralized learning and continual\nlearning. Then, we review existing solutions from three levels to represent how\nexisting techniques alleviate the catastrophic forgetting and distribution\nshift. Additionally, we delve into the various types of heterogeneity issues,\nsecurity, and privacy attributes, as well as real-world applications across\nthree prevalent scenarios. Furthermore, we establish a large-scale benchmark to\nrevisit this problem and analyze the performance of the state-of-the-art NCCL\napproaches. Finally, we discuss the important challenges and future research\ndirections in NCCL.\n","authors":["Yichen Li","Haozhao Wang","Wenchao Xu","Tianzhe Xiao","Hong Liu","Minzhu Tu","Yuying Wang","Xin Yang","Rui Zhang","Shui Yu","Song Guo","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2412.13840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13834v1","updated":"2024-12-18T13:24:09Z","published":"2024-12-18T13:24:09Z","title":"Maybe you are looking for CroQS: Cross-modal Query Suggestion for\n  Text-to-Image Retrieval","summary":"  Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/\n","authors":["Giacomo Pacini","Fabio Carrara","Nicola Messina","Nicola Tonellotto","Giuseppe Amato","Fabrizio Falchi"],"pdf_url":"https://arxiv.org/pdf/2412.13834v1.pdf","comment":"15 pages, 5 figures. To be published as full paper in the Proceedings\n  of the European Conference on Information Retrieval (ECIR) 2025"},{"id":"http://arxiv.org/abs/2412.13810v1","updated":"2024-12-18T12:57:56Z","published":"2024-12-18T12:57:56Z","title":"CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers?","summary":"  We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design.\nOur approach is based on a powerful Vision and Large Language Model (VLLM) as a\nplanner and a tool-augmentation paradigm using CAD-specific modules.\nCAD-Assistant addresses multimodal user queries by generating actions that are\niteratively executed on a Python interpreter equipped with the FreeCAD\nsoftware, accessed via its Python API. Our framework is able to assess the\nimpact of generated CAD commands on geometry and adapts subsequent actions\nbased on the evolving state of the CAD design. We consider a wide range of\nCAD-specific tools including Python libraries, modules of the FreeCAD Python\nAPI, helpful routines, rendering functions and other specialized modules. We\nevaluate our method on multiple CAD benchmarks and qualitatively demonstrate\nthe potential of tool-augmented VLLMs as generic CAD task solvers across\ndiverse CAD workflows.\n","authors":["Dimitrios Mallis","Ahmet Serdar Karadeniz","Sebastian Cavada","Danila Rukhovich","Niki Foteinopoulou","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2412.13810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13809v1","updated":"2024-12-18T12:57:49Z","published":"2024-12-18T12:57:49Z","title":"Extreme Multi-label Completion for Semantic Document Labelling with\n  Taxonomy-Aware Parallel Learning","summary":"  In Extreme Multi Label Completion (XMLCo), the objective is to predict the\nmissing labels of a collection of documents. Together with XML Classification,\nXMLCo is arguably one of the most challenging document classification tasks, as\nthe very high number of labels (at least ten of thousands) is generally very\nlarge compared to the number of available labelled documents in the training\ndataset. Such a task is often accompanied by a taxonomy that encodes the labels\norganic relationships, and many methods have been proposed to leverage this\nhierarchy to improve the results of XMLCo algorithms. In this paper, we propose\na new approach to this problem, TAMLEC (Taxonomy-Aware Multi-task Learning for\nExtreme multi-label Completion). TAMLEC divides the problem into several\nTaxonomy-Aware Tasks, i.e. subsets of labels adapted to the hierarchical paths\nof the taxonomy, and trains on these tasks using a dynamic Parallel Feature\nsharing approach, where some parts of the model are shared between tasks while\nothers are task-specific. Then, at inference time, TAMLEC uses the labels\navailable in a document to infer the appropriate tasks and to predict missing\nlabels. To achieve this result, TAMLEC uses a modified transformer architecture\nthat predicts ordered sequences of labels on a Weak-Semilattice structure that\nis naturally induced by the tasks. This approach yields multiple advantages.\nFirst, our experiments on real-world datasets show that TAMLEC outperforms\nstate-of-the-art methods for various XMLCo problems. Second, TAMLEC is by\nconstruction particularly suited for few-shots XML tasks, where new tasks or\nlabels are introduced with only few examples, and extensive evaluations\nhighlight its strong performance compared to existing methods.\n","authors":["Julien Audiffren","Christophe Broillet","Ljiljana Dolamic","Philippe Cudré-Mauroux"],"pdf_url":"https://arxiv.org/pdf/2412.13809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04377v2","updated":"2024-12-18T12:55:49Z","published":"2024-12-05T17:52:35Z","title":"A Hitchhiker's Guide to Understanding Performances of Two-Class\n  Classifiers","summary":"  Properly understanding the performances of classifiers is essential in\nvarious scenarios. However, the literature often relies only on one or two\nstandard scores to compare classifiers, which fails to capture the nuances of\napplication-specific requirements, potentially leading to suboptimal classifier\nselection. Recently, a paper on the foundations of the theory of\nperformance-based ranking introduced a tool, called the Tile, that organizes an\ninfinity of ranking scores into a 2D map. Thanks to the Tile, it is now\npossible to evaluate and compare classifiers efficiently, displaying all\npossible application-specific preferences instead of having to rely on a pair\nof scores. In this paper, we provide a first hitchhiker's guide for\nunderstanding the performances of two-class classifiers by presenting four\nscenarios, each showcasing a different user profile: a theoretical analyst, a\nmethod designer, a benchmarker, and an application developer. Particularly, we\nshow that we can provide different interpretative flavors that are adapted to\nthe user's needs by mapping different values on the Tile. As an illustration,\nwe leverage the newly introduced Tile tool and the different flavors to rank\nand analyze the performances of 74 state-of-the-art semantic segmentation\nmodels in two-class classification through the eyes of the four user profiles.\nThrough these user profiles, we demonstrate that the Tile effectively captures\nthe behavior of classifiers in a single visualization, while accommodating an\ninfinite number of ranking scores.\n","authors":["Anaïs Halin","Sébastien Piérard","Anthony Cioppa","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04309v2","updated":"2024-12-18T12:50:29Z","published":"2024-12-05T16:27:59Z","title":"The Tile: A 2D Map of Ranking Scores for Two-Class Classification","summary":"  In the computer vision and machine learning communities, as well as in many\nother research domains, rigorous evaluation of any new method, including\nclassifiers, is essential. One key component of the evaluation process is the\nability to compare and rank methods. However, ranking classifiers and\naccurately comparing their performances, especially when taking\napplication-specific preferences into account, remains challenging. For\ninstance, commonly used evaluation tools like Receiver Operating Characteristic\n(ROC) and Precision/Recall (PR) spaces display performances based on two\nscores. Hence, they are inherently limited in their ability to compare\nclassifiers across a broader range of scores and lack the capability to\nestablish a clear ranking among classifiers. In this paper, we present a novel\nversatile tool, named the Tile, that organizes an infinity of ranking scores in\na single 2D map for two-class classifiers, including common evaluation scores\nsuch as the accuracy, the true positive rate, the positive predictive value,\nJaccard's coefficient, and all F-beta scores. Furthermore, we study the\nproperties of the underlying ranking scores, such as the influence of the\npriors or the correspondences with the ROC space, and depict how to\ncharacterize any other score by comparing them to the Tile. Overall, we\ndemonstrate that the Tile is a powerful tool that effectively captures all the\nrankings in a single visualization and allows interpreting them.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01973v2","updated":"2024-12-18T12:46:01Z","published":"2024-11-04T10:50:03Z","title":"The Certainty Ratio $C_ρ$: a novel metric for assessing the\n  reliability of classifier predictions","summary":"  Evaluating the performance of classifiers is critical in machine learning,\nparticularly in high-stakes applications where the reliability of predictions\ncan significantly impact decision-making. Traditional performance measures,\nsuch as accuracy and F-score, often fail to account for the uncertainty\ninherent in classifier predictions, leading to potentially misleading\nassessments. This paper introduces the Certainty Ratio ($C_\\rho$), a novel\nmetric designed to quantify the contribution of confident (certain) versus\nuncertain predictions to any classification performance measure. By integrating\nthe Probabilistic Confusion Matrix ($CM^\\star$) and decomposing predictions\ninto certainty and uncertainty components, $C_\\rho$ provides a more\ncomprehensive evaluation of classifier reliability. Experimental results across\n21 datasets and multiple classifiers, including Decision Trees, Naive-Bayes,\n3-Nearest Neighbors, and Random Forests, demonstrate that $C_\\rho$ reveals\ncritical insights that conventional metrics often overlook. These findings\nemphasize the importance of incorporating probabilistic information into\nclassifier evaluation, offering a robust tool for researchers and practitioners\nseeking to improve model trustworthiness in complex environments.\n","authors":["Jesus S. Aguilar-Ruiz"],"pdf_url":"https://arxiv.org/pdf/2411.01973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04227v2","updated":"2024-12-18T12:45:58Z","published":"2024-12-05T15:05:25Z","title":"Foundations of the Theory of Performance-Based Ranking","summary":"  Ranking entities such as algorithms, devices, methods, or models based on\ntheir performances, while accounting for application-specific preferences, is a\nchallenge. To address this challenge, we establish the foundations of a\nuniversal theory for performance-based ranking. First, we introduce a rigorous\nframework built on top of both the probability and order theories. Our new\nframework encompasses the elements necessary to (1) manipulate performances as\nmathematical objects, (2) express which performances are worse than or\nequivalent to others, (3) model tasks through a variable called satisfaction,\n(4) consider properties of the evaluation, (5) define scores, and (6) specify\napplication-specific preferences through a variable called importance. On top\nof this framework, we propose the first axiomatic definition of performance\norderings and performance-based rankings. Then, we introduce a universal\nparametric family of scores, called ranking scores, that can be used to\nestablish rankings satisfying our axioms, while considering\napplication-specific preferences. Finally, we show, in the case of two-class\nclassification, that the family of ranking scores encompasses well-known\nperformance scores, including the accuracy, the true positive rate (recall,\nsensitivity), the true negative rate (specificity), the positive predictive\nvalue (precision), and F1. However, we also show that some other scores\ncommonly used to compare classifiers are unsuitable to derive performance\norderings satisfying the axioms. Therefore, this paper provides the computer\nvision and machine learning communities with a rigorous framework for\nevaluating and ranking entities.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19673v2","updated":"2024-12-18T12:42:10Z","published":"2024-04-30T16:06:04Z","title":"Neural Controlled Differential Equations with Quantum Hidden Evolutions","summary":"  We introduce a class of neural controlled differential equation inspired by\nquantum mechanics. Neural quantum controlled differential equations (NQDEs)\nmodel the dynamics by analogue of the Schr\\\"{o}dinger equation. Specifically,\nthe hidden state represents the wave function, and its collapse leads to an\ninterpretation of the classification probability. We implement and compare the\nresults of four variants of NQDEs on a toy spiral classification problem.\n","authors":["Lingyi Yang","Zhen Shao"],"pdf_url":"https://arxiv.org/pdf/2404.19673v2.pdf","comment":"Code available at: https://github.com/lingyiyang/NQDE"},{"id":"http://arxiv.org/abs/2412.13795v1","updated":"2024-12-18T12:39:53Z","published":"2024-12-18T12:39:53Z","title":"Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN","summary":"  Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN.\n","authors":["Pengxiang Li","Lu Yin","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11744v2","updated":"2024-12-18T12:34:00Z","published":"2024-12-16T13:03:18Z","title":"Conditional Diffusion Models Based Conditional Independence Testing","summary":"  Conditional independence (CI) testing is a fundamental task in modern\nstatistics and machine learning. The conditional randomization test (CRT) was\nrecently introduced to test whether two random variables, $X$ and $Y$, are\nconditionally independent given a potentially high-dimensional set of random\nvariables, $Z$. The CRT operates exceptionally well under the assumption that\nthe conditional distribution $X|Z$ is known. However, since this distribution\nis typically unknown in practice, accurately approximating it becomes crucial.\nIn this paper, we propose using conditional diffusion models (CDMs) to learn\nthe distribution of $X|Z$. Theoretically and empirically, it is shown that CDMs\nclosely approximate the true conditional distribution. Furthermore, CDMs offer\na more accurate approximation of $X|Z$ compared to GANs, potentially leading to\na CRT that performs better than those based on GANs. To accommodate complex\ndependency structures, we utilize a computationally efficient classifier-based\nconditional mutual information (CMI) estimator as our test statistic. The\nproposed testing procedure performs effectively without requiring assumptions\nabout specific distribution forms or feature dependencies, and is capable of\nhandling mixed-type conditioning sets that include both continuous and discrete\nvariables. Theoretical analysis shows that our proposed test achieves a valid\ncontrol of the type I error. A series of experiments on synthetic data\ndemonstrates that our new test effectively controls both type-I and type-II\nerrors, even in high dimensional scenarios.\n","authors":["Yanfeng Yang","Shuai Li","Yingjie Zhang","Zhuoran Sun","Hai Shu","Ziqi Chen","Renming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11744v2.pdf","comment":"17 pages, 7 figures, aaai 2025"},{"id":"http://arxiv.org/abs/2412.13790v1","updated":"2024-12-18T12:32:43Z","published":"2024-12-18T12:32:43Z","title":"Toward Efficient Data-Free Unlearning","summary":"  Machine unlearning without access to real data distribution is challenging.\nThe existing method based on data-free distillation achieved unlearning by\nfiltering out synthetic samples containing forgetting information but struggled\nto distill the retaining-related knowledge efficiently. In this work, we\nanalyze that such a problem is due to over-filtering, which reduces the\nsynthesized retaining-related information. We propose a novel method, Inhibited\nSynthetic PostFilter (ISPF), to tackle this challenge from two perspectives:\nFirst, the Inhibited Synthetic, by reducing the synthesized forgetting\ninformation; Second, the PostFilter, by fully utilizing the retaining-related\ninformation in synthesized samples. Experimental results demonstrate that the\nproposed ISPF effectively tackles the challenge and outperforms existing\nmethods.\n","authors":["Chenhao Zhang","Shaofei Shen","Weitong Chen","Miao Xu"],"pdf_url":"https://arxiv.org/pdf/2412.13790v1.pdf","comment":"15 pages, 10 figures, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13779v1","updated":"2024-12-18T12:16:41Z","published":"2024-12-18T12:16:41Z","title":"Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization","summary":"  Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.\n","authors":["Yichen Li","Yuying Wang","Tianzhe Xiao","Haozhao Wang","Yining Qi","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2412.13779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13769v1","updated":"2024-12-18T12:06:52Z","published":"2024-12-18T12:06:52Z","title":"QuLTSF: Long-Term Time Series Forecasting with Quantum Machine Learning","summary":"  Long-term time series forecasting (LTSF) involves predicting a large number\nof future values of a time series based on the past values and is an essential\ntask in a wide range of domains including weather forecasting, stock market\nanalysis, disease outbreak prediction. Over the decades LTSF algorithms have\ntransitioned from statistical models to deep learning models like transformer\nmodels. Despite the complex architecture of transformer based LTSF models `Are\nTransformers Effective for Time Series Forecasting? (Zeng et al., 2023)' showed\nthat simple linear models can outperform the state-of-the-art transformer based\nLTSF models. Recently, quantum machine learning (QML) is evolving as a domain\nto enhance the capabilities of classical machine learning models. In this paper\nwe initiate the application of QML to LTSF problems by proposing QuLTSF, a\nsimple hybrid QML model for multivariate LTSF. Through extensive experiments on\na widely used weather dataset we show the advantages of QuLTSF over the\nstate-of-the-art classical linear models, in terms of reduced mean squared\nerror and mean absolute error.\n","authors":["Hari Hara Suthan Chittoor","Paul Robert Griffin","Ariel Neufeld","Jayne Thompson","Mile Gu"],"pdf_url":"https://arxiv.org/pdf/2412.13769v1.pdf","comment":"submitted for conference publication"},{"id":"http://arxiv.org/abs/2412.13762v1","updated":"2024-12-18T11:59:24Z","published":"2024-12-18T11:59:24Z","title":"Cultivating Archipelago of Forests: Evolving Robust Decision Trees\n  through Island Coevolution","summary":"  Decision trees are widely used in machine learning due to their simplicity\nand interpretability, but they often lack robustness to adversarial attacks and\ndata perturbations. The paper proposes a novel island-based coevolutionary\nalgorithm (ICoEvoRDF) for constructing robust decision tree ensembles. The\nalgorithm operates on multiple islands, each containing populations of decision\ntrees and adversarial perturbations. The populations on each island evolve\nindependently, with periodic migration of top-performing decision trees between\nislands. This approach fosters diversity and enhances the exploration of the\nsolution space, leading to more robust and accurate decision tree ensembles.\nICoEvoRDF utilizes a popular game theory concept of mixed Nash equilibrium for\nensemble weighting, which further leads to improvement in results. ICoEvoRDF is\nevaluated on 20 benchmark datasets, demonstrating its superior performance\ncompared to state-of-the-art methods in optimizing both adversarial accuracy\nand minimax regret. The flexibility of ICoEvoRDF allows for the integration of\ndecision trees from various existing methods, providing a unified framework for\ncombining diverse solutions. Our approach offers a promising direction for\ndeveloping robust and interpretable machine learning models\n","authors":["Adam Żychowski","Andrew Perrault","Jacek Mańdziuk"],"pdf_url":"https://arxiv.org/pdf/2412.13762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00200v5","updated":"2024-12-18T11:51:43Z","published":"2023-04-01T02:07:08Z","title":"Diffusion map particle systems for generative modeling","summary":"  We propose a novel diffusion map particle system (DMPS) for generative\nmodeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient\ndescent (LAWGD). Diffusion maps are used to approximate the generator of the\ncorresponding Langevin diffusion process from samples, and hence to learn the\nunderlying data-generating manifold. On the other hand, LAWGD enables efficient\nsampling from the target distribution given a suitable choice of kernel, which\nwe construct here via a spectral approximation of the generator, computed with\ndiffusion maps. Our method requires no offline training and minimal tuning, and\ncan outperform other approaches on data sets of moderate dimension.\n","authors":["Fengyi Li","Youssef Marzouk"],"pdf_url":"https://arxiv.org/pdf/2304.00200v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13757v1","updated":"2024-12-18T11:47:54Z","published":"2024-12-18T11:47:54Z","title":"Federated Source-free Domain Adaptation for Classification: Weighted\n  Cluster Aggregation for Unlabeled Data","summary":"  Federated learning (FL) commonly assumes that the server or some clients have\nlabeled data, which is often impractical due to annotation costs and privacy\nconcerns. Addressing this problem, we focus on a source-free domain adaptation\ntask, where (1) the server holds a pre-trained model on labeled source domain\ndata, (2) clients possess only unlabeled data from various target domains, and\n(3) the server and clients cannot access the source data in the adaptation\nphase. This task is known as Federated source-Free Domain Adaptation (FFREEDA).\nSpecifically, we focus on classification tasks, while the previous work solely\nstudies semantic segmentation. Our contribution is the novel Federated learning\nwith Weighted Cluster Aggregation (FedWCA) method, designed to mitigate both\ndomain shifts and privacy concerns with only unlabeled data. FedWCA comprises\nthree phases: private and parameter-free clustering of clients to obtain\ndomain-specific global models on the server, weighted aggregation of the global\nmodels for the clustered clients, and local domain adaptation with\npseudo-labeling. Experimental results show that FedWCA surpasses several\nexisting methods and baselines in FFREEDA, establishing its effectiveness and\npracticality.\n","authors":["Junki Mori","Kosuke Kihara","Taiki Miyagawa","Akinori F. Ebihara","Isamu Teranishi","Hisashi Kashima"],"pdf_url":"https://arxiv.org/pdf/2412.13757v1.pdf","comment":"Accepted by WACV 2025"},{"id":"http://arxiv.org/abs/2412.13754v1","updated":"2024-12-18T11:44:19Z","published":"2024-12-18T11:44:19Z","title":"Optimal Exact Recovery in Semi-Supervised Learning: A Study of Spectral\n  Methods and Graph Convolutional Networks","summary":"  We delve into the challenge of semi-supervised node classification on the\nContextual Stochastic Block Model (CSBM) dataset. Here, nodes from the\ntwo-cluster Stochastic Block Model (SBM) are coupled with feature vectors,\nwhich are derived from a Gaussian Mixture Model (GMM) that corresponds to their\nrespective node labels. With only a subset of the CSBM node labels accessible\nfor training, our primary objective becomes the accurate classification of the\nremaining nodes. Venturing into the transductive learning landscape, we, for\nthe first time, pinpoint the information-theoretical threshold for the exact\nrecovery of all test nodes in CSBM. Concurrently, we design an optimal spectral\nestimator inspired by Principal Component Analysis (PCA) with the training\nlabels and essential data from both the adjacency matrix and feature vectors.\nWe also evaluate the efficacy of graph ridge regression and Graph Convolutional\nNetworks (GCN) on this synthetic dataset. Our findings underscore that graph\nridge regression and GCN possess the ability to achieve the information\nthreshold of exact recovery in a manner akin to the optimal estimator when\nusing the optimal weighted self-loops. This highlights the potential role of\nfeature learning in augmenting the proficiency of GCN, especially in the realm\nof semi-supervised learning.\n","authors":["Hai-Xiao Wang","Zhichao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13754v1.pdf","comment":"Accepted by ICML 2024. The conference version can be accessed via\n  https://openreview.net/forum?id=8m4V6Fx6ma"},{"id":"http://arxiv.org/abs/2411.01850v2","updated":"2024-12-18T11:25:55Z","published":"2024-11-04T07:05:02Z","title":"ManiBox: Enhancing Spatial Grasping Generalization via Scalable\n  Simulation Data Generation","summary":"  Learning a precise robotic grasping policy is crucial for embodied agents\noperating in complex real-world manipulation tasks. Despite significant\nadvancements, most models still struggle with accurate spatial positioning of\nobjects to be grasped. We first show that this spatial generalization challenge\nstems primarily from the extensive data requirements for adequate spatial\nunderstanding. However, collecting such data with real robots is prohibitively\nexpensive, and relying on simulation data often leads to visual generalization\ngaps upon deployment. To overcome these challenges, we then focus on\nstate-based policy generalization and present \\textbf{ManiBox}, a novel\nbounding-box-guided manipulation method built on a simulation-based\nteacher-student framework. The teacher policy efficiently generates scalable\nsimulation data using bounding boxes, which are proven to uniquely determine\nthe objects' spatial positions. The student policy then utilizes these\nlow-dimensional spatial states to enable zero-shot transfer to real robots.\nThrough comprehensive evaluations in simulated and real-world environments,\nManiBox demonstrates a marked improvement in spatial grasping generalization\nand adaptability to diverse objects and backgrounds. Further, our empirical\nstudy into scaling laws for policy performance indicates that spatial volume\ngeneralization scales with data volume in a power law. For a certain level of\nspatial volume, the success rate of grasping empirically follows\nMichaelis-Menten kinetics relative to data volume, showing a saturation effect\nas data increases. Our videos and code are available in\nhttps://thkkk.github.io/manibox.\n","authors":["Hengkai Tan","Xuezhou Xu","Chengyang Ying","Xinyi Mao","Songming Liu","Xingxing Zhang","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.01850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13738v1","updated":"2024-12-18T11:15:32Z","published":"2024-12-18T11:15:32Z","title":"Uncertainty separation via ensemble quantile regression","summary":"  This paper introduces a novel and scalable framework for uncertainty\nestimation and separation with applications in data driven modeling in science\nand engineering tasks where reliable uncertainty quantification is critical.\nLeveraging an ensemble of quantile regression (E-QR) models, our approach\nenhances aleatoric uncertainty estimation while preserving the quality of\nepistemic uncertainty, surpassing competing methods, such as Deep Ensembles\n(DE) and Monte Carlo (MC) dropout. To address challenges in separating\nuncertainty types, we propose an algorithm that iteratively improves separation\nthrough progressive sampling in regions of high uncertainty. Our framework is\nscalable to large datasets and demonstrates superior performance on synthetic\nbenchmarks, offering a robust tool for uncertainty quantification in\ndata-driven applications.\n","authors":["Navid Ansari","Hans-Peter Seidel","Vahid Babaei"],"pdf_url":"https://arxiv.org/pdf/2412.13738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13729v1","updated":"2024-12-18T11:08:25Z","published":"2024-12-18T11:08:25Z","title":"THÖR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared\n  Industrial Spaces","summary":"  Accurate human activity and trajectory prediction are crucial for ensuring\nsafe and reliable human-robot interactions in dynamic environments, such as\nindustrial settings, with mobile robots. Datasets with fine-grained action\nlabels for moving people in industrial environments with mobile robots are\nscarce, as most existing datasets focus on social navigation in public spaces.\nThis paper introduces the TH\\\"OR-MAGNI Act dataset, a substantial extension of\nthe TH\\\"OR-MAGNI dataset, which captures participant movements alongside robots\nin diverse semantic and spatial contexts. TH\\\"OR-MAGNI Act provides 8.3 hours\nof manually labeled participant actions derived from egocentric videos recorded\nvia eye-tracking glasses. These actions, aligned with the provided TH\\\"OR-MAGNI\nmotion cues, follow a long-tailed distribution with diversified acceleration,\nvelocity, and navigation distance profiles. We demonstrate the utility of\nTH\\\"OR-MAGNI Act for two tasks: action-conditioned trajectory prediction and\njoint action and trajectory prediction. We propose two efficient\ntransformer-based models that outperform the baselines to address these tasks.\nThese results underscore the potential of TH\\\"OR-MAGNI Act to develop\npredictive models for enhanced human-robot interaction in complex environments.\n","authors":["Tiago Rodrigues de Almeida","Tim Schreiter","Andrey Rudenko","Luigi Palmieiri","Johannes A. Stork","Achim J. Lilienthal"],"pdf_url":"https://arxiv.org/pdf/2412.13729v1.pdf","comment":"This paper has been accepted to the the 20th edition of the IEEE/ACM\n  International Conference on Human-Robot Interaction (HRI'25), which will be\n  held in Melbourne, Australia on March 4-6, 2025. Code:\n  https://github.com/tmralmeida/thor-magni-actions"},{"id":"http://arxiv.org/abs/2412.13724v1","updated":"2024-12-18T11:04:58Z","published":"2024-12-18T11:04:58Z","title":"USEFUSE: Utile Stride for Enhanced Performance in Fused Layer\n  Architecture of Deep Neural Networks","summary":"  Convolutional Neural Networks (CNNs) are crucial in various applications, but\ntheir deployment on resource-constrained edge devices poses challenges. This\nstudy presents the Sum-of-Products (SOP) units for convolution, which utilize\nlow-latency left-to-right bit-serial arithmetic to minimize response time and\nenhance overall performance. The study proposes a methodology for fusing\nmultiple convolution layers to reduce off-chip memory communication and\nincrease overall performance. An effective mechanism detects and skips\ninefficient convolutions after ReLU layers, minimizing power consumption\nwithout compromising accuracy. Furthermore, efficient tile movement guarantees\nuniform access to the fusion pyramid. An analysis demonstrates the utile stride\nstrategy improves operational intensity. Two designs cater to varied demands:\none focuses on minimal response time for mission-critical applications, and\nanother focuses on resource-constrained devices with comparable latency. This\napproach notably reduced redundant computations, improving the efficiency of\nCNN deployment on edge devices.\n","authors":["Muhammad Sohail Ibrahim","Muhammad Usman","Jeong-A Lee"],"pdf_url":"https://arxiv.org/pdf/2412.13724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13722v1","updated":"2024-12-18T11:03:26Z","published":"2024-12-18T11:03:26Z","title":"Data-driven Discovery of Biophysical T Cell Receptor Co-specificity\n  Rules","summary":"  The biophysical interactions between the T cell receptor (TCR) and its\nligands determine the specificity of the cellular immune response. However, the\nimmense diversity of receptors and ligands has made it challenging to discover\ngeneralizable rules across the distinct binding affinity landscapes created by\ndifferent ligands. Here, we present an optimization framework for discovering\nbiophysical rules that predict whether TCRs share specificity to a ligand.\nApplying this framework to TCRs associated with a collection of SARS-CoV-2\npeptides we establish how co-specificity depends on the type and position of\namino-acid differences between receptors. We also demonstrate that the inferred\nrules generalize to ligands not seen during training. Our analysis reveals that\nmatching of steric properties between substituted amino acids is important for\nreceptor co-specificity, in contrast with the hydrophobic properties that more\nprominently determine evolutionary substitutability. We furthermore find that\npositions not in direct contact with the peptide still significantly impact\nspecificity. These findings highlight the potential for data-driven approaches\nto uncover the molecular mechanisms underpinning the specificity of adaptive\nimmune responses.\n","authors":["Andrew G. T. Pyo","Yuta Nagano","Martina Milighetti","James Henderson","Curtis G. Callan Jr.","Benny Chain","Ned S. Wingreen","Andreas Tiffeau-Mayer"],"pdf_url":"https://arxiv.org/pdf/2412.13722v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.12672v4","updated":"2024-12-18T11:01:18Z","published":"2024-10-16T15:36:13Z","title":"Context Matters: Leveraging Contextual Features for Time Series\n  Forecasting","summary":"  Time series forecasts are often influenced by exogenous contextual features\nin addition to their corresponding history. For example, in financial settings,\nit is hard to accurately predict a stock price without considering public\nsentiments and policy decisions in the form of news articles, tweets, etc.\nThough this is common knowledge, the current state-of-the-art (SOTA)\nforecasting models fail to incorporate such contextual information, owing to\nits heterogeneity and multimodal nature. To address this, we introduce\nContextFormer, a novel plug-and-play method to surgically integrate multimodal\ncontextual information into existing pre-trained forecasting models.\nContextFormer effectively distills forecast-specific information from rich\nmultimodal contexts, including categorical, continuous, time-varying, and even\ntextual information, to significantly enhance the performance of existing base\nforecasters. ContextFormer outperforms SOTA forecasting models by up to 30% on\na range of real-world datasets spanning energy, traffic, environmental, and\nfinancial domains.\n","authors":["Sameep Chattopadhyay","Pulkit Paliwal","Sai Shankar Narasimhan","Shubhankar Agarwal","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.12672v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13716v1","updated":"2024-12-18T10:55:43Z","published":"2024-12-18T10:55:43Z","title":"Model Decides How to Tokenize: Adaptive DNA Sequence Tokenization with\n  MxDNA","summary":"  Foundation models have made significant strides in understanding the genomic\nlanguage of DNA sequences. However, previous models typically adopt the\ntokenization methods designed for natural language, which are unsuitable for\nDNA sequences due to their unique characteristics. In addition, the optimal\napproach to tokenize DNA remains largely under-explored, and may not be\nintuitively understood by humans even if discovered. To address these\nchallenges, we introduce MxDNA, a novel framework where the model autonomously\nlearns an effective DNA tokenization strategy through gradient decent. MxDNA\nemploys a sparse Mixture of Convolution Experts coupled with a deformable\nconvolution to model the tokenization process, with the discontinuous,\noverlapping, and ambiguous nature of meaningful genomic segments explicitly\nconsidered. On Nucleotide Transformer Benchmarks and Genomic Benchmarks, MxDNA\ndemonstrates superior performance to existing methods with less pretraining\ndata and time, highlighting its effectiveness. Finally, we show that MxDNA\nlearns unique tokenization strategy distinct to those of previous methods and\ncaptures genomic functionalities at a token level during self-supervised\npretraining. Our MxDNA aims to provide a new perspective on DNA tokenization,\npotentially offering broad applications in various domains and yielding\nprofound insights.\n","authors":["Lifeng Qiao","Peng Ye","Yuchen Ren","Weiqiang Bai","Chaoqi Liang","Xinzhu Ma","Nanqing Dong","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2412.13716v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.13715v1","updated":"2024-12-18T10:53:36Z","published":"2024-12-18T10:53:36Z","title":"SSE-SAM: Balancing Head and Tail Classes Gradually through Stage-Wise\n  SAM","summary":"  Real-world datasets often exhibit a long-tailed distribution, where vast\nmajority of classes known as tail classes have only few samples. Traditional\nmethods tend to overfit on these tail classes. Recently, a new approach called\nImbalanced SAM (ImbSAM) is proposed to leverage the generalization benefits of\nSharpness-Aware Minimization (SAM) for long-tailed distributions. The main\nstrategy is to merely enhance the smoothness of the loss function for tail\nclasses. However, we argue that improving generalization in long-tail scenarios\nrequires a careful balance between head and tail classes. We show that neither\nSAM nor ImbSAM alone can fully achieve this balance. For SAM, we prove that\nalthough it enhances the model's generalization ability by escaping saddle\npoint in the overall loss landscape, it does not effectively address this for\ntail-class losses. Conversely, while ImbSAM is more effective at avoiding\nsaddle points in tail classes, the head classes are trained insufficiently,\nresulting in significant performance drops. Based on these insights, we propose\nStage-wise Saddle Escaping SAM (SSE-SAM), which uses complementary strengths of\nImbSAM and SAM in a phased approach. Initially, SSE-SAM follows the majority\nsample to avoid saddle points of the head-class loss. During the later phase,\nit focuses on tail-classes to help them escape saddle points. Our experiments\nconfirm that SSE-SAM has better ability in escaping saddles both on head and\ntail classes, and shows performance improvements.\n","authors":["Xingyu Lyu","Qianqian Xu","Zhiyong Yang","Shaojie Lyu","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13714v1","updated":"2024-12-18T10:53:30Z","published":"2024-12-18T10:53:30Z","title":"AnchorInv: Few-Shot Class-Incremental Learning of Physiological Signals\n  via Representation Space Guided Inversion","summary":"  Deep learning models have demonstrated exceptional performance in a variety\nof real-world applications. These successes are often attributed to strong base\nmodels that can generalize to novel tasks with limited supporting data while\nkeeping prior knowledge intact. However, these impressive results are based on\nthe availability of a large amount of high-quality data, which is often lacking\nin specialized biomedical applications. In such fields, models are usually\ndeveloped with limited data that arrive incrementally with novel categories.\nThis requires the model to adapt to new information while preserving existing\nknowledge. Few-Shot Class-Incremental Learning (FSCIL) methods offer a\npromising approach to addressing these challenges, but they also depend on\nstrong base models that face the same aforementioned limitations. To overcome\nthese constraints, we propose AnchorInv following the straightforward and\nefficient buffer-replay strategy. Instead of selecting and storing raw data,\nAnchorInv generates synthetic samples guided by anchor points in the feature\nspace. This approach protects privacy and regularizes the model for adaptation.\nWhen evaluated on three public physiological time series datasets, AnchorInv\nexhibits efficient knowledge forgetting prevention and improved adaptation to\nnovel classes, surpassing state-of-the-art baselines.\n","authors":["Chenqi Li","Boyan Gao","Gabriel Jones","Timothy Denison","Tingting Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.13714v1.pdf","comment":"AAAI-25 Extended Version"},{"id":"http://arxiv.org/abs/2406.02347v3","updated":"2024-12-18T10:45:06Z","published":"2024-06-04T14:23:27Z","title":"Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few\n  Steps Image Generation","summary":"  In this paper, we propose an efficient, fast, and versatile distillation\nmethod to accelerate the generation of pre-trained diffusion models: Flash\nDiffusion. The method reaches state-of-the-art performances in terms of FID and\nCLIP-Score for few steps image generation on the COCO2014 and COCO2017\ndatasets, while requiring only several GPU hours of training and fewer\ntrainable parameters than existing methods. In addition to its efficiency, the\nversatility of the method is also exposed across several tasks such as\ntext-to-image, inpainting, face-swapping, super-resolution and using different\nbackbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\alpha$),\nas well as adapters. In all cases, the method allowed to reduce drastically the\nnumber of sampling steps while maintaining very high-quality image generation.\nThe official implementation is available at\nhttps://github.com/gojasper/flash-diffusion.\n","authors":["Clément Chadebec","Onur Tasar","Eyal Benaroche","Benjamin Aubin"],"pdf_url":"https://arxiv.org/pdf/2406.02347v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13697v1","updated":"2024-12-18T10:41:44Z","published":"2024-12-18T10:41:44Z","title":"Splitting criteria for ordinal decision trees: an experimental study","summary":"  Ordinal Classification (OC) is a machine learning field that addresses\nclassification tasks where the labels exhibit a natural order. Unlike nominal\nclassification, which treats all classes as equally distinct, OC takes the\nordinal relationship into account, producing more accurate and relevant\nresults. This is particularly critical in applications where the magnitude of\nclassification errors has implications. Despite this, OC problems are often\ntackled using nominal methods, leading to suboptimal solutions. Although\ndecision trees are one of the most popular classification approaches, ordinal\ntree-based approaches have received less attention when compared to other\nclassifiers. This work conducts an experimental study of tree-based\nmethodologies specifically designed to capture ordinal relationships. A\ncomprehensive survey of ordinal splitting criteria is provided, standardising\nthe notations used in the literature for clarity. Three ordinal splitting\ncriteria, Ordinal Gini (OGini), Weighted Information Gain (WIG), and Ranking\nImpurity (RI), are compared to the nominal counterparts of the first two (Gini\nand information gain), by incorporating them into a decision tree classifier.\nAn extensive repository considering 45 publicly available OC datasets is\npresented, supporting the first experimental comparison of ordinal and nominal\nsplitting criteria using well-known OC evaluation metrics. Statistical analysis\nof the results highlights OGini as the most effective ordinal splitting\ncriterion to date. Source code, datasets, and results are made available to the\nresearch community.\n","authors":["Rafael Ayllón-Gavilán","Francisco José Martínez-Estudillo","David Guijo-Rubio","César Hervás-Martínez","Pedro Antonio Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2412.13697v1.pdf","comment":"11 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.23318v2","updated":"2024-12-18T10:32:31Z","published":"2024-10-29T21:38:54Z","title":"Denoising Diffusion Probabilistic Models for Magnetic Resonance\n  Fingerprinting","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI, enabling the mapping of multiple tissue properties from a\nsingle, accelerated scan. However, achieving accurate reconstructions remains\nchallenging, particularly in highly accelerated and undersampled acquisitions,\nwhich are crucial for reducing scan times. While deep learning techniques have\nadvanced image reconstruction, the recent introduction of diffusion models\noffers new possibilities for imaging tasks, though their application in the\nmedical field is still emerging. Notably, diffusion models have not yet been\nexplored for the MRF problem. In this work, we propose for the first time a\nconditional diffusion probabilistic model for MRF image reconstruction.\nQualitative and quantitative comparisons on in-vivo brain scan data demonstrate\nthat the proposed approach can outperform established deep learning and\ncompressed sensing algorithms for MRF reconstruction. Extensive ablation\nstudies also explore strategies to improve computational efficiency of our\napproach.\n","authors":["Perla Mayo","Carolin M. Pirkl","Alin Achim","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2410.23318v2.pdf","comment":"13 pages, 5 figures, 3 tables, 2 algorithms"},{"id":"http://arxiv.org/abs/2404.17157v2","updated":"2024-12-18T10:32:06Z","published":"2024-04-26T05:01:08Z","title":"Neuro-Symbolic Embedding for Short and Effective Feature Selection via\n  Autoregressive Generation","summary":"  Feature selection aims to identify the optimal feature subset for enhancing\ndownstream models. Effective feature selection can remove redundant features,\nsave computational resources, accelerate the model learning process, and\nimprove the model overall performance. However, existing works are often\ntime-intensive to identify the effective feature subset within high-dimensional\nfeature spaces. Meanwhile, these methods mainly utilize a single downstream\ntask performance as the selection criterion, leading to the selected subsets\nthat are not only redundant but also lack generalizability. To bridge these\ngaps, we reformulate feature selection through a neuro-symbolic lens and\nintroduce a novel generative framework aimed at identifying short and effective\nfeature subsets. More specifically, we found that feature ID tokens of the\nselected subset can be formulated as symbols to reflect the intricate\ncorrelations among features. Thus, in this framework, we first create a data\ncollector to automatically collect numerous feature selection samples\nconsisting of feature ID tokens, model performance, and the measurement of\nfeature subset redundancy. Building on the collected data, an\nencoder-decoder-evaluator learning paradigm is developed to preserve the\nintelligence of feature selection into a continuous embedding space for\nefficient search. Within the learned embedding space, we leverage a\nmulti-gradient search algorithm to find more robust and generalized embeddings\nwith the objective of improving model performance and reducing feature subset\nredundancy. These embeddings are then utilized to reconstruct the feature ID\ntokens for executing the final feature selection. Ultimately, comprehensive\nexperiments and case studies are conducted to validate the effectiveness of the\nproposed framework.\n","authors":["Nanxu Gong","Wangyang Ying","Dongjie Wang","Yanjie Fu"],"pdf_url":"https://arxiv.org/pdf/2404.17157v2.pdf","comment":"Accepted to ACM TIST"},{"id":"http://arxiv.org/abs/2412.13690v1","updated":"2024-12-18T10:28:51Z","published":"2024-12-18T10:28:51Z","title":"Personalized Clustering via Targeted Representation Learning","summary":"  Clustering traditionally aims to reveal a natural grouping structure model\nfrom unlabeled data. However, this model may not always align with users'\npreference. In this paper, we propose a personalized clustering method that\nexplicitly performs targeted representation learning by interacting with users\nvia modicum task information (e.g., $\\textit{must-link}$ or\n$\\textit{cannot-link}$ pairs) to guide the clustering direction. We query users\nwith the most informative pairs, i.e., those pairs most hard to cluster and\nthose most easy to miscluster, to facilitate the representation learning in\nterms of the clustering preference. Moreover, by exploiting attention\nmechanism, the targeted representation is learned and augmented. By leveraging\nthe targeted representation and constrained constrastive loss as well,\npersonalized clustering is obtained. Theoretically, we verify that the risk of\npersonalized clustering is tightly bounded, guaranteeing that active queries to\nusers do mitigate the clustering risk. Experimentally, extensive results show\nthat our method performs well across different clustering tasks and datasets,\neven with a limited number of queries.\n","authors":["Xiwen Geng","Suyun Zhao","Yixin Yu","Borui Peng","Pan Du","Hong Chen","Cuiping Li","Mengdie Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13690v1.pdf","comment":"Accepted to AAAI 2025 main conference"},{"id":"http://arxiv.org/abs/2405.16203v2","updated":"2024-12-18T10:25:12Z","published":"2024-05-25T12:27:21Z","title":"Evolutionary Large Language Model for Automated Feature Transformation","summary":"  Feature transformation aims to reconstruct the feature space of raw features\nto enhance the performance of downstream models. However, the exponential\ngrowth in the combinations of features and operations poses a challenge, making\nit difficult for existing methods to efficiently explore a wide space.\nAdditionally, their optimization is solely driven by the accuracy of downstream\nmodels in specific domains, neglecting the acquisition of general feature\nknowledge. To fill this research gap, we propose an evolutionary LLM framework\nfor automated feature transformation. This framework consists of two parts: 1)\nconstructing a multi-population database through an RL data collector while\nutilizing evolutionary algorithm strategies for database maintenance, and 2)\nutilizing the ability of Large Language Model (LLM) in sequence understanding,\nwe employ few-shot prompts to guide LLM in generating superior samples based on\nfeature transformation sequence distinction. Leveraging the multi-population\ndatabase initially provides a wide search scope to discover excellent\npopulations. Through culling and evolution, the high-quality populations are\nafforded greater opportunities, thereby furthering the pursuit of optimal\nindividuals. Through the integration of LLMs with evolutionary algorithms, we\nachieve efficient exploration within a vast space, while harnessing feature\nknowledge to propel optimization, thus realizing a more adaptable search\nparadigm. Finally, we empirically demonstrate the effectiveness and generality\nof our proposed method.\n","authors":["Nanxu Gong","Chandan K. Reddy","Wangyang Ying","Haifeng Chen","Yanjie Fu"],"pdf_url":"https://arxiv.org/pdf/2405.16203v2.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2210.14164v4","updated":"2024-12-18T10:16:59Z","published":"2022-10-19T21:52:01Z","title":"Understanding Key Point Cloud Features for Development Three-dimensional\n  Adversarial Attacks","summary":"  Adversarial attacks pose serious challenges for deep neural network\n(DNN)-based analysis of various input signals. In the case of three-dimensional\npoint clouds, methods have been developed to identify points that play a key\nrole in network decision, and these become crucial in generating existing\nadversarial attacks. For example, a saliency map approach is a popular method\nfor identifying adversarial drop points, whose removal would significantly\nimpact the network decision. This paper seeks to enhance the understanding of\nthree-dimensional adversarial attacks by exploring which point cloud features\nare most important for predicting adversarial points. Specifically, Fourteen\nkey point cloud features such as edge intensity and distance from the centroid\nare defined, and multiple linear regression is employed to assess their\npredictive power for adversarial points. Based on critical feature selection\ninsights, a new attack method has been developed to evaluate whether the\nselected features can generate an attack successfully. Unlike traditional\nattack methods that rely on model-specific vulnerabilities, this approach\nfocuses on the intrinsic characteristics of the point clouds themselves. It is\ndemonstrated that these features can predict adversarial points across four\ndifferent DNN architectures, Point Network (PointNet), PointNet++, Dynamic\nGraph Convolutional Neural Networks (DGCNN), and Point Convolutional Network\n(PointConv) outperforming random guessing and achieving results comparable to\nsaliency map-based attacks. This study has important engineering applications,\nsuch as enhancing the security and robustness of three-dimensional point\ncloud-based systems in fields like robotics and autonomous driving.\n","authors":["Hanieh Naderi","Chinthaka Dinesh","Ivan V. Bajic","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2210.14164v4.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.12916v2","updated":"2024-12-18T10:16:59Z","published":"2024-12-17T13:50:20Z","title":"Graph Spring Neural ODEs for Link Sign Prediction","summary":"  Signed graphs allow for encoding positive and negative relations between\nnodes and are used to model various online activities. Node representation\nlearning for signed graphs is a well-studied task with important applications\nsuch as sign prediction. While the size of datasets is ever-increasing, recent\nmethods often sacrifice scalability for accuracy. We propose a novel\nmessage-passing layer architecture called Graph Spring Network (GSN) modeled\nafter spring forces. We combine it with a Graph Neural Ordinary Differential\nEquations (ODEs) formalism to optimize the system dynamics in embedding space\nto solve a downstream prediction task. Once the dynamics is learned, embedding\ngeneration for novel datasets is done by solving the ODEs in time using a\nnumerical integration scheme. Our GSN layer leverages the fast-to-compute edge\nvector directions and learnable scalar functions that only depend on nodes'\ndistances in latent space to compute the nodes' positions. Conversely, Graph\nConvolution and Graph Attention Network layers rely on learnable vector\nfunctions that require the full positions of input nodes in latent space. We\npropose a specific implementation called Spring-Neural-Network (SPR-NN) using a\nset of small neural networks mimicking attracting and repulsing spring forces\nthat we train for link sign prediction. Experiments show that our method\nachieves accuracy close to the state-of-the-art methods with node generation\ntime speedup factors of up to 28,000 on large graphs.\n","authors":["Andrin Rehmann","Alexandre Bovet"],"pdf_url":"https://arxiv.org/pdf/2412.12916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14535v4","updated":"2024-12-18T10:10:56Z","published":"2024-10-18T15:23:29Z","title":"Comparing Differentiable and Dynamic Ray Tracing: Introducing the\n  Multipath Lifetime Map","summary":"  With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle\ncommunications, radio propagation modeling tools must adapt to the rapidly\nchanging nature of the radio channel. Recently, both Differentiable and Dynamic\nRay Tracing frameworks have emerged to address these challenges. However, there\nis often confusion about how these approaches differ and which one should be\nused in specific contexts. In this paper, we provide an overview of these two\ntechniques and a comparative analysis against two state-of-the-art tools:\n3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise\ncharacterization of the scope of these methods, we introduce a novel\nsimulation-based metric, the Multipath Lifetime Map, which enables the\nevaluation of spatial and temporal coherence in radio channels only based on\nthe geometrical description of the environment. Finally, our metrics are\nevaluated on a classic urban street canyon scenario, yielding similar results\nto those obtained from measurement campaigns.\n","authors":["Jérome Eertmans","Enrico Maria Vittuci","Vittorio Degli-Esposti","Laurent Jacques","Claude Oestges"],"pdf_url":"https://arxiv.org/pdf/2410.14535v4.pdf","comment":"5 pages, 5 figures, 1 table, accepted at EuCAP 2025"},{"id":"http://arxiv.org/abs/2412.13679v1","updated":"2024-12-18T10:07:54Z","published":"2024-12-18T10:07:54Z","title":"On Enhancing Root Cause Analysis with SQL Summaries for Failures in\n  Database Workload Replays at SAP HANA","summary":"  Capturing the workload of a database and replaying this workload for a new\nversion of the database can be an effective approach for regression testing.\nHowever, false positive errors caused by many factors such as data privacy\nlimitations, time dependency or non-determinism in multi-threaded environment\ncan negatively impact the effectiveness. Therefore, we employ a machine\nlearning based framework to automate the root cause analysis of failures found\nduring replays. However, handling unseen novel issues not found in the training\ndata is one general challenge of machine learning approaches with respect to\ngeneralizability of the learned model. We describe how we continue to address\nthis challenge for more robust long-term solutions. From our experience,\nretraining with new failures is inadequate due to features overlapping across\ndistinct root causes. Hence, we leverage a large language model (LLM) to\nanalyze failed SQL statements and extract concise failure summaries as an\nadditional feature to enhance the classification process. Our experiments show\nthe F1-Macro score improved by 4.77% for our data. We consider our approach\nbeneficial for providing end users with additional information to gain more\ninsights into the found issues and to improve the assessment of the replay\nresults.\n","authors":["Neetha Jambigi","Joshua Hammesfahr","Moritz Mueller","Thomas Bach","Michael Felderer"],"pdf_url":"https://arxiv.org/pdf/2412.13679v1.pdf","comment":"The 35th IEEE International Symposium on Software Reliability\n  Engineering"},{"id":"http://arxiv.org/abs/2412.13678v1","updated":"2024-12-18T10:05:43Z","published":"2024-12-18T10:05:43Z","title":"Clio: Privacy-Preserving Insights into Real-World AI Use","summary":"  How are AI assistants being used in the real world? While model providers in\ntheory have a window into this impact via their users' data, both privacy\nconcerns and practical challenges have made analyzing this data difficult. To\naddress these issues, we present Clio (Claude insights and observations), a\nprivacy-preserving platform that uses AI assistants themselves to analyze and\nsurface aggregated usage patterns across millions of conversations, without the\nneed for human reviewers to read raw conversations. We validate this can be\ndone with a high degree of accuracy and privacy by conducting extensive\nevaluations. We demonstrate Clio's usefulness in two broad ways. First, we\nshare insights about how models are being used in the real world from one\nmillion Claude.ai Free and Pro conversations, ranging from providing advice on\nhairstyles to providing guidance on Git operations and concepts. We also\nidentify the most common high-level use cases on Claude.ai (coding, writing,\nand research tasks) as well as patterns that differ across languages (e.g.,\nconversations in Japanese discuss elder care and aging populations at\nhigher-than-typical rates). Second, we use Clio to make our systems safer by\nidentifying coordinated attempts to abuse our systems, monitoring for unknown\nunknowns during critical periods like launches of new capabilities or major\nworld events, and improving our existing monitoring systems. We also discuss\nthe limitations of our approach, as well as risks and ethical concerns. By\nenabling analysis of real-world AI usage, Clio provides a scalable platform for\nempirically grounded AI safety and governance.\n","authors":["Alex Tamkin","Miles McCain","Kunal Handa","Esin Durmus","Liane Lovitt","Ankur Rathi","Saffron Huang","Alfred Mountfield","Jerry Hong","Stuart Ritchie","Michael Stern","Brian Clarke","Landon Goldberg","Theodore R. Sumers","Jared Mueller","William McEachen","Wes Mitchell","Shan Carter","Jack Clark","Jared Kaplan","Deep Ganguli"],"pdf_url":"https://arxiv.org/pdf/2412.13678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13796v4","updated":"2024-12-18T10:03:15Z","published":"2024-05-22T16:21:02Z","title":"Generalizing Weather Forecast to Fine-grained Temporal Scales via\n  Physics-AI Hybrid Modeling","summary":"  Data-driven artificial intelligence (AI) models have made significant\nadvancements in weather forecasting, particularly in medium-range and\nnowcasting. However, most data-driven weather forecasting models are black-box\nsystems that focus on learning data mapping rather than fine-grained physical\nevolution in the time dimension. Consequently, the limitations in the temporal\nscale of datasets prevent these models from forecasting at finer time scales.\nThis paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which\nGeneralizes weather forecasts to Finer-grained Temporal scales beyond training\ndataset. Specifically, we employ a carefully designed PDE kernel to simulate\nphysical evolution on a small time scale (e.g., 300 seconds) and use a parallel\nneural networks with a learnable router for bias correction. Furthermore, we\nintroduce a lead time-aware training framework to promote the generalization of\nthe model at different lead times. The weight analysis of physics-AI modules\nindicates that physics conducts major evolution while AI performs corrections\nadaptively. Extensive experiments show that WeatherGFT trained on an hourly\ndataset, achieves state-of-the-art performance across multiple lead times and\nexhibits the capability to generalize 30-minute forecasts.\n","authors":["Wanghan Xu","Fenghua Ling","Wenlong Zhang","Tao Han","Hao Chen","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2405.13796v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04728v2","updated":"2024-12-18T10:00:35Z","published":"2024-09-07T06:24:50Z","title":"Urban traffic analysis and forecasting through shared Koopman eigenmodes","summary":"  Predicting traffic flow in data-scarce cities is challenging due to limited\nhistorical data. To address this, we leverage transfer learning by identifying\nperiodic patterns common to data-rich cities using a customized variant of\nDynamic Mode Decomposition (DMD): constrained Hankelized DMD (TrHDMD). This\nmethod uncovers common eigenmodes (urban heartbeats) in traffic patterns and\ntransfers them to data-scarce cities, significantly enhancing prediction\nperformance. TrHDMD reduces the need for extensive training datasets by\nutilizing prior knowledge from other cities. By applying Koopman operator\ntheory to multi-city loop detector data, we identify stable, interpretable, and\ntime-invariant traffic modes. Injecting ``urban heartbeats'' into forecasting\ntasks improves prediction accuracy and has the potential to enhance traffic\nmanagement strategies for cities with varying data infrastructures. Our work\nintroduces cross-city knowledge transfer via shared Koopman eigenmodes,\noffering actionable insights and reliable forecasts for data-scarce urban\nenvironments.\n","authors":["Chuhan Yang","Fares B. Mehouachi","Monica Menendez","Saif Eddin Jabari"],"pdf_url":"https://arxiv.org/pdf/2409.04728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12797v3","updated":"2024-12-18T09:54:22Z","published":"2022-02-25T16:17:23Z","title":"Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement\n  Learning Approach","summary":"  Dynamic mechanism design studies how mechanism designers should allocate\nresources among agents in a time-varying environment. We consider the problem\nwhere the agents interact with the mechanism designer according to an unknown\nMarkov Decision Process (MDP), where agent rewards and the mechanism designer's\nstate evolve according to an episodic MDP with unknown reward functions and\ntransition kernels. We focus on the online setting with linear function\napproximation and propose novel learning algorithms to recover the dynamic\nVickrey-Clarke-Grove (VCG) mechanism over multiple rounds of interaction. A key\ncontribution of our approach is incorporating reward-free online Reinforcement\nLearning (RL) to aid exploration over a rich policy space to estimate prices in\nthe dynamic VCG mechanism. We show that the regret of our proposed method is\nupper bounded by $\\tilde{\\mathcal{O}}(T^{2/3})$ and further devise a lower\nbound to show that our algorithm is efficient, incurring the same $\\Omega(T^{2\n/ 3})$ regret as the lower bound, where $T$ is the total number of rounds. Our\nwork establishes the regret guarantee for online RL in solving dynamic\nmechanism design problems without prior knowledge of the underlying model.\n","authors":["Shuang Qiu","Boxiang Lyu","Qinglin Meng","Zhaoran Wang","Zhuoran Yang","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2202.12797v3.pdf","comment":"Accepted in JMLR 2024"},{"id":"http://arxiv.org/abs/2412.13670v1","updated":"2024-12-18T09:53:12Z","published":"2024-12-18T09:53:12Z","title":"AntiLeak-Bench: Preventing Data Contamination by Automatically\n  Constructing Benchmarks with Updated Real-World Knowledge","summary":"  Data contamination hinders fair LLM evaluation by introducing test data into\nnewer models' training sets. Existing studies solve this challenge by updating\nbenchmarks with newly collected data. However, they fail to guarantee\ncontamination-free evaluation as the newly collected data may contain\npre-existing knowledge, and their benchmark updates rely on intensive human\nlabor. To address these issues, we in this paper propose AntiLeak-Bench, an\nautomated anti-leakage benchmarking framework. Instead of simply using newly\ncollected data, we construct samples with explicitly new knowledge absent from\nLLMs' training sets, which thus ensures strictly contamination-free evaluation.\nWe further design a fully automated workflow to build and update our benchmark\nwithout human labor. This significantly reduces the cost of benchmark\nmaintenance to accommodate emerging LLMs. Through extensive experiments, we\nhighlight that data contamination likely exists before LLMs' cutoff time and\ndemonstrate AntiLeak-Bench effectively overcomes this challenge.\n","authors":["Xiaobao Wu","Liangming Pan","Yuxi Xie","Ruiwen Zhou","Shuai Zhao","Yubo Ma","Mingzhe Du","Rui Mao","Anh Tuan Luu","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13667v1","updated":"2024-12-18T09:50:00Z","published":"2024-12-18T09:50:00Z","title":"Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for\n  Precise Causal Discovery","summary":"  Causal inference is an imperative foundation for decision-making across\ndomains, such as smart health, AI for drug discovery and AIOps. Traditional\nstatistical causal discovery methods, while well-established, predominantly\nrely on observational data and often overlook the semantic cues inherent in\ncause-and-effect relationships. The advent of Large Language Models (LLMs) has\nushered in an affordable way of leveraging the semantic cues for\nknowledge-driven causal discovery, but the development of LLMs for causal\ndiscovery lags behind other areas, particularly in the exploration of\nmulti-modality data. To bridge the gap, we introduce MATMCD, a multi-agent\nsystem powered by tool-augmented LLMs. MATMCD has two key agents: a Data\nAugmentation agent that retrieves and processes modality-augmented data, and a\nCausal Constraint agent that integrates multi-modal data for knowledge-driven\ninference. Delicate design of the inner-workings ensures successful cooperation\nof the agents. Our empirical study across seven datasets suggests the\nsignificant potential of multi-modality enhanced causal discovery.\n","authors":["ChengAo Shen","Zhengzhang Chen","Dongsheng Luo","Dongkuan Xu","Haifeng Chen","Jingchao Ni"],"pdf_url":"https://arxiv.org/pdf/2412.13667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13035v3","updated":"2024-12-18T09:48:32Z","published":"2024-09-19T18:11:59Z","title":"TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement\n  Learning","summary":"  The increasing prevalence of large language models (LLMs) such as GPT-4 in\nvarious applications has led to a surge in the size of prompts required for\noptimal performance, leading to challenges in computational efficiency. Prompt\ncompression aims to reduce the inference cost by minimizing input tokens\nwithout compromising on the task performance. However, existing prompt\ncompression techniques either rely on sub-optimal metrics such as information\nentropy or model it as a task-agnostic token classification problem that fails\nto capture task-specific information. To address these issues, we propose a\nnovel and efficient reinforcement learning (RL) based task-aware prompt\ncompression method. To ensure low latency requirements, we leverage existing\nTransformer encoder-based token classification model while guiding the learning\nprocess with task-specific reward signals using lightweight REINFORCE\nalgorithm. We evaluate the performance of our method on three diverse and\nchallenging tasks including text summarization, question answering and code\nsummarization. We demonstrate that our RL-guided compression method improves\nthe task performance by 8% - 189% across these three scenarios over\nstate-of-the-art compression techniques while satisfying the same compression\nrate and latency requirements.\n","authors":["Shivam Shandilya","Menglin Xia","Supriyo Ghosh","Huiqiang Jiang","Jue Zhang","Qianhui Wu","Victor Rühle"],"pdf_url":"https://arxiv.org/pdf/2409.13035v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15024v2","updated":"2024-12-18T09:47:25Z","published":"2024-11-22T15:55:19Z","title":"DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models","summary":"  Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.\n","authors":["Keda Tao","Can Qin","Haoxuan You","Yang Sui","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15024v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13665v1","updated":"2024-12-18T09:47:19Z","published":"2024-12-18T09:47:19Z","title":"Time-Reversible Bridges of Data with Machine Learning","summary":"  The analysis of dynamical systems is a fundamental tool in the natural\nsciences and engineering. It is used to understand the evolution of systems as\nlarge as entire galaxies and as small as individual molecules. With predefined\nconditions on the evolution of dy-namical systems, the underlying differential\nequations have to fulfill specific constraints in time and space. This class of\nproblems is known as boundary value problems. This thesis presents novel\napproaches to learn time-reversible deterministic and stochastic dynamics\nconstrained by initial and final conditions. The dynamics are inferred by\nmachine learning algorithms from observed data, which is in contrast to the\ntraditional approach of solving differential equations by numerical\nintegration. The work in this thesis examines a set of problems of increasing\ndifficulty each of which is concerned with learning a different aspect of the\ndynamics. Initially, we consider learning deterministic dynamics from ground\ntruth solutions which are constrained by deterministic boundary conditions.\nSecondly, we study a boundary value problem in discrete state spaces, where the\nforward dynamics follow a stochastic jump process and the boundary conditions\nare discrete probability distributions. In particular, the stochastic dynamics\nof a specific jump process, the Ehrenfest process, is considered and the\nreverse time dynamics are inferred with machine learning. Finally, we\ninvestigate the problem of inferring the dynamics of a continuous-time\nstochastic process between two probability distributions without any reference\ninformation. Here, we propose a novel criterion to learn time-reversible\ndynamics of two stochastic processes to solve the Schr\\\"odinger Bridge Problem.\n","authors":["Ludwig Winkler"],"pdf_url":"https://arxiv.org/pdf/2412.13665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13662v1","updated":"2024-12-18T09:39:12Z","published":"2024-12-18T09:39:12Z","title":"When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement\n  Learning?","summary":"  Learning policies from high-dimensional visual inputs, such as pixels and\npoint clouds, is crucial in various applications. Visual reinforcement learning\nis a promising approach that directly trains policies from visual observations,\nalthough it faces challenges in sample efficiency and computational costs. This\nstudy conducts an empirical comparison of State-to-Visual DAgger, a two-stage\nframework that initially trains a state policy before adopting online imitation\nto learn a visual policy, and Visual RL across a diverse set of tasks. We\nevaluate both methods across 16 tasks from three benchmarks, focusing on their\nasymptotic performance, sample efficiency, and computational costs.\nSurprisingly, our findings reveal that State-to-Visual DAgger does not\nuniversally outperform Visual RL but shows significant advantages in\nchallenging tasks, offering more consistent performance. In contrast, its\nbenefits in sample efficiency are less pronounced, although it often reduces\nthe overall wall-clock time required for training. Based on our findings, we\nprovide recommendations for practitioners and hope that our results contribute\nvaluable perspectives for future research in visual policy learning.\n","authors":["Tongzhou Mu","Zhaoyang Li","Stanisław Wiktor Strzelecki","Xiu Yuan","Yunchao Yao","Litian Liang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2412.13662v1.pdf","comment":"Accepted by The 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.12759v2","updated":"2024-12-18T09:34:58Z","published":"2024-12-17T10:24:29Z","title":"Versatile Ordering Network: An Attention-based Neural Network for\n  Ordering Across Scales and Quality Metrics","summary":"  Ordering has been extensively studied in many visualization applications,\nsuch as axis and matrix reordering, for the simple reason that the order will\ngreatly impact the perceived pattern of data. Many quality metrics concerning\ndata pattern, perception, and aesthetics are proposed, and respective\noptimization algorithms are developed. However, the optimization problems\nrelated to ordering are often difficult to solve (e.g., TSP is NP-complete),\nand developing specialized optimization algorithms is costly. In this paper, we\npropose Versatile Ordering Network (VON), which automatically learns the\nstrategy to order given a quality metric. VON uses the quality metric to\nevaluate its solutions, and leverages reinforcement learning with a greedy\nrollout baseline to improve itself. This keeps the metric transparent and\nallows VON to optimize over different metrics. Additionally, VON uses the\nattention mechanism to collect information across scales and reposition the\ndata points with respect to the current context. This allows VONs to deal with\ndata points following different distributions. We examine the effectiveness of\nVON under different usage scenarios and metrics. The results demonstrate that\nVON can produce comparable results to specialized solvers. The code is\navailable at https://github.com/sysuvis/VON.\n","authors":["Zehua Yu","Weihan Zhang","Sihan Pan","Jun Tao"],"pdf_url":"https://arxiv.org/pdf/2412.12759v2.pdf","comment":"has been accepted by TVCG on 11-Dec-2024"},{"id":"http://arxiv.org/abs/2407.15161v2","updated":"2024-12-18T09:07:47Z","published":"2024-07-21T13:33:08Z","title":"FFHFlow: A Flow-based Variational Approach for Learning Diverse\n  Dexterous Grasps with Shape-Aware Introspection","summary":"  Synthesizing diverse dexterous grasps from uncertain partial observation is\nan important yet challenging task for physically intelligent embodiments.\nPrevious works on generative grasp synthesis fell short of precisely capturing\nthe complex grasp distribution and reasoning about shape uncertainty in the\nunstructured and often partially perceived reality. In this work, we introduce\na novel model that can generate diverse grasps for a multi-fingered hand while\nintrospectively handling perceptual uncertainty and recognizing unknown object\ngeometry to avoid performance degradation. Specifically, we devise a Deep\nLatent Variable Model (DLVM) based on Normalizing Flows (NFs), facilitating\nhierarchical and expressive latent representation for modeling versatile\ngrasps. Our model design counteracts typical pitfalls of its popular\nalternative in generative grasping, i.e., conditional Variational Autoencoders\n(cVAEs) whose performance is limited by mode collapse and miss-specified prior\nissues. Moreover, the resultant feature hierarchy and the exact flow likelihood\ncomputation endow our model with shape-aware introspective capabilities,\nenabling it to quantify the shape uncertainty of partial point clouds and\ndetect objects of novel geometry. We further achieve performance gain by fusing\nthis information with a discriminative grasp evaluator, facilitating a novel\nhybrid way for grasp evaluation. Comprehensive simulated and real-world\nexperiments show that the proposed idea gains superior performance and higher\nrun-time efficiency against strong baselines, including diffusion models. We\nalso demonstrate substantial benefits of greater diversity for grasping objects\nin clutter and a confined workspace in the real world.\n","authors":["Qian Feng","Jianxiang Feng","Zhaopeng Chen","Rudolph Triebel","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2407.15161v2.pdf","comment":"First two authors contributed equally, whose ordering decided via\n  coin-tossing. Under Reivew"},{"id":"http://arxiv.org/abs/2412.13630v1","updated":"2024-12-18T09:06:16Z","published":"2024-12-18T09:06:16Z","title":"Policy Decorator: Model-Agnostic Online Refinement for Large Policy\n  Model","summary":"  Recent advancements in robot learning have used imitation learning with large\nmodels and extensive demonstrations to develop effective policies. However,\nthese models are often limited by the quantity, quality, and diversity of\ndemonstrations. This paper explores improving offline-trained imitation\nlearning models through online interactions with the environment. We introduce\nPolicy Decorator, which uses a model-agnostic residual policy to refine large\nimitation learning models during online interactions. By implementing\ncontrolled exploration strategies, Policy Decorator enables stable,\nsample-efficient online learning. Our evaluation spans eight tasks across two\nbenchmarks-ManiSkill and Adroit-and involves two state-of-the-art imitation\nlearning models (Behavior Transformer and Diffusion Policy). The results show\nPolicy Decorator effectively improves the offline-trained policies and\npreserves the smooth motion of imitation learning models, avoiding the erratic\nbehaviors of pure RL policies. See our project page\n(https://policydecorator.github.io) for videos.\n","authors":["Xiu Yuan","Tongzhou Mu","Stone Tao","Yunhao Fang","Mengke Zhang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2412.13630v1.pdf","comment":"Explore videos, data, code, and more at\n  https://policydecorator.github.io"},{"id":"http://arxiv.org/abs/2412.13627v1","updated":"2024-12-18T09:05:19Z","published":"2024-12-18T09:05:19Z","title":"TAUDiff: Improving statistical downscaling for extreme weather events\n  using generative diffusion models","summary":"  Deterministic regression-based downscaling models for climate variables often\nsuffer from spectral bias, which can be mitigated by generative models like\ndiffusion models. To enable efficient and reliable simulation of extreme\nweather events, it is crucial to achieve rapid turnaround, dynamical\nconsistency, and accurate spatio-temporal spectral recovery. We propose an\nefficient correction diffusion model, TAUDiff, that combines a deterministic\nspatio-temporal model for mean field downscaling with a smaller generative\ndiffusion model for recovering the fine-scale stochastic features. We\ndemonstrate the efficacy of this approach on downscaling atmospheric wind\nvelocity fields obtained from coarse GCM simulations. Our approach can not only\nensure quicker simulation of extreme events but also reduce overall carbon\nfootprint due to low inference times.\n","authors":["Rahul Sundar","Nishant Parashar","Antoine Blanchard","Boyko Dodov"],"pdf_url":"https://arxiv.org/pdf/2412.13627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07639v2","updated":"2024-12-18T09:04:32Z","published":"2024-12-10T16:19:08Z","title":"Offline Multi-Agent Reinforcement Learning via In-Sample Sequential\n  Policy Optimization","summary":"  Offline Multi-Agent Reinforcement Learning (MARL) is an emerging field that\naims to learn optimal multi-agent policies from pre-collected datasets.\nCompared to single-agent case, multi-agent setting involves a large joint\nstate-action space and coupled behaviors of multiple agents, which bring extra\ncomplexity to offline policy optimization. In this work, we revisit the\nexisting offline MARL methods and show that in certain scenarios they can be\nproblematic, leading to uncoordinated behaviors and out-of-distribution (OOD)\njoint actions. To address these issues, we propose a new offline MARL\nalgorithm, named In-Sample Sequential Policy Optimization (InSPO). InSPO\nsequentially updates each agent's policy in an in-sample manner, which not only\navoids selecting OOD joint actions but also carefully considers teammates'\nupdated policies to enhance coordination. Additionally, by thoroughly exploring\nlow-probability actions in the behavior policy, InSPO can well address the\nissue of premature convergence to sub-optimal solutions. Theoretically, we\nprove InSPO guarantees monotonic policy improvement and converges to quantal\nresponse equilibrium (QRE). Experimental results demonstrate the effectiveness\nof our method compared to current state-of-the-art offline MARL methods.\n","authors":["Zongkai Liu","Qian Lin","Chao Yu","Xiawei Wu","Yile Liang","Donghui Li","Xuetao Ding"],"pdf_url":"https://arxiv.org/pdf/2412.07639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13623v1","updated":"2024-12-18T09:04:07Z","published":"2024-12-18T09:04:07Z","title":"Unifying Attribution-Based Explanations Using Functional Decomposition","summary":"  The black box problem in machine learning has led to the introduction of an\never-increasing set of explanation methods for complex models. These\nexplanations have different properties, which in turn has led to the problem of\nmethod selection: which explanation method is most suitable for a given use\ncase? In this work, we propose a unifying framework of attribution-based\nexplanation methods, which provides a step towards a rigorous study of the\nsimilarities and differences of explanations. We first introduce removal-based\nattribution methods (RBAMs), and show that an extensively broad selection of\nexisting methods can be viewed as such RBAMs. We then introduce the canonical\nadditive decomposition (CAD). This is a general construction for additively\ndecomposing any function based on the central idea of removing (groups of)\nfeatures. We proceed to show that indeed every valid additive decomposition is\nan instance of the CAD, and that any removal-based attribution method is\nassociated with a specific CAD. Next, we show that any removal-based\nattribution method can be completely defined as a game-theoretic value or\ninteraction index for a specific (possibly constant-shifted) cooperative game,\nwhich is defined using the corresponding CAD of the method. We then use this\nintrinsic connection to define formal descriptions of specific behaviours of\nexplanation methods, which we also call functional axioms, and identify\nsufficient conditions on the corresponding CAD and game-theoretic value or\ninteraction index of an attribution method under which the attribution method\nis guaranteed to adhere to these functional axioms. Finally, we show how this\nunifying framework can be used to develop new, efficient approximations for\nexisting explanation methods.\n","authors":["Arne Gevaert","Yvan Saeys"],"pdf_url":"https://arxiv.org/pdf/2412.13623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02329v2","updated":"2024-12-18T08:56:43Z","published":"2024-06-04T13:58:28Z","title":"On Affine Homotopy between Language Encoders","summary":"  Pre-trained language encoders -- functions that represent text as vectors --\nare an integral component of many NLP tasks. We tackle a natural question in\nlanguage encoder analysis: What does it mean for two encoders to be similar? We\ncontend that a faithful measure of similarity needs to be \\emph{intrinsic},\nthat is, task-independent, yet still be informative of \\emph{extrinsic}\nsimilarity -- the performance on downstream tasks. It is common to consider two\nencoders similar if they are \\emph{homotopic}, i.e., if they can be aligned\nthrough some transformation. In this spirit, we study the properties of\n\\emph{affine} alignment of language encoders and its implications on extrinsic\nsimilarity. We find that while affine alignment is fundamentally an asymmetric\nnotion of similarity, it is still informative of extrinsic similarity. We\nconfirm this on datasets of natural language representations. Beyond providing\nuseful bounds on extrinsic similarity, affine intrinsic similarity also allows\nus to begin uncovering the structure of the space of pre-trained encoders by\ndefining an order over them.\n","authors":["Robin SM Chan","Reda Boumasmoud","Anej Svete","Yuxin Ren","Qipeng Guo","Zhijing Jin","Shauli Ravfogel","Mrinmaya Sachan","Bernhard Schölkopf","Mennatallah El-Assady","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.02329v2.pdf","comment":"10 pages, Accepted at NeurIPS 2024 (Main)"},{"id":"http://arxiv.org/abs/2311.07315v3","updated":"2024-12-18T08:39:02Z","published":"2023-11-13T13:10:52Z","title":"An introduction to reinforcement learning for neuroscience","summary":"  Reinforcement learning (RL) has a rich history in neuroscience, from early\nwork on dopamine as a reward prediction error signal (Schultz et al., 1997) to\nrecent work proposing that the brain could implement a form of 'distributional\nreinforcement learning' popularized in machine learning (Dabney et al., 2020).\nThere has been a close link between theoretical advances in reinforcement\nlearning and neuroscience experiments throughout this literature, and the\ntheories describing the experimental data have therefore become increasingly\ncomplex. Here, we provide an introduction and mathematical background to many\nof the methods that have been used in systems neroscience. We start with an\noverview of the RL problem and classical temporal difference algorithms,\nfollowed by a discussion of 'model-free', 'model-based', and intermediate RL\nalgorithms. We then introduce deep reinforcement learning and discuss how this\nframework has led to new insights in neuroscience. This includes a particular\nfocus on meta-reinforcement learning (Wang et al., 2018) and distributional RL\n(Dabney et al., 2020). Finally, we discuss potential shortcomings of the RL\nformalism for neuroscience and highlight open questions in the field. Code that\nimplements the methods discussed and generates the figures is also provided.\n","authors":["Kristopher T. Jensen"],"pdf_url":"https://arxiv.org/pdf/2311.07315v3.pdf","comment":"Code available at:\n  https://colab.research.google.com/drive/1ZC4lR8kTO48yySDZtcOEdMKd3NqY_ly1?usp=sharing"},{"id":"http://arxiv.org/abs/2412.13607v1","updated":"2024-12-18T08:35:40Z","published":"2024-12-18T08:35:40Z","title":"PreMixer: MLP-Based Pre-training Enhanced MLP-Mixers for Large-scale\n  Traffic Forecasting","summary":"  In urban computing, precise and swift forecasting of multivariate time series\ndata from traffic networks is crucial. This data incorporates additional\nspatial contexts such as sensor placements and road network layouts, and\nexhibits complex temporal patterns that amplify challenges for predictive\nlearning in traffic management, smart mobility demand, and urban planning.\nConsequently, there is an increasing need to forecast traffic flow across\nbroader geographic regions and for higher temporal coverage. However, current\nresearch encounters limitations because of the inherent inefficiency of model\nand their unsuitability for large-scale traffic network applications due to\nmodel complexity. This paper proposes a novel framework, named PreMixer,\ndesigned to bridge this gap. It features a predictive model and a pre-training\nmechanism, both based on the principles of Multi-Layer Perceptrons (MLP). The\nPreMixer comprehensively consider temporal dependencies of traffic patterns in\ndifferent time windows and processes the spatial dynamics as well.\nAdditionally, we integrate spatio-temporal positional encoding to manage\nspatiotemporal heterogeneity without relying on predefined graphs. Furthermore,\nour innovative pre-training model uses a simple patch-wise MLP to conduct\nmasked time series modeling, learning from long-term historical data segmented\ninto patches to generate enriched contextual representations. This approach\nenhances the downstream forecasting model without incurring significant time\nconsumption or computational resource demands owing to improved learning\nefficiency and data handling flexibility. Our framework achieves comparable\nstate-of-the-art performance while maintaining high computational efficiency,\nas verified by extensive experiments on large-scale traffic datasets.\n","authors":["Tongtong Zhang","Zhiyong Cui","Bingzhang Wang","Yilong Ren","Haiyang Yu","Pan Deng","Yinhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03120v3","updated":"2024-12-18T08:34:23Z","published":"2024-12-04T08:39:45Z","title":"Sinkhorn Algorithm for Sequentially Composed Optimal Transports","summary":"  Sinkhorn algorithm is the de-facto standard approximation algorithm for\noptimal transport, which has been applied to a variety of applications,\nincluding image processing and natural language processing. In theory, the\nproof of its convergence follows from the convergence of the Sinkhorn--Knopp\nalgorithm for the matrix scaling problem, and Altschuler et al. show that its\nworst-case time complexity is in near-linear time. Very recently, sequentially\ncomposed optimal transports were proposed by Watanabe and Isobe as a\nhierarchical extension of optimal transports. In this paper, we present an\nefficient approximation algorithm, namely Sinkhorn algorithm for sequentially\ncomposed optimal transports, for its entropic regularization. Furthermore, we\npresent a theoretical analysis of the Sinkhorn algorithm, namely (i) its\nexponential convergence to the optimal solution with respect to the Hilbert\npseudometric, and (ii) a worst-case complexity analysis for the case of one\nsequential composition.\n","authors":["Kazuki Watanabe","Noboru Isobe"],"pdf_url":"https://arxiv.org/pdf/2412.03120v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.10824v2","updated":"2024-12-18T08:25:55Z","published":"2024-12-14T13:05:05Z","title":"Diffusion Model from Scratch","summary":"  Diffusion generative models are currently the most popular generative models.\nHowever, their underlying modeling process is quite complex, and starting\ndirectly with the seminal paper Denoising Diffusion Probability Model (DDPM)\ncan be challenging. This paper aims to assist readers in building a\nfoundational understanding of generative models by tracing the evolution from\nVAEs to DDPM through detailed mathematical derivations and a problem-oriented\nanalytical approach. It also explores the core ideas and improvement strategies\nof current mainstream methodologies, providing guidance for undergraduate and\ngraduate students interested in learning about diffusion models.\n","authors":["Wang Zhen","Dong Yunyun"],"pdf_url":"https://arxiv.org/pdf/2412.10824v2.pdf","comment":"There were problems with the typography of our illustrations, and\n  there were problems with the derivation of the 200-step formula"},{"id":"http://arxiv.org/abs/2412.13592v1","updated":"2024-12-18T08:15:55Z","published":"2024-12-18T08:15:55Z","title":"PASCO (PArallel Structured COarsening): an overlay to speed up graph\n  clustering algorithms","summary":"  Clustering the nodes of a graph is a cornerstone of graph analysis and has\nbeen extensively studied. However, some popular methods are not suitable for\nvery large graphs: e.g., spectral clustering requires the computation of the\nspectral decomposition of the Laplacian matrix, which is not applicable for\nlarge graphs with a large number of communities. This work introduces PASCO, an\noverlay that accelerates clustering algorithms. Our method consists of three\nsteps: 1-We compute several independent small graphs representing the input\ngraph by applying an efficient and structure-preserving coarsening algorithm.\n2-A clustering algorithm is run in parallel onto each small graph and provides\nseveral partitions of the initial graph. 3-These partitions are aligned and\ncombined with an optimal transport method to output the final partition. The\nPASCO framework is based on two key contributions: a novel global algorithm\nstructure designed to enable parallelization and a fast, empirically validated\ngraph coarsening algorithm that preserves structural properties. We demonstrate\nthe strong performance of 1 PASCO in terms of computational efficiency,\nstructural preservation, and output partition quality, evaluated on both\nsynthetic and real-world graph datasets.\n","authors":["Etienne Lasalle","Rémi Vaudaine","Titouan Vayer","Pierre Borgnat","Rémi Gribonval","Paulo Gonçalves","Màrton Karsai"],"pdf_url":"https://arxiv.org/pdf/2412.13592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09563v2","updated":"2024-12-18T08:15:09Z","published":"2024-06-13T20:12:09Z","title":"e-COP : Episodic Constrained Optimization of Policies","summary":"  In this paper, we present the $\\texttt{e-COP}$ algorithm, the first policy\noptimization algorithm for constrained Reinforcement Learning (RL) in episodic\n(finite horizon) settings. Such formulations are applicable when there are\nseparate sets of optimization criteria and constraints on a system's behavior.\nWe approach this problem by first establishing a policy difference lemma for\nthe episodic setting, which provides the theoretical foundation for the\nalgorithm. Then, we propose to combine a set of established and novel solution\nideas to yield the $\\texttt{e-COP}$ algorithm that is easy to implement and\nnumerically stable, and provide a theoretical guarantee on optimality under\ncertain scaling assumptions. Through extensive empirical analysis using\nbenchmarks in the Safety Gym suite, we show that our algorithm has similar or\nbetter performance than SoTA (non-episodic) algorithms adapted for the episodic\nsetting. The scalability of the algorithm opens the door to its application in\nsafety-constrained Reinforcement Learning from Human Feedback for Large\nLanguage or Diffusion Models.\n","authors":["Akhil Agnihotri","Rahul Jain","Deepak Ramachandran","Sahil Singla"],"pdf_url":"https://arxiv.org/pdf/2406.09563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12687v2","updated":"2024-12-18T08:14:35Z","published":"2024-12-17T09:08:18Z","title":"Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models","summary":"  This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),\nwherein the SLM locally measures its output uncertainty and skips both uplink\ntransmissions and LLM operations for tokens that are likely to be accepted.\nThis opportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputations by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping.\n","authors":["Seungeun Oh","Jinhyuk Kim","Jihong Park","Seung-Woo Ko","Tony Q. S. Quek","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12687v2.pdf","comment":"6 pages, 6 figures; This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2412.13589v1","updated":"2024-12-18T08:12:55Z","published":"2024-12-18T08:12:55Z","title":"SemiDFL: A Semi-Supervised Paradigm for Decentralized Federated Learning","summary":"  Decentralized federated learning (DFL) realizes cooperative model training\namong connected clients without relying on a central server, thereby mitigating\ncommunication bottlenecks and eliminating the single-point failure issue\npresent in centralized federated learning (CFL). Most existing work on DFL\nfocuses on supervised learning, assuming each client possesses sufficient\nlabeled data for local training. However, in real-world applications, much of\nthe data is unlabeled. We address this by considering a challenging yet\npractical semisupervised learning (SSL) scenario in DFL, where clients may have\nvarying data sources: some with few labeled samples, some with purely unlabeled\ndata, and others with both. In this work, we propose SemiDFL, the first\nsemi-supervised DFL method that enhances DFL performance in SSL scenarios by\nestablishing a consensus in both data and model spaces. Specifically, we\nutilize neighborhood information to improve the quality of pseudo-labeling,\nwhich is crucial for effectively leveraging unlabeled data. We then design a\nconsensusbased diffusion model to generate synthesized data, which is used in\ncombination with pseudo-labeled data to create mixed datasets. Additionally, we\ndevelop an adaptive aggregation method that leverages the model accuracy of\nsynthesized data to further enhance SemiDFL performance. Through extensive\nexperimentation, we demonstrate the remarkable performance superiority of the\nproposed DFL-Semi method over existing CFL and DFL schemes in both IID and\nnon-IID SSL scenarios.\n","authors":["Xinyang Liu","Pengchao Han","Xuan Li","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13589v1.pdf","comment":"Accepted by AAAI 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2412.14158v1","updated":"2024-12-18T18:53:22Z","published":"2024-12-18T18:53:22Z","title":"AKiRa: Augmentation Kit on Rays for optical video generation","summary":"  Recent advances in text-conditioned video diffusion have greatly improved\nvideo quality. However, these methods offer limited or sometimes no control to\nusers on camera aspects, including dynamic camera motion, zoom, distorted lens\nand focus shifts. These motion and optical aspects are crucial for adding\ncontrollability and cinematic elements to generation frameworks, ultimately\nresulting in visual content that draws focus, enhances mood, and guides\nemotions according to filmmakers' controls. In this paper, we aim to close the\ngap between controllable video generation and camera optics. To achieve this,\nwe propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework\nthat builds and trains a camera adapter with a complex camera model over an\nexisting video generation backbone. It enables fine-tuned control over camera\nmotion as well as complex optical parameters (focal length, distortion,\naperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh.\nExtensive experiments demonstrate AKiRa's effectiveness in combining and\ncomposing camera optics while outperforming all state-of-the-art methods. This\nwork sets a new landmark in controlled and optically enhanced video generation,\npaving the way for future optical video generation methods.\n","authors":["Xi Wang","Robin Courant","Marc Christie","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2412.14158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14056v1","updated":"2024-12-18T17:06:21Z","published":"2024-12-18T17:06:21Z","title":"A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future","summary":"  Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.\n","authors":["Shilin Sun","Wenbin An","Feng Tian","Fang Nan","Qidong Liu","Jun Liu","Nazaraf Shah","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14056v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.14018v1","updated":"2024-12-18T16:34:51Z","published":"2024-12-18T16:34:51Z","title":"SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical\n  Video Generation","summary":"  Medical video generation has transformative potential for enhancing surgical\nunderstanding and pathology insights through precise and controllable visual\nrepresentations. However, current models face limitations in controllability\nand authenticity. To bridge this gap, we propose SurgSora, a\nmotion-controllable surgical video generation framework that uses a single\ninput frame and user-controllable motion cues. SurgSora consists of three key\nmodules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB\nand depth features from the input frame and integrates them with segmentation\ncues to capture detailed spatial features of complex anatomical structures; the\nDecoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D\nfeatures at multiple scales to enhance temporal understanding and object\nspatial dynamics; and the Trajectory Controller (TC), which allows users to\nspecify motion directions and estimates sparse optical flow, guiding the video\ngeneration process. The fused features are used as conditions for a frozen\nStable Diffusion model to produce realistic, temporally coherent surgical\nvideos. Extensive evaluations demonstrate that SurgSora outperforms\nstate-of-the-art methods in controllability and authenticity, showing its\npotential to advance surgical video generation for medical education, training,\nand research.\n","authors":["Tong Chen","Shuya Yang","Junyi Wang","Long Bai","Hongliang Ren","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14005v1","updated":"2024-12-18T16:20:21Z","published":"2024-12-18T16:20:21Z","title":"Real-Time Position-Aware View Synthesis from Single-View Input","summary":"  Recent advancements in view synthesis have significantly enhanced immersive\nexperiences across various computer graphics and multimedia applications,\nincluding telepresence, and entertainment. By enabling the generation of new\nperspectives from a single input view, view synthesis allows users to better\nperceive and interact with their environment. However, many state-of-the-art\nmethods, while achieving high visual quality, face limitations in real-time\nperformance, which makes them less suitable for live applications where low\nlatency is critical. In this paper, we present a lightweight, position-aware\nnetwork designed for real-time view synthesis from a single input image and a\ntarget camera pose. The proposed framework consists of a Position Aware\nEmbedding, modeled with a multi-layer perceptron, which efficiently maps\npositional information from the target pose to generate high dimensional\nfeature maps. These feature maps, along with the input image, are fed into a\nRendering Network that merges features from dual encoder branches to resolve\nboth high level semantics and low level details, producing a realistic new view\nof the scene. Experimental results demonstrate that our method achieves\nsuperior efficiency and visual quality compared to existing approaches,\nparticularly in handling complex translational movements without explicit\ngeometric operations like warping. This work marks a step toward enabling\nreal-time view synthesis from a single image for live and interactive\napplications.\n","authors":["Manu Gond","Emin Zerman","Sebastian Knorr","Mårten Sjöström"],"pdf_url":"https://arxiv.org/pdf/2412.14005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13743v1","updated":"2024-12-18T11:21:39Z","published":"2024-12-18T11:21:39Z","title":"User-Generated Content and Editors in Games: A Comprehensive Survey","summary":"  User-Generated Content (UGC) refers to any form of content, such as posts and\nimages, created by users rather than by professionals. In recent years, UGC has\nbecome an essential part of the evolving video game industry, influencing both\ngame culture and community dynamics. The ability for users to actively\ncontribute to the games they engage with has shifted the landscape of gaming\nfrom a one-directional entertainment experience into a collaborative,\nuser-driven ecosystem. Therefore, this growing trend highlights the urgent need\nfor summarizing the current UGC development in game industry. Our conference\npaper has systematically classified the existing UGC in games and the UGC\neditors separately into four types. However, the previous survey lacks the\ndepth and precision necessary to capture the wide-ranging and increasingly\ncomplex nature of UGC. To this end, as an extension of previous work, this\npaper presents a refined and expanded classification of UGC and UGC editors\nwithin video games, offering a more robust and comprehensive framework with\nrepresentative cases that better reflects the diversity and nuances of\ncontemporary user-generated contributions. Moreover, we provide our insights on\nthe future of UGC, involving game culture, game genre and user creative\ntendencies, artificial intelligence, its potential ethical considerations, and\nrelationship between games, users and communities.\n","authors":["Yuyue Liu","Haihan Duan","Wei Cai"],"pdf_url":"https://arxiv.org/pdf/2412.13743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13614v1","updated":"2024-12-18T08:49:01Z","published":"2024-12-18T08:49:01Z","title":"Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity\n  Linking","summary":"  Visual Entity Linking (VEL) is a crucial task for achieving fine-grained\nvisual understanding, matching objects within images (visual mentions) to\nentities in a knowledge base. Previous VEL tasks rely on textual inputs, but\nwriting queries for complex scenes can be challenging. Visual inputs like\nclicks or bounding boxes offer a more convenient alternative. Therefore, we\npropose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses\npixel masks from visual inputs to refer to objects, supplementing reference\nmethods for VEL. To facilitate research on this task, we have constructed the\nMaskOVEN-Wiki dataset through an entirely automatic reverse region-entity\nannotation framework. This dataset contains over 5 million annotations aligning\npixel-level regions with entity-level labels, which will advance visual\nunderstanding towards fine-grained. Moreover, as pixel masks correspond to\nsemantic regions in an image, we enhance previous patch-interacted attention\nwith region-interacted attention by a visual semantic tokenization approach.\nManual evaluation results indicate that the reverse annotation framework\nachieved a 94.8% annotation success rate. Experimental results show that models\ntrained on this dataset improved accuracy by 18 points compared to zero-shot\nmodels. Additionally, the semantic tokenization method achieved a 5-point\naccuracy improvement over the trained baseline.\n","authors":["Zhengfei Xu","Sijia Zhao","Yanchao Hao","Xiaolong Liu","Lili Li","Yuyang Yin","Bo Li","Xi Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2412.13614v1.pdf","comment":"AAAI 2025;Dataset are released at\n  https://github.com/NP-NET-research/PL-VEL"},{"id":"http://arxiv.org/abs/2412.13609v1","updated":"2024-12-18T08:36:35Z","published":"2024-12-18T08:36:35Z","title":"Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production","summary":"  Sign Language Production (SLP) aims to generate semantically consistent sign\nvideos from textual statements, where the conversion from textual glosses to\nsign poses (G2P) is a crucial step. Existing G2P methods typically treat sign\nposes as discrete three-dimensional coordinates and directly fit them, which\noverlooks the relative positional relationships among joints. To this end, we\nprovide a new perspective, constraining joint associations and gesture details\nby modeling the limb bones to improve the accuracy and naturalness of the\ngenerated poses. In this work, we propose a pioneering iconicity disentangled\ndiffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD\nincorporates a novel Iconicity Disentanglement (ID) module to bridge the gap\nbetween relative positions among joints. The ID module disentangles the\nconventional 3D joint representation into a 4D bone representation, comprising\nthe 3D spatial direction vector and 1D spatial distance vector between adjacent\njoints. Additionally, an Attribute Controllable Diffusion (ACD) module is\nintroduced to further constrain joint associations, in which the attribute\nseparation layer aims to separate the bone direction and length attributes, and\nthe attribute control layer is designed to guide the pose generation by\nleveraging the above attributes. The ACD module utilizes the gloss embeddings\nas semantic conditions and finally generates sign poses from noise embeddings.\nExtensive experiments on PHOENIX14T and USTC-CSL datasets validate the\neffectiveness of our method. The code is available at:\nhttps://github.com/NaVi-start/Sign-IDD.\n","authors":["Shengeng Tang","Jiayi He","Dan Guo","Yanyan Wei","Feng Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.13609v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13462v1","updated":"2024-12-18T03:18:03Z","published":"2024-12-18T03:18:03Z","title":"SAVGBench: Benchmarking Spatially Aligned Audio-Video Generation","summary":"  This work addresses the lack of multimodal generative models capable of\nproducing high-quality videos with spatially aligned audio. While recent\nadvancements in generative models have been successful in video generation,\nthey often overlook the spatial alignment between audio and visuals, which is\nessential for immersive experiences. To tackle this problem, we establish a new\nresearch direction in benchmarking Spatially Aligned Audio-Video Generation\n(SAVG). We propose three key components for the benchmark: dataset, baseline,\nand metrics. We introduce a spatially aligned audio-visual dataset, derived\nfrom an audio-visual dataset consisting of multichannel audio, video, and\nspatiotemporal annotations of sound events. We propose a baseline audio-visual\ndiffusion model focused on stereo audio-visual joint learning to accommodate\nspatial sound. Finally, we present metrics to evaluate video and spatial audio\nquality, including a new spatial audio-visual alignment metric. Our\nexperimental result demonstrates that gaps exist between the baseline model and\nground truth in terms of video and audio quality, and spatial alignment between\nboth modalities.\n","authors":["Kazuki Shimada","Christian Simon","Takashi Shibuya","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2412.13462v1.pdf","comment":"5 pages, 3 figures"}],"Performance":[{"id":"http://arxiv.org/abs/2412.04377v2","updated":"2024-12-18T12:55:49Z","published":"2024-12-05T17:52:35Z","title":"A Hitchhiker's Guide to Understanding Performances of Two-Class\n  Classifiers","summary":"  Properly understanding the performances of classifiers is essential in\nvarious scenarios. However, the literature often relies only on one or two\nstandard scores to compare classifiers, which fails to capture the nuances of\napplication-specific requirements, potentially leading to suboptimal classifier\nselection. Recently, a paper on the foundations of the theory of\nperformance-based ranking introduced a tool, called the Tile, that organizes an\ninfinity of ranking scores into a 2D map. Thanks to the Tile, it is now\npossible to evaluate and compare classifiers efficiently, displaying all\npossible application-specific preferences instead of having to rely on a pair\nof scores. In this paper, we provide a first hitchhiker's guide for\nunderstanding the performances of two-class classifiers by presenting four\nscenarios, each showcasing a different user profile: a theoretical analyst, a\nmethod designer, a benchmarker, and an application developer. Particularly, we\nshow that we can provide different interpretative flavors that are adapted to\nthe user's needs by mapping different values on the Tile. As an illustration,\nwe leverage the newly introduced Tile tool and the different flavors to rank\nand analyze the performances of 74 state-of-the-art semantic segmentation\nmodels in two-class classification through the eyes of the four user profiles.\nThrough these user profiles, we demonstrate that the Tile effectively captures\nthe behavior of classifiers in a single visualization, while accommodating an\ninfinite number of ranking scores.\n","authors":["Anaïs Halin","Sébastien Piérard","Anthony Cioppa","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04309v2","updated":"2024-12-18T12:50:29Z","published":"2024-12-05T16:27:59Z","title":"The Tile: A 2D Map of Ranking Scores for Two-Class Classification","summary":"  In the computer vision and machine learning communities, as well as in many\nother research domains, rigorous evaluation of any new method, including\nclassifiers, is essential. One key component of the evaluation process is the\nability to compare and rank methods. However, ranking classifiers and\naccurately comparing their performances, especially when taking\napplication-specific preferences into account, remains challenging. For\ninstance, commonly used evaluation tools like Receiver Operating Characteristic\n(ROC) and Precision/Recall (PR) spaces display performances based on two\nscores. Hence, they are inherently limited in their ability to compare\nclassifiers across a broader range of scores and lack the capability to\nestablish a clear ranking among classifiers. In this paper, we present a novel\nversatile tool, named the Tile, that organizes an infinity of ranking scores in\na single 2D map for two-class classifiers, including common evaluation scores\nsuch as the accuracy, the true positive rate, the positive predictive value,\nJaccard's coefficient, and all F-beta scores. Furthermore, we study the\nproperties of the underlying ranking scores, such as the influence of the\npriors or the correspondences with the ROC space, and depict how to\ncharacterize any other score by comparing them to the Tile. Overall, we\ndemonstrate that the Tile is a powerful tool that effectively captures all the\nrankings in a single visualization and allows interpreting them.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04227v2","updated":"2024-12-18T12:45:58Z","published":"2024-12-05T15:05:25Z","title":"Foundations of the Theory of Performance-Based Ranking","summary":"  Ranking entities such as algorithms, devices, methods, or models based on\ntheir performances, while accounting for application-specific preferences, is a\nchallenge. To address this challenge, we establish the foundations of a\nuniversal theory for performance-based ranking. First, we introduce a rigorous\nframework built on top of both the probability and order theories. Our new\nframework encompasses the elements necessary to (1) manipulate performances as\nmathematical objects, (2) express which performances are worse than or\nequivalent to others, (3) model tasks through a variable called satisfaction,\n(4) consider properties of the evaluation, (5) define scores, and (6) specify\napplication-specific preferences through a variable called importance. On top\nof this framework, we propose the first axiomatic definition of performance\norderings and performance-based rankings. Then, we introduce a universal\nparametric family of scores, called ranking scores, that can be used to\nestablish rankings satisfying our axioms, while considering\napplication-specific preferences. Finally, we show, in the case of two-class\nclassification, that the family of ranking scores encompasses well-known\nperformance scores, including the accuracy, the true positive rate (recall,\nsensitivity), the true negative rate (specificity), the positive predictive\nvalue (precision), and F1. However, we also show that some other scores\ncommonly used to compare classifiers are unsuitable to derive performance\norderings satisfying the axioms. Therefore, this paper provides the computer\nvision and machine learning communities with a rigorous framework for\nevaluating and ranking entities.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13737v1","updated":"2024-12-18T11:14:30Z","published":"2024-12-18T11:14:30Z","title":"On the Compression of Language Models for Code: An Empirical Study on\n  CodeBERT","summary":"  Language models have proven successful across a wide range of software\nengineering tasks, but their significant computational costs often hinder their\npractical adoption. To address this challenge, researchers have begun applying\nvarious compression strategies to improve the efficiency of language models for\ncode. These strategies aim to optimize inference latency and memory usage,\nthough often at the cost of reduced model effectiveness. However, there is\nstill a significant gap in understanding how these strategies influence the\nefficiency and effectiveness of language models for code. Here, we empirically\ninvestigate the impact of three well-known compression strategies -- knowledge\ndistillation, quantization, and pruning -- across three different classes of\nsoftware engineering tasks: vulnerability detection, code summarization, and\ncode search. Our findings reveal that the impact of these strategies varies\ngreatly depending on the task and the specific compression method employed.\nPractitioners and researchers can use these insights to make informed decisions\nwhen selecting the most appropriate compression strategy, balancing both\nefficiency and effectiveness based on their specific needs.\n","authors":["Giordano d'Aloisio","Luca Traini","Federica Sarro","Antinisca Di Marco"],"pdf_url":"https://arxiv.org/pdf/2412.13737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13724v1","updated":"2024-12-18T11:04:58Z","published":"2024-12-18T11:04:58Z","title":"USEFUSE: Utile Stride for Enhanced Performance in Fused Layer\n  Architecture of Deep Neural Networks","summary":"  Convolutional Neural Networks (CNNs) are crucial in various applications, but\ntheir deployment on resource-constrained edge devices poses challenges. This\nstudy presents the Sum-of-Products (SOP) units for convolution, which utilize\nlow-latency left-to-right bit-serial arithmetic to minimize response time and\nenhance overall performance. The study proposes a methodology for fusing\nmultiple convolution layers to reduce off-chip memory communication and\nincrease overall performance. An effective mechanism detects and skips\ninefficient convolutions after ReLU layers, minimizing power consumption\nwithout compromising accuracy. Furthermore, efficient tile movement guarantees\nuniform access to the fusion pyramid. An analysis demonstrates the utile stride\nstrategy improves operational intensity. Two designs cater to varied demands:\none focuses on minimal response time for mission-critical applications, and\nanother focuses on resource-constrained devices with comparable latency. This\napproach notably reduced redundant computations, improving the efficiency of\nCNN deployment on edge devices.\n","authors":["Muhammad Sohail Ibrahim","Muhammad Usman","Jeong-A Lee"],"pdf_url":"https://arxiv.org/pdf/2412.13724v1.pdf","comment":null}],"Database":[{"id":"http://arxiv.org/abs/2412.14146v1","updated":"2024-12-18T18:44:08Z","published":"2024-12-18T18:44:08Z","title":"Advanced Reasoning and Transformation Engine for Multi-Step Insight\n  Synthesis in Data Analytics with Large Language Models","summary":"  This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics.\n","authors":["Atin Sakkeer Hussain"],"pdf_url":"https://arxiv.org/pdf/2412.14146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12827v2","updated":"2024-12-18T16:06:42Z","published":"2023-10-19T15:24:49Z","title":"Privately Answering Queries on Skewed Data via Per Record Differential\n  Privacy","summary":"  We consider the problem of the private release of statistics (like aggregate\npayrolls) where it is critical to preserve the contribution made by a small\nnumber of outlying large entities. We propose a privacy formalism, per-record\nzero concentrated differential privacy (PzCDP), where the privacy loss\nassociated with each record is a public function of that record's value. Unlike\nother formalisms which provide different privacy losses to different records,\nPRzCDP's privacy loss depends explicitly on the confidential data. We define\nour formalism, derive its properties, and propose mechanisms which satisfy\nPRzCDP that are uniquely suited to publishing skewed or heavy-tailed\nstatistics, where a small number of records contribute substantially to query\nanswers. This targeted relaxation helps overcome the difficulties of applying\nstandard DP to these data products.\n","authors":["Jeremy Seeman","William Sexton","David Pujol","Ashwin Machanavajjhala"],"pdf_url":"https://arxiv.org/pdf/2310.12827v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13965v1","updated":"2024-12-18T15:44:59Z","published":"2024-12-18T15:44:59Z","title":"What If: Causal Analysis with Graph Databases","summary":"  Graphs are expressive abstractions representing more effectively\nrelationships in data and enabling data science tasks. They are also a widely\nadopted paradigm in causal inference focusing on causal directed acyclic\ngraphs. Causal DAGs (Directed Acyclic Graphs) are manually curated by domain\nexperts, but they are never validated, stored and integrated as data artifacts\nin a graph data management system. In this paper, we delineate our vision to\nalign these two paradigms, namely causal analysis and property graphs, the\nlatter being the cornerstone of modern graph databases. To articulate this\nvision, a paradigm shift is required leading to rethinking property graph data\nmodels with hypernodes and structural equations, graph query semantics and\nquery constructs, and the definition of graph views to account for causality\noperators. Moreover, several research problems and challenges arise aiming at\nautomatically extracting causal models from the underlying graph observational\ndata, aligning and integrating disparate causal graph models into unified ones\nalong with their maintenance upon the changes in the underlying data. The above\nvision will allow to make graph databases aware of causal knowledge and pave\nthe way to data-driven personalized decision-making in several scientific\nfields.\n","authors":["Amedeo Pachera","Mattia Palmiotto","Angela Bonifati","Andrea Mauri"],"pdf_url":"https://arxiv.org/pdf/2412.13965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13918v1","updated":"2024-12-18T14:58:06Z","published":"2024-12-18T14:58:06Z","title":"Localized RETE for Incremental Graph Queries with Nested Graph\n  Conditions","summary":"  The growing size of graph-based modeling artifacts in model-driven\nengineering calls for techniques that enable efficient execution of graph\nqueries. Incremental approaches based on the RETE algorithm provide an adequate\nsolution in many scenarios, but are generally designed to search for query\nresults over the entire graph. However, in certain situations, a user may only\nbe interested in query results for a subgraph, for instance when a developer is\nworking on a large model of which only a part is loaded into their workspace.\nIn this case, the global execution semantics can result in significant\ncomputational overhead.\n  To mitigate the outlined shortcoming, in this article we propose an extension\nof the RETE approach that enables local, yet fully incremental execution of\ngraph queries, while still guaranteeing completeness of results with respect to\nthe relevant subgraph.\n  We empirically evaluate the presented approach via experiments inspired by a\nscenario from software development and with queries and data from an\nindependent social network benchmark. The experimental results indicate that\nthe proposed technique can significantly improve performance regarding memory\nconsumption and execution time in favorable cases, but may incur a noticeable\noverhead in unfavorable cases.\n","authors":["Matthias Barkowsky","Holger Giese"],"pdf_url":"https://arxiv.org/pdf/2412.13918v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.01145"},{"id":"http://arxiv.org/abs/2412.13679v1","updated":"2024-12-18T10:07:54Z","published":"2024-12-18T10:07:54Z","title":"On Enhancing Root Cause Analysis with SQL Summaries for Failures in\n  Database Workload Replays at SAP HANA","summary":"  Capturing the workload of a database and replaying this workload for a new\nversion of the database can be an effective approach for regression testing.\nHowever, false positive errors caused by many factors such as data privacy\nlimitations, time dependency or non-determinism in multi-threaded environment\ncan negatively impact the effectiveness. Therefore, we employ a machine\nlearning based framework to automate the root cause analysis of failures found\nduring replays. However, handling unseen novel issues not found in the training\ndata is one general challenge of machine learning approaches with respect to\ngeneralizability of the learned model. We describe how we continue to address\nthis challenge for more robust long-term solutions. From our experience,\nretraining with new failures is inadequate due to features overlapping across\ndistinct root causes. Hence, we leverage a large language model (LLM) to\nanalyze failed SQL statements and extract concise failure summaries as an\nadditional feature to enhance the classification process. Our experiments show\nthe F1-Macro score improved by 4.77% for our data. We consider our approach\nbeneficial for providing end users with additional information to gain more\ninsights into the found issues and to improve the assessment of the replay\nresults.\n","authors":["Neetha Jambigi","Joshua Hammesfahr","Moritz Mueller","Thomas Bach","Michael Felderer"],"pdf_url":"https://arxiv.org/pdf/2412.13679v1.pdf","comment":"The 35th IEEE International Symposium on Software Reliability\n  Engineering"},{"id":"http://arxiv.org/abs/2412.03611v2","updated":"2024-12-18T07:01:15Z","published":"2024-12-04T14:00:50Z","title":"Learning-based Sketches for Frequency Estimation in Data Streams without\n  Ground Truth","summary":"  Estimating the frequency of items on the high-volume, fast data stream has\nbeen extensively studied in many areas, such as database and network\nmeasurement. Traditional sketch algorithms only allow to give very rough\nestimates with limited memory cost, whereas some learning-augmented algorithms\nhave been proposed recently, their offline framework requires actual\nfrequencies that are challenging to access in general for training, and speed\nis too slow for real-time processing, despite the still coarse-grained\naccuracy.\n  To this end, we propose a more practical learning-based estimation framework\nnamely UCL-sketch, by following the line of equation-based sketch to estimate\nper-key frequencies. In a nutshell, there are two key techniques: online\ntraining via equivalent learning without ground truth, and highly scalable\narchitecture with logical estimation buckets. We implemented experiments on\nboth real-world and synthetic datasets. The results demonstrate that our method\ngreatly outperforms existing state-of-the-art sketches regarding per-key\naccuracy and distribution, while preserving resource efficiency. Our code is\nattached in the supplementary material, and will be made publicly available at\nhttps://github.com/Y-debug-sys/UCL-sketch.\n","authors":["Xinyu Yuan","Yan Qiao","Meng Li","Zhenchun Wei","Cuiying Feng"],"pdf_url":"https://arxiv.org/pdf/2412.03611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13520v1","updated":"2024-12-18T05:45:39Z","published":"2024-12-18T05:45:39Z","title":"ROMAS: A Role-Based Multi-Agent System for Database monitoring and\n  Planning","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in data analytics when integrated with Multi-Agent Systems (MAS).\nHowever, these systems often struggle with complex tasks that involve diverse\nfunctional requirements and intricate data processing challenges, necessitating\ncustomized solutions that lack broad applicability. Furthermore, current MAS\nfail to emulate essential human-like traits such as self-planning,\nself-monitoring, and collaborative work in dynamic environments, leading to\ninefficiencies and resource wastage. To address these limitations, we propose\nROMAS, a novel Role-Based M ulti-A gent System designed to adapt to various\nscenarios while enabling low code development and one-click deployment. ROMAS\nhas been effectively deployed in DB-GPT [Xue et al., 2023a, 2024b], a\nwell-known project utilizing LLM-powered database analytics, showcasing its\npractical utility in real-world scenarios. By integrating role-based\ncollaborative mechanisms for self-monitoring and self-planning, and leveraging\nexisting MAS capabilities to enhance database interactions, ROMAS offers a more\neffective and versatile solution. Experimental evaluations of ROMAS demonstrate\nits superiority across multiple scenarios, highlighting its potential to\nadvance the field of multi-agent data analytics.\n","authors":["Yi Huang","Fangyin Cheng","Fan Zhou","Jiahui Li","Jian Gong","Hongjun Yang","Zhidong Fan","Caigao Jiang","Siqiao Xue","Faqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13520v1.pdf","comment":null}]},"2024-12-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2412.15210v1","updated":"2024-12-19T18:59:46Z","published":"2024-12-19T18:59:46Z","title":"Tokenisation is NP-Complete","summary":"  In this work, we prove the NP-completeness of two variants of tokenisation,\ndefined as the problem of compressing a dataset to at most $\\delta$ symbols by\neither finding a vocabulary directly (direct tokenisation), or selecting a\nsequence of merge operations (bottom-up tokenisation).\n","authors":["Philip Whittington","Gregor Bachmann","Tiago Pimentel"],"pdf_url":"https://arxiv.org/pdf/2412.15210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15204v1","updated":"2024-12-19T18:59:17Z","published":"2024-12-19T18:59:17Z","title":"LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks","summary":"  This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.\n","authors":["Yushi Bai","Shangqing Tu","Jiajie Zhang","Hao Peng","Xiaozhi Wang","Xin Lv","Shulin Cao","Jiazheng Xu","Lei Hou","Yuxiao Dong","Jie Tang","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2412.15204v1.pdf","comment":"25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.15194v1","updated":"2024-12-19T18:58:04Z","published":"2024-12-19T18:58:04Z","title":"MMLU-CF: A Contamination-free Multi-task Language Understanding\n  Benchmark","summary":"  Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.\n","authors":["Qihao Zhao","Yangyu Huang","Tengchao Lv","Lei Cui","Qinzheng Sun","Shaoguang Mao","Xin Zhang","Ying Xin","Qiufeng Yin","Scarlett Li","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.15194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15189v1","updated":"2024-12-19T18:57:11Z","published":"2024-12-19T18:57:11Z","title":"Face the Facts! Evaluating RAG-based Fact-checking Pipelines in\n  Realistic Settings","summary":"  Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under\nmore realistic scenarios, RAG-based methods for the generation of verdicts -\ni.e., short texts discussing the veracity of a claim - evaluating them on\nstylistically complex claims and heterogeneous, yet reliable, knowledge bases.\nOur findings show a complex landscape, where, for example, LLM-based retrievers\noutperform other retrieval techniques, though they still struggle with\nheterogeneous knowledge bases; larger models excel in verdict faithfulness,\nwhile smaller models provide better context adherence, with human evaluations\nfavouring zero-shot and one-shot approaches for informativeness, and fine-tuned\nmodels for emotional alignment.\n","authors":["Daniel Russo","Stefano Menini","Jacopo Staiano","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2412.15189v1.pdf","comment":"Code and data at https://github.com/drusso98/face-the-facts"},{"id":"http://arxiv.org/abs/2412.15188v1","updated":"2024-12-19T18:56:24Z","published":"2024-12-19T18:56:24Z","title":"LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation","summary":"  We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.\n","authors":["Weijia Shi","Xiaochuang Han","Chunting Zhou","Weixin Liang","Xi Victoria Lin","Luke Zettlemoyer","Lili Yu"],"pdf_url":"https://arxiv.org/pdf/2412.15188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15177v1","updated":"2024-12-19T18:51:30Z","published":"2024-12-19T18:51:30Z","title":"Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative\n  Querying","summary":"  Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided.\n","authors":["Federico Castagna","Isabel Sassoon","Simon Parsons"],"pdf_url":"https://arxiv.org/pdf/2412.15177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05199v2","updated":"2024-12-19T18:46:21Z","published":"2024-11-07T21:51:07Z","title":"CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement","summary":"  Large Language Models (LLMs) have revolutionized code generation but require\nsignificant resources and often over-generalize, limiting their task-specific\nefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective\nalternative. However, standard supervised approaches rely only on correct\nexamples, missing valuable insights from failures. We introduce CodeLutra, a\nframework that leverages both correct and incorrect code attempts. Instead of\nusing only correct solutions, CodeLutra applies iterative preference-based\nrefinement, comparing successful and failed outputs to better approximate\ndesired results. This approach narrows the performance gap with\nstate-of-the-art larger models without requiring massive datasets or auxiliary\nmodels. For instance, on a challenging data science coding task, using only 500\nsamples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's\nlevel. By learning from both successes and mistakes, CodeLutra provides a\nscalable and efficient path to high-quality code generation, making smaller\nopen-source models more competitive with leading closed-source alternatives.\n","authors":["Leitian Tao","Xiang Chen","Tong Yu","Tung Mai","Ryan Rossi","Yixuan Li","Saayan Mitra"],"pdf_url":"https://arxiv.org/pdf/2411.05199v2.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.15156v1","updated":"2024-12-19T18:32:21Z","published":"2024-12-19T18:32:21Z","title":"Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM","summary":"  Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.\n","authors":["Yatai Ji","Jiacheng Zhang","Jie Wu","Shilong Zhang","Shoufa Chen","Chongjian GE","Peize Sun","Weifeng Chen","Wenqi Shao","Xuefeng Xiao","Weilin Huang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2412.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15151v1","updated":"2024-12-19T18:28:41Z","published":"2024-12-19T18:28:41Z","title":"Language Models as Continuous Self-Evolving Data Engineers","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities.\n","authors":["Peidong Wang","Ming Wang","Zhiming Ma","Xiaocui Yang","Shi Feng","Daling Wang","Yifei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15127v1","updated":"2024-12-19T18:08:04Z","published":"2024-12-19T18:08:04Z","title":"Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness","summary":"  The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios.\n","authors":["Haotian Zheng","Jinke Ren","Yushan Sun","Ruichen Zhang","Wenbo Zhang","Zhen Li","Dusit Niyato","Shuguang Cui","Yatong Han"],"pdf_url":"https://arxiv.org/pdf/2412.15127v1.pdf","comment":"12 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2412.15118v1","updated":"2024-12-19T17:59:42Z","published":"2024-12-19T17:59:42Z","title":"Outcome-Refining Process Supervision for Code Generation","summary":"  Large Language Models have demonstrated remarkable capabilities in code\ngeneration, yet they often struggle with complex programming tasks that require\ndeep algorithmic reasoning. While process supervision through learned reward\nmodels shows promise in guiding reasoning steps, it requires expensive training\ndata and suffers from unreliable evaluation. We propose Outcome-Refining\nProcess Supervision, a novel paradigm that treats outcome refinement itself as\nthe process to be supervised. Our framework leverages concrete execution\nsignals to ground the supervision of reasoning steps, while using\ntree-structured exploration to maintain multiple solution trajectories\nsimultaneously. Experiments demonstrate that our approach enables even smaller\nmodels to achieve high success accuracy and performance metrics on competitive\nprogramming tasks, creates more reliable verification than traditional reward\nmodels without requiring training PRMs. Our approach achieves significant\nimprovements across 5 models and 3 datasets: an average of 26.9% increase in\ncorrectness and 42.2% in efficiency. The results suggest that providing\nstructured reasoning space with concrete verification signals is crucial for\nsolving complex programming tasks. We open-source all our code and data at:\nhttps://github.com/zhuohaoyu/ORPS\n","authors":["Zhuohao Yu","Weizheng Gu","Yidong Wang","Zhengran Zeng","Jindong Wang","Wei Ye","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15118v1.pdf","comment":"18 pages, 5 figures, Code: https://github.com/zhuohaoyu/ORPS"},{"id":"http://arxiv.org/abs/2409.18472v2","updated":"2024-12-19T17:57:43Z","published":"2024-09-27T06:18:55Z","title":"URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological\n  and Multilingual Knowledge Base","summary":"  URIEL is a knowledge base offering geographical, phylogenetic, and\ntypological vector representations for 7970 languages. It includes distance\nmeasures between these vectors for 4005 languages, which are accessible via the\nlang2vec tool. Despite being frequently cited, URIEL is limited in terms of\nlinguistic inclusion and overall usability. To tackle these challenges, we\nintroduce URIEL+, an enhanced version of URIEL and lang2vec that addresses\nthese limitations. In addition to expanding typological feature coverage for\n2898 languages, URIEL+ improves the user experience with robust, customizable\ndistance calculations to better suit the needs of users. These upgrades also\noffer competitive performance on downstream tasks and provide distances that\nbetter align with linguistic distance studies.\n","authors":["Aditya Khan","Mason Shipton","David Anugraha","Kaiyao Duan","Phuong H. Hoang","Eric Khiu","A. Seza Doğruöz","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2409.18472v2.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.15115v1","updated":"2024-12-19T17:56:09Z","published":"2024-12-19T17:56:09Z","title":"Qwen2.5 Technical Report","summary":"  In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.\n","authors":[" Qwen"," :","An Yang","Baosong Yang","Beichen Zhang","Binyuan Hui","Bo Zheng","Bowen Yu","Chengyuan Li","Dayiheng Liu","Fei Huang","Haoran Wei","Huan Lin","Jian Yang","Jianhong Tu","Jianwei Zhang","Jianxin Yang","Jiaxi Yang","Jingren Zhou","Junyang Lin","Kai Dang","Keming Lu","Keqin Bao","Kexin Yang","Le Yu","Mei Li","Mingfeng Xue","Pei Zhang","Qin Zhu","Rui Men","Runji Lin","Tianhao Li","Tingyu Xia","Xingzhang Ren","Xuancheng Ren","Yang Fan","Yang Su","Yichang Zhang","Yu Wan","Yuqiong Liu","Zeyu Cui","Zhenru Zhang","Zihan Qiu"],"pdf_url":"https://arxiv.org/pdf/2412.15115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15113v1","updated":"2024-12-19T17:55:42Z","published":"2024-12-19T17:55:42Z","title":"Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture","summary":"  Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million parameters,\nfocusing on attention head values, with results also indicating improved ICL\nperformance at this larger and more naturalistic scale.\n","authors":["Thomas F Burns","Tomoki Fukai","Christopher J Earls"],"pdf_url":"https://arxiv.org/pdf/2412.15113v1.pdf","comment":"18 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.04619v3","updated":"2024-12-19T17:51:34Z","published":"2024-12-05T21:12:37Z","title":"Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization","summary":"  Language models (LMs), like other neural networks, often favor shortcut\nheuristics based on surface-level patterns. Although LMs behave like n-gram\nmodels early in training, they must eventually learn hierarchical syntactic\nrepresentations to correctly apply grammatical rules out-of-distribution (OOD).\nIn this work, we use case studies of English grammar to explore how complex,\ndiverse training data drives models to generalize OOD. We construct a framework\nthat unifies our understanding of random variation with training dynamics, rule\nselection with memorization, and data diversity with complexity. We show that\nthese factors are nuanced, and that intermediate levels of diversity and\ncomplexity lead to inconsistent behavior across random seeds and to unstable\ntraining dynamics. Our findings emphasize the critical role of training data in\nshaping generalization patterns and illuminate how competing model strategies\nlead to inconsistent generalization outcomes across random seeds. Code is\navailable at https://github.com/sunnytqin/concept_comp.git.\n","authors":["Tian Qin","Naomi Saphra","David Alvarez-Melis"],"pdf_url":"https://arxiv.org/pdf/2412.04619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15101v1","updated":"2024-12-19T17:48:23Z","published":"2024-12-19T17:48:23Z","title":"Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering\n  with Temporal Adaptability","summary":"  Retrieve-augmented generation (RAG) frameworks have emerged as a promising\nsolution to multi-hop question answering(QA) tasks since it enables large\nlanguage models (LLMs) to incorporate external knowledge and mitigate their\ninherent knowledge deficiencies. Despite this progress, existing RAG\nframeworks, which usually follows the retrieve-then-read paradigm, often\nstruggle with multi-hop QA with temporal information since it has difficulty\nretrieving and synthesizing accurate time-related information. To address the\nchallenge, this paper proposes a novel framework called review-then-refine,\nwhich aims to enhance LLM performance in multi-hop QA scenarios with temporal\ninformation. Our approach begins with a review phase, where decomposed\nsub-queries are dynamically rewritten with temporal information, allowing for\nsubsequent adaptive retrieval and reasoning process. In addition, we implement\nadaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing\nthe potential for hallucinations. In the subsequent refine phase, the LLM\nsynthesizes the retrieved information from each sub-query along with its\ninternal knowledge to formulate a coherent answer. Extensive experimental\nresults across multiple datasets demonstrate the effectiveness of our proposed\nframework, highlighting its potential to significantly improve multi-hop QA\ncapabilities in LLMs.\n","authors":["Xiangsen Chen","Xuming Hu","Nan Tang"],"pdf_url":"https://arxiv.org/pdf/2412.15101v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.15098v1","updated":"2024-12-19T17:46:13Z","published":"2024-12-19T17:46:13Z","title":"A Cross-Domain Study of the Use of Persuasion Techniques in Online\n  Disinformation","summary":"  Disinformation, irrespective of domain or language, aims to deceive or\nmanipulate public opinion, typically through employing advanced persuasion\ntechniques. Qualitative and quantitative research on the weaponisation of\npersuasion techniques in disinformation has been mostly topic-specific (e.g.,\nCOVID-19) with limited cross-domain studies, resulting in a lack of\ncomprehensive understanding of these strategies. This study employs a\nstate-of-the-art persuasion technique classifier to conduct a large-scale,\nmulti-domain analysis of the role of 16 persuasion techniques in disinformation\nnarratives. It shows how different persuasion techniques are employed\ndisproportionately in different disinformation domains. We also include a\ndetailed case study on climate change disinformation, highlighting how\nlinguistic, psychological, and cultural factors shape the adaptation of\npersuasion strategies to fit unique thematic contexts.\n","authors":["João A. Leite","Olesya Razuvayevskaya","Carolina Scarton","Kalina Bontcheva"],"pdf_url":"https://arxiv.org/pdf/2412.15098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13702v2","updated":"2024-12-19T17:36:38Z","published":"2024-12-18T10:45:24Z","title":"Typhoon 2: A Family of Open Text and Multimodal Thai Large Language\n  Models","summary":"  This paper introduces Typhoon 2, a series of text and multimodal large\nlanguage models optimized for the Thai language. The series includes models for\ntext, vision, and audio. Typhoon2-Text builds on state-of-the-art open models,\nsuch as Llama 3 and Qwen2, and we perform continual pre-training on a mixture\nof English and Thai data. We employ post-training techniques to enhance Thai\nlanguage performance while preserving the base models' original capabilities.\nWe release text models across a range of sizes, from 1 to 70 billion\nparameters, available in both base and instruction-tuned variants. To guardrail\ntext generation, we release Typhoon2-Safety, a classifier enhanced for Thai\ncultures and language. Typhoon2-Vision improves Thai document understanding\nwhile retaining general visual capabilities, such as image captioning.\nTyphoon2-Audio introduces an end-to-end speech-to-speech model architecture\ncapable of processing audio, speech, and text inputs and generating both text\nand speech outputs.\n","authors":["Kunat Pipatanakul","Potsawee Manakul","Natapong Nitarach","Warit Sirichotedumrong","Surapon Nonesung","Teetouch Jaknamon","Parinthapat Pengpun","Pittawat Taveekitworachai","Adisai Na-Thalang","Sittipong Sripaisarnmongkol","Krisanapong Jirayoot","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2412.13702v2.pdf","comment":"technical report, 55 pages"},{"id":"http://arxiv.org/abs/2412.15084v1","updated":"2024-12-19T17:29:44Z","published":"2024-12-19T17:29:44Z","title":"AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward\n  Modeling","summary":"  In this paper, we introduce AceMath, a suite of frontier math models that\nexcel in solving complex math problems, along with highly effective reward\nmodels capable of evaluating generated solutions and reliably identifying the\ncorrect ones. To develop the instruction-tuned math models, we propose a\nsupervised fine-tuning (SFT) process that first achieves competitive\nperformance across general domains, followed by targeted fine-tuning for the\nmath domain using a carefully curated set of prompts and synthetically\ngenerated responses. The resulting model, AceMath-72B-Instruct greatly\noutperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop\nmath-specialized reward model, we first construct AceMath-RewardBench, a\ncomprehensive and robust benchmark for evaluating math reward models across\ndiverse problems and difficulty levels. After that, we present a systematic\napproach to build our math reward models. The resulting model, AceMath-72B-RM,\nconsistently outperforms state-of-the-art reward models. Furthermore, when\ncombining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest\naverage rm@8 score across the math reasoning benchmarks. We will release model\nweights, training data, and evaluation benchmarks at:\nhttps://research.nvidia.com/labs/adlr/acemath\n","authors":["Zihan Liu","Yang Chen","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2412.15084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15077v1","updated":"2024-12-19T17:26:07Z","published":"2024-12-19T17:26:07Z","title":"Till the Layers Collapse: Compressing a Deep Neural Network through the\n  Lenses of Batch Normalization Layers","summary":"  Today, deep neural networks are widely used since they can handle a variety\nof complex tasks. Their generality makes them very powerful tools in modern\ntechnology. However, deep neural networks are often overparameterized. The\nusage of these large models consumes a lot of computation resources. In this\npaper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers\n\\textbf{C}ollapse (TLC), which compresses deep neural networks through the\nlenses of batch normalization layers. By reducing the depth of these networks,\nour method decreases deep neural networks' computational requirements and\noverall latency. We validate our method on popular models such as Swin-T,\nMobileNet-V2, and RoBERTa, across both image classification and natural\nlanguage processing (NLP) tasks.\n","authors":["Zhu Liao","Nour Hezbri","Victor Quétu","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2412.15077v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15060v1","updated":"2024-12-19T17:08:11Z","published":"2024-12-19T17:08:11Z","title":"ConfliBERT: A Language Model for Political Conflict","summary":"  Conflict scholars have used rule-based approaches to extract information\nabout political violence from news reports and texts. Recent Natural Language\nProcessing developments move beyond rigid rule-based approaches. We review our\nrecent ConfliBERT language model (Hu et al. 2022) to process political and\nviolence related texts. The model can be used to extract actor and action\nclassifications from texts about political conflict. When fine-tuned, results\nshow that ConfliBERT has superior performance in accuracy, precision and recall\nover other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama\n3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also\nhundreds of times faster than these more generalist LLMs. These results are\nillustrated using texts from the BBC, re3d, and the Global Terrorism Dataset\n(GTD).\n","authors":["Patrick T. Brandt","Sultan Alsarra","Vito J. D`Orazio","Dagmar Heintze","Latifur Khan","Shreyas Meher","Javier Osorio","Marcus Sianan"],"pdf_url":"https://arxiv.org/pdf/2412.15060v1.pdf","comment":"30 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2412.15035v1","updated":"2024-12-19T16:46:54Z","published":"2024-12-19T16:46:54Z","title":"LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps","summary":"  Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities.\n","authors":["Felix Friedrich","Simone Tedeschi","Patrick Schramowski","Manuel Brack","Roberto Navigli","Huu Nguyen","Bo Li","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2412.15035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14512v3","updated":"2024-12-19T16:37:00Z","published":"2024-08-25T04:32:45Z","title":"LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings","summary":"  Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.\n","authors":["Duo Wang","Yuan Zuo","Fengzhi Li","Junjie Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10868v4","updated":"2024-12-19T16:22:07Z","published":"2024-06-16T09:36:32Z","title":"Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts","summary":"  Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ndecoder-only LLMs, such as Llama and Mistral? (2) How can we address the\nchallenge of long-form (or free-form) text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction.\n","authors":["Lihu Chen","Adam Dejl","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2406.10868v4.pdf","comment":"AAAI 2025 Main Track"},{"id":"http://arxiv.org/abs/2411.10912v2","updated":"2024-12-19T16:20:49Z","published":"2024-11-16T23:29:32Z","title":"SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment","summary":"  When different groups' values differ, one approach to model alignment is to\nsteer models at inference time towards each group's preferences. However,\ntechniques like in-context learning only consider similarity when drawing\nfew-shot examples and not cross-group differences in values. We propose SPICA,\na framework that accounts for group-level differences during in-context example\nretrieval. SPICA introduces three designs: scenario banks, group-informed\nretrieval metrics, and in-context alignment prompts. From an evaluation of\nSPICA on an alignment task collecting inputs from four demographic groups ($n =\n544$), our metrics retrieve in-context examples that more closely match\nobserved preferences, with the best prompt configuration using multiple\ncontrastive responses to demonstrate examples. In an end-to-end evaluation ($n\n= 120$), we observe that SPICA is higher rated than similarity-based retrieval,\nwith groups seeing up to a +0.16 point improvement on a 5 point scale.\nAdditionally, gains from SPICA were more uniform, with all groups benefiting\nfrom alignment rather than only some. Finally, we find that while a\ngroup-agnostic approach can align to aggregated values, it is not most suited\nfor divergent groups.\n","authors":["Quan Ze Chen","K. J. Kevin Feng","Chan Young Park","Amy X. Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.10912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15004v1","updated":"2024-12-19T16:20:22Z","published":"2024-12-19T16:20:22Z","title":"Large Language Models and Code Security: A Systematic Literature Review","summary":"  Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.\n","authors":["Enna Basic","Alberto Giaretta"],"pdf_url":"https://arxiv.org/pdf/2412.15004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08406v2","updated":"2024-12-19T16:09:47Z","published":"2024-09-12T21:39:01Z","title":"Knowledge Tagging with Large Language Model based Multi-Agent System","summary":"  Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts.\n","authors":["Hang Li","Tianlong Xu","Ethan Chang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2409.08406v2.pdf","comment":"Accepted by AAAI 2025 (AAAI/IAAI 2025 Innovative Application Award)"},{"id":"http://arxiv.org/abs/2412.14986v1","updated":"2024-12-19T15:58:53Z","published":"2024-12-19T15:58:53Z","title":"Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small\n  Language Models Write Young Students Texts","summary":"  Large Language Models (LLMs) have been used to generate texts in response to\ndifferent writing tasks: reports, essays, story telling. However, language\nmodels do not have a meta-representation of the text writing process, nor\ninherent communication learning needs, comparable to those of young human\nstudents. This paper introduces a fine-grained linguistic and textual analysis\nof multilingual Small Language Models' (SLMs) writing. With our method,\nChain-of-MetaWriting, SLMs can imitate some steps of the human writing process,\nsuch as planning and evaluation. We mainly focused on short story and essay\nwriting tasks in French for schoolchildren and undergraduate students\nrespectively. Our results show that SLMs encounter difficulties in assisting\nyoung students on sensitive topics such as violence in the schoolyard, and they\nsometimes use words too complex for the target audience. In particular, the\noutput is quite different from the human produced texts in term of text\ncohesion and coherence regarding temporal connectors, topic progression,\nreference.\n","authors":["Ioana Buhnila","Georgeta Cislaru","Amalia Todirascu"],"pdf_url":"https://arxiv.org/pdf/2412.14986v1.pdf","comment":"Accepted at WRAICOGS 2025 (Writing Aids at the Crossroads of AI,\n  Cognitive Science, and NLP) co-located with COLING 2025"},{"id":"http://arxiv.org/abs/2412.11745v2","updated":"2024-12-19T15:55:45Z","published":"2024-12-16T13:03:43Z","title":"Beyond Dataset Creation: Critical View of Annotation Variation and Bias\n  Probing of a Dataset for Online Radical Content Detection","summary":"  The proliferation of radical content on online platforms poses significant\nrisks, including inciting violence and spreading extremist ideologies. Despite\nongoing research, existing datasets and models often fail to address the\ncomplexities of multilingual and diverse data. To bridge this gap, we introduce\na publicly available multilingual dataset annotated with radicalization levels,\ncalls for action, and named entities in English, French, and Arabic. This\ndataset is pseudonymized to protect individual privacy while preserving\ncontextual information. Beyond presenting our freely available dataset, we\nanalyze the annotation process, highlighting biases and disagreements among\nannotators and their implications for model performance. Additionally, we use\nsynthetic data to investigate the influence of socio-demographic traits on\nannotation patterns and model predictions. Our work offers a comprehensive\nexamination of the challenges and opportunities in building robust datasets for\nradical content detection, emphasizing the importance of fairness and\ntransparency in model development.\n","authors":["Arij Riabi","Virginie Mouilleron","Menel Mahamdi","Wissam Antoun","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2412.11745v2.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.13765v2","updated":"2024-12-19T15:50:54Z","published":"2024-12-18T12:01:53Z","title":"LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms","summary":"  Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned TXLM-RoBERTa using\nhuman-annotated sentiment datasets to enhance prediction accuracy and utilized\nLLama 3B, and Gemma 9B from Ollama.\n","authors":["Ali Hamdi","Ahmed Abdelmoneim Mazrou","Mohamed Shaltout"],"pdf_url":"https://arxiv.org/pdf/2412.13765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14965v1","updated":"2024-12-19T15:44:04Z","published":"2024-12-19T15:44:04Z","title":"Movie2Story: A framework for understanding videos and telling stories in\n  the form of novel text","summary":"  Multimodal video-to-text models have made considerable progress, primarily in\ngenerating brief descriptions of video content. However, there is still a\ndeficiency in generating rich long-form text descriptions that integrate both\nvideo and audio. In this paper, we introduce a framework called M2S, designed\nto generate novel-length text by combining audio, video, and character\nrecognition. M2S includes modules for video long-form text description and\ncomprehension, audio-based analysis of emotion, speech rate, and character\nalignment, and visual-based character recognition alignment. By integrating\nmultimodal information using the large language model GPT4o, M2S stands out in\nthe field of multimodal text generation. We demonstrate the effectiveness and\naccuracy of M2S through comparative experiments and human evaluation.\nAdditionally, the model framework has good scalability and significant\npotential for future research.\n","authors":["Kangning Li","Zheyang Jia","Anyu Ying"],"pdf_url":"https://arxiv.org/pdf/2412.14965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14964v1","updated":"2024-12-19T15:44:01Z","published":"2024-12-19T15:44:01Z","title":"Knowledge Injection via Prompt Distillation","summary":"  In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights.\n","authors":["Kalle Kujanpää","Harri Valpola","Alexander Ilin"],"pdf_url":"https://arxiv.org/pdf/2412.14964v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.14959v1","updated":"2024-12-19T15:39:31Z","published":"2024-12-19T15:39:31Z","title":"Understanding the Dark Side of LLMs' Intrinsic Self-Correction","summary":"  Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/.\n","authors":["Qingjie Zhang","Han Qiu","Di Wang","Haoting Qian","Yiming Li","Tianwei Zhang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.14959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13647v2","updated":"2024-12-19T15:37:55Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20054v2","updated":"2024-12-19T15:30:40Z","published":"2024-06-28T17:07:06Z","title":"To Word Senses and Beyond: Inducing Concepts with Contextualized\n  Language Models","summary":"  Polysemy and synonymy are two crucial interrelated facets of lexical\nambiguity. While both phenomena are widely documented in lexical resources and\nhave been studied extensively in NLP, leading to dedicated systems, they are\noften being considered independently in practical problems. While many tasks\ndealing with polysemy (e.g. Word Sense Disambiguiation or Induction) highlight\nthe role of word's senses, the study of synonymy is rooted in the study of\nconcepts, i.e. meanings shared across the lexicon. In this paper, we introduce\nConcept Induction, the unsupervised task of learning a soft clustering among\nwords that defines a set of concepts directly from data. This task generalizes\nWord Sense Induction. We propose a bi-level approach to Concept Induction that\nleverages both a local lemma-centric view and a global cross-lexicon view to\ninduce concepts. We evaluate the obtained clustering on SemCor's annotated data\nand obtain good performance (BCubed F1 above 0.60). We find that the local and\nthe global levels are mutually beneficial to induce concepts and also senses in\nour setting. Finally, we create static embeddings representing our induced\nconcepts and use them on the Word-in-Context task, obtaining competitive\nperformance with the State-of-the-Art.\n","authors":["Bastien Liétard","Pascal Denis","Mikaella Keller"],"pdf_url":"https://arxiv.org/pdf/2406.20054v2.pdf","comment":"Published in EMNLP 2024 main conference proceedings"},{"id":"http://arxiv.org/abs/2408.10839v2","updated":"2024-12-19T15:25:41Z","published":"2024-08-20T13:34:17Z","title":"Benchmarking Large Language Models for Math Reasoning Tasks","summary":"  The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.\n","authors":["Kathrin Seßler","Yao Rong","Emek Gözlüklü","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2408.10839v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.11795v2","updated":"2024-12-19T15:21:44Z","published":"2024-12-16T14:07:39Z","title":"ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible\n  Speech Synthesis","summary":"  Prosody contains rich information beyond the literal meaning of words, which\nis crucial for the intelligibility of speech. Current models still fall short\nin phrasing and intonation; they not only miss or misplace breaks when\nsynthesizing long sentences with complex structures but also produce unnatural\nintonation. We propose ProsodyFM, a prosody-aware text-to-speech synthesis\n(TTS) model with a flow-matching (FM) backbone that aims to enhance the\nphrasing and intonation aspects of prosody. ProsodyFM introduces two key\ncomponents: a Phrase Break Encoder to capture initial phrase break locations,\nfollowed by a Duration Predictor for the flexible adjustment of break\ndurations; and a Terminal Intonation Encoder which learns a bank of intonation\nshape tokens combined with a novel Pitch Processor for more robust modeling of\nhuman-perceived intonation change. ProsodyFM is trained with no explicit\nprosodic labels and yet can uncover a broad spectrum of break durations and\nintonation patterns. Experimental results demonstrate that ProsodyFM can\neffectively improve the phrasing and intonation aspects of prosody, thereby\nenhancing the overall intelligibility compared to four state-of-the-art (SOTA)\nmodels. Out-of-distribution experiments show that this prosody improvement can\nfurther bring ProsodyFM superior generalizability for unseen complex sentences\nand speakers. Our case study intuitively illustrates the powerful and\nfine-grained controllability of ProsodyFM over phrasing and intonation.\n","authors":["Xiangheng He","Junjie Chen","Zixing Zhang","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2412.11795v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.07991v4","updated":"2024-12-19T15:16:49Z","published":"2024-10-10T14:48:57Z","title":"Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets","summary":"  The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.\n","authors":["Tommaso Giorgi","Lorenzo Cima","Tiziano Fagni","Marco Avvenuti","Stefano Cresci"],"pdf_url":"https://arxiv.org/pdf/2410.07991v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09378v2","updated":"2024-12-19T15:14:18Z","published":"2024-12-12T15:46:43Z","title":"From Bench to Bedside: A Review of Clinical Trials in Drug Discovery and\n  Development","summary":"  Clinical trials are an indispensable part of the drug development process,\nbridging the gap between basic research and clinical application. During the\ndevelopment of new drugs, clinical trials are used not only to evaluate the\nsafety and efficacy of the drug but also to explore its dosage, treatment\nregimens, and potential side effects. This review discusses the various stages\nof clinical trials, including Phase I (safety assessment), Phase II\n(preliminary efficacy evaluation), Phase III (large-scale validation), and\nPhase IV (post-marketing surveillance), highlighting the characteristics of\neach phase and their interrelationships. Additionally, the paper addresses the\nmajor challenges encountered in clinical trials, such as ethical issues,\nsubject recruitment difficulties, diversity and representativeness concerns,\nand proposes strategies for overcoming these challenges. With the advancement\nof technology, innovative technologies such as artificial intelligence, big\ndata, and digitalization are gradually transforming clinical trial design and\nimplementation, improving trial efficiency and data quality. The article also\nlooks forward to the future of clinical trials, particularly the impact of\nemerging therapies such as gene therapy and immunotherapy on trial design, as\nwell as the importance of regulatory reforms and global collaboration. In\nconclusion, the core role of clinical trials in drug development will continue\nto drive the progress of innovative drug development and clinical treatment.\n","authors":["Tianyang Wang","Ming Liu","Benji Peng","Xinyuan Song","Charles Zhang","Xintian Sun","Qian Niu","Junyu Liu","Silin Chen","Keyu Chen","Ming Li","Pohsun Feng","Ziqian Bi","Yunze Wang","Yichao Zhang","Cheng Fei","Lawrence KQ Yan"],"pdf_url":"https://arxiv.org/pdf/2412.09378v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2407.04693v2","updated":"2024-12-19T15:11:47Z","published":"2024-07-05T17:56:38Z","title":"ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models","summary":"  Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.\n","authors":["Yuzhe Gu","Ziwei Ji","Wenwei Zhang","Chengqi Lyu","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2407.04693v2.pdf","comment":"Accepted by NeurIPS 2024. Dataset, code, and model are released at\n  https://github.com/open-compass/ANAH"},{"id":"http://arxiv.org/abs/2411.16300v3","updated":"2024-12-19T15:11:46Z","published":"2024-11-25T11:35:08Z","title":"BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment","summary":"  Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.\n","authors":["Shaolei Zhang","Kehao Zhang","Qingkai Fang","Shoutao Guo","Yan Zhou","Xiaodong Liu","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2411.16300v3.pdf","comment":"BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing\n  2's code and models: https://github.com/ictnlp/BayLing"},{"id":"http://arxiv.org/abs/2312.00326v5","updated":"2024-12-19T15:07:38Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v5.pdf","comment":"19 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.14922v1","updated":"2024-12-19T15:00:18Z","published":"2024-12-19T15:00:18Z","title":"RobustFT: Robust Supervised Fine-tuning for Large Language Models under\n  Noisy Response","summary":"  Supervised fine-tuning (SFT) plays a crucial role in adapting large language\nmodels (LLMs) to specific domains or tasks. However, as demonstrated by\nempirical experiments, the collected data inevitably contains noise in\npractical applications, which poses significant challenges to model performance\non downstream tasks. Therefore, there is an urgent need for a noise-robust SFT\nframework to enhance model capabilities in downstream tasks. To address this\nchallenge, we introduce a robust SFT framework (RobustFT) that performs noise\ndetection and relabeling on downstream task data. For noise identification, our\napproach employs a multi-expert collaborative system with inference-enhanced\nmodels to achieve superior noise detection. In the denoising phase, we utilize\na context-enhanced strategy, which incorporates the most relevant and confident\nknowledge followed by careful assessment to generate reliable annotations.\nAdditionally, we introduce an effective data selection mechanism based on\nresponse entropy, ensuring only high-quality samples are retained for\nfine-tuning. Extensive experiments conducted on multiple LLMs across five\ndatasets demonstrate RobustFT's exceptional performance in noisy scenarios.\n","authors":["Junyu Luo","Xiao Luo","Kaize Ding","Jingyang Yuan","Zhiping Xiao","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.14922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14905v1","updated":"2024-12-19T14:37:11Z","published":"2024-12-19T14:37:11Z","title":"Dehallucinating Parallel Context Extension for Retrieval-Augmented\n  Generation","summary":"  Large language models (LLMs) are susceptible to generating hallucinated\ninformation, despite the integration of retrieval-augmented generation (RAG).\nParallel context extension (PCE) is a line of research attempting to\neffectively integrating parallel (unordered) contexts, while it still suffers\nfrom hallucinations when adapted to RAG scenarios. In this paper, we propose\nDePaC (Dehallucinating Parallel Context Extension), which alleviates the\nhallucination problem with context-aware negative training and\ninformation-calibrated aggregation. DePaC is designed to alleviate two types of\nin-context hallucination: fact fabrication (i.e., LLMs present claims that are\nnot supported by the contexts) and fact omission (i.e., LLMs fail to present\nclaims that can be supported by the contexts). Specifically, (1) for fact\nfabrication, we apply the context-aware negative training that fine-tunes the\nLLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to\nanswer when contexts are not related to questions; (2) for fact omission, we\npropose the information-calibrated aggregation which prioritizes context\nwindows with higher information increment from their contexts. The experimental\nresults on nine RAG tasks demonstrate that DePaC significantly alleviates the\ntwo types of hallucination and consistently achieves better performances on\nthese tasks.\n","authors":["Zexiong Ma","Shengnan An","Zeqi Lin","Yanzhen Zou","Jian-Guang Lou","Bing Xie"],"pdf_url":"https://arxiv.org/pdf/2412.14905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14872v1","updated":"2024-12-19T14:11:15Z","published":"2024-12-19T14:11:15Z","title":"Why language models collapse when trained on recursively generated text","summary":"  Language models (LMs) have been widely used to generate text on the Internet.\nThe generated text is often collected into the training corpus of the next\ngenerations of LMs. Previous work has experimentally found that LMs collapse\nwhen trained on recursively generated text. This paper contributes to existing\nknowledge from two aspects. We present a theoretical proof of LM collapse. Our\nproof reveals the cause of LM collapse and proves that all auto-regressive LMs\nwill definitely collapse. We present a new finding: the performance of LMs\ngradually declines when trained on recursively generated text until they\nperform no better than a randomly initialized LM. The trained LMs produce large\namounts of repetitive text and perform poorly across a wide range of natural\nlanguage tasks. The above proof and new findings deepen our understanding of LM\ncollapse and offer valuable insights that may inspire new training techniques\nto mitigate this threat.\n","authors":["Lecheng Wang","Xianjie Shi","Ge Li","Jia Li","Yihong Dong","Xuanming Zhang","Wenpin Jiao","Hong Mei"],"pdf_url":"https://arxiv.org/pdf/2412.14872v1.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.14867v1","updated":"2024-12-19T14:03:22Z","published":"2024-12-19T14:03:22Z","title":"Graph-Convolutional Networks: Named Entity Recognition and Large\n  Language Model Embedding in Document Clustering","summary":"  Recent advances in machine learning, particularly Large Language Models\n(LLMs) such as BERT and GPT, provide rich contextual embeddings that improve\ntext representation. However, current document clustering approaches often\nignore the deeper relationships between named entities (NEs) and the potential\nof LLM embeddings. This paper proposes a novel approach that integrates Named\nEntity Recognition (NER) and LLM embeddings within a graph-based framework for\ndocument clustering. The method builds a graph with nodes representing\ndocuments and edges weighted by named entity similarity, optimized using a\ngraph-convolutional network (GCN). This ensures a more effective grouping of\nsemantically related documents. Experimental results indicate that our approach\noutperforms conventional co-occurrence-based methods in clustering, notably for\ndocuments rich in named entities.\n","authors":["Imed Keraghel","Mohamed Nadif"],"pdf_url":"https://arxiv.org/pdf/2412.14867v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.06228v2","updated":"2024-12-19T13:57:05Z","published":"2024-11-09T16:17:14Z","title":"An $\\mathbf{L^*}$ Algorithm for Deterministic Weighted Regular Languages","summary":"  Extracting finite state automata (FSAs) from black-box models offers a\npowerful approach to gaining interpretable insights into complex model\nbehaviors. To support this pursuit, we present a weighted variant of Angluin's\n(1987) $\\mathbf{L^*}$ algorithm for learning FSAs. We stay faithful to the\noriginal algorithm, devising a way to exactly learn deterministic weighted FSAs\nwhose weights support division. Furthermore, we formulate the learning process\nin a manner that highlights the connection with FSA minimization, showing how\n$\\mathbf{L^*}$ directly learns a minimal automaton for the target language.\n","authors":["Clemente Pasti","Talu Karagöz","Anej Svete","Franz Nowak","Reda Boumasmoud","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.06228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14860v1","updated":"2024-12-19T13:55:48Z","published":"2024-12-19T13:55:48Z","title":"Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling","summary":"  Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reflect on the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Models to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches.\n","authors":["Junyi Li","Hwee Tou Ng"],"pdf_url":"https://arxiv.org/pdf/2412.14860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14849v1","updated":"2024-12-19T13:39:47Z","published":"2024-12-19T13:39:47Z","title":"DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for\n  Few-Shot Aspect-Based Sentiment Analysis","summary":"  Recently developed large language models (LLMs) have presented promising new\navenues to address data scarcity in low-resource scenarios. In few-shot\naspect-based sentiment analysis (ABSA), previous efforts have explored data\naugmentation techniques, which prompt LLMs to generate new samples by modifying\nexisting ones. However, these methods fail to produce adequately diverse data,\nimpairing their effectiveness. Besides, some studies apply in-context learning\nfor ABSA by using specific instructions and a few selected examples as prompts.\nThough promising, LLMs often yield labels that deviate from task requirements.\nTo overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data\nsynthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize\ndata from two complementary perspectives: \\textit{key-point-driven} and\n\\textit{instance-driven}, which effectively generate diverse and high-quality\nABSA samples in low-resource settings. Furthermore, a \\textit{label refinement}\nmodule is integrated to improve the synthetic labels. Extensive experiments\ndemonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA\nsolutions and other LLM-oriented data generation methods.\n","authors":["Hongling Xu","Yice Zhang","Qianlong Wang","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2412.14849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14847v1","updated":"2024-12-19T13:39:24Z","published":"2024-12-19T13:39:24Z","title":"A Survey of RWKV","summary":"  The Receptance Weighted Key Value (RWKV) model offers a novel alternative to\nthe Transformer architecture, merging the benefits of recurrent and\nattention-based systems. Unlike conventional Transformers, which depend heavily\non self-attention, RWKV adeptly captures long-range dependencies with minimal\ncomputational demands. By utilizing a recurrent framework, RWKV addresses some\ncomputational inefficiencies found in Transformers, particularly in tasks with\nlong sequences. RWKV has recently drawn considerable attention for its robust\nperformance across multiple domains. Despite its growing popularity, no\nsystematic review of the RWKV model exists. This paper seeks to fill this gap\nas the first comprehensive review of the RWKV architecture, its core\nprinciples, and its varied applications, such as natural language generation,\nnatural language understanding, and computer vision. We assess how RWKV\ncompares to traditional Transformer models, highlighting its capability to\nmanage long sequences efficiently and lower computational costs. Furthermore,\nwe explore the challenges RWKV encounters and propose potential directions for\nfuture research and advancement. We consistently maintain the related\nopen-source materials at: https://github.com/MLGroupJLU/RWKV-Survey.\n","authors":["Zhiyuan Li","Tingyu Xia","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2412.14847v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2412.14843v1","updated":"2024-12-19T13:36:18Z","published":"2024-12-19T13:36:18Z","title":"Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas","summary":"  The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training.\n","authors":["Pietro Bernardelle","Leon Fröhling","Stefano Civelli","Riccardo Lunardi","Kevin Roiter","Gianluca Demartini"],"pdf_url":"https://arxiv.org/pdf/2412.14843v1.pdf","comment":"4 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.14838v1","updated":"2024-12-19T13:28:42Z","published":"2024-12-19T13:28:42Z","title":"DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs","summary":"  Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.\n","authors":["Xiabin Zhou","Wenbin Wang","Minyan Zeng","Jiaxian Guo","Xuebo Liu","Li Shen","Min Zhang","Liang Ding"],"pdf_url":"https://arxiv.org/pdf/2412.14838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14835v1","updated":"2024-12-19T13:25:39Z","published":"2024-12-19T13:25:39Z","title":"Progressive Multimodal Reasoning via Active Retrieval","summary":"  Multi-step multimodal reasoning tasks pose significant challenges for\nmultimodal large language models (MLLMs), and finding effective ways to enhance\ntheir performance in such scenarios remains an unresolved issue. In this paper,\nwe propose AR-MCTS, a universal framework designed to progressively improve the\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\nTree Search (MCTS). Our approach begins with the development of a unified\nretrieval module that retrieves key supporting insights for solving complex\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\nautomated multimodal reasoning verification, we employ the MCTS algorithm\ncombined with an active retrieval mechanism, which enables the automatic\ngeneration of step-wise annotations. This strategy dynamically retrieves key\ninsights for each reasoning step, moving beyond traditional beam search\nsampling to improve the diversity and reliability of the reasoning space.\nAdditionally, we introduce a process reward model that aligns progressively to\nsupport the automatic verification of multimodal reasoning tasks. Experimental\nresults across three complex multimodal reasoning benchmarks confirm the\neffectiveness of the AR-MCTS framework in enhancing the performance of various\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\nsampling diversity and accuracy, yielding reliable multimodal reasoning.\n","authors":["Guanting Dong","Chenghao Zhang","Mengjie Deng","Yutao Zhu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.14835v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.14829v1","updated":"2024-12-19T13:19:19Z","published":"2024-12-19T13:19:19Z","title":"Mention Attention for Pronoun Translation","summary":"  Most pronouns are referring expressions, computers need to resolve what do\nthe pronouns refer to, and there are divergences on pronoun usage across\nlanguages. Thus, dealing with these divergences and translating pronouns is a\nchallenge in machine translation. Mentions are referring candidates of pronouns\nand have closer relations with pronouns compared to general tokens. We assume\nthat extracting additional mention features can help pronoun translation.\nTherefore, we introduce an additional mention attention module in the decoder\nto pay extra attention to source mentions but not non-mention tokens. Our\nmention attention module not only extracts features from source mentions, but\nalso considers target-side context which benefits pronoun translation. In\naddition, we also introduce two mention classifiers to train models to\nrecognize mentions, whose outputs guide the mention attention. We conduct\nexperiments on the WMT17 English-German translation task, and evaluate our\nmodels on general translation and pronoun translation, using BLEU, APT, and\ncontrastive evaluation metrics. Our proposed model outperforms the baseline\nTransformer model in terms of APT and BLEU scores, this confirms our hypothesis\nthat we can improve pronoun translation by paying additional attention to\nsource mentions, and shows that our introduced additional modules do not have\nnegative effect on the general translation quality.\n","authors":["Gongbo Tang","Christian Hardmeier"],"pdf_url":"https://arxiv.org/pdf/2412.14829v1.pdf","comment":"camera-ready version of the paper accepted by JCRAI-23 conference, in\n  ACL format"},{"id":"http://arxiv.org/abs/2408.17072v2","updated":"2024-12-19T13:16:40Z","published":"2024-08-30T07:57:30Z","title":"MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for\n  Retrieval-Augmented Large Language Models","summary":"  In a real-world RAG system, the current query often involves spoken ellipses\nand ambiguous references from dialogue contexts, necessitating query rewriting\nto better describe user's information needs. However, traditional context-based\nrewriting has minimal enhancement on downstream generation tasks due to the\nlengthy process from query rewriting to response generation. Some researchers\ntry to utilize reinforcement learning with generation feedback to assist the\nrewriter, but these sparse rewards provide little guidance in most cases,\nleading to unstable training and generation results. We find that user's needs\nare also reflected in the gold document, retrieved documents and ground truth.\nTherefore, by feeding back these multi-aspect dense rewards to query rewriting,\nmore stable and satisfactory responses can be achieved. In this paper, we\npropose a novel query rewriting method MaFeRw, which improves RAG performance\nby integrating multi-aspect feedback from both the retrieval process and\ngenerated results. Specifically, we first use manual data to train a T5 model\nfor the rewriter initialization. Next, we design three metrics as reinforcement\nlearning feedback: the similarity between the rewritten query and the gold\ndocument, the ranking metrics, and ROUGE between the generation and the ground\ntruth. Inspired by RLAIF, we train three kinds of reward models for the above\nmetrics to achieve more efficient training. Finally, we combine the scores of\nthese reward models as feedback, and use PPO algorithm to explore the optimal\nquery rewriting strategy. Experimental results on two conversational RAG\ndatasets demonstrate that MaFeRw achieves superior generation metrics and more\nstable training compared to baselines.\n","authors":["Yujing Wang","Hainan Zhang","Liang Pang","Binghui Guo","Hongwei Zheng","Zhiming Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.17072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14809v1","updated":"2024-12-19T12:57:47Z","published":"2024-12-19T12:57:47Z","title":"ResoFilter: Rine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis","summary":"  Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance.\n","authors":["Zeao Tu","Xiangdi Meng","Yu He","Zihan Yao","Tianyu Qi","Jun Liu","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2412.14809v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2412.14780v1","updated":"2024-12-19T12:06:24Z","published":"2024-12-19T12:06:24Z","title":"Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model\n  Fine-tuning","summary":"  When using agent-task datasets to enhance agent capabilities for Large\nLanguage Models (LLMs), current methodologies often treat all tokens within a\nsample equally. However, we argue that tokens serving different roles -\nspecifically, reasoning tokens versus boilerplate tokens (e.g., those governing\noutput format) - differ significantly in importance and learning complexity,\nnecessitating their disentanglement and distinct treatment. To address this, we\npropose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token\ndiscrimination. SHAD classifies tokens by exploiting predictability differences\nobserved after shuffling input-output combinations across samples: boilerplate\ntokens, due to their repetitive nature among samples, maintain predictability,\nwhereas reasoning tokens do not. Using SHAD, we propose the\nReasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes\nreasoning tokens during fine-tuning, yielding notable performance gains over\ncommon Supervised Fine-Tuning (SFT).\n","authors":["Ziang Ye","Zhenru Zhang","Yang Zhang","Jianxin Ma","Junyang Lin","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2412.14780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14771v1","updated":"2024-12-19T11:55:51Z","published":"2024-12-19T11:55:51Z","title":"ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine","summary":"  Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments.\n","authors":["Rabee Qasem","Mohannad Hendi","Banan Tantour"],"pdf_url":"https://arxiv.org/pdf/2412.14771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14769v1","updated":"2024-12-19T11:51:57Z","published":"2024-12-19T11:51:57Z","title":"PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in\n  Left-Behind Children","summary":"  Left-behind children (LBCs), numbering over 66 million in China, face severe\nmental health challenges due to parental migration for work. Early screening\nand identification of at-risk LBCs is crucial, yet challenging due to the\nsevere shortage of mental health professionals, especially in rural areas.\nWhile the House-Tree-Person (HTP) test shows higher child participation rates,\nits requirement for expert interpretation limits its application in\nresource-scarce regions. To address this challenge, we propose PsyDraw, a\nmulti-agent system based on Multimodal Large Language Models that assists\nmental health professionals in analyzing HTP drawings. The system employs\nspecialized agents for feature extraction and psychological interpretation,\noperating in two stages: comprehensive feature analysis and professional report\ngeneration. Evaluation of HTP drawings from 290 primary school students reveals\nthat 71.03% of the analyzes achieved High Consistency with professional\nevaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The\nsystem identified 31.03% of cases requiring professional attention,\ndemonstrating its effectiveness as a preliminary screening tool. Currently\ndeployed in pilot schools, \\method shows promise in supporting mental health\nprofessionals, particularly in resource-limited areas, while maintaining high\nprofessional standards in psychological assessment.\n","authors":["Yiqun Zhang","Xiaocui Yang","Xiaobai Li","Siyuan Yu","Yi Luan","Shi Feng","Daling Wang","Yifei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.14769v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2412.14751v1","updated":"2024-12-19T11:30:07Z","published":"2024-12-19T11:30:07Z","title":"Query pipeline optimization for cancer patient question answering\n  systems","summary":"  Retrieval-augmented generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by using query pipelines to retrieve relevant external\ninformation and grounding responses in retrieved knowledge. However, query\npipeline optimization for cancer patient question-answering (CPQA) systems\nrequires separately optimizing multiple components with domain-specific\nconsiderations. We propose a novel three-aspect optimization approach for the\nRAG query pipeline in CPQA systems, utilizing public biomedical databases like\nPubMed and PubMed Central. Our optimization includes: (1) document retrieval,\nutilizing a comparative analysis of NCBI resources and introducing Hybrid\nSemantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,\nidentifying optimal pairings of dense retrievers and rerankers; and (3)\nsemantic representation, introducing Semantic Enhanced Overlap Segmentation\n(SEOS) for improved contextual understanding. On a custom-developed dataset\ntailored for cancer-related inquiries, our optimized RAG approach improved the\nanswer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and\nabout 3% over a naive RAG setup. This study highlights the importance of\ndomain-specific query optimization in realizing the full potential of RAG and\nprovides a robust framework for building more accurate and reliable CPQA\nsystems, advancing the development of RAG-based biomedical systems.\n","authors":["Maolin He","Rena Gao","Mike Conway","Brian E. Chapman"],"pdf_url":"https://arxiv.org/pdf/2412.14751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14737v1","updated":"2024-12-19T11:10:36Z","published":"2024-12-19T11:10:36Z","title":"On Verbalized Confidence Scores for LLMs","summary":"  The rise of large language models (LLMs) and their tight integration into our\ndaily life make it essential to dedicate efforts towards their trustworthiness.\nUncertainty quantification for LLMs can establish more human trust into their\nresponses, but also allows LLM agents to make more informed decisions based on\neach other's uncertainty. To estimate the uncertainty in a response, internal\ntoken logits, task-specific proxy models, or sampling of multiple responses are\ncommonly used. This work focuses on asking the LLM itself to verbalize its\nuncertainty with a confidence score as part of its output tokens, which is a\npromising way for prompt- and model-agnostic uncertainty quantification with\nlow overhead. Using an extensive benchmark, we assess the reliability of\nverbalized confidence scores with respect to different datasets, models, and\nprompt methods. Our results reveal that the reliability of these scores\nstrongly depends on how the model is asked, but also that it is possible to\nextract well-calibrated confidence scores with certain prompt methods. We argue\nthat verbalized confidence scores can become a simple but effective and\nversatile uncertainty quantification method in the future. Our code is\navailable at https://github.com/danielyxyang/llm-verbalized-uq .\n","authors":["Daniel Yang","Yao-Hung Hubert Tsai","Makoto Yamada"],"pdf_url":"https://arxiv.org/pdf/2412.14737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18403v2","updated":"2024-12-19T11:07:09Z","published":"2024-06-26T14:56:13Z","title":"LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks","summary":"  There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators.\n","authors":["Anna Bavaresco","Raffaella Bernardi","Leonardo Bertolazzi","Desmond Elliott","Raquel Fernández","Albert Gatt","Esam Ghaleb","Mario Giulianelli","Michael Hanna","Alexander Koller","André F. T. Martins","Philipp Mondorf","Vera Neplenbroek","Sandro Pezzelle","Barbara Plank","David Schlangen","Alessandro Suglia","Aditya K Surikuchi","Ece Takmaz","Alberto Testoni"],"pdf_url":"https://arxiv.org/pdf/2406.18403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11242v2","updated":"2024-12-19T10:33:13Z","published":"2024-12-15T16:47:16Z","title":"TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs","summary":"  Specializing large language models (LLMs) for local deployment in\ndomain-specific use cases is necessary for strong performance while meeting\nlatency and privacy constraints. However, conventional task-specific adaptation\napproaches do not show simultaneous memory saving and inference speedup at\ndeployment time. Practical compression techniques like quantization and pruning\nrequire dedicated hardware or kernel support to achieve measured inference\nspeedup. We develop TrimLLM based on the layer-wise specialization phenomenon\nwe empirically observed and verified on contemporary LLMs. TrimLLM reduces the\ndepth of LLMs via progressive layer dropping. We show it retains LLMs' capacity\nin specific domains and achieves inference speedup irrespective of hardware and\ndeep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for\ninference; models adapted on medical, legal, and financial datasets all\ndemonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to\n$3.1\\times$ speedup on A100 when compared to state-of-the-art model compression\nalgorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio.\n","authors":["Lanxiang Hu","Tajana Rosing","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07675v3","updated":"2024-12-19T10:11:42Z","published":"2024-12-10T17:02:58Z","title":"RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting","summary":"  Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.\n","authors":["Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2412.07675v3.pdf","comment":"Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117"},{"id":"http://arxiv.org/abs/2412.14689v1","updated":"2024-12-19T09:43:39Z","published":"2024-12-19T09:43:39Z","title":"How to Synthesize Text Data without Model Collapse?","summary":"  Model collapse in synthetic data indicates that iterative training on\nself-generated data leads to a gradual decline in performance. With the\nproliferation of AI models, synthetic data will fundamentally reshape the web\ndata ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend\nof synthetic and human-produced data. In this paper, we focus on two questions:\nwhat is the impact of synthetic data on language model training, and how to\nsynthesize data without model collapse? We first pre-train language models\nacross different proportions of synthetic data, revealing a negative\ncorrelation between the proportion of synthetic data and model performance. We\nfurther conduct statistical analysis on synthetic data to uncover\ndistributional shift phenomenon and over-concentration of n-gram features.\nInspired by the above findings, we propose token editing on human-produced data\nto obtain semi-synthetic data. As a proof of concept, we theoretically\ndemonstrate that token-level editing can prevent model collapse, as the test\nerror is constrained by a finite upper bound. We conduct extensive experiments\non pre-training from scratch, continual pre-training, and supervised\nfine-tuning. The results validate our theoretical proof that token-level\nediting improves data quality and enhances model performance.\n","authors":["Xuekai Zhu","Daixuan Cheng","Hengli Li","Kaiyan Zhang","Ermo Hua","Xingtai Lv","Ning Ding","Zhouhan Lin","Zilong Zheng","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14686v1","updated":"2024-12-19T09:40:17Z","published":"2024-12-19T09:40:17Z","title":"Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity\n  Benchmark for Multimodal Fake News Detection","summary":"  Social platforms, while facilitating access to information, have also become\nsaturated with a plethora of fake news, resulting in negative consequences.\nAutomatic multimodal fake news detection is a worthwhile pursuit. Existing\nmultimodal fake news datasets only provide binary labels of real or fake.\nHowever, real news is alike, while each fake news is fake in its own way. These\ndatasets fail to reflect the mixed nature of various types of multimodal fake\nnews. To bridge the gap, we construct an attributing multi-granularity\nmultimodal fake news detection dataset \\amg, revealing the inherent fake\npattern. Furthermore, we propose a multi-granularity clue alignment model \\our\nto achieve multimodal fake news detection and attribution. Experimental results\ndemonstrate that \\amg is a challenging dataset, and its attribution setting\nopens up new avenues for future research.\n","authors":["Hao Guo","Zihan Ma","Zhi Zeng","Minnan Luo","Weixin Zeng","Jiuyang Tang","Xiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14675v1","updated":"2024-12-19T09:29:08Z","published":"2024-12-19T09:29:08Z","title":"LLMs as mediators: Can they diagnose conflicts accurately?","summary":"  Prior research indicates that to be able to mediate conflict, observers of\ndisagreements between parties must be able to reliably distinguish the sources\nof their disagreement as stemming from differences in beliefs about what is\ntrue (causality) vs. differences in what they value (morality). In this paper,\nwe test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this\ntask and whether one or other type of disagreement proves particularly\nchallenging for LLM's to diagnose. We replicate study 1 in Ko\\c{c}ak et al.\n(2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We\nfind that both LLMs have similar semantic understanding of the distinction\nbetween causal and moral codes as humans and can reliably distinguish between\nthem. When asked to diagnose the source of disagreement in a conversation, both\nLLMs, compared to humans, exhibit a tendency to overestimate the extent of\ncausal disagreement and underestimate the extent of moral disagreement in the\nmoral misalignment condition. This tendency is especially pronounced for GPT 4\nwhen using a proximate scale that relies on concrete language specific to an\nissue. GPT 3.5 does not perform as well as GPT4 or humans when using either the\nproximate or the distal scale. The study provides a first test of the potential\nfor using LLMs to mediate conflict by diagnosing the root of disagreements in\ncausal and evaluative codes.\n","authors":["Özgecan Koçak","Phanish Puranam","Afşar Yegin"],"pdf_url":"https://arxiv.org/pdf/2412.14675v1.pdf","comment":"27 pages, 2 appendices, 21 tables (incl appendices)"},{"id":"http://arxiv.org/abs/2412.06926v3","updated":"2024-12-19T09:24:39Z","published":"2024-12-09T19:11:54Z","title":"When Every Token Counts: Optimal Segmentation for Low-Resource Language\n  Models","summary":"  Traditional greedy tokenization methods have been a critical step in Natural\nLanguage Processing (NLP), influencing how text is converted into tokens and\ndirectly impacting model performance. While subword tokenizers like Byte-Pair\nEncoding (BPE) are widely used, questions remain about their optimality across\nmodel scales and languages. In this work, we demonstrate through extensive\nexperiments that an optimal BPE configuration significantly reduces token count\ncompared to greedy segmentation, yielding improvements in token-saving\npercentages and performance benefits, particularly for smaller models. We\nevaluate tokenization performance across various intrinsic and extrinsic tasks,\nincluding generation and classification. Our findings suggest that\ncompression-optimized tokenization strategies could provide substantial\nadvantages for multilingual and low-resource language applications,\nhighlighting a promising direction for further research and inclusive NLP.\n","authors":["Bharath Raj S","Garvit Suri","Vikrant Dewangan","Raghav Sonavane"],"pdf_url":"https://arxiv.org/pdf/2412.06926v3.pdf","comment":"LoResLM @ COLING 2025"},{"id":"http://arxiv.org/abs/2412.14670v1","updated":"2024-12-19T09:21:39Z","published":"2024-12-19T09:21:39Z","title":"Analysis and Visualization of Linguistic Structures in Large Language\n  Models: Neural Representations of Verb-Particle Constructions in BERT","summary":"  This study investigates the internal representations of verb-particle\ncombinations within transformer-based large language models (LLMs),\nspecifically examining how these models capture lexical and syntactic nuances\nat different neural network layers. Employing the BERT architecture, we analyse\nthe representational efficacy of its layers for various verb-particle\nconstructions such as 'agree on', 'come back', and 'give up'. Our methodology\nincludes a detailed dataset preparation from the British National Corpus,\nfollowed by extensive model training and output analysis through techniques\nlike multi-dimensional scaling (MDS) and generalized discrimination value (GDV)\ncalculations. Results show that BERT's middle layers most effectively capture\nsyntactic structures, with significant variability in representational accuracy\nacross different verb categories. These findings challenge the conventional\nuniformity assumed in neural network processing of linguistic elements and\nsuggest a complex interplay between network architecture and linguistic\nrepresentation. Our research contributes to a better understanding of how deep\nlearning models comprehend and process language, offering insights into the\npotential and limitations of current neural approaches to linguistic analysis.\nThis study not only advances our knowledge in computational linguistics but\nalso prompts further research into optimizing neural architectures for enhanced\nlinguistic precision.\n","authors":["Hassane Kissane","Achim Schilling","Patrick Krauss"],"pdf_url":"https://arxiv.org/pdf/2412.14670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14660v1","updated":"2024-12-19T09:10:07Z","published":"2024-12-19T09:10:07Z","title":"Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.\n","authors":["Zijun Chen","Wenbo Hu","Guande He","Zhijie Deng","Zheng Zhang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.14660v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.14656v1","updated":"2024-12-19T09:07:38Z","published":"2024-12-19T09:07:38Z","title":"Length Controlled Generation for Black-box LLMs","summary":"  Large language models (LLMs) have demonstrated impressive instruction\nfollowing capabilities, while still struggling to accurately manage the length\nof the generated text, which is a fundamental requirement in many real-world\napplications. Existing length control methods involve fine-tuning the\nparameters of LLMs, which is inefficient and suboptimal for practical use. In\nthis paper, we propose a novel iterative sampling framework for text length\ncontrol, integrating the Metropolis-Hastings algorithm with an importance\nsampling acceleration strategy. This framework efficiently and reliably\nregulates LLMs to generate length-constrained text without modifying the\nunderlying parameters, thereby preserving the original capabilities of LLMs.\nExperimental results demonstrate that our framework achieves almost 100\\%\nsuccess rates of length control on Llama3.1 for tasks such as length-controlled\nabstractive summarization and length-constrained instruction following, with\nminimal additional computational overhead. This also highlights the significant\npotential of our method for precise length control across a broader range of\napplications, without compromising the versatility of LLMs.\n","authors":["Yuxuan Gu","Wenjie Wang","Xiaocheng Feng","Weihong Zhong","Kun Zhu","Lei Huang","Tat-Seng Chua","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2412.14656v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.14642v1","updated":"2024-12-19T08:51:16Z","published":"2024-12-19T08:51:16Z","title":"TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation","summary":"  In this paper, we propose Text-based Open Molecule Generation Benchmark\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom). Each task further contains three\nsubtasks, with each subtask comprising 5,000 test samples. Given the inherent\ncomplexity of open molecule generation, we have also developed an automated\nevaluation system that helps measure both the quality and the accuracy of the\ngenerated molecules. Our comprehensive benchmarking of 25 LLMs reveals the\ncurrent limitations and potential areas for improvement in text-guided molecule\ndiscovery. Furthermore, with the assistance of OpenMolIns, a specialized\ninstruction tuning dataset proposed for solving challenges raised by\nTOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even\nsurpassing GPT-3.5-turbo by 46.5\\% on TOMG-Bench. Our codes and datasets are\navailable through https://github.com/phenixace/TOMG-Bench.\n","authors":["Jiatong Li","Junxian Li","Yunqing Liu","Dongzhan Zhou","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2412.14642v1.pdf","comment":"A benchmark for text-based open molecule generation"},{"id":"http://arxiv.org/abs/2409.17603v2","updated":"2024-12-19T08:39:05Z","published":"2024-09-26T07:40:03Z","title":"Deep CLAS: Deep Contextual Listen, Attend and Spell","summary":"  Contextual-LAS (CLAS) has been shown effective in improving Automatic Speech\nRecognition (ASR) of rare words. It relies on phrase-level contextual modeling\nand attention-based relevance scoring without explicit contextual constraint\nwhich lead to insufficient use of contextual information. In this work, we\npropose deep CLAS to use contextual information better. We introduce bias loss\nforcing model to focus on contextual information. The query of bias attention\nis also enriched to improve the accuracy of the bias attention score. To get\nfine-grained contextual information, we replace phrase-level encoding with\ncharacter-level encoding and encode contextual information with conformer\nrather than LSTM. Moreover, we directly use the bias attention score to correct\nthe output probability distribution of the model. Experiments using the public\nAISHELL-1 and AISHELL-NER. On AISHELL-1, compared to CLAS baselines, deep CLAS\nobtains a 65.78% relative recall and a 53.49% relative F1-score increase in the\nnamed entity recognition scene.\n","authors":["Mengzhi Wang","Shifu Xiong","Genshun Wan","Hang Chen","Jianqing Gao","Lirong Dai"],"pdf_url":"https://arxiv.org/pdf/2409.17603v2.pdf","comment":"Submitted to JUSTC"},{"id":"http://arxiv.org/abs/2412.14626v1","updated":"2024-12-19T08:28:18Z","published":"2024-12-19T08:28:18Z","title":"Learning to Generate Research Idea with Dynamic Control","summary":"  The rapid advancements in large language models (LLMs) have demonstrated\ntheir potential to accelerate scientific discovery, particularly in automating\nthe process of research ideation. LLM-based systems have shown promise in\ngenerating hypotheses and research ideas. However, current approaches\npredominantly rely on prompting-based pre-trained models, limiting their\nability to optimize generated content effectively. Moreover, they also lack the\ncapability to deal with the complex interdependence and inherent restrictions\namong novelty, feasibility, and effectiveness, which remains challenging due to\nthe inherent trade-offs among these dimensions, such as the\ninnovation-feasibility conflict. To address these limitations, we for the first\ntime propose fine-tuning LLMs to be better idea proposers and introduce a novel\nframework that employs a two-stage approach combining Supervised Fine-Tuning\n(SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model\nlearns foundational patterns from pairs of research papers and follow-up ideas.\nIn the RL stage, multi-dimensional reward modeling, guided by fine-grained\nfeedback, evaluates and optimizes the generated ideas across key metrics.\nDimensional controllers enable dynamic adjustment of generation, while a\nsentence-level decoder ensures context-aware emphasis during inference. Our\nframework provides a balanced approach to research ideation, achieving\nhigh-quality outcomes by dynamically navigating the trade-offs among novelty,\nfeasibility, and effectiveness.\n","authors":["Ruochen Li","Liqiang Jing","Chi Han","Jiawei Zhou","Xinya Du"],"pdf_url":"https://arxiv.org/pdf/2412.14626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14011v2","updated":"2024-12-19T08:27:54Z","published":"2024-12-18T16:29:45Z","title":"Towards an optimised evaluation of teachers' discourse: The case of\n  engaging messages","summary":"  Evaluating teachers' skills is crucial for enhancing education quality and\nstudent outcomes. Teacher discourse, significantly influencing student\nperformance, is a key component. However, coding this discourse can be\nlaborious. This study addresses this issue by introducing a new methodology for\noptimising the assessment of teacher discourse. The research consisted of two\nstudies, both within the framework of engaging messages used by secondary\neducation teachers. The first study involved training two large language models\non real-world examples from audio-recorded lessons over two academic years to\nidentify and classify the engaging messages from the lessons' transcripts. This\nresulted in sensitivities of 84.31% and 91.11%, and specificities of 97.69% and\n86.36% in identification and classification, respectively. The second study\napplied these models to transcripts of audio-recorded lessons from a third\nacademic year to examine the frequency and distribution of message types by\neducational level and moment of the academic year. Results showed teachers\npredominantly use messages emphasising engagement benefits, linked to improved\noutcomes, while one-third highlighted non-engagement disadvantages, associated\nwith increased anxiety. The use of engaging messages declined in Grade 12 and\ntowards the academic year's end. These findings suggest potential interventions\nto optimise engaging message use, enhancing teaching quality and student\noutcomes.\n","authors":["Samuel Falcon","Jaime Leon"],"pdf_url":"https://arxiv.org/pdf/2412.14011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14617v1","updated":"2024-12-19T08:06:09Z","published":"2024-12-19T08:06:09Z","title":"How good is GPT at writing political speeches for the White House?","summary":"  Using large language models (LLMs), computers are able to generate a written\ntext in response to a us er request. As this pervasive technology can be\napplied in numerous contexts, this study analyses the written style of one LLM\ncalled GPT by comparing its generated speeches with those of the recent US\npresidents. To achieve this objective, the State of the Union (SOTU) addresses\nwritten by Reagan to Biden are contrasted to those produced by both GPT-3.5 and\nGPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma\n\"we\" and produce shorter messages with, on average, longer sentences. Moreover,\nGPT opts for an optimistic tone, opting more often for political (e.g.,\npresident, Congress), symbolic (e.g., freedom), and abstract terms (e.g.,\nfreedom). Even when imposing an author's style to GPT, the resulting speech\nremains distinct from addresses written by the target author. Finally, the two\nGPT versions present distinct characteristics, but both appear overall\ndissimilar to true presidential messages.\n","authors":["Jacques Savoy"],"pdf_url":"https://arxiv.org/pdf/2412.14617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14613v1","updated":"2024-12-19T08:03:16Z","published":"2024-12-19T08:03:16Z","title":"HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic\n  Evaluation Using a Vision Language Model","summary":"  Vision-language models (VLMs) have shown impressive abilities in text and\nimage understanding. However, existing metrics for evaluating the text\ngenerated by VLMs focus exclusively on overall quality, leading to two\nlimitations: 1) it is challenging to identify which aspects of the text need\nimprovement from the overall score; 2) metrics may overlook specific evaluation\ncriteria when predicting an overall score. To address these limitations, we\npropose HarmonicEval, a reference-free evaluation metric that aggregates\ncriterion-wise scores to produce the overall score in a bottom-up manner.\nFurthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)\ndataset, which comprises 18,000 expert human judgments across four\nvision-language tasks. Our experiments demonstrate that HarmonicEval achieves\nhigher correlations with human judgments than conventional metrics while\nproviding numerical scores for each criterion.\n","authors":["Masanari Ohi","Masahiro Kaneko","Naoaki Okazaki","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2412.14613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14612v1","updated":"2024-12-19T08:02:08Z","published":"2024-12-19T08:02:08Z","title":"KARRIEREWEGE: A Large Scale Career Path Prediction Dataset","summary":"  Accurate career path prediction can support many stakeholders, like job\nseekers, recruiters, HR, and project managers. However, publicly available data\nand tools for career path prediction are scarce. In this work, we introduce\nKARRIEREWEGE, a comprehensive, publicly available dataset containing over 500k\ncareer paths, significantly surpassing the size of previously available\ndatasets. We link the dataset to the ESCO taxonomy to offer a valuable resource\nfor predicting career trajectories. To tackle the problem of free-text inputs\ntypically found in resumes, we enhance it by synthesizing job titles and\ndescriptions resulting in KARRIEREWEGE+. This allows for accurate predictions\nfrom unstructured data, closely aligning with real-world application\nchallenges. We benchmark existing state-of-the-art (SOTA) models on our dataset\nand a prior benchmark and observe improved performance and robustness,\nparticularly for free-text use cases, due to the synthesized data.\n","authors":["Elena Senger","Yuri Campbell","Rob van der Goot","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2412.14612v1.pdf","comment":"Accepted at COLING Industry Track"},{"id":"http://arxiv.org/abs/2412.14596v1","updated":"2024-12-19T07:31:40Z","published":"2024-12-19T07:31:40Z","title":"LDP: Generalizing to Multilingual Visual Information Extraction by\n  Language Decoupled Pretraining","summary":"  Visual Information Extraction (VIE) plays a crucial role in the comprehension\nof semi-structured documents, and several pre-trained models have been\ndeveloped to enhance performance. However, most of these works are monolingual\n(usually English). Due to the extremely unbalanced quantity and quality of\npre-training corpora between English and other languages, few works can extend\nto non-English scenarios. In this paper, we conduct systematic experiments to\nshow that vision and layout modality hold invariance among images with\ndifferent languages. If decoupling language bias from document images, a\nvision-layout-based model can achieve impressive cross-lingual generalization.\nAccordingly, we present a simple but effective multilingual training paradigm\nLDP (Language Decoupled Pre-training) for better utilization of monolingual\npre-training data. Our proposed model LDM (Language Decoupled Model) is first\npre-trained on the language-independent data, where the language knowledge is\ndecoupled by a diffusion model, and then the LDM is fine-tuned on the\ndownstream languages. Extensive experiments show that the LDM outperformed all\nSOTA multilingual pre-trained models, and also maintains competitiveness on\ndownstream monolingual/English benchmarks.\n","authors":["Huawen Shen","Gengluo Li","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14596v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2411.12262v2","updated":"2024-12-19T07:29:23Z","published":"2024-11-19T06:21:51Z","title":"Low-resource Machine Translation: what for? who for? An observational\n  study on a dedicated Tetun language translation service","summary":"  Low-resource machine translation (MT) presents a diversity of community needs\nand application challenges that remain poorly understood. To complement surveys\nand focus groups, which tend to rely on small samples of respondents, we\npropose an observational study on actual usage patterns of a specialized MT\nservice for the Tetun language, which is the lingua franca in Timor-Leste. Our\nanalysis of 100,000 translation requests reveals patterns that challenge\nassumptions based on existing corpora. We find that users, many of them\nstudents on mobile devices, typically translate text from a high-resource\nlanguage into Tetun across diverse domains including science, healthcare, and\ndaily life. This contrasts sharply with available Tetun corpora, which are\ndominated by news articles covering government and social issues. Our results\nsuggest that MT systems for minority languages like Tetun should prioritize\naccuracy on domains relevant to educational contexts, in the high-resource to\nlow-resource direction. More broadly, this study demonstrates how observational\nanalysis can inform low-resource language technology development, by grounding\nresearch in practical community needs.\n","authors":["Raphael Merx","Adérito José Guterres Correia","Hanna Suominen","Ekaterina Vylomova"],"pdf_url":"https://arxiv.org/pdf/2411.12262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14588v1","updated":"2024-12-19T07:14:13Z","published":"2024-12-19T07:14:13Z","title":"Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning","summary":"  In legal practice, judges apply the trichotomous dogmatics of criminal law,\nsequentially assessing the elements of the offense, unlawfulness, and\nculpability to determine whether an individual's conduct constitutes a crime.\nAlthough current legal large language models (LLMs) show promising accuracy in\njudgment prediction, they lack trichotomous reasoning capabilities due to the\nabsence of an appropriate benchmark dataset, preventing them from predicting\ninnocent outcomes. As a result, every input is automatically assigned a charge,\nlimiting their practical utility in legal contexts. To bridge this gap, we\nintroduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with\nInnocent Verdicts. Adhering to the trichotomous dogmatics, we extend three\nwidely-used legal datasets through LLM-based augmentation and manual\nverification. Our experiments with state-of-the-art legal LLMs and novel\nstrategies that integrate trichotomous reasoning into zero-shot prompting and\nfine-tuning reveal: (1) current legal LLMs have significant room for\nimprovement, with even the best models achieving an F1 score of less than 0.3\non LJPIV; and (2) our strategies notably enhance both in-domain and\ncross-domain judgment prediction accuracy, especially for cases resulting in an\ninnocent verdict.\n","authors":["Kepu Zhang","Haoyue Yang","Xu Tang","Weijie Yu","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2412.14588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14584v1","updated":"2024-12-19T07:06:01Z","published":"2024-12-19T07:06:01Z","title":"Simulation-Free Hierarchical Latent Policy Planning for Proactive\n  Dialogues","summary":"  Recent advancements in proactive dialogues have garnered significant\nattention, particularly for more complex objectives (e.g. emotion support and\npersuasion). Unlike traditional task-oriented dialogues, proactive dialogues\ndemand advanced policy planning and adaptability, requiring rich scenarios and\ncomprehensive policy repositories to develop such systems. However, existing\napproaches tend to rely on Large Language Models (LLMs) for user simulation and\nonline learning, leading to biases that diverge from realistic scenarios and\nresult in suboptimal efficiency. Moreover, these methods depend on manually\ndefined, context-independent, coarse-grained policies, which not only incur\nhigh expert costs but also raise concerns regarding their completeness. In our\nwork, we highlight the potential for automatically discovering policies\ndirectly from raw, real-world dialogue records. To this end, we introduce a\nnovel dialogue policy planning framework, LDPP. It fully automates the process\nfrom mining policies in dialogue records to learning policy planning.\nSpecifically, we employ a variant of the Variational Autoencoder to discover\nfine-grained policies represented as latent vectors. After automatically\nannotating the data with these latent policy labels, we propose an Offline\nHierarchical Reinforcement Learning (RL) algorithm in the latent space to\ndevelop effective policy planning capabilities. Our experiments demonstrate\nthat LDPP outperforms existing methods on two proactive scenarios, even\nsurpassing ChatGPT with only a 1.8-billion-parameter LLM.\n","authors":["Tao He","Lizi Liao","Yixin Cao","Yuanxing Liu","Yiheng Sun","Zerui Chen","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2412.14584v1.pdf","comment":"24 pages, 5 fgiures, AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14581v1","updated":"2024-12-19T07:01:25Z","published":"2024-12-19T07:01:25Z","title":"CORD: Balancing COnsistency and Rank Distillation for Robust\n  Retrieval-Augmented Generation","summary":"  With the adoption of retrieval-augmented generation (RAG), large language\nmodels (LLMs) are expected to ground their generation to the retrieved\ncontexts. Yet, this is hindered by position bias of LLMs, failing to evenly\nattend to all contexts. Previous work has addressed this by synthesizing\ncontexts with perturbed positions of gold segment, creating a\nposition-diversified train set. We extend this intuition to propose consistency\nregularization with augmentation and distillation. First, we augment each\ntraining instance with its position perturbation to encourage consistent\npredictions, regardless of ordering. We also distill behaviors of this pair,\nalthough it can be counterproductive in certain RAG scenarios where the given\norder from the retriever is crucial for generation quality. We thus propose\nCORD, balancing COnsistency and Rank Distillation. CORD adaptively samples\nnoise-controlled perturbations from an interpolation space, ensuring both\nconsistency and respect for the rank prior. Empirical results show this balance\nenables CORD to outperform consistently in diverse RAG benchmarks.\n","authors":["Youngwon Lee","Seung-won Hwang","Daniel Campos","Filip Graliński","Zhewei Yao","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2412.14581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07629v2","updated":"2024-12-19T06:53:37Z","published":"2024-12-10T16:08:14Z","title":"Piece of Table: A Divide-and-Conquer Approach for Selecting Sub-Tables\n  in Table Question Answering","summary":"  Applying language models (LMs) to tables is challenging due to the inherent\nstructural differences between two-dimensional tables and one-dimensional text\nfor which the LMs were originally designed. Furthermore, when applying\nlinearized tables to LMs, the maximum token lengths often imposed in\nself-attention calculations make it difficult to comprehensively understand the\ncontext spread across large tables. To address these challenges, we present\nPieTa (Piece of Table), a new framework for sub-table-based question answering\n(QA). PieTa operates through an iterative process of dividing tables into\nsmaller windows, using LMs to select relevant cells within each window, and\nmerging these cells into a sub-table. This multi-resolution approach captures\ndependencies across multiple rows and columns while avoiding the limitations\ncaused by long context inputs. Instantiated as a simple iterative sub-table\nunion algorithm, PieTa demonstrates improved performance over previous\nsub-table-based QA approaches.\n","authors":["Wonjin Lee","Kyumin Kim","Sungjae Lee","Jihun Lee","Kwang In Kim"],"pdf_url":"https://arxiv.org/pdf/2412.07629v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14574v1","updated":"2024-12-19T06:44:59Z","published":"2024-12-19T06:44:59Z","title":"Sliding Windows Are Not the End: Exploring Full Ranking with\n  Long-Context Large Language Models","summary":"  Large Language Models (LLMs) have shown exciting performance in listwise\npassage ranking. Due to the limited input length, existing methods often adopt\nthe sliding window strategy. Such a strategy, though effective, is inefficient\nas it involves repetitive and serialized processing, which usually re-evaluates\nrelevant passages multiple times. As a result, it incurs redundant API costs,\nwhich are proportional to the number of inference tokens. The development of\nlong-context LLMs enables the full ranking of all passages within a single\ninference, avoiding redundant API costs. In this paper, we conduct a\ncomprehensive study of long-context LLMs for ranking tasks in terms of\nefficiency and effectiveness. Surprisingly, our experiments reveal that full\nranking with long-context LLMs can deliver superior performance in the\nsupervised fine-tuning setting with a huge efficiency improvement. Furthermore,\nwe identify two limitations of fine-tuning the full ranking model based on\nexisting methods: (1) sliding window strategy fails to produce a full ranking\nlist as a training label, and (2) the language modeling loss cannot emphasize\ntop-ranked passage IDs in the label. To alleviate these issues, we propose a\nnew complete listwise label construction approach and a novel importance-aware\nlearning objective for full ranking. Experiments show the superior performance\nof our method over baselines. Our codes are available at\n\\url{https://github.com/8421BCD/fullrank}.\n","authors":["Wenhan Liu","Xinyu Ma","Yutao Zhu","Ziliang Zhao","Shuaiqiang Wang","Dawei Yin","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2412.14574v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.07663v2","updated":"2024-12-19T06:34:31Z","published":"2024-08-14T16:51:21Z","title":"Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining\n  of Probability Distributions","summary":"  Large language models are susceptible to jailbreak attacks, which can result\nin the generation of harmful content. While prior defenses mitigate these risks\nby perturbing or inspecting inputs, they ignore competing objectives, the\nunderlying cause of alignment failures. In this paper, we propose\nAlignment-Enhanced Decoding (AED), a novel defense that employs adaptive\ndecoding to address the root causes of jailbreak issues. We first define the\nCompetitive Index to quantify alignment failures and utilize feedback from\nself-evaluation to compute post-alignment logits. Then, AED adaptively combines\nAED and post-alignment logits with the original logits to obtain harmless and\nhelpful distributions. Consequently, our method enhances safety alignment while\nmaintaining helpfulness. We conduct experiments across five models and four\ncommon jailbreaks, with the results validating the effectiveness of our\napproach. Code is available at https://github.com/GIGABaozi/AED.git.\n","authors":["Quan Liu","Zhenhong Zhou","Longzhu He","Yi Liu","Wei Zhang","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2408.07663v2.pdf","comment":"Accepted by EMNLP 2024, 15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13663v2","updated":"2024-12-19T06:32:26Z","published":"2024-12-18T09:39:44Z","title":"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast, Memory Efficient, and Long Context Finetuning and Inference","summary":"  Encoder-only transformer models such as BERT offer a great performance-size\ntradeoff for retrieval and classification tasks with respect to larger\ndecoder-only models. Despite being the workhorse of numerous production\npipelines, there have been limited Pareto improvements to BERT since its\nrelease. In this paper, we introduce ModernBERT, bringing modern model\noptimizations to encoder-only models and representing a major Pareto\nimprovement over older encoders. Trained on 2 trillion tokens with a native\n8192 sequence length, ModernBERT models exhibit state-of-the-art results on a\nlarge pool of evaluations encompassing diverse classification tasks and both\nsingle and multi-vector retrieval on different domains (including code). In\naddition to strong downstream performance, ModernBERT is also the most speed\nand memory efficient encoder and is designed for inference on common GPUs.\n","authors":["Benjamin Warner","Antoine Chaffin","Benjamin Clavié","Orion Weller","Oskar Hallström","Said Taghadouini","Alexis Gallagher","Raja Biswas","Faisal Ladhak","Tom Aarsen","Nathan Cooper","Griffin Adams","Jeremy Howard","Iacopo Poli"],"pdf_url":"https://arxiv.org/pdf/2412.13663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19813v3","updated":"2024-12-19T06:27:44Z","published":"2024-07-29T09:05:10Z","title":"Improving Retrieval Augmented Language Model with Self-Reasoning","summary":"  The Retrieval-Augmented Language Model (RALM) has shown remarkable\nperformance on knowledge-intensive tasks by incorporating external knowledge\nduring inference, which mitigates the factual hallucinations inherited in large\nlanguage models (LLMs). Despite these advancements, challenges persist in the\nimplementation of RALMs, particularly concerning their reliability and\ntraceability. To be specific, the irrelevant document retrieval may result in\nunhelpful response generation or even deteriorate the performance of LLMs,\nwhile the lack of proper citations in generated outputs complicates efforts to\nverify the trustworthiness of the models. To this end, we propose a novel\nself-reasoning framework aimed at improving the reliability and traceability of\nRALMs, whose core idea is to leverage reasoning trajectories generated by the\nLLM itself. The framework involves constructing self-reason trajectories with\nthree processes: a relevance-aware process, an evidence-aware selective\nprocess, and a trajectory analysis process. We have evaluated our framework\nacross four public datasets (two short-form QA datasets, one long-form QA\ndataset, and one fact verification dataset) to demonstrate the superiority of\nour method, which can outperform existing state-of-the-art models and can\nachieve comparable performance with GPT-4, while only using 2,000 training\nsamples.\n","authors":["Yuan Xia","Jingbo Zhou","Zhenhui Shi","Jun Chen","Haifeng Huang"],"pdf_url":"https://arxiv.org/pdf/2407.19813v3.pdf","comment":"AAAI 2025 (main conference)"},{"id":"http://arxiv.org/abs/2412.14556v1","updated":"2024-12-19T06:14:20Z","published":"2024-12-19T06:14:20Z","title":"CitaLaw: Enhancing LLM with Citations in Legal Domain","summary":"  In this paper, we propose CitaLaw, the first benchmark designed to evaluate\nLLMs' ability to produce legally sound responses with appropriate citations.\nCitaLaw features a diverse set of legal questions for both laypersons and\npractitioners, paired with a comprehensive corpus of law articles and precedent\ncases as a reference pool. This framework enables LLM-based systems to retrieve\nsupporting citations from the reference corpus and align these citations with\nthe corresponding sentences in their responses. Moreover, we introduce\nsyllogism-inspired evaluation methods to assess the legal alignment between\nretrieved references and LLM-generated responses, as well as their consistency\nwith user questions. Extensive experiments on 2 open-domain and 7\nlegal-specific LLMs demonstrate that integrating legal references substantially\nenhances response quality. Furthermore, our proposed syllogism-based evaluation\nmethod exhibits strong agreement with human judgments.\n","authors":["Kepu Zhang","Weijie Yu","Sunhao Dai","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2412.14556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08125v2","updated":"2024-12-19T05:46:29Z","published":"2024-12-11T06:21:33Z","title":"Progressive Multi-granular Alignments for Grounded Reasoning in Large\n  Vision-Language Models","summary":"  Existing Large Vision-Language Models (LVLMs) excel at matching concepts\nacross multi-modal inputs but struggle with compositional concepts and\nhigh-level relationships between entities. This paper introduces Progressive\nmulti-granular Vision-Language alignments (PromViL), a novel framework to\nenhance LVLMs' ability in performing grounded compositional visual reasoning\ntasks. Our approach constructs a hierarchical structure of multi-modal\nalignments, ranging from simple to complex concepts. By progressively aligning\ntextual descriptions with corresponding visual regions, our model learns to\nleverage contextual information from lower levels to inform higher-level\nreasoning. To facilitate this learning process, we introduce a data generation\nprocess that creates a novel dataset derived from Visual Genome, providing a\nwide range of nested compositional vision-language pairs. Experimental results\ndemonstrate that our PromViL framework significantly outperforms baselines on\nvarious visual grounding and compositional question answering tasks. The code\nis available at: https://github.com/lqh52/PromViL.\n","authors":["Quang-Hung Le","Long Hoang Dang","Ngan Le","Truyen Tran","Thao Minh Le"],"pdf_url":"https://arxiv.org/pdf/2412.08125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00451v3","updated":"2024-12-19T05:32:59Z","published":"2024-10-01T07:11:55Z","title":"Unleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large\n  Language Models","summary":"  Despite significant ongoing efforts in safety alignment, large language\nmodels (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks\nthat can induce harmful behaviors, including through the use of adversarial\nsuffixes. Building on prior research, we hypothesize that these adversarial\nsuffixes are not mere bugs but may represent features that can dominate the\nLLM's behavior. To evaluate this hypothesis, we conduct several experiments.\nFirst, we demonstrate that benign features can be effectively made to function\nas adversarial suffixes, i.e., we develop a feature extraction method to\nextract sample-agnostic features from benign dataset in the form of suffixes\nand show that these suffixes may effectively compromise safety alignment.\nSecond, we show that adversarial suffixes generated from jailbreak attacks may\ncontain meaningful features, i.e., appending the same suffix to different\nprompts results in responses exhibiting specific characteristics. Third, we\nshow that such benign-yet-safety-compromising features can be easily introduced\nthrough fine-tuning using only benign datasets. As a result, we are able to\ncompletely eliminate GPT's safety alignment in a blackbox setting through\nfinetuning with only benign data. Our code and data is available at\n\\url{https://github.com/suffix-maybe-feature/adver-suffix-maybe-features}.\n","authors":["Wei Zhao","Zhe Li","Yige Li","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.00451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13362v3","updated":"2024-12-19T05:26:14Z","published":"2024-06-19T09:07:31Z","title":"VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models","summary":"  Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.\n","authors":["Haowen Hou","Peigen Zeng","Fei Ma","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2406.13362v3.pdf","comment":"Accepted at COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2412.14533v1","updated":"2024-12-19T05:11:16Z","published":"2024-12-19T05:11:16Z","title":"ClusterTalk: Corpus Exploration Framework using Multi-Dimensional\n  Exploratory Search","summary":"  Exploratory search of large text corpora is essential in domains like\nbiomedical research, where large amounts of research literature are\ncontinuously generated. This paper presents ClusterTalk (The demo video and\nsource code are available at: https://github.com/achouhan93/ClusterTalk), a\nframework for corpus exploration using multi-dimensional exploratory search.\nOur system integrates document clustering with faceted search, allowing users\nto interactively refine their exploration and ask corpus and document-level\nqueries. Compared to traditional one-dimensional search approaches like keyword\nsearch or clustering, this system improves the discoverability of information\nby encouraging a deeper interaction with the corpus. We demonstrate the\nfunctionality of the ClusterTalk framework based on four million PubMed\nabstracts for the four-year time frame.\n","authors":["Ashish Chouhan","Saifeldin Mandour","Michael Gertz"],"pdf_url":"https://arxiv.org/pdf/2412.14533v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2404.01349v2","updated":"2024-12-19T05:05:46Z","published":"2024-03-31T22:22:53Z","title":"Fairness in Large Language Models: A Taxonomic Survey","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains. However, despite their promising performance in numerous\nreal-world applications, most of these algorithms lack fairness considerations.\nConsequently, they may lead to discriminatory outcomes against certain\ncommunities, particularly marginalized populations, prompting extensive study\nin fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in\ntraditional machine learning, entails exclusive backgrounds, taxonomies, and\nfulfillment techniques. To this end, this survey presents a comprehensive\noverview of recent advances in the existing literature concerning fair LLMs.\nSpecifically, a brief introduction to LLMs is provided, followed by an analysis\nof factors contributing to bias in LLMs. Additionally, the concept of fairness\nin LLMs is discussed categorically, summarizing metrics for evaluating bias in\nLLMs and existing algorithms for promoting fairness. Furthermore, resources for\nevaluating bias in LLMs, including toolkits and datasets, are summarized.\nFinally, existing research challenges and open questions are discussed.\n","authors":["Zhibo Chu","Zichong Wang","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.01349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08108v2","updated":"2024-12-19T05:01:33Z","published":"2024-12-11T05:23:34Z","title":"Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language\n  Models Across Both Images and Text with a Single Perturbation","summary":"  Large Vision-Language Models (VLMs) have demonstrated remarkable performance\nacross multimodal tasks by integrating vision encoders with large language\nmodels (LLMs). However, these models remain vulnerable to adversarial attacks.\nAmong such attacks, Universal Adversarial Perturbations (UAPs) are especially\npowerful, as a single optimized perturbation can mislead the model across\nvarious input images. In this work, we introduce a novel UAP specifically\ndesigned for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),\ncapable of universally deceiving VLMs across both image and text inputs. To\nsuccessfully disrupt the vision encoder's fundamental process, we analyze the\ncore components of the attention mechanism. After identifying value vectors in\nthe middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a\nlabel-free manner with a frozen model. Despite being developed as a black-box\nto the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently\noutperforming baseline methods across vision-language tasks. Extensive ablation\nstudies and analyses further demonstrate the robustness of Doubly-UAP and\nprovide insights into how it influences internal attention mechanisms.\n","authors":["Hee-Seon Kim","Minbeom Kim","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.08108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14528v1","updated":"2024-12-19T04:51:06Z","published":"2024-12-19T04:51:06Z","title":"Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge\n  Distillation on Language Models","summary":"  Knowledge distillation (KD) has become a prevalent technique for compressing\nlarge language models (LLMs). Existing KD methods are constrained by the need\nfor identical tokenizers (i.e., vocabularies) between teacher and student\nmodels, limiting their versatility in handling LLMs of different architecture\nfamilies. In this paper, we introduce the Multi-Level Optimal Transport\n(MultiLevelOT), a novel approach that advances the optimal transport for\nuniversal cross-tokenizer knowledge distillation. Our method aligns the logit\ndistributions of the teacher and the student at both token and sequence levels\nusing diverse cost matrices, eliminating the need for dimensional or\ntoken-by-token correspondence. At the token level, MultiLevelOT integrates both\nglobal and local information by jointly optimizing all tokens within a sequence\nto enhance robustness. At the sequence level, we efficiently capture complex\ndistribution structures of logits via the Sinkhorn distance, which approximates\nthe Wasserstein distance for divergence measures. Extensive experiments on\ntasks such as extractive QA, generative QA, and summarization demonstrate that\nthe MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under\nvarious settings. Our approach is robust to different student and teacher\nmodels across model families, architectures, and parameter sizes.\n","authors":["Xiao Cui","Mo Zhu","Yulei Qin","Liang Xie","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2412.14528v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2411.16646v2","updated":"2024-12-19T04:50:45Z","published":"2024-11-25T18:28:26Z","title":"Self-Generated Critiques Boost Reward Modeling for Language Models","summary":"  Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.\n","authors":["Yue Yu","Zhengxing Chen","Aston Zhang","Liang Tan","Chenguang Zhu","Richard Yuanzhe Pang","Yundi Qian","Xuewei Wang","Suchin Gururangan","Chao Zhang","Melanie Kambadur","Dhruv Mahajan","Rui Hou"],"pdf_url":"https://arxiv.org/pdf/2411.16646v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2412.00869v2","updated":"2024-12-19T04:38:59Z","published":"2024-12-01T16:15:14Z","title":"KnowledgePrompts: Exploring the Abilities of Large Language Models to\n  Solve Proportional Analogies via Knowledge-Enhanced Prompting","summary":"  Making analogies is fundamental to cognition. Proportional analogies, which\nconsist of four terms, are often used to assess linguistic and cognitive\nabilities. For instance, completing analogies like \"Oxygen is to Gas as <blank>\nis to <blank>\" requires identifying the semantic relationship (e.g., \"type of\")\nbetween the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair\nthat shares the same relationship (e.g., \"Aluminum\" and \"Metal\"). In this work,\nwe introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for\nproportional analogy completion and evaluate the performance of contemporary\nLarge Language Models (LLMs) in various knowledge-enhanced prompt settings.\nSpecifically, we augment prompts with three types of knowledge: exemplar,\nstructured, and targeted. Our results show that despite extensive training\ndata, solving proportional analogies remains challenging for current LLMs, with\nthe best model achieving an accuracy of 55%. Notably, we find that providing\ntargeted knowledge can better assist models in completing proportional\nanalogies compared to providing exemplars or collections of structured\nknowledge. Our code and data are available at:\nhttps://github.com/Thiliniiw/KnowledgePrompts/\n","authors":["Thilini Wijesiriwardene","Ruwan Wickramarachchi","Sreeram Vennam","Vinija Jain","Aman Chadha","Amitava Das","Ponnurangam Kumaraguru","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2412.00869v2.pdf","comment":"Accepted at COLING 2025"},{"id":"http://arxiv.org/abs/2305.09574v2","updated":"2024-12-19T04:35:08Z","published":"2023-05-16T16:11:48Z","title":"UOR: Universal Backdoor Attacks on Pre-trained Language Models","summary":"  Backdoors implanted in pre-trained language models (PLMs) can be transferred\nto various downstream tasks, which exposes a severe security threat. However,\nmost existing backdoor attacks against PLMs are un-targeted and task-specific.\nFew targeted and task-agnostic methods use manually pre-defined triggers and\noutput representations, which prevent the attacks from being more effective and\ngeneral. In this paper, we first summarize the requirements that a more\nthreatening backdoor attack against PLMs should satisfy, and then propose a new\nbackdoor attack method called UOR, which breaks the bottleneck of the previous\napproach by turning manual selection into automatic optimization. Specifically,\nwe define poisoned supervised contrastive learning which can automatically\nlearn the more uniform and universal output representations of triggers for\nvarious PLMs. Moreover, we use gradient search to select appropriate trigger\nwords which can be adaptive to different PLMs and vocabularies. Experiments\nshow that our method can achieve better attack performance on various text\nclassification tasks compared to manual methods. Further, we tested our method\non PLMs with different architectures, different usage paradigms, and more\ndifficult tasks, which demonstrated the universality of our method.\n","authors":["Wei Du","Peixuan Li","Boqun Li","Haodong Zhao","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2305.09574v2.pdf","comment":"ACL-Findings 2024"},{"id":"http://arxiv.org/abs/2412.14516v1","updated":"2024-12-19T04:31:56Z","published":"2024-12-19T04:31:56Z","title":"Cal-DPO: Calibrated Direct Preference Optimization for Language Model\n  Alignment","summary":"  We study the problem of aligning large language models (LLMs) with human\npreference data. Contrastive preference optimization has shown promising\nresults in aligning LLMs with available preference data by optimizing the\nimplicit reward associated with the policy. However, the contrastive objective\nfocuses mainly on the relative values of implicit rewards associated with two\nresponses while ignoring their actual values, resulting in suboptimal alignment\nwith human preferences. To address this limitation, we propose calibrated\ndirect preference optimization (Cal-DPO), a simple yet effective algorithm. We\nshow that substantial improvement in alignment with the given preferences can\nbe achieved simply by calibrating the implicit reward to ensure that the\nlearned implicit rewards are comparable in scale to the ground-truth rewards.\nWe demonstrate the theoretical advantages of Cal-DPO over existing approaches.\nThe results of our experiments on a variety of standard benchmarks show that\nCal-DPO remarkably improves off-the-shelf methods.\n","authors":["Teng Xiao","Yige Yuan","Huaisheng Zhu","Mingxiao Li","Vasant G Honavar"],"pdf_url":"https://arxiv.org/pdf/2412.14516v1.pdf","comment":"Accepted by NeurIPS 2024 Main"},{"id":"http://arxiv.org/abs/2412.14510v1","updated":"2024-12-19T04:18:51Z","published":"2024-12-19T04:18:51Z","title":"PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization","summary":"  The emergence of Retrieval-augmented generation (RAG) has alleviated the\nissues of outdated and hallucinatory content in the generation of large\nlanguage models (LLMs), yet it still reveals numerous limitations. When a\ngeneral-purpose LLM serves as the RAG generator, it often suffers from\ninadequate response informativeness, response robustness, and citation quality.\nPast approaches to tackle these limitations, either by incorporating additional\nsteps beyond generating responses or optimizing the generator through\nsupervised fine-tuning (SFT), still failed to align with the RAG requirement\nthoroughly. Consequently, optimizing the RAG generator from multiple preference\nperspectives while maintaining its end-to-end LLM form remains a challenge. To\nbridge this gap, we propose Multiple Perspective Preference Alignment for\nRetrieval-Augmented Generation (PA-RAG), a method for optimizing the generator\nof RAG systems to align with RAG requirements comprehensively. Specifically, we\nconstruct high-quality instruction fine-tuning data and multi-perspective\npreference data by sampling varied quality responses from the generator across\ndifferent prompt documents quality scenarios. Subsequently, we optimize the\ngenerator using SFT and Direct Preference Optimization (DPO). Extensive\nexperiments conducted on four question-answer datasets across three LLMs\ndemonstrate that PA-RAG can significantly enhance the performance of RAG\ngenerators. Our code and datasets are available at\nhttps://github.com/wujwyi/PA-RAG.\n","authors":["Jiayi Wu","Hengyi Cai","Lingyong Yan","Hao Sun","Xiang Li","Shuaiqiang Wang","Dawei Yin","Ming Gao"],"pdf_url":"https://arxiv.org/pdf/2412.14510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14501v1","updated":"2024-12-19T03:48:40Z","published":"2024-12-19T03:48:40Z","title":"Do Large Language Models Defend Inferentialist Semantics?: On the\n  Logical Expressivism and Anti-Representationalism of LLMs","summary":"  The philosophy of language, which has historically been developed through an\nanthropocentric lens, is now being forced to move towards post-anthropocentrism\ndue to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude\n(Anthropic), which are considered to possess linguistic abilities comparable to\nthose of humans. Traditionally, LLMs have been explained through distributional\nsemantics as their foundational semantics. However, recent research is\nexploring alternative foundational semantics beyond distributional semantics.\nThis paper proposes Robert Brandom's inferentialist semantics as an suitable\nfoundational semantics for LLMs, specifically focusing on the issue of\nlinguistic representationalism within this post-anthropocentric trend. Here, we\nshow that the anti-representationalism and logical expressivism of inferential\nsemantics, as well as quasi-compositionality, are useful in interpreting the\ncharacteristics and behaviors of LLMs. Further, we propose a \\emph{consensus\ntheory of truths} for LLMs. This paper argues that the characteristics of LLMs\nchallenge mainstream assumptions in philosophy of language, such as semantic\nexternalism and compositionality. We believe the argument in this paper leads\nto a re-evaluation of anti\\hyphen{}representationalist views of language,\npotentially leading to new developments in the philosophy of language.\n","authors":["Yuzuki Arai","Sho Tsugawa"],"pdf_url":"https://arxiv.org/pdf/2412.14501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14480v1","updated":"2024-12-19T03:04:34Z","published":"2024-12-19T03:04:34Z","title":"GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question\n  Answering","summary":"  In Embodied Question Answering (EQA), agents must explore and develop a\nsemantic understanding of an unseen environment in order to answer a situated\nquestion with confidence. This remains a challenging problem in robotics, due\nto the difficulties in obtaining useful semantic representations, updating\nthese representations online, and leveraging prior world knowledge for\nefficient exploration and planning. Aiming to address these limitations, we\npropose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic\nscene graphs (3DSGs) and task relevant images as multi-modal memory for\ngrounding Vision-Language Models (VLMs) to perform EQA tasks in unseen\nenvironments. We employ a hierarchical planning approach that exploits the\nhierarchical nature of 3DSGs for structured planning and semantic-guided\nexploration. Through experiments in simulation on the HM-EQA dataset and in the\nreal world in home and office environments, we demonstrate that our method\noutperforms key baselines by completing EQA tasks with higher success rates and\nfewer planning steps.\n","authors":["Saumya Saxena","Blake Buchanan","Chris Paxton","Bingqing Chen","Narunas Vaskevicius","Luigi Palmieri","Jonathan Francis","Oliver Kroemer"],"pdf_url":"https://arxiv.org/pdf/2412.14480v1.pdf","comment":"Project website: https://saumyasaxena.github.io/grapheqa"},{"id":"http://arxiv.org/abs/2310.13008v2","updated":"2024-12-19T02:54:42Z","published":"2023-10-16T07:26:24Z","title":"DavIR: Data Selection via Implicit Reward for Large Language Models","summary":"  We introduce DavIR, a model-based data selection method for post-training\nLarge Language Models. DavIR generalizes Reducible Holdout Loss to core-set\nselection problem of causal language modeling, and quantifies the learnability\nof a given datum with respect to a pre-trained LLM based on relative reduction\nin loss during fine-tuning, a metric we show to be closely related to the\nimplicit reward model described in Direct Preference Optimization (DPO). We\nshow that 6% of Alpaca dataset selected with DavIR can steer both the LLaMA and\nGemma model family to produce superior performance compared to the same models\ntrained on the full 52K dataset. We also show that Alpaca dataset compressed\nwith DavIR can be combined with GSM8K dataset to effectively balance\nopen-domain freeform QA and mathematical reasoning capabilities. Finally, we\napply the DavIR objective to DPO and develop a normalized DavIR-DPO objective\nwhich improves alignment performance of Zephyr-7B-SFT model by 8% (relative) on\nAlpacaEval, compared against training on vanilla DPO objective.\n","authors":["Haotian Zhou","Tingkai Liu","Qianli Ma","Yufeng Zhang","Jianbo Yuan","Pengfei Liu","Yang You","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.13008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14475v1","updated":"2024-12-19T02:49:55Z","published":"2024-12-19T02:49:55Z","title":"MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval","summary":"  Despite the rapidly growing demand for multimodal retrieval, progress in this\nfield remains severely constrained by a lack of training data. In this paper,\nwe introduce MegaPairs, a novel data synthesis method that leverages vision\nlanguage models (VLMs) and open-domain images, together with a massive\nsynthetic dataset generated from this method. Our empirical analysis shows that\nMegaPairs generates high-quality data, enabling the multimodal retriever to\nsignificantly outperform the baseline model trained on 70$\\times$ more data\nfrom existing datasets. Moreover, since MegaPairs solely relies on general\nimage corpora and open-source VLMs, it can be easily scaled up, enabling\ncontinuous improvements in retrieval performance. In this stage, we produced\nmore than 26 million training instances and trained several models of varying\nsizes using this data. These new models achieve state-of-the-art zero-shot\nperformance across 4 popular composed image retrieval (CIR) benchmarks and the\nhighest overall performance on the 36 datasets provided by MMEB. They also\ndemonstrate notable performance improvements with additional downstream\nfine-tuning. Our produced dataset, well-trained models, and data synthesis\npipeline will be made publicly available to facilitate the future development\nof this field.\n","authors":["Junjie Zhou","Zheng Liu","Ze Liu","Shitao Xiao","Yueze Wang","Bo Zhao","Chen Jason Zhang","Defu Lian","Yongping Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.14475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14471v1","updated":"2024-12-19T02:39:26Z","published":"2024-12-19T02:39:26Z","title":"Why We Build Local Large Language Models: An Observational Analysis from\n  35 Japanese and Multilingual LLMs","summary":"  Why do we build local large language models (LLMs)? What should a local LLM\nlearn from the target language? Which abilities can be transferred from other\nlanguages? Do language-specific scaling laws exist? To explore these research\nquestions, we evaluated 35 Japanese, English, and multilingual LLMs on 19\nevaluation benchmarks for Japanese and English, taking Japanese as a local\nlanguage. Adopting an observational approach, we analyzed correlations of\nbenchmark scores, and conducted principal component analysis (PCA) on the\nscores to derive \\textit{ability factors} of local LLMs. We found that training\non English text can improve the scores of academic subjects in Japanese\n(JMMLU). In addition, it is unnecessary to specifically train on Japanese text\nto enhance abilities for solving Japanese code generation, arithmetic\nreasoning, commonsense, and reading comprehension tasks. In contrast, training\non Japanese text could improve question-answering tasks about Japanese\nknowledge and English-Japanese translation, which indicates that abilities for\nsolving these two tasks can be regarded as \\textit{Japanese abilities} for\nLLMs. Furthermore, we confirmed that the Japanese abilities scale with the\ncomputational budget for Japanese text.\n","authors":["Koshiro Saito","Sakae Mizuki","Masanari Ohi","Taishi Nakamura","Taihei Shiotani","Koki Maeda","Youmi Ma","Kakeru Hattori","Kazuki Fujii","Takumi Okamoto","Shigeki Ishida","Hiroya Takamura","Rio Yokota","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2412.14471v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2407.21792v2","updated":"2024-12-19T02:39:01Z","published":"2024-07-31T17:59:24Z","title":"Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?","summary":"  As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with both upstream model capabilities and training\ncompute, potentially enabling \"safetywashing\" -- where capability improvements\nare misrepresented as safety advancements. Based on these findings, we propose\nan empirical foundation for developing more meaningful safety metrics and\ndefine AI safety in a machine learning research context as a set of clearly\ndelineated research goals that are empirically separable from generic\ncapabilities advancements. In doing so, we aim to provide a more rigorous\nframework for AI safety research, advancing the science of safety evaluations\nand clarifying the path towards measurable progress.\n","authors":["Richard Ren","Steven Basart","Adam Khoja","Alice Gatti","Long Phan","Xuwang Yin","Mantas Mazeika","Alexander Pan","Gabriel Mukobi","Ryan H. Kim","Stephen Fitz","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2407.21792v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.18118v3","updated":"2024-12-19T02:37:00Z","published":"2024-06-26T07:15:44Z","title":"SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance","summary":"  As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality.\n","authors":["Caishuang Huang","Wanxu Zhao","Rui Zheng","Huijie Lv","Shihan Dou","Sixian Li","Xiao Wang","Enyu Zhou","Junjie Ye","Yuming Yang","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.18118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14205v3","updated":"2024-12-19T02:35:48Z","published":"2024-05-23T06:03:19Z","title":"Agent Planning with World Knowledge Model","summary":"  Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.\n","authors":["Shuofei Qiao","Runnan Fang","Ningyu Zhang","Yuqi Zhu","Xiang Chen","Shumin Deng","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14205v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14470v1","updated":"2024-12-19T02:35:15Z","published":"2024-12-19T02:35:15Z","title":"Agent-SafetyBench: Evaluating the Safety of LLM Agents","summary":"  As large language models (LLMs) are increasingly deployed as agents, their\nintegration into interactive environments and tool use introduce new safety\nchallenges beyond those associated with the models themselves. However, the\nabsence of comprehensive benchmarks for evaluating agent safety presents a\nsignificant barrier to effective assessment and further improvement. In this\npaper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to\nevaluate the safety of LLM agents. Agent-SafetyBench encompasses 349\ninteraction environments and 2,000 test cases, evaluating 8 categories of\nsafety risks and covering 10 common failure modes frequently encountered in\nunsafe interactions. Our evaluation of 16 popular LLM agents reveals a\nconcerning result: none of the agents achieves a safety score above 60%. This\nhighlights significant safety challenges in LLM agents and underscores the\nconsiderable need for improvement. Through quantitative analysis, we identify\ncritical failure modes and summarize two fundamental safety detects in current\nLLM agents: lack of robustness and lack of risk awareness. Furthermore, our\nfindings suggest that reliance on defense prompts alone is insufficient to\naddress these safety issues, emphasizing the need for more advanced and robust\nstrategies. We release Agent-SafetyBench at\n\\url{https://github.com/thu-coai/Agent-SafetyBench} to facilitate further\nresearch and innovation in agent safety evaluation and improvement.\n","authors":["Zhexin Zhang","Shiyao Cui","Yida Lu","Jingzhuo Zhou","Junxiao Yang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.14470v1.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.14461v1","updated":"2024-12-19T02:21:41Z","published":"2024-12-19T02:21:41Z","title":"From Human Annotation to LLMs: SILICON Annotation Workflow for\n  Management Research","summary":"  Unstructured text data annotation and analysis are fundamental to management\nresearch, often relying on human annotators through crowdsourcing platforms.\nWhile Large Language Models (LLMs) promise to provide a cost-effective and\nefficient alternative to human annotation, there lacks a systematic workflow\nthat evaluate when LLMs are suitable or how to proceed with LLM-based text\nannotation in a reproducible manner. This paper addresses this methodological\ngap by introducing the ``SILICON\" (\\textbf{S}ystematic \\textbf{I}nference with\n\\textbf{L}LMs for \\textbf{I}nformation \\textbf{C}lassificati\\textbf{o}n and\n\\textbf{N}otation) workflow. The workflow integrates established principles of\nhuman annotation with systematic prompt optimization and model selection,\naddressing challenges such as developing robust annotation guidelines,\nestablishing high-quality human baselines, optimizing prompts, and ensuring\nreproducibility across LLMs. We validate the SILICON workflow through seven\ncase studies covering common management research tasks, including business\nproposal evaluation, dialog intent and breakdown analysis, review attribute\ndetection. Our findings highlight the importance of validating annotation\nguideline agreement, the superiority of expert-developed human baselines over\ncrowdsourced ones, the iterative nature of prompt optimization, and the\nnecessity of testing multiple LLMs. Notably, we propose a regression-based\nmethodology to empirically compare LLM outputs across prompts and models. Our\nworkflow advances management research by establishing reproducible processes\nfor LLM-based annotation that maintain scientific rigor. We provide practical\nguidance for researchers to effectively navigate the evolving landscape of\ngenerative AI tools effectively while maintaining transparency and\nreproducibility.\n","authors":["Xiang Cheng","Raveesh Mayya","João Sedoc"],"pdf_url":"https://arxiv.org/pdf/2412.14461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14768v3","updated":"2024-12-19T02:18:54Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.13144v3","updated":"2024-12-19T02:14:09Z","published":"2024-06-19T01:37:10Z","title":"DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party\n  Dialogue Understanding of Conversational Agents","summary":"  Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversational agents, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nagents often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, an agent is assigned the role of a character from\npopular TV shows, requiring it to respond to spontaneous questions using past\ndialogue information and to distinguish between known and unknown information.\nKey features of DialSim include assessing the agent's ability to respond within\na reasonable time limit, handling long-term multi-party dialogues, and\nevaluating performance under randomized questioning with LongDialQA, a novel,\nhigh-quality question-answering dataset. Our experiments using DialSim reveal\nthe strengths and weaknesses of the latest conversational agents, offering\nvaluable insights for future advancements in conversational AI. DialSim is\navailable at https://dialsim.github.io/.\n","authors":["Jiho Kim","Woosog Chay","Hyeonji Hwang","Daeun Kyung","Hyunseung Chung","Eunbyeol Cho","Yohan Jo","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2406.13144v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v3","updated":"2024-12-19T02:10:00Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v3.pdf","comment":"NeurIPS 2024, 26 pages"},{"id":"http://arxiv.org/abs/2412.14454v1","updated":"2024-12-19T02:09:59Z","published":"2024-12-19T02:09:59Z","title":"Are Longer Prompts Always Better? Prompt Selection in Large Language\n  Models for Recommendation Systems","summary":"  In large language models (LLM)-based recommendation systems (LLM-RSs),\naccurately predicting user preferences by leveraging the general knowledge of\nLLMs is possible without requiring extensive training data. By converting\nrecommendation tasks into natural language inputs called prompts, LLM-RSs can\nefficiently solve issues that have been difficult to address due to data\nscarcity but are crucial in applications such as cold-start and cross-domain\nproblems. However, when applying this in practice, selecting the prompt that\nmatches tasks and data is essential. Although numerous prompts have been\nproposed in LLM-RSs and representing the target user in prompts significantly\nimpacts recommendation accuracy, there are still no clear guidelines for\nselecting specific prompts.\n  In this paper, we categorize and analyze prompts from previous research to\nestablish practical prompt selection guidelines. Through 450 experiments with\n90 prompts and five real-world datasets, we examined the relationship between\nprompts and dataset characteristics in recommendation accuracy. We found that\nno single prompt consistently outperforms others; thus, selecting prompts on\nthe basis of dataset characteristics is crucial. Here, we propose a prompt\nselection method that achieves higher accuracy with minimal validation data.\nBecause increasing the number of prompts to explore raises costs, we also\nintroduce a cost-efficient strategy using high-performance and cost-efficient\nLLMs, significantly reducing exploration costs while maintaining high\nprediction accuracy. Our work offers valuable insights into the prompt\nselection, advancing accurate and efficient LLM-RSs.\n","authors":["Genki Kusano","Kosuke Akimoto","Kunihiro Takeoka"],"pdf_url":"https://arxiv.org/pdf/2412.14454v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2412.14436v1","updated":"2024-12-19T01:35:47Z","published":"2024-12-19T01:35:47Z","title":"ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain\n  Adaptation with an Astronomy Case Study","summary":"  Recent advances in language modeling demonstrate the need for high-quality\ndomain-specific training data, especially for tasks that require specialized\nknowledge. General-purpose models, while versatile, often lack the depth needed\nfor expert-level tasks because of limited domain-specific information. Domain\nadaptation training can enhance these models, but it demands substantial,\nhigh-quality data. To address this, we propose ORBIT, a cost-efficient\nmethodology for curating massive, high-quality domain-specific datasets from\nnoisy web sources, tailored for training specialist large language models.\nUsing astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu\ndataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning\n\\textsc{LLaMA-3-8B} on a 1B-token astronomy subset improved performance on the\nMMLU astronomy benchmark from 69\\% to 76\\% and achieved top results on\nAstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA)\noutperformed \\textsc{LLaMA-3-8B-base}, with GPT-4o evaluations preferring it in\n73\\% of cases across 1000 astronomy-specific questions. Additionally, we\nvalidated ORBIT's generalizability by applying it to law and medicine,\nachieving a significant improvement of data quality compared to an unfiltered\nbaseline. We open-source the ORBIT methodology, including the curated datasets,\nthe codebase, and the resulting model at\n\\href{https://github.com/ModeEric/ORBIT-Llama}{https://github.com/ModeEric/ORBIT-Llama}.\n","authors":["Eric Modesitt","Ke Yang","Spencer Hulsey","Chengxiang Zhai","Volodymyr Kindratenko"],"pdf_url":"https://arxiv.org/pdf/2412.14436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14426v1","updated":"2024-12-19T00:41:40Z","published":"2024-12-19T00:41:40Z","title":"All-in-One Tuning and Structural Pruning for Domain-Specific LLMs","summary":"  Existing pruning techniques for large language models (LLMs) targeting\ndomain-specific applications typically follow a two-stage process: pruning the\npretrained general-purpose LLMs and then fine-tuning the pruned LLMs on\nspecific domains. However, the pruning decisions, derived from the pretrained\nweights, remain unchanged during fine-tuning, even if the weights have been\nupdated. Therefore, such a combination of the pruning decisions and the\nfinetuned weights may be suboptimal, leading to non-negligible performance\ndegradation. To address these limitations, we propose ATP: All-in-One Tuning\nand Structural Pruning, a unified one-stage structural pruning and fine-tuning\napproach that dynamically identifies the current optimal substructure\nthroughout the fine-tuning phase via a trainable pruning decision generator.\nMoreover, given the limited available data for domain-specific applications,\nLow-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In\nATP, we introduce LoRA-aware forward and sparsity regularization to ensure that\nthe substructures corresponding to the learned pruning decisions can be\ndirectly removed after the ATP process. ATP outperforms the state-of-the-art\ntwo-stage pruning methods on tasks in the legal and healthcare domains. More\nspecifically, ATP recovers up to 88% and 91% performance of the dense model\nwhen pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.\n","authors":["Lei Lu","Zhepeng Wang","Ruexue Bao","Mengbing Wang","Fangyi Li","Yawen Wu","Weiwen Jiang","Jie Xu","Yanzhi Wang","Shangqian Gao"],"pdf_url":"https://arxiv.org/pdf/2412.14426v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.15216v1","updated":"2024-12-19T18:59:58Z","published":"2024-12-19T18:59:58Z","title":"UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit\n  Consistency","summary":"  We propose an unsupervised model for instruction-based image editing that\neliminates the need for ground-truth edited images during training. Existing\nsupervised methods depend on datasets containing triplets of input image,\nedited image, and edit instruction. These are generated by either existing\nediting methods or human-annotations, which introduce biases and limit their\ngeneralization ability. Our method addresses these challenges by introducing a\nnovel editing mechanism called Cycle Edit Consistency (CEC), which applies\nforward and backward edits in one training step and enforces consistency in\nimage and attention spaces. This allows us to bypass the need for ground-truth\nedited images and unlock training for the first time on datasets comprising\neither real image-caption pairs or image-caption-edit triplets. We empirically\nshow that our unsupervised technique performs better across a broader range of\nedits with high fidelity and precision. By eliminating the need for\npre-existing datasets of triplets, reducing biases associated with supervised\nmethods, and proposing CEC, our work represents a significant advancement in\nunblocking scaling of instruction-based image editing.\n","authors":["Enis Simsar","Alessio Tonioni","Yongqin Xian","Thomas Hofmann","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2412.15216v1.pdf","comment":"Project page: https://enis.dev/uip2p/"},{"id":"http://arxiv.org/abs/2412.15215v1","updated":"2024-12-19T18:59:57Z","published":"2024-12-19T18:59:57Z","title":"EnvGS: Modeling View-Dependent Appearance with Environment Gaussian","summary":"  Reconstructing complex reflections in real-world scenes from 2D images is\nessential for achieving photorealistic novel view synthesis. Existing methods\nthat utilize environment maps to model reflections from distant lighting often\nstruggle with high-frequency reflection details and fail to account for\nnear-field reflections. In this work, we introduce EnvGS, a novel approach that\nemploys a set of Gaussian primitives as an explicit 3D representation for\ncapturing reflections of environments. These environment Gaussian primitives\nare incorporated with base Gaussian primitives to model the appearance of the\nwhole scene. To efficiently render these environment Gaussian primitives, we\ndeveloped a ray-tracing-based renderer that leverages the GPU's RT core for\nfast rendering. This allows us to jointly optimize our model for high-quality\nreconstruction while maintaining real-time rendering speeds. Results from\nmultiple real-world and synthetic datasets demonstrate that our method produces\nsignificantly more detailed reflections, achieving the best rendering quality\nin real-time novel view synthesis.\n","authors":["Tao Xie","Xi Chen","Zhen Xu","Yiman Xie","Yudong Jin","Yujun Shen","Sida Peng","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15215v1.pdf","comment":"Project page: https://zju3dv.github.io/envgs/"},{"id":"http://arxiv.org/abs/2412.15213v1","updated":"2024-12-19T18:59:56Z","published":"2024-12-19T18:59:56Z","title":"Flowing from Words to Pixels: A Framework for Cross-Modality Evolution","summary":"  Diffusion models, and their generalization, flow matching, have had a\nremarkable impact on the field of media generation. Here, the conventional\napproach is to learn the complex mapping from a simple source distribution of\nGaussian noise to the target media distribution. For cross-modal tasks such as\ntext-to-image generation, this same mapping from noise to image is learnt\nwhilst including a conditioning mechanism in the model. One key and thus far\nrelatively unexplored feature of flow matching is that, unlike Diffusion\nmodels, they are not constrained for the source distribution to be noise.\nHence, in this paper, we propose a paradigm shift, and ask the question of\nwhether we can instead train flow matching models to learn a direct mapping\nfrom the distribution of one modality to the distribution of another, thus\nobviating the need for both the noise distribution and conditioning mechanism.\nWe present a general and simple framework, CrossFlow, for cross-modal flow\nmatching. We show the importance of applying Variational Encoders to the input\ndata, and introduce a method to enable Classifier-free guidance. Surprisingly,\nfor text-to-image, CrossFlow with a vanilla transformer without cross attention\nslightly outperforms standard flow matching, and we show that it scales better\nwith training steps and model size, while also allowing for interesting latent\narithmetic which results in semantically meaningful edits in the output space.\nTo demonstrate the generalizability of our approach, we also show that\nCrossFlow is on par with or outperforms the state-of-the-art for various\ncross-modal / intra-modal mapping tasks, viz. image captioning, depth\nestimation, and image super-resolution. We hope this paper contributes to\naccelerating progress in cross-modal media generation.\n","authors":["Qihao Liu","Xi Yin","Alan Yuille","Andrew Brown","Mannat Singh"],"pdf_url":"https://arxiv.org/pdf/2412.15213v1.pdf","comment":"Project page: https://cross-flow.github.io/"},{"id":"http://arxiv.org/abs/2412.15214v1","updated":"2024-12-19T18:59:56Z","published":"2024-12-19T18:59:56Z","title":"LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis","summary":"  The intuitive nature of drag-based interaction has led to its growing\nadoption for controlling object trajectories in image-to-video synthesis.\nStill, existing methods that perform dragging in the 2D space usually face\nambiguity when handling out-of-plane movements. In this work, we augment the\ninteraction with a new dimension, i.e., the depth dimension, such that users\nare allowed to assign a relative depth for each point on the trajectory. That\nway, our new interaction paradigm not only inherits the convenience from 2D\ndragging, but facilitates trajectory control in the 3D space, broadening the\nscope of creativity. We propose a pioneering method for 3D trajectory control\nin image-to-video synthesis by abstracting object masks into a few cluster\npoints. These points, accompanied by the depth information and the instance\ninformation, are finally fed into a video diffusion model as the control\nsignal. Extensive experiments validate the effectiveness of our approach,\ndubbed LeviTor, in precisely manipulating the object movements when producing\nphoto-realistic videos from static images. Project page:\nhttps://ppetrichor.github.io/levitor.github.io/\n","authors":["Hanlin Wang","Hao Ouyang","Qiuyu Wang","Wen Wang","Ka Leong Cheng","Qifeng Chen","Yujun Shen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.15214v1.pdf","comment":"Project page available at\n  https://ppetrichor.github.io/levitor.github.io/"},{"id":"http://arxiv.org/abs/2412.15211v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Generative Multiview Relighting for 3D Reconstruction under Extreme\n  Illumination Variation","summary":"  Reconstructing the geometry and appearance of objects from photographs taken\nin different environments is difficult as the illumination and therefore the\nobject appearance vary across captured images. This is particularly challenging\nfor more specular objects whose appearance strongly depends on the viewing\ndirection. Some prior approaches model appearance variation across images using\na per-image embedding vector, while others use physically-based rendering to\nrecover the materials and per-image illumination. Such approaches fail at\nfaithfully recovering view-dependent appearance given significant variation in\ninput illumination and tend to produce mostly diffuse results. We present an\napproach that reconstructs objects from images taken under different\nilluminations by first relighting the images under a single reference\nillumination with a multiview relighting diffusion model and then\nreconstructing the object's geometry and appearance with a radiance field\narchitecture that is robust to the small remaining inconsistencies among the\nrelit images. We validate our proposed approach on both synthetic and real\ndatasets and demonstrate that it greatly outperforms existing techniques at\nreconstructing high-fidelity appearance from images taken under extreme\nillumination variation. Moreover, our approach is particularly effective at\nrecovering view-dependent \"shiny\" appearance which cannot be reconstructed by\nprior methods.\n","authors":["Hadi Alzayer","Philipp Henzler","Jonathan T. Barron","Jia-Bin Huang","Pratul P. Srinivasan","Dor Verbin"],"pdf_url":"https://arxiv.org/pdf/2412.15211v1.pdf","comment":"Project page: https://relight-to-reconstruct.github.io/"},{"id":"http://arxiv.org/abs/2412.15212v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Scaling 4D Representations","summary":"  Scaling has not yet been convincingly demonstrated for pure self-supervised\nlearning from video. However, prior work has focused evaluations on\nsemantic-related tasks $\\unicode{x2013}$ action classification, ImageNet\nclassification, etc. In this paper we focus on evaluating self-supervised\nlearning on non-semantic vision tasks that are more spatial (3D) and temporal\n(+1D = 4D), such as camera pose estimation, point and object tracking, and\ndepth estimation. We show that by learning from very large video datasets,\nmasked auto-encoding (MAE) with transformer video models actually scales,\nconsistently improving performance on these 4D tasks, as model size increases\nfrom 20M all the way to the largest by far reported self-supervised video model\n$\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with\nmany recent image and video models demonstrates the benefits of scaling 4D\nrepresentations.\n","authors":["João Carreira","Dilara Gokay","Michael King","Chuhan Zhang","Ignacio Rocco","Aravindh Mahendran","Thomas Albert Keck","Joseph Heyward","Skanda Koppula","Etienne Pot","Goker Erdogan","Yana Hasson","Yi Yang","Klaus Greff","Guillaume Le Moing","Sjoerd van Steenkiste","Daniel Zoran","Drew A. Hudson","Pedro Vélez","Luisa Polanía","Luke Friedman","Chris Duvarney","Ross Goroshin","Kelsey Allen","Jacob Walker","Rishabh Kabra","Eric Aboussouan","Jennifer Sun","Thomas Kipf","Carl Doersch","Viorica Pătrăucean","Dima Damen","Pauline Luc","Mehdi S. M. Sajjadi","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2412.15212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15209v1","updated":"2024-12-19T18:59:44Z","published":"2024-12-19T18:59:44Z","title":"PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation","summary":"  Despite significant advancements in Large Vision-Language Models (LVLMs),\nexisting pixel-grounding models operate on single-image settings, limiting\ntheir ability to perform detailed, fine-grained comparisons across multiple\nimages. Conversely, current multi-image understanding models lack pixel-level\ngrounding. Our work addresses this gap by introducing the task of multi-image\npixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates\npixel-level grounding with robust multi-image reasoning capabilities to produce\ncontextually rich, pixel-grounded explanations. Central to PRIMA is an\nefficient vision module that queries fine-grained visual representations across\nmultiple images, reducing TFLOPs by $25.3\\%$. To support training and\nevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark\nconsisting of $\\sim$224K question-answer pairs that require fine-grained visual\nunderstanding across multiple images. Experimental results demonstrate PRIMA\noutperforms state-of-the-art baselines.\n","authors":["Muntasir Wahed","Kiet A. Nguyen","Adheesh Sunil Juvekar","Xinzhuo Li","Xiaona Zhou","Vedant Shah","Tianjiao Yu","Pinar Yanardag","Ismini Lourentzou"],"pdf_url":"https://arxiv.org/pdf/2412.15209v1.pdf","comment":"Project page: https://plan-lab.github.io/prima"},{"id":"http://arxiv.org/abs/2412.15208v1","updated":"2024-12-19T18:59:40Z","published":"2024-12-19T18:59:40Z","title":"OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving","summary":"  Since the advent of Multimodal Large Language Models (MLLMs), they have made\na significant impact across a wide range of real-world applications,\nparticularly in Autonomous Driving (AD). Their ability to process complex\nvisual data and reason about intricate driving scenarios has paved the way for\na new paradigm in end-to-end AD systems. However, the progress of developing\nend-to-end models for AD has been slow, as existing fine-tuning methods demand\nsubstantial resources, including extensive computational power, large-scale\ndatasets, and significant funding. Drawing inspiration from recent advancements\nin inference computing, we propose OpenEMMA, an open-source end-to-end\nframework based on MLLMs. By incorporating the Chain-of-Thought reasoning\nprocess, OpenEMMA achieves significant improvements compared to the baseline\nwhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates\neffectiveness, generalizability, and robustness across a variety of challenging\ndriving scenarios, offering a more efficient and effective approach to\nautonomous driving. We release all the codes in\nhttps://github.com/taco-group/OpenEMMA.\n","authors":["Shuo Xing","Chengyuan Qian","Yuping Wang","Hongyuan Hua","Kexin Tian","Yang Zhou","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15206v1","updated":"2024-12-19T18:59:33Z","published":"2024-12-19T18:59:33Z","title":"AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models\n  for Autonomous Driving","summary":"  Recent advancements in large vision language models (VLMs) tailored for\nautonomous driving (AD) have shown strong scene understanding and reasoning\ncapabilities, making them undeniable candidates for end-to-end driving systems.\nHowever, limited work exists on studying the trustworthiness of DriveVLMs -- a\ncritical factor that directly impacts public transportation safety. In this\npaper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for\nlarge vision-language models in autonomous driving (DriveVLMs), considering\ndiverse perspectives -- including trustfulness, safety, robustness, privacy,\nand fairness. We constructed the largest visual question-answering dataset for\ninvestigating trustworthiness issues in driving scenarios, comprising over 10k\nunique scenes and 18k queries. We evaluated six publicly available VLMs,\nspanning from generalist to specialist, from open-source to commercial models.\nOur exhaustive evaluations have unveiled previously undiscovered\nvulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found\nthat the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform\nspecialized models fine-tuned for driving in terms of overall trustworthiness.\nDriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing\nsensitive information. Additionally, both generalist and specialist VLMs remain\nsusceptible to adversarial attacks and struggle to ensure unbiased\ndecision-making across diverse environments and populations. Our findings call\nfor immediate and decisive action to address the trustworthiness of DriveVLMs\n-- an issue of critical importance to public safety and the welfare of all\ncitizens relying on autonomous transportation systems. Our benchmark is\npublicly available at \\url{https://github.com/taco-group/AutoTrust}, and the\nleaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.\n","authors":["Shuo Xing","Hongyuan Hua","Xiangbo Gao","Shenzhe Zhu","Renjie Li","Kexin Tian","Xiaopeng Li","Heng Huang","Tianbao Yang","Zhangyang Wang","Yang Zhou","Huaxiu Yao","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15206v1.pdf","comment":"55 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.15205v1","updated":"2024-12-19T18:59:31Z","published":"2024-12-19T18:59:31Z","title":"FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching","summary":"  Autoregressive (AR) modeling has achieved remarkable success in natural\nlanguage processing by enabling models to generate text with coherence and\ncontextual understanding through next token prediction. Recently, in image\ngeneration, VAR proposes scale-wise autoregressive modeling, which extends the\nnext token prediction to the next scale prediction, preserving the 2D structure\nof images. However, VAR encounters two primary challenges: (1) its complex and\nrigid scale design limits generalization in next scale prediction, and (2) the\ngenerator's dependence on a discrete tokenizer with the same complex scale\nstructure restricts modularity and flexibility in updating the tokenizer. To\naddress these limitations, we introduce FlowAR, a general next scale prediction\nmethod featuring a streamlined scale design, where each subsequent scale is\nsimply double the previous one. This eliminates the need for VAR's intricate\nmulti-scale residual tokenizer and enables the use of any off-the-shelf\nVariational AutoEncoder (VAE). Our simplified design enhances generalization in\nnext scale prediction and facilitates the integration of Flow Matching for\nhigh-quality image synthesis. We validate the effectiveness of FlowAR on the\nchallenging ImageNet-256 benchmark, demonstrating superior generation\nperformance compared to previous methods. Codes will be available at\n\\url{https://github.com/OliverRensu/FlowAR}.\n","authors":["Sucheng Ren","Qihang Yu","Ju He","Xiaohui Shen","Alan Yuille","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15200v1","updated":"2024-12-19T18:58:46Z","published":"2024-12-19T18:58:46Z","title":"DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation\n  for High-quality 3D Asset Creation","summary":"  Procedural Content Generation (PCG) is powerful in creating high-quality 3D\ncontents, yet controlling it to produce desired shapes is difficult and often\nrequires extensive parameter tuning. Inverse Procedural Content Generation aims\nto automatically find the best parameters under the input condition. However,\nexisting sampling-based and neural network-based methods still suffer from\nnumerous sample iterations or limited controllability. In this work, we present\nDI-PCG, a novel and efficient method for Inverse PCG from general image\nconditions. At its core is a lightweight diffusion transformer model, where PCG\nparameters are directly treated as the denoising target and the observed images\nas conditions to control parameter generation. DI-PCG is efficient and\neffective. With only 7.6M network parameters and 30 GPU hours to train, it\ndemonstrates superior performance in recovering parameters accurately, and\ngeneralizing well to in-the-wild images. Quantitative and qualitative\nexperiment results validate the effectiveness of DI-PCG in inverse PCG and\nimage-to-3D generation tasks. DI-PCG offers a promising approach for efficient\ninverse PCG and represents a valuable exploration step towards a 3D generation\npath that models how to construct a 3D asset using parametric models.\n","authors":["Wang Zhao","Yan-Pei Cao","Jiale Xu","Yuejiang Dong","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.15200v1.pdf","comment":"Project page: https://thuzhaowang.github.io/projects/DI-PCG/"},{"id":"http://arxiv.org/abs/2412.15199v1","updated":"2024-12-19T18:58:36Z","published":"2024-12-19T18:58:36Z","title":"LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation","summary":"  This paper targets the challenge of real-time LiDAR re-simulation in dynamic\ndriving scenarios. Recent approaches utilize neural radiance fields combined\nwith the physical modeling of LiDAR sensors to achieve high-fidelity\nre-simulation results. Unfortunately, these methods face limitations due to\nhigh computational demands in large-scale scenes and cannot perform real-time\nLiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel\nframework that supports real-time, physically accurate LiDAR re-simulation for\ndriving scenes. Our primary contribution is the development of an efficient and\neffective rendering pipeline, which integrates Gaussian primitives and\nhardware-accelerated ray tracing technology. Specifically, we model the\nphysical properties of LiDAR sensors using Gaussian primitives with learnable\nparameters and incorporate scene graphs to handle scene dynamics. Building upon\nthis scene representation, our framework first constructs a bounding volume\nhierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views\nthrough a differentiable rendering algorithm. Importantly, our framework\nsupports realistic rendering with flexible scene editing operations and various\nsensor configurations. Extensive experiments across multiple public benchmarks\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nrendering quality and efficiency. Our project page is at\nhttps://zju3dv.github.io/lidar-rt.\n","authors":["Chenxu Zhou","Lvchang Fu","Sida Peng","Yunzhi Yan","Zhanhua Zhang","Yong Chen","Jiazhi Xia","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15199v1.pdf","comment":"Project page: https://zju3dv.github.io/lidar-rt"},{"id":"http://arxiv.org/abs/2412.15195v1","updated":"2024-12-19T18:58:14Z","published":"2024-12-19T18:58:14Z","title":"Preventing Local Pitfalls in Vector Quantization via Optimal Transport","summary":"  Vector-quantized networks (VQNs) have exhibited remarkable performance across\nvarious tasks, yet they are prone to training instability, which complicates\nthe training process due to the necessity for techniques such as subtle\ninitialization and model distillation. In this study, we identify the local\nminima issue as the primary cause of this instability. To address this, we\nintegrate an optimal transport method in place of the nearest neighbor search\nto achieve a more globally informed assignment. We introduce OptVQ, a novel\nvector quantization method that employs the Sinkhorn algorithm to optimize the\noptimal transport problem, thereby enhancing the stability and efficiency of\nthe training process. To mitigate the influence of diverse data distributions\non the Sinkhorn algorithm, we implement a straightforward yet effective\nnormalization strategy. Our comprehensive experiments on image reconstruction\ntasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses\ncurrent state-of-the-art VQNs in reconstruction quality.\n","authors":["Borui Zhang","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.15195v1.pdf","comment":"Code is available at https://github.com/zbr17/OptVQ"},{"id":"http://arxiv.org/abs/2412.15191v1","updated":"2024-12-19T18:57:21Z","published":"2024-12-19T18:57:21Z","title":"AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation","summary":"  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video\ngeneration that leverages the activations of frozen video and audio diffusion\nmodels for temporally-aligned cross-modal conditioning. The key to our\nframework is a Fusion Block that enables bidirectional information exchange\nbetween our backbone video and audio diffusion models through a\ntemporally-aligned self attention operation. Unlike prior work that uses\nfeature extractors pretrained for other tasks for the conditioning signal,\nAV-Link can directly leverage features obtained by the complementary modality\nin a single framework i.e. video features to generate audio, or audio features\nto generate video. We extensively evaluate our design choices and demonstrate\nthe ability of our method to achieve synchronized and high-quality audiovisual\ncontent, showcasing its potential for applications in immersive media\ngeneration. Project Page: snap-research.github.io/AVLink/\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Ivan Skorokhodov","Alper Canberk","Kwot Sin Lee","Vicente Ordonez","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2412.15191v1.pdf","comment":"Project Page: snap-research.github.io/AVLink/"},{"id":"http://arxiv.org/abs/2412.15190v1","updated":"2024-12-19T18:57:13Z","published":"2024-12-19T18:57:13Z","title":"EarthDial: Turning Multi-sensory Earth Observations to Interactive\n  Dialogues","summary":"  Automated analysis of vast Earth observation data via interactive\nVision-Language Models (VLMs) can unlock new opportunities for environmental\nmonitoring, disaster response, and resource management. Existing generic VLMs\ndo not perform well on Remote Sensing data, while the recent Geo-spatial VLMs\nremain restricted to a fixed resolution and few sensor modalities. In this\npaper, we introduce EarthDial, a conversational assistant specifically designed\nfor Earth Observation (EO) data, transforming complex, multi-sensory Earth\nobservations into interactive, natural language dialogues. EarthDial supports\nmulti-spectral, multi-temporal, and multi-resolution imagery, enabling a wide\nrange of remote sensing tasks, including classification, detection, captioning,\nquestion answering, visual reasoning, and visual grounding. To achieve this, we\nintroduce an extensive instruction tuning dataset comprising over 11.11M\ninstruction pairs covering RGB, Synthetic Aperture Radar (SAR), and\nmultispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,\nEarthDial handles bi-temporal and multi-temporal sequence analysis for\napplications like change detection. Our extensive experimental results on 37\ndownstream applications demonstrate that EarthDial outperforms existing generic\nand domain-specific models, achieving better generalization across various EO\ntasks.\n","authors":["Sagar Soni","Akshay Dudhane","Hiyam Debary","Mustansar Fiaz","Muhammad Akhtar Munir","Muhammad Sohail Danish","Paolo Fraccaro","Campbell D Watson","Levente J Klein","Fahad Shahbaz Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2412.15190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15188v1","updated":"2024-12-19T18:56:24Z","published":"2024-12-19T18:56:24Z","title":"LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation","summary":"  We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.\n","authors":["Weijia Shi","Xiaochuang Han","Chunting Zhou","Weixin Liang","Xi Victoria Lin","Luke Zettlemoyer","Lili Yu"],"pdf_url":"https://arxiv.org/pdf/2412.15188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15185v1","updated":"2024-12-19T18:55:25Z","published":"2024-12-19T18:55:25Z","title":"Tiled Diffusion","summary":"  Image tiling -- the seamless connection of disparate images to create a\ncoherent visual field -- is crucial for applications such as texture creation,\nvideo game asset development, and digital art. Traditionally, tiles have been\nconstructed manually, a method that poses significant limitations in\nscalability and flexibility. Recent research has attempted to automate this\nprocess using generative models. However, current approaches primarily focus on\ntiling textures and manipulating models for single-image generation, without\ninherently supporting the creation of multiple interconnected tiles across\ndiverse domains. This paper presents Tiled Diffusion, a novel approach that\nextends the capabilities of diffusion models to accommodate the generation of\ncohesive tiling patterns across various domains of image synthesis that require\ntiling. Our method supports a wide range of tiling scenarios, from self-tiling\nto complex many-to-many connections, enabling seamless integration of multiple\nimages. Tiled Diffusion automates the tiling process, eliminating the need for\nmanual intervention and enhancing creative possibilities in various\napplications, such as seamlessly tiling of existing images, tiled texture\ncreation, and 360{\\deg} synthesis.\n","authors":["Or Madar","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2412.15185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07449v2","updated":"2024-12-19T18:51:28Z","published":"2024-11-12T00:20:11Z","title":"Tracing the Roots: Leveraging Temporal Dynamics in Diffusion\n  Trajectories for Origin Attribution","summary":"  Diffusion models have revolutionized image synthesis, garnering significant\nresearch interest in recent years. Diffusion is an iterative algorithm in which\nsamples are generated step-by-step, starting from pure noise. This process\nintroduces the notion of diffusion trajectories, i.e., paths from the standard\nGaussian distribution to the target image distribution. In this context, we\nstudy discriminative algorithms operating on these trajectories. Specifically,\ngiven a pre-trained diffusion model, we consider the problem of classifying\nimages as part of the training dataset, generated by the model or originating\nfrom an external source. Our approach demonstrates the presence of patterns\nacross steps that can be leveraged for classification. We also conduct ablation\nstudies, which reveal that using higher-order gradient features to characterize\nthe trajectories leads to significant performance gains and more robust\nalgorithms.\n","authors":["Andreas Floros","Seyed-Mohsen Moosavi-Dezfooli","Pier Luigi Dragotti"],"pdf_url":"https://arxiv.org/pdf/2411.07449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15171v1","updated":"2024-12-19T18:46:55Z","published":"2024-12-19T18:46:55Z","title":"SqueezeMe: Efficient Gaussian Avatars for VR","summary":"  Gaussian Splatting has enabled real-time 3D human avatars with unprecedented\nlevels of visual quality. While previous methods require a desktop GPU for\nreal-time inference of a single avatar, we aim to squeeze multiple Gaussian\navatars onto a portable virtual reality headset with real-time drivable\ninference. We begin by training a previous work, Animatable Gaussians, on a\nhigh quality dataset captured with 512 cameras. The Gaussians are animated by\ncontrolling base set of Gaussians with linear blend skinning (LBS) motion and\nthen further adjusting the Gaussians with a neural network decoder to correct\ntheir appearance. When deploying the model on a Meta Quest 3 VR headset, we\nfind two major computational bottlenecks: the decoder and the rendering. To\naccelerate the decoder, we train the Gaussians in UV-space instead of\npixel-space, and we distill the decoder to a single neural network layer.\nFurther, we discover that neighborhoods of Gaussians can share a single\ncorrective from the decoder, which provides an additional speedup. To\naccelerate the rendering, we develop a custom pipeline in Vulkan that runs on\nthe mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently\nat 72 FPS on a VR headset. Demo videos are at\nhttps://forresti.github.io/squeezeme.\n","authors":["Shunsuke Saito","Stanislav Pidhorskyi","Igor Santesteban","Forrest Iandola","Divam Gupta","Anuj Pahuja","Nemanja Bartolovic","Frank Yu","Emanuel Garbin","Tomas Simon"],"pdf_url":"https://arxiv.org/pdf/2412.15171v1.pdf","comment":"Initial version"},{"id":"http://arxiv.org/abs/2412.15159v1","updated":"2024-12-19T18:34:50Z","published":"2024-12-19T18:34:50Z","title":"OnlineVPO: Align Video Diffusion Model with Online Video-Centric\n  Preference Optimization","summary":"  In recent years, the field of text-to-video (T2V) generation has made\nsignificant strides. Despite this progress, there is still a gap between\ntheoretical advancements and practical application, amplified by issues like\ndegraded image quality and flickering artifacts. Recent advancements in\nenhancing the video diffusion model (VDM) through feedback learning have shown\npromising results. However, these methods still exhibit notable limitations,\nsuch as misaligned feedback and inferior scalability. To tackle these issues,\nwe introduce OnlineVPO, a more efficient preference learning approach tailored\nspecifically for video diffusion models. Our method features two novel designs,\nfirstly, instead of directly using image-based reward feedback, we leverage the\nvideo quality assessment (VQA) model trained on synthetic data as the reward\nmodel to provide distribution and modality-aligned feedback on the video\ndiffusion model. Additionally, we introduce an online DPO algorithm to address\nthe off-policy optimization and scalability issue in existing video preference\nlearning frameworks. By employing the video reward model to offer concise video\nfeedback on the fly, OnlineVPO offers effective and efficient preference\nguidance. Extensive experiments on the open-source video-diffusion model\ndemonstrate OnlineVPO as a simple yet effective and more importantly scalable\npreference learning algorithm for video diffusion models, offering valuable\ninsights for future advancements in this domain.\n","authors":["Jiacheng Zhang","Jie Wu","Weifeng Chen","Yatai Ji","Xuefeng Xiao","Weilin Huang","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.15159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15156v1","updated":"2024-12-19T18:32:21Z","published":"2024-12-19T18:32:21Z","title":"Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM","summary":"  Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.\n","authors":["Yatai Ji","Jiacheng Zhang","Jie Wu","Shilong Zhang","Shoufa Chen","Chongjian GE","Peize Sun","Weifeng Chen","Wenqi Shao","Xuefeng Xiao","Weilin Huang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2412.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15150v1","updated":"2024-12-19T18:28:37Z","published":"2024-12-19T18:28:37Z","title":"Leveraging Color Channel Independence for Improved Unsupervised Object\n  Detection","summary":"  Object-centric architectures can learn to extract distinct object\nrepresentations from visual scenes, enabling downstream applications on the\nobject level. Similarly to autoencoder-based image models, object-centric\napproaches have been trained on the unsupervised reconstruction loss of images\nencoded by RGB color spaces. In our work, we challenge the common assumption\nthat RGB images are the optimal color space for unsupervised learning in\ncomputer vision. We discuss conceptually and empirically that other color\nspaces, such as HSV, bear essential characteristics for object-centric\nrepresentation learning, like robustness to lighting conditions. We further\nshow that models improve when requiring them to predict additional color\nchannels. Specifically, we propose to transform the predicted targets to the\nRGB-S space, which extends RGB with HSV's saturation component and leads to\nmarkedly better reconstruction and disentanglement for five common evaluation\ndatasets. The use of composite color spaces can be implemented with basically\nno computational overhead, is agnostic of the models' architecture, and is\nuniversally applicable across a wide range of visual computing tasks and\ntraining types. The findings of our approach encourage additional\ninvestigations in computer vision tasks beyond object-centric learning.\n","authors":["Bastian Jäckl","Yannick Metz","Udo Schlegel","Daniel A. Keim","Maximilian T. Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.15150v1.pdf","comment":"38 pages incl. references, 16 figures"},{"id":"http://arxiv.org/abs/2412.15129v1","updated":"2024-12-19T18:09:42Z","published":"2024-12-19T18:09:42Z","title":"Jet: A Modern Transformer-Based Normalizing Flow","summary":"  In the past, normalizing generative flows have emerged as a promising class\nof generative models for natural images. This type of model has many modeling\nadvantages: the ability to efficiently compute log-likelihood of the input\ndata, fast generation and simple overall structure. Normalizing flows remained\na topic of active research but later fell out of favor, as visual quality of\nthe samples was not competitive with other model classes, such as GANs,\nVQ-VAE-based approaches or diffusion models. In this paper we revisit the\ndesign of the coupling-based normalizing flow models by carefully ablating\nprior design choices and using computational blocks based on the Vision\nTransformer architecture, not convolutional neural networks. As a result, we\nachieve state-of-the-art quantitative and qualitative performance with a much\nsimpler architecture. While the overall visual quality is still behind the\ncurrent state-of-the-art models, we argue that strong normalizing flow models\ncan help advancing research frontier by serving as building components of more\npowerful generative models.\n","authors":["Alexander Kolesnikov","André Susano Pinto","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2412.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15119v1","updated":"2024-12-19T17:59:54Z","published":"2024-12-19T17:59:54Z","title":"Parallelized Autoregressive Visual Generation","summary":"  Autoregressive models have emerged as a powerful approach for visual\ngeneration but suffer from slow inference speed due to their sequential\ntoken-by-token prediction process. In this paper, we propose a simple yet\neffective approach for parallelized autoregressive visual generation that\nimproves generation efficiency while preserving the advantages of\nautoregressive modeling. Our key insight is that parallel generation depends on\nvisual token dependencies-tokens with weak dependencies can be generated in\nparallel, while strongly dependent adjacent tokens are difficult to generate\ntogether, as their independent sampling may lead to inconsistencies. Based on\nthis observation, we develop a parallel generation strategy that generates\ndistant tokens with weak dependencies in parallel while maintaining sequential\ngeneration for strongly dependent local tokens. Our approach can be seamlessly\nintegrated into standard autoregressive models without modifying the\narchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that\nour method achieves a 3.6x speedup with comparable quality and up to 9.5x\nspeedup with minimal quality degradation across both image and video generation\ntasks. We hope this work will inspire future research in efficient visual\ngeneration and unified autoregressive modeling. Project page:\nhttps://epiphqny.github.io/PAR-project.\n","authors":["Yuqing Wang","Shuhuai Ren","Zhijie Lin","Yujin Han","Haoyuan Guo","Zhenheng Yang","Difan Zou","Jiashi Feng","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2412.15119v1.pdf","comment":"Project page: https://epiphqny.github.io/PAR-project"},{"id":"http://arxiv.org/abs/2412.11917v3","updated":"2024-12-19T17:57:59Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v3.pdf","comment":"AAAI-25 (extended version), Code: https://github.com/CompVis/DisCLIP"},{"id":"http://arxiv.org/abs/2412.15106v1","updated":"2024-12-19T17:51:49Z","published":"2024-12-19T17:51:49Z","title":"Knowing Where to Focus: Attention-Guided Alignment for Text-based Person\n  Search","summary":"  In the realm of Text-Based Person Search (TBPS), mainstream methods aim to\nexplore more efficient interaction frameworks between text descriptions and\nvisual data. However, recent approaches encounter two principal challenges.\nFirstly, the widely used random-based Masked Language Modeling (MLM) considers\nall the words in the text equally during training. However, massive\nsemantically vacuous words ('with', 'the', etc.) be masked fail to contribute\nefficient interaction in the cross-modal MLM and hampers the representation\nalignment. Secondly, manual descriptions in TBPS datasets are tedious and\ninevitably contain several inaccuracies. To address these issues, we introduce\nan Attention-Guided Alignment (AGA) framework featuring two innovative\ncomponents: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module\n(TEM). AGM dynamically masks semantically meaningful words by aggregating the\nattention weight derived from the text encoding process, thereby cross-modal\nMLM can capture information related to the masked word from text context and\nimages and align their representations. Meanwhile, TEM alleviates low-quality\nrepresentations caused by repetitive and erroneous text descriptions by\nreplacing those semantically meaningful words with MLM's prediction. It not\nonly enriches text descriptions but also prevents overfitting. Extensive\nexperiments across three challenging benchmarks demonstrate the effectiveness\nof our AGA, achieving new state-of-the-art results with Rank-1 accuracy\nreaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid,\nrespectively.\n","authors":["Lei Tan","Weihao Li","Pingyang Dai","Jie Chen","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2412.15106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13788v2","updated":"2024-12-19T17:51:42Z","published":"2024-03-20T17:51:53Z","title":"DepthFM: Fast Monocular Depth Estimation with Flow Matching","summary":"  Current discriminative depth estimation methods often produce blurry\nartifacts, while generative approaches suffer from slow sampling due to\ncurvatures in the noise-to-depth transport. Our method addresses these\nchallenges by framing depth estimation as a direct transport between image and\ndepth distributions. We are the first to explore flow matching in this field,\nand we demonstrate that its interpolation trajectories enhance both training\nand sampling efficiency while preserving high performance. While generative\nmodels typically require extensive training data, we mitigate this dependency\nby integrating external knowledge from a pre-trained image diffusion model,\nenabling effective transfer even across differing objectives. To further boost\nour model performance, we employ synthetic data and utilize image-depth pairs\ngenerated by a discriminative model on an in-the-wild image dataset. As a\ngenerative model, our model can reliably estimate depth confidence, which\nprovides an additional advantage. Our approach achieves competitive zero-shot\nperformance on standard benchmarks of complex natural scenes while improving\nsampling efficiency and only requiring minimal synthetic data for training.\n","authors":["Ming Gui","Johannes Schusterbauer","Ulrich Prestel","Pingchuan Ma","Dmytro Kotovenko","Olga Grebenkova","Stefan Andreas Baumann","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13788v2.pdf","comment":"AAAI 2025, Project Page: https://github.com/CompVis/depth-fm"},{"id":"http://arxiv.org/abs/2412.15095v1","updated":"2024-12-19T17:45:08Z","published":"2024-12-19T17:45:08Z","title":"A Full Transformer-based Framework for Automatic Pain Estimation using\n  Videos","summary":"  The automatic estimation of pain is essential in designing an optimal pain\nmanagement system offering reliable assessment and reducing the suffering of\npatients. In this study, we present a novel full transformer-based framework\nconsisting of a Transformer in Transformer (TNT) model and a Transformer\nleveraging cross-attention and self-attention blocks. Elaborating on videos\nfrom the BioVid database, we demonstrate state-of-the-art performances, showing\nthe efficacy, efficiency, and generalization capability across all the primary\npain estimation tasks.\n","authors":["Stefanos Gkikas","Manolis Tsiknakis"],"pdf_url":"https://arxiv.org/pdf/2412.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15077v1","updated":"2024-12-19T17:26:07Z","published":"2024-12-19T17:26:07Z","title":"Till the Layers Collapse: Compressing a Deep Neural Network through the\n  Lenses of Batch Normalization Layers","summary":"  Today, deep neural networks are widely used since they can handle a variety\nof complex tasks. Their generality makes them very powerful tools in modern\ntechnology. However, deep neural networks are often overparameterized. The\nusage of these large models consumes a lot of computation resources. In this\npaper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers\n\\textbf{C}ollapse (TLC), which compresses deep neural networks through the\nlenses of batch normalization layers. By reducing the depth of these networks,\nour method decreases deep neural networks' computational requirements and\noverall latency. We validate our method on popular models such as Swin-T,\nMobileNet-V2, and RoBERTa, across both image classification and natural\nlanguage processing (NLP) tasks.\n","authors":["Zhu Liao","Nour Hezbri","Victor Quétu","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2412.15077v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15058v1","updated":"2024-12-19T17:06:53Z","published":"2024-12-19T17:06:53Z","title":"MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging\n  Datasets with In-Context Guidance","summary":"  Medical researchers and clinicians often need to perform novel segmentation\ntasks on a set of related images. Existing methods for segmenting a new dataset\nare either interactive, requiring substantial human effort for each image, or\nrequire an existing set of manually labeled images. We introduce a system,\nMultiverSeg, that enables practitioners to rapidly segment an entire new\ndataset without requiring access to any existing labeled data from that task or\ndomain. Along with the image to segment, the model takes user interactions such\nas clicks, bounding boxes or scribbles as input, and predicts a segmentation.\nAs the user segments more images, those images and segmentations become\nadditional inputs to the model, providing context. As the context set of\nlabeled images grows, the number of interactions required to segment each new\nimage decreases. We demonstrate that MultiverSeg enables users to interactively\nsegment new datasets efficiently, by amortizing the number of interactions per\nimage to achieve an accurate segmentation. Compared to using a state-of-the-art\ninteractive segmentation method, using MultiverSeg reduced the total number of\nscribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images\nfrom unseen tasks. We release code and model weights at\nhttps://multiverseg.csail.mit.edu\n","authors":["Hallee E. Wong","Jose Javier Gonzalez Ortiz","John Guttag","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2412.15058v1.pdf","comment":"Project Website: https://multiverseg.csail.mit.edu Keywords:\n  interactive segmentation, in-context learning, medical image analysis,\n  biomedical imaging, image annotation, visual prompting"},{"id":"http://arxiv.org/abs/2412.15054v1","updated":"2024-12-19T17:02:03Z","published":"2024-12-19T17:02:03Z","title":"GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and\n  Facilitative Playbacks Evaluation","summary":"  The advances in the development of Facilitative Playbacks extracted from\nHigh-Speed videoendoscopic sequences of the vocal folds are hindered by a\nnotable lack of publicly available datasets annotated with the semantic\nsegmentations corresponding to the area of the glottal gap. This fact also\nlimits the reproducibility and further exploration of existing research in this\nfield.\n  To address this gap, GIRAFE is a data repository designed to facilitate the\ndevelopment of advanced techniques for the semantic segmentation, analysis, and\nfast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The\nrepository includes 65 high-speed videoendoscopic recordings from a cohort of\n50 patients (30 female, 20 male). The dataset comprises 15 recordings from\nhealthy controls, 26 from patients with diagnosed voice disorders, and 24 with\nan unknown health condition. All of them were manually annotated by an expert,\nincluding the masks corresponding to the semantic segmentation of the glottal\ngap. The repository is also complemented with the automatic segmentation of the\nglottal area using different state-of-the-art approaches.\n  This data set has already supported several studies, which demonstrates its\nusefulness for the development of new glottal gap segmentation algorithms from\nHigh-Speed-Videoendoscopic sequences to improve or create new Facilitative\nPlaybacks. Despite these advances and others in the field, the broader\nchallenge of performing an accurate and completely automatic semantic\nsegmentation method of the glottal area remains open.\n","authors":["G. Andrade-Miranda","K. Chatzipapas","J. D. Arias-Londoño","J. I. Godino-Llorente"],"pdf_url":"https://arxiv.org/pdf/2412.15054v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.15050v1","updated":"2024-12-19T16:57:45Z","published":"2024-12-19T16:57:45Z","title":"Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion","summary":"  Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area.\n","authors":["Zhifei Chen","Tianshuo Xu","Wenhang Ge","Leyi Wu","Dongyu Yan","Jing He","Luozhou Wang","Lu Zeng","Shunsi Zhang","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03767v2","updated":"2024-12-19T16:45:52Z","published":"2023-01-10T03:10:32Z","title":"Metric Compatible Training for Online Backfilling in Large-Scale\n  Retrieval","summary":"  Backfilling is the process of re-extracting all gallery embeddings from\nupgraded models in image retrieval systems. It inevitably requires a\nprohibitively large amount of computational cost and even entails the downtime\nof the service. Although backward-compatible learning sidesteps this challenge\nby tackling query-side representations, this leads to suboptimal solutions in\nprinciple because gallery embeddings cannot benefit from model upgrades. We\naddress this dilemma by introducing an online backfilling algorithm, which\nenables us to achieve a progressive performance improvement during the\nbackfilling process while not sacrificing the final performance of new model\nafter the completion of backfilling. To this end, we first propose a simple\ndistance rank merge technique for online backfilling. Then, we incorporate a\nreverse transformation module for more effective and efficient merging, which\nis further enhanced by adopting a metric-compatible contrastive learning\napproach. These two components help to make the distances of old and new models\ncompatible, resulting in desirable merge results during backfilling with no\nextra computational overhead. Extensive experiments show the effectiveness of\nour framework on four standard benchmarks in various settings.\n","authors":["Seonguk Seo","Mustafa Gokhan Uzunbas","Bohyung Han","Sara Cao","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2301.03767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15032v1","updated":"2024-12-19T16:44:01Z","published":"2024-12-19T16:44:01Z","title":"DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT\n  Space","summary":"  This paper explores image modeling from the frequency space and introduces\nDCTdiff, an end-to-end diffusion generative paradigm that efficiently models\nimages in the discrete cosine transform (DCT) space. We investigate the design\nspace of DCTdiff and reveal the key design factors. Experiments on different\nframeworks (UViT, DiT), generation tasks, and various diffusion samplers\ndemonstrate that DCTdiff outperforms pixel-based diffusion models regarding\ngenerative quality and training efficiency. Remarkably, DCTdiff can seamlessly\nscale up to high-resolution generation without using the latent diffusion\nparadigm. Finally, we illustrate several intriguing properties of DCT image\nmodeling. For example, we provide a theoretical proof of why `image diffusion\ncan be seen as spectral autoregression', bridging the gap between diffusion and\nautoregressive models. The effectiveness of DCTdiff and the introduced\nproperties suggest a promising direction for image modeling in the frequency\nspace. The code is at \\url{https://github.com/forever208/DCTdiff}.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Haozhe Jia","Lanmiao Liu","Martin Beneš","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2412.15032v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2302.10634v2","updated":"2024-12-19T16:41:57Z","published":"2023-02-21T12:48:44Z","title":"A Deep Learning-Based and Fully Automated Pipeline for Regurgitant\n  Mitral Valve Anatomy Analysis from 3D Echocardiography","summary":"  3D transesophageal echocardiography (3DTEE), is the recommended method for\ndiagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of\nthe mitral valve (MV), allowing for precise segmentation and measurement of the\nregurgitant valve anatomy. However, manual TEE segmentations are time-consuming\nand prone to intra-operator variability, affecting the reliability of the\nmeasurements. To address this, we developed a fully automated pipeline using a\n3D convolutional neural network (CNN) to segment MV substructures (annulus,\nanterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN,\nbased on a multi-decoder residual U-Net architecture, was trained and tested on\na dataset comprising 100 3DTEE images with corresponding segmentations. Within\nthe pipeline, a custom algorithm refines the CNN-based segmentations and\nextracts MV models, from which anatomical landmarks and features are\nquantified. The accuracy of the proposed method was assessed using Dice score\nand mean surface distance (MSD) against ground truth segmentations, and the\nextracted anatomical parameters were compared against a semiautomated\ncommercial software TomTec Image Arena. The trained 3D CNN achieved an average\nDice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the\nannulus, anterior and posterior leaflet. The proposed CNN architecture\noutperformed a baseline residual U-Net architecture in MV substructure\nsegmentation, and the refinement of the predicted annulus segmentation improved\nMSD by 8.36%. The annular and leaflet linear measurements differed by less than\n7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained\nwith TomTec Image Arena. The proposed pipeline was faster than the commercial\nsoftware, with a modeling time of 12.54 s and a quantification time of 54.42 s.\n","authors":["Riccardo Munafò","Simone Saitta","Giacomo Ingallina","Paolo Denti","Francesco Maisano","Eustachio Agricola","Alberto Redaelli","Emiliano Votta"],"pdf_url":"https://arxiv.org/pdf/2302.10634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15023v1","updated":"2024-12-19T16:37:19Z","published":"2024-12-19T16:37:19Z","title":"Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and\n  Semantic Controls","summary":"  Sound designers and Foley artists usually sonorize a scene, such as from a\nmovie or video game, by manually annotating and sonorizing each action of\ninterest in the video. In our case, the intent is to leave full creative\ncontrol to sound designers with a tool that allows them to bypass the more\nrepetitive parts of their work, thus being able to focus on the creative\naspects of sound production. We achieve this presenting Stable-V2A, a two-stage\nmodel consisting of: an RMS-Mapper that estimates an envelope representative of\nthe audio characteristics associated with the input video; and Stable-Foley, a\ndiffusion model based on Stable Audio Open that generates audio semantically\nand temporally aligned with the target video. Temporal alignment is guaranteed\nby the use of the envelope as a ControlNet input, while semantic alignment is\nachieved through the use of sound representations chosen by the designer as\ncross-attention conditioning of the diffusion process. We train and test our\nmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. In\naddition, to test our model on a case study of interest, we introduce Walking\nThe Maps, a dataset of videos extracted from video games depicting animated\ncharacters walking in different locations. Samples and code available on our\ndemo page at https://ispamm.github.io/Stable-V2A.\n","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunità","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.15023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15010v1","updated":"2024-12-19T16:22:37Z","published":"2024-12-19T16:22:37Z","title":"Robust Federated Learning in the Face of Covariate Shift: A Magnitude\n  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation","summary":"  The development of highly sophisticated neural networks has allowed for fast\nprogress in every field of computer vision, however, applications where\nannotated data is prohibited due to privacy or security concerns remain\nchallenging. Federated Learning (FL) offers a promising framework for\nindividuals aiming to collaboratively develop a shared model while preserving\ndata privacy. Nevertheless, our findings reveal that variations in data\ndistribution among clients can profoundly affect FL methodologies, primarily\ndue to instabilities in the aggregation process. We also propose a novel FL\nframework to mitigate the adverse effects of covariate shifts among federated\nclients by combining individual parameter pruning and regularization techniques\nto improve the robustness of individual clients' models to aggregate. Each\nclient's model is optimized through magnitude-based pruning and the addition of\ndropout and noise injection layers to build more resilient decision pathways in\nthe networks and improve the robustness of the model's parameter aggregation\nstep. The proposed framework is capable of extracting robust representations\neven in the presence of very large covariate shifts among client data\ndistributions and in the federation of a small number of clients. Empirical\nfindings substantiate the effectiveness of our proposed methodology across\ncommon benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.\nFurthermore, we introduce the CelebA-Gender dataset, specifically designed to\nevaluate performance on a more realistic domain. The proposed method is capable\nof extracting robust representations even in the presence of both high and low\ncovariate shifts among client data distributions.\n","authors":["Ozgu Goksu","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2412.15010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14988v1","updated":"2024-12-19T16:00:10Z","published":"2024-12-19T16:00:10Z","title":"Stitch Contrast and Segment_Learning a Human Action Segmentation Model\n  Using Trimmed Skeleton Videos","summary":"  Existing skeleton-based human action classification models rely on\nwell-trimmed action-specific skeleton videos for both training and testing,\nprecluding their scalability to real-world applications where untrimmed videos\nexhibiting concatenated actions are predominant. To overcome this limitation,\nrecently introduced skeleton action segmentation models involve un-trimmed\nskeleton videos into end-to-end training. The model is optimized to provide\nframe-wise predictions for any length of testing videos, simultaneously\nrealizing action localization and classification. Yet, achieving such an\nimprovement im-poses frame-wise annotated skeleton videos, which remains\ntime-consuming in practice. This paper features a novel framework for\nskeleton-based action segmentation trained on short trimmed skeleton videos,\nbut that can run on longer un-trimmed videos. The approach is implemented in\nthree steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral\nskeleton stitching scheme that treats trimmed skeleton videos as elementary\nhuman motions that compose a semantic space and can be sampled to generate\nmulti-action stitched se-quences. Contrast learns contrastive representations\nfrom stitched sequences with a novel discrimination pretext task that enables a\nskeleton encoder to learn meaningful action-temporal contexts to improve action\nsegmentation. Finally, Segment relates the proposed method to action\nsegmentation by learning a segmentation layer while handling particular da-ta\navailability. Experiments involve a trimmed source dataset and an untrimmed\ntarget dataset in an adaptation formulation for real-world skeleton-based human\naction segmentation to evaluate the effectiveness of the proposed method.\n","authors":["Haitao Tian","Pierre Payeur"],"pdf_url":"https://arxiv.org/pdf/2412.14988v1.pdf","comment":"Accepted as AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08941v3","updated":"2024-12-19T15:59:19Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14974v1","updated":"2024-12-19T15:48:51Z","published":"2024-12-19T15:48:51Z","title":"Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse\n  Articulated Objects with Rich Annotations","summary":"  The acquisition of substantial volumes of 3D articulated object data is\nexpensive and time-consuming, and consequently the scarcity of 3D articulated\nobject data becomes an obstacle for deep learning methods to achieve remarkable\nperformance in various articulated object understanding tasks. Meanwhile,\npairing these object data with detailed annotations to enable training for\nvarious tasks is also difficult and labor-intensive to achieve. In order to\nexpeditiously gather a significant number of 3D articulated objects with\ncomprehensive and detailed annotations for training, we propose Articulated\nObject Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox\nconsists of i) descriptions of articulated objects by means of a generalized\nstructure program along with their analytic correspondence to the objects'\npoint cloud, ii) procedural rules about manipulations on the structure program\nto synthesize large-scale and diverse new articulated objects, and iii)\nmathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to\nprovide annotations to the synthesized object. Arti-PG has two appealing\nproperties for providing training data for articulated object understanding\ntasks: i) objects are created with unlimited variations in shape through\nprogram-oriented structure manipulation, ii) Arti-PG is widely applicable to\ndiverse tasks by easily providing comprehensive and detailed annotations.\nArti-PG now supports the procedural generation of 26 categories of articulate\nobjects and provides annotations across a wide range of both vision and\nmanipulation tasks, and we provide exhaustive experiments which fully\ndemonstrate its advantages. We will make Arti-PG toolbox publicly available for\nthe community to use.\n","authors":["Jianhua Sun","Yuxuan Li","Jiude Wei","Longfei Xu","Nange Wang","Yining Zhang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2412.14974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14969v1","updated":"2024-12-19T15:47:31Z","published":"2024-12-19T15:47:31Z","title":"PhotoHolmes: a Python library for forgery detection in digital images","summary":"  In this paper, we introduce PhotoHolmes, an open-source Python library\ndesigned to easily run and benchmark forgery detection methods on digital\nimages. The library includes implementations of popular and state-of-the-art\nmethods, dataset integration tools, and evaluation metrics. Utilizing the\nBenchmark tool in PhotoHolmes, users can effortlessly compare various methods.\nThis facilitates an accurate and reproducible comparison between their own\nmethods and those in the existing literature. Furthermore, PhotoHolmes includes\na command-line interface (CLI) to easily run the methods implemented in the\nlibrary on any suspicious image. As such, image forgery methods become more\naccessible to the community. The library has been built with extensibility and\nmodularity in mind, which makes adding new methods, datasets and metrics to the\nlibrary a straightforward process. The source code is available at\nhttps://github.com/photoholmes/photoholmes.\n","authors":["Julián O'Flaherty","Rodrigo Paganini","Juan Pablo Sotelo","Julieta Umpiérrez","Marina Gardella","Matías Tailanian","Pablo Musé"],"pdf_url":"https://arxiv.org/pdf/2412.14969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14965v1","updated":"2024-12-19T15:44:04Z","published":"2024-12-19T15:44:04Z","title":"Movie2Story: A framework for understanding videos and telling stories in\n  the form of novel text","summary":"  Multimodal video-to-text models have made considerable progress, primarily in\ngenerating brief descriptions of video content. However, there is still a\ndeficiency in generating rich long-form text descriptions that integrate both\nvideo and audio. In this paper, we introduce a framework called M2S, designed\nto generate novel-length text by combining audio, video, and character\nrecognition. M2S includes modules for video long-form text description and\ncomprehension, audio-based analysis of emotion, speech rate, and character\nalignment, and visual-based character recognition alignment. By integrating\nmultimodal information using the large language model GPT4o, M2S stands out in\nthe field of multimodal text generation. We demonstrate the effectiveness and\naccuracy of M2S through comparative experiments and human evaluation.\nAdditionally, the model framework has good scalability and significant\npotential for future research.\n","authors":["Kangning Li","Zheyang Jia","Anyu Ying"],"pdf_url":"https://arxiv.org/pdf/2412.14965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14963v1","updated":"2024-12-19T15:43:05Z","published":"2024-12-19T15:43:05Z","title":"IDOL: Instant Photorealistic 3D Human Creation from a Single Image","summary":"  Creating a high-fidelity, animatable 3D full-body avatar from a single image\nis a challenging task due to the diverse appearance and poses of humans and the\nlimited availability of high-quality training data. To achieve fast and\nhigh-quality human reconstruction, this work rethinks the task from the\nperspectives of dataset, model, and representation. First, we introduce a\nlarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K\ndiverse, photorealistic sets of human images. Each set contains 24-view frames\nin specific human poses, generated using a pose-controllable\nimage-to-multi-view model. Next, leveraging the diversity in views, poses, and\nappearances within HuGe100K, we develop a scalable feed-forward transformer\nmodel to predict a 3D human Gaussian representation in a uniform space from a\ngiven human image. This model is trained to disentangle human pose, body shape,\nclothing geometry, and texture. The estimated Gaussians can be animated without\npost-processing. We conduct comprehensive experiments to validate the\neffectiveness of the proposed dataset and method. Our model demonstrates the\nability to efficiently reconstruct photorealistic humans at 1K resolution from\na single input image using a single GPU instantly. Additionally, it seamlessly\nsupports various applications, as well as shape and texture editing tasks.\n","authors":["Yiyu Zhuang","Jiaxi Lv","Hao Wen","Qing Shuai","Ailing Zeng","Hao Zhu","Shifeng Chen","Yujiu Yang","Xun Cao","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14963v1.pdf","comment":"21 pages, 15 figures, includes main content, supplementary materials,\n  and references"},{"id":"http://arxiv.org/abs/2412.14961v1","updated":"2024-12-19T15:42:21Z","published":"2024-12-19T15:42:21Z","title":"TDCNet: Transparent Objects Depth Completion with CNN-Transformer\n  Dual-Branch Parallel Network","summary":"  The sensing and manipulation of transparent objects present a critical\nchallenge in industrial and laboratory robotics. Conventional sensors face\nchallenges in obtaining the full depth of transparent objects due to the\nrefraction and reflection of light on their surfaces and their lack of visible\ntexture. Previous research has attempted to obtain complete depth maps of\ntransparent objects from RGB and damaged depth maps (collected by depth sensor)\nusing deep learning models. However, existing methods fail to fully utilize the\noriginal depth map, resulting in limited accuracy for deep completion. To solve\nthis problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel\nnetwork for transparent object depth completion. The proposed framework\nconsists of two different branches: one extracts features from partial depth\nmaps, while the other processes RGB-D images. Experimental results demonstrate\nthat our model achieves state-of-the-art performance across multiple public\ndatasets. Our code and the pre-trained model are publicly available at\nhttps://github.com/XianghuiFan/TDCNet.\n","authors":["Xianghui Fan","Chao Ye","Anping Deng","Xiaotian Wu","Mengyang Pan","Hang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14957v1","updated":"2024-12-19T15:38:15Z","published":"2024-12-19T15:38:15Z","title":"Dream to Manipulate: Compositional World Models Empowering Robot\n  Imitation Learning with Imagination","summary":"  A world model provides an agent with a representation of its environment,\nenabling it to predict the causal consequences of its actions. Current world\nmodels typically cannot directly and explicitly imitate the actual environment\nin front of a robot, often resulting in unrealistic behaviors and\nhallucinations that make them unsuitable for real-world applications. In this\npaper, we introduce a new paradigm for constructing world models that are\nexplicit representations of the real world and its dynamics. By integrating\ncutting-edge advances in real-time photorealism with Gaussian Splatting and\nphysics simulators, we propose the first compositional manipulation world\nmodel, which we call DreMa. DreMa replicates the observed world and its\ndynamics, allowing it to imagine novel configurations of objects and predict\nthe future consequences of robot actions. We leverage this capability to\ngenerate new data for imitation learning by applying equivariant\ntransformations to a small set of demonstrations. Our evaluations across\nvarious settings demonstrate significant improvements in both accuracy and\nrobustness by incrementing actions and object distributions, reducing the data\nneeded to learn a policy and improving the generalization of the agents. As a\nhighlight, we show that a real Franka Emika Panda robot, powered by DreMa's\nimagination, can successfully learn novel physical tasks from just a single\nexample per task variation (one-shot policy learning). Our project page and\nsource code can be found in https://leobarcellona.github.io/DreamToManipulate/\n","authors":["Leonardo Barcellona","Andrii Zadaianchuk","Davide Allegro","Samuele Papa","Stefano Ghidoni","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2412.14957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13647v2","updated":"2024-12-19T15:37:55Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14954v1","updated":"2024-12-19T15:36:30Z","published":"2024-12-19T15:36:30Z","title":"Corn Ear Detection and Orientation Estimation Using Deep Learning","summary":"  Monitoring growth behavior of maize plants such as the development of ears\ncan give key insights into the plant's health and development. Traditionally,\nthe measurement of the angle of ears is performed manually, which can be\ntime-consuming and prone to human error. To address these challenges, this\npaper presents a computer vision-based system for detecting and tracking ears\nof corn in an image sequence. The proposed system could accurately detect,\ntrack, and predict the ear's orientation, which can be useful in monitoring\ntheir growth behavior. This can significantly save time compared to manual\nmeasurement and enables additional areas of ear orientation research and\npotential increase in efficiencies for maize production. Using an object\ndetector with keypoint detection, the algorithm proposed could detect 90\npercent of all ears. The cardinal estimation had a mean absolute error (MAE) of\n18 degrees, compared to a mean 15 degree difference between two people\nmeasuring by hand. These results demonstrate the feasibility of using computer\nvision techniques for monitoring maize growth and can lead to further research\nin this area.\n","authors":["Nathan Sprague","John Evans","Michael Mardikes"],"pdf_url":"https://arxiv.org/pdf/2412.14954v1.pdf","comment":"22 pages;15 figures"},{"id":"http://arxiv.org/abs/2406.16710v2","updated":"2024-12-19T15:28:26Z","published":"2024-06-24T15:11:35Z","title":"ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait\n  Image","summary":"  While recent works have achieved great success on image-to-3D object\ngeneration, high quality and fidelity 3D head generation from a single image\nremains a great challenge. Previous text-based methods for generating 3D heads\nwere limited by text descriptions and image-based methods struggled to produce\nhigh-quality head geometry. To handle this challenging problem, we propose a\nnovel framework, ID-Sculpt, to generate high-quality 3D heads while preserving\ntheir identities. Our work incorporates the identity information of the\nportrait image into three parts: 1) geometry initialization, 2) geometry\nsculpting, and 3) texture generation stages. Given a reference portrait image,\nwe first align the identity features with text features to realize ID-aware\nguidance enhancement, which contains the control signals representing the face\ninformation. We then use the canny map, ID features of the portrait image, and\na pre-trained text-to-normal/depth diffusion model to generate ID-aware\ngeometry supervision, and 3D-GAN inversion is employed to generate ID-aware\ngeometry initialization. Furthermore, with the ability to inject identity\ninformation into 3D head generation, we use ID-aware guidance to calculate\nID-aware Score Distillation (ISD) for geometry sculpting. For texture\ngeneration, we adopt the ID Consistent Texture Inpainting and Refinement which\nprogressively expands the view for texture inpainting to obtain an\ninitialization UV texture map. We then use the ID-aware guidance to provide\nimage-level supervision for noisy multi-view images to obtain a refined texture\nmap. Extensive experiments demonstrate that we can generate high-quality 3D\nheads with accurate geometry and texture from a single in-the-wild portrait\nimage.\n","authors":["Jinkun Hao","Junshu Tang","Jiangning Zhang","Ran Yi","Yijia Hong","Moran Li","Weijian Cao","Yating Wang","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2406.16710v2.pdf","comment":"Accepted by AAAI 2025; Project page:\n  https://jinkun-hao.github.io/ID-Sculpt/"},{"id":"http://arxiv.org/abs/2411.10958v2","updated":"2024-12-19T15:26:20Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrixes $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$\nto enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS)\nof SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on\nRTX4090, respectively. Comprehensive experiments confirm that our approach\nincurs negligible end-to-end metrics loss across diverse models, including\nthose for large language processing, image generation, and video generation.\nThe codes are available at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14939v1","updated":"2024-12-19T15:15:03Z","published":"2024-12-19T15:15:03Z","title":"GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction","summary":"  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n","authors":["Zesong Yang","Ru Zhang","Jiale Shi","Zixiang Ai","Boming Zhao","Hujun Bao","Luwei Yang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.14939v1.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://zju3dv.github.io/gurecon/"},{"id":"http://arxiv.org/abs/2302.11947v2","updated":"2024-12-19T15:13:46Z","published":"2023-02-23T11:44:43Z","title":"Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight\n  Convolutional Neural Networks","summary":"  The health and safety hazards posed by worn crane lifting ropes mandate\nperiodic inspection for damage. This task is time-consuming, prone to human\nerror, halts operation, and may result in the premature disposal of ropes.\nTherefore, we propose using efficient deep learning and computer vision methods\nto automate the process of detecting damaged ropes. Specifically, we present a\nvision-based system for detecting damage in synthetic fiber rope images using\nlightweight convolutional neural networks. We develop a camera-based apparatus\nto photograph the lifting rope's surface, while in operation, and capture the\nprogressive wear-and-tear as well as the more significant degradation in the\nrope's health state. Experts from Konecranes annotate the collected images in\naccordance with the rope's condition; normal or damaged. Then, we pre-process\nthe images, systematically design a deep learning model, evaluate its detection\nand prediction performance, analyze its computational complexity, and compare\nit with various other models. Experimental results show the proposed model\noutperforms other similar techniques with 96.5% accuracy, 94.8% precision,\n98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the\nmodel's real-time operation, low memory footprint, robustness to various\nenvironmental and operational conditions, and adequacy for deployment in\nindustrial applications such as lifting, mooring, towing, climbing, and\nsailing.\n","authors":["Tuomas Jalonen","Mohammad Al-Sa'd","Roope Mellanen","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2302.11947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14925v1","updated":"2024-12-19T15:02:50Z","published":"2024-12-19T15:02:50Z","title":"Automatic Spectral Calibration of Hyperspectral Images:Method, Dataset\n  and Benchmark","summary":"  Hyperspectral image (HSI) densely samples the world in both the space and\nfrequency domain and therefore is more distinctive than RGB images. Usually,\nHSI needs to be calibrated to minimize the impact of various illumination\nconditions. The traditional way to calibrate HSI utilizes a physical reference,\nwhich involves manual operations, occlusions, and/or limits camera mobility.\nThese limitations inspire this paper to automatically calibrate HSIs using a\nlearning-based method. Towards this goal, a large-scale HSI calibration dataset\nis created, which has 765 high-quality HSI pairs covering diversified natural\nscenes and illuminations. The dataset is further expanded to 7650 pairs by\ncombining with 10 different physically measured illuminations. A spectral\nillumination transformer (SIT) together with an illumination attention module\nis proposed. Extensive benchmarks demonstrate the SoTA performance of the\nproposed SIT. The benchmarks also indicate that low-light conditions are more\nchallenging than normal conditions. The dataset and codes are available\nonline:https://github.com/duranze/Automatic-spectral-calibration-of-HSI\n","authors":["Zhuoran Du","Shaodi You","Cheng Cheng","Shikui Wei"],"pdf_url":"https://arxiv.org/pdf/2412.14925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04272v2","updated":"2024-12-19T15:02:37Z","published":"2024-09-06T13:28:05Z","title":"Cycle Pixel Difference Network for Crisp Edge Detection","summary":"  Edge detection, as a fundamental task in computer vision, has garnered\nincreasing attention. The advent of deep learning has significantly advanced\nthis field. However, recent deep learning-based methods generally face two\nsignificant issues: 1) reliance on large-scale pre-trained weights, and 2)\ngeneration of thick edges. We construct a U-shape encoder-decoder model named\nCPD-Net that successfully addresses these two issues simultaneously. In\nresponse to issue 1), we propose a novel cycle pixel difference convolution\n(CPDC), which effectively integrates edge prior knowledge with modern\nconvolution operations, consequently successfully eliminating the dependence on\nlarge-scale pre-trained weights. As for issue 2), we construct a multi-scale\ninformation enhancement module (MSEM) and a dual residual connection-based\n(DRC) decoder to enhance the edge location ability of the model, thereby\ngenerating crisp and clean contour maps. Comprehensive experiments conducted on\nfour standard benchmarks demonstrate that our method achieves competitive\nperformance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760\nand AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our\napproach provides a novel perspective for addressing these challenges in edge\ndetection.\n","authors":["Changsong Liu","Wei Zhang","Yanyan Liu","Mingyang Li","Wenlin Li","Yimeng Fan","Xiangnan Bai","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.04272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16571v4","updated":"2024-12-19T14:47:05Z","published":"2024-04-25T12:34:23Z","title":"MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth\n  Estimation of Endoscopic Images","summary":"  Photometric constraint is indispensable for self-supervised monocular depth\nestimation. It involves warping a source image onto a target view using\nestimated depth&pose, and then minimizing the difference between the warped and\ntarget images. However, the endoscopic built-in light causes significant\nbrightness fluctuations, and thus makes the photometric constraint unreliable.\nPrevious efforts only mitigate this relying on extra models to calibrate image\nbrightness. In this paper, we propose MonoPCC to address the brightness\ninconsistency radically by reshaping the photometric constraint into a cycle\nform. Instead of only warping the source image, MonoPCC constructs a closed\nloop consisting of two opposite forward-backward warping paths: from target to\nsource and then back to target. Thus, the target image finally receives an\nimage cycle-warped from itself, which naturally makes the constraint invariant\nto brightness changes. Moreover, MonoPCC transplants the source image's\nphase-frequency into the intermediate warped image to avoid structure lost, and\nalso stabilizes the training via an exponential moving average (EMA) strategy\nto avoid frequent changes in the forward warping. The comprehensive and\nextensive experimental results on four endoscopic datasets demonstrate that our\nproposed MonoPCC shows a great robustness to the brightness inconsistency, and\nexceeds other state-of-the-arts by reducing the absolute relative error by at\nleast 7.27%, 9.38%, 9.90% and 3.17%, respectively.\n","authors":["Zhiwei Wang","Ying Zhou","Shiquan He","Ting Li","Fan Huang","Qiang Ding","Xinxia Feng","Mei Liu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2404.16571v4.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.18512v2","updated":"2024-12-19T14:46:05Z","published":"2023-11-30T12:40:23Z","title":"Union-over-Intersections: Object Detection beyond Winner-Takes-All","summary":"  This paper revisits the problem of predicting box locations in object\ndetection architectures. Typically, each box proposal or box query aims to\ndirectly maximize the intersection-over-union score with the ground truth,\nfollowed by a winner-takes-all non-maximum suppression where only the highest\nscoring box in each region is retained. We observe that both steps are\nsub-optimal: the first involves regressing proposals to the entire ground\ntruth, which is a difficult task even with large receptive fields, and the\nsecond neglects valuable information from boxes other than the top candidate.\nInstead of regressing proposals to the whole ground truth, we propose a simpler\napproach: regress only to the area of intersection between the proposal and the\nground truth. This avoids the need for proposals to extrapolate beyond their\nvisual scope, improving localization accuracy. Rather than adopting a\nwinner-takes-all strategy, we take the union over the regressed intersections\nof all boxes in a region to generate the final box outputs. Our plug-and-play\nmethod integrates seamlessly into proposal-based, grid-based, and query-based\ndetection architectures with minimal modifications, consistently improving\nobject localization and instance segmentation. We demonstrate its broad\napplicability and versatility across various detection and segmentation tasks.\n","authors":["Aritra Bhowmik","Pascal Mettes","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2311.18512v2.pdf","comment":"17 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2312.06259v2","updated":"2024-12-19T14:38:47Z","published":"2023-12-11T09:57:09Z","title":"Point Cloud Semantic Segmentation with Sparse and Inhomogeneous\n  Annotations","summary":"  Utilizing uniformly distributed sparse annotations, weakly supervised\nlearning alleviates the heavy reliance on fine-grained annotations in point\ncloud semantic segmentation tasks. However, few works discuss the inhomogeneity\nof sparse annotations, albeit it is common in real-world scenarios. Therefore,\nthis work introduces the probability density function into the gradient\nsampling approximation method to qualitatively analyze the impact of annotation\nsparsity and inhomogeneity under weakly supervised learning. Based on our\nanalysis, we propose an Adaptive Annotation Distribution Network (AADNet)\ncapable of robust learning on arbitrarily distributed sparse annotations.\nSpecifically, we propose a label-aware point cloud downsampling strategy to\nincrease the proportion of annotations involved in the training stage.\nFurthermore, we design the multiplicative dynamic entropy as the gradient\ncalibration function to mitigate the gradient bias caused by non-uniformly\ndistributed sparse annotations and explicitly reduce the epistemic uncertainty.\nWithout any prior restrictions and additional information, our proposed method\nachieves comprehensive performance improvements at multiple label rates and\ndifferent annotation distributions.\n","authors":["Zhiyi Pan","Nan Zhang","Wei Gao","Shan Liu","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2312.06259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14902v1","updated":"2024-12-19T14:32:11Z","published":"2024-12-19T14:32:11Z","title":"MagicNaming: Consistent Identity Generation by Finding a \"Name Space\" in\n  T2I Diffusion Models","summary":"  Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable\nof generating famous persons by simply referring to their names. Is it possible\nto make such models generate generic identities as simple as the famous ones,\ne.g., just use a name? In this paper, we explore the existence of a \"Name\nSpace\", where any point in the space corresponds to a specific identity.\nFortunately, we find some clues in the feature space spanned by text embedding\nof celebrities' names. Specifically, we first extract the embeddings of\ncelebrities' names in the Laion5B dataset with the text encoder of diffusion\nmodels. Such embeddings are used as supervision to learn an encoder that can\npredict the name (actually an embedding) of a given face image. We\nexperimentally find that such name embeddings work well in promising the\ngenerated image with good identity consistency. Note that like the names of\ncelebrities, our predicted name embeddings are disentangled from the semantics\nof text inputs, making the original generation capability of text-to-image\nmodels well-preserved. Moreover, by simply plugging such name embeddings, all\nvariants (e.g., from Civitai) derived from the same base model (i.e., SDXL)\nreadily become identity-aware text-to-image models. Project homepage:\n\\url{https://magicfusion.github.io/MagicNaming/}.\n","authors":["Jing Zhao","Heliang Zheng","Chaoyue Wang","Long Lan","Wanrong Hunag","Yuhua Tang"],"pdf_url":"https://arxiv.org/pdf/2412.14902v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13099v2","updated":"2024-12-19T14:23:45Z","published":"2024-12-17T17:10:02Z","title":"Accuracy Limits as a Barrier to Biometric System Security","summary":"  Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.\n","authors":["Axel Durbet","Paul-Marie Grollemund","Pascal Lafourcade","Kevin Thiry-Atighehchi"],"pdf_url":"https://arxiv.org/pdf/2412.13099v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.14118v2","updated":"2024-12-19T14:18:57Z","published":"2024-12-18T18:04:12Z","title":"GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images","summary":"  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n","authors":["Ziyang Xu","Huangxuan Zhao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14118v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2411.13093v2","updated":"2024-12-19T14:17:13Z","published":"2024-11-20T07:44:34Z","title":"Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension","summary":"  Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.\n","authors":["Yongdong Luo","Xiawu Zheng","Xiao Yang","Guilin Li","Haojia Lin","Jinfa Huang","Jiayi Ji","Fei Chao","Jiebo Luo","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.13093v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.14880v1","updated":"2024-12-19T14:17:09Z","published":"2024-12-19T14:17:09Z","title":"Multimodal Hypothetical Summary for Retrieval-based Multi-image Question\n  Answering","summary":"  Retrieval-based multi-image question answering (QA) task involves retrieving\nmultiple question-related images and synthesizing these images to generate an\nanswer. Conventional \"retrieve-then-answer\" pipelines often suffer from\ncascading errors because the training objective of QA fails to optimize the\nretrieval stage. To address this issue, we propose a novel method to\neffectively introduce and reference retrieved information into the QA. Given\nthe image set to be retrieved, we employ a multimodal large language model\n(visual perspective) and a large language model (textual perspective) to obtain\nmultimodal hypothetical summary in question-form and description-form. By\ncombining visual and textual perspectives, MHyS captures image content more\nspecifically and replaces real images in retrieval, which eliminates the\nmodality gap by transforming into text-to-text retrieval and helps improve\nretrieval. To more advantageously introduce retrieval with QA, we employ\ncontrastive learning to align queries (questions) with MHyS. Moreover, we\npropose a coarse-to-fine strategy for calculating both sentence-level and\nword-level similarity scores, to further enhance retrieval and filter out\nirrelevant details. Our approach achieves a 3.7% absolute improvement over\nstate-of-the-art methods on RETVQA and a 14.5% improvement over CLIP.\nComprehensive experiments and detailed ablation studies demonstrate the\nsuperiority of our method.\n","authors":["Peize Li","Qingyi Si","Peng Fu","Zheng Lin","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14880v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14873v1","updated":"2024-12-19T14:11:49Z","published":"2024-12-19T14:11:49Z","title":"Zero-Shot Artifact2Artifact: Self-incentive artifact removal for\n  photoacoustic imaging without any data","summary":"  Photoacoustic imaging (PAI) uniquely combines optical contrast with the\npenetration depth of ultrasound, making it critical for clinical applications.\nHowever, the quality of 3D PAI is often degraded due to reconstruction\nartifacts caused by the sparse and angle-limited configuration of detector\narrays. Existing iterative or deep learning-based methods are either\ntime-consuming or require large training datasets, significantly limiting their\npractical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a\nzero-shot self-supervised artifact removal method based on a super-lightweight\nnetwork, which leverages the fact that reconstruction artifacts are sensitive\nto irregularities caused by data loss. By introducing random perturbations to\nthe acquired PA data, it spontaneously generates subset data, which in turn\nstimulates the network to learn the artifact patterns in the reconstruction\nresults, thus enabling zero-shot artifact removal. This approach requires\nneither training data nor prior knowledge of the artifacts, and is capable of\nartifact removal for 3D PAI. For maximum amplitude projection (MAP) images or\nslice images in 3D PAI acquired with arbitrarily sparse or angle-limited\ndetector arrays, ZS-A2A employs a self-incentive strategy to complete artifact\nremoval and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in\nboth simulation study and $ in\\ vivo $ animal experiments. Results demonstrate\nthat ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing\nzero-shot methods, and for the $ in\\ vivo $ rat liver, ZS-A2A improves CNR from\n17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in\nthe following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.\n","authors":["Shuang Li","Qian Chen","Chulhong Kim","Seongwook Choi","Yibing Wang","Yu Zhang","Changhui Li"],"pdf_url":"https://arxiv.org/pdf/2412.14873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13620v3","updated":"2024-12-19T14:08:42Z","published":"2022-04-28T16:35:04Z","title":"Generative Adversarial Networks for Image Super-Resolution: A Survey","summary":"  Single image super-resolution (SISR) has played an important role in the\nfield of image processing. Recent generative adversarial networks (GANs) can\nachieve excellent results on low-resolution images with small samples. However,\nthere are little literatures summarizing different GANs in SISR. In this paper,\nwe conduct a comparative study of GANs from different perspectives. We first\ntake a look at developments of GANs. Second, we present popular architectures\nfor GANs in big and small samples for image applications. Then, we analyze\nmotivations, implementations and differences of GANs based optimization methods\nand discriminative learning for image super-resolution in terms of supervised,\nsemi-supervised and unsupervised manners, where these GANs are analyzed via\nintegrating different network architectures, prior knowledge, loss functions\nand multiple tasks. Next, we compare performance of these popular GANs on\npublic datasets via quantitative and qualitative analysis in SISR. Finally, we\nhighlight challenges of GANs and potential research points for SISR.\n","authors":["Chunwei Tian","Xuanyu Zhang","Qi Zhu","Bob Zhang","Jerry Chun-Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2204.13620v3.pdf","comment":"31pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.07024v3","updated":"2024-12-19T14:07:44Z","published":"2024-07-09T16:44:04Z","title":"Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization","summary":"  The vocabulary size in temporal action localization (TAL) is limited by the\nscarcity of large-scale annotated datasets. To overcome this, recent works\nintegrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL\n(OV-TAL). However, despite the success of VLMs trained on extensive datasets,\nexisting OV-TAL methods still rely on human-labeled TAL datasets of limited\nsize to train action localizers, limiting their generalizability. In this\npaper, we explore the scalability of self-training with unlabeled YouTube\nvideos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic\naction localizer is trained on a human-labeled TAL dataset to generate\npseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled\ndataset is then used to train the localizer. Extensive experiments demonstrate\nthat leveraging web-scale videos in self-training significantly enhances the\ngeneralizability of an action localizer. Additionally, we identify limitations\nin existing OV-TAL evaluation schemes and propose a new benchmark for thorough\nassessment. Finally, we showcase the TAL performance of the large multimodal\nmodel Gemini-1.5 on our new benchmark. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL.\n","authors":["Jeongseok Hyun","Su Ho Han","Hyolim Kang","Joon-Young Lee","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07024v3.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2412.14870v1","updated":"2024-12-19T14:06:56Z","published":"2024-12-19T14:06:56Z","title":"Large-scale School Mapping using Weakly Supervised Deep Learning for\n  Universal School Connectivity","summary":"  Improving global school connectivity is critical for ensuring inclusive and\nequitable quality education. To reliably estimate the cost of connecting\nschools, governments and connectivity providers require complete and accurate\nschool location data - a resource that is often scarce in many low- and\nmiddle-income countries. To address this challenge, we propose a\ncost-effective, scalable approach to locating schools in high-resolution\nsatellite images using weakly supervised deep learning techniques. Our best\nmodels, which combine vision transformers and convolutional neural networks,\nachieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging\nexplainable AI techniques, our approach can approximate the precise\ngeographical coordinates of the school locations using only low-cost,\nclassification-level annotations. To demonstrate the scalability of our method,\nwe generate nationwide maps of school location predictions in African countries\nand present a detailed analysis of our results, using Senegal as our case\nstudy. Finally, we demonstrate the immediate usability of our work by\nintroducing an interactive web mapping tool to streamline human-in-the-loop\nmodel validation efforts by government partners. This work successfully\nshowcases the real-world utility of deep learning and satellite images for\nplanning regional infrastructure and accelerating universal school\nconnectivity.\n","authors":["Isabelle Tingzon","Utku Can Ozturk","Ivan Dotu"],"pdf_url":"https://arxiv.org/pdf/2412.14870v1.pdf","comment":"Accepted at AAAI-25 Special Track on AI for Social Impact (AISI)"},{"id":"http://arxiv.org/abs/2412.14869v1","updated":"2024-12-19T14:06:44Z","published":"2024-12-19T14:06:44Z","title":"AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional\n  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature\n  Screening","summary":"  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood\nwithin the skull, which occurs due to the rupture of blood vessels in or around\nthe brain. If this condition is not diagnosed in a timely manner and\nappropriately treated, it can lead to serious complications such as decreased\nconsciousness, permanent neurological disabilities, or even death.The primary\naim of this study is to detect the occurrence or non-occurrence of ICH,\nfollowed by determining the type of subdural hemorrhage (SDH). These tasks are\nframed as two separate binary classification problems. By adding two layers to\nthe co-scale convolutional attention (CCA) classifier architecture, we\nintroduce a novel approach for ICH detection. In the first layer, after\nextracting features from different slices of computed tomography (CT) scan\nimages, we combine these features and select the 50 components that capture the\nhighest variance in the data, considering them as informative features. We then\nassess the discriminative power of these features using the bootstrap forest\nalgorithm, discarding those that lack sufficient discriminative ability between\ndifferent classes. This algorithm explicitly determines the contribution of\neach feature to the final prediction, assisting us in developing an explainable\nAI model. The features feed into a boosting neural network as a latent feature\nspace. In the second layer, we introduce a novel uncertainty-based fuzzy\nintegral operator to fuse information from different CT scan slices. This\noperator, by accounting for the dependencies between consecutive slices,\nsignificantly improves detection accuracy.\n","authors":["Mehdi Hosseini Chagahi","Md. Jalil Piran","Niloufar Delfan","Behzad Moshiri","Jaber Hatam Parikhan"],"pdf_url":"https://arxiv.org/pdf/2412.14869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20213v4","updated":"2024-12-19T13:46:40Z","published":"2024-03-29T14:50:43Z","title":"VHM: Versatile and Honest Vision Language Model for Remote Sensing Image\n  Analysis","summary":"  This paper develops a Versatile and Honest vision language Model (VHM) for\nremote sensing image analysis. VHM is built on a large-scale remote sensing\nimage-text dataset with rich-content captions (VersaD), and an honest\ninstruction dataset comprising both factual and deceptive questions (HnstD).\nUnlike prevailing remote sensing image-text datasets, in which image captions\nfocus on a few prominent objects and their relationships, VersaD captions\nprovide detailed information about image properties, object attributes, and the\noverall scene. This comprehensive captioning enables VHM to thoroughly\nunderstand remote sensing images and perform diverse remote sensing tasks.\nMoreover, different from existing remote sensing instruction datasets that only\ninclude factual questions, HnstD contains additional deceptive questions\nstemming from the non-existence of objects. This feature prevents VHM from\nproducing affirmative answers to nonsense queries, thereby ensuring its\nhonesty. In our experiments, VHM significantly outperforms various vision\nlanguage models on common tasks of scene classification, visual question\nanswering, and visual grounding. Additionally, VHM achieves competent\nperformance on several unexplored tasks, such as building vectorizing,\nmulti-label classification and honest question answering. We will release the\ncode, data and model weights at https://github.com/opendatalab/VHM .\n","authors":["Chao Pang","Xingxing Weng","Jiang Wu","Jiayu Li","Yi Liu","Jiaxing Sun","Weijia Li","Shuai Wang","Litong Feng","Gui-Song Xia","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2403.20213v4.pdf","comment":"Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding\n  author: Gui-Song Xia, Conghui He"},{"id":"http://arxiv.org/abs/2411.04865v4","updated":"2024-12-19T13:45:39Z","published":"2024-11-07T16:58:18Z","title":"ZAHA: Introducing the Level of Facade Generalization and the Large-Scale\n  Point Cloud Facade Semantic Segmentation Benchmark Dataset","summary":"  Facade semantic segmentation is a long-standing challenge in photogrammetry\nand computer vision. Although the last decades have witnessed the influx of\nfacade segmentation methods, there is a lack of comprehensive facade classes\nand data covering the architectural variability. In ZAHA, we introduce Level of\nFacade Generalization (LoFG), novel hierarchical facade classes designed based\non international urban modeling standards, ensuring compatibility with\nreal-world challenging classes and uniform methods' comparison. Realizing the\nLoFG, we present to date the largest semantic 3D facade segmentation dataset,\nproviding 601 million annotated points at five and 15 classes of LoFG2 and\nLoFG3, respectively. Moreover, we analyze the performance of baseline semantic\nsegmentation methods on our introduced LoFG classes and data, complementing it\nwith a discussion on the unresolved challenges for facade segmentation. We\nfirmly believe that ZAHA shall facilitate further development of 3D facade\nsemantic segmentation methods, enabling robust segmentation indispensable in\ncreating urban digital twins.\n","authors":["Olaf Wysocki","Yue Tan","Thomas Froech","Yan Xia","Magdalena Wysocki","Ludwig Hoegner","Daniel Cremers","Christoph Holst"],"pdf_url":"https://arxiv.org/pdf/2411.04865v4.pdf","comment":"Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV))"},{"id":"http://arxiv.org/abs/2412.13913v2","updated":"2024-12-19T13:41:08Z","published":"2024-12-18T14:53:38Z","title":"A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye\n  View Detection","summary":"  Camera-based Bird's Eye View (BEV) perception models receive increasing\nattention for their crucial role in autonomous driving, a domain where concerns\nabout the robustness and reliability of deep learning have been raised. While\nonly a few works have investigated the effects of randomly generated semantic\nperturbations, aka natural corruptions, on the multi-view BEV detection task,\nwe develop a black-box robustness evaluation framework that adversarially\noptimises three common semantic perturbations: geometric transformation, colour\nshifting, and motion blur, to deceive BEV models, serving as the first approach\nin this emerging field. To address the challenge posed by optimising the\nsemantic perturbation, we design a smoothed, distance-based surrogate function\nto replace the mAP metric and introduce SimpleDIRECT, a deterministic\noptimisation algorithm that utilises observed slopes to guide the optimisation\nprocess. By comparing with randomised perturbation and two optimisation\nbaselines, we demonstrate the effectiveness of the proposed framework.\nAdditionally, we provide a benchmark on the semantic robustness of ten recent\nBEV models. The results reveal that PolarFormer, which emphasises geometric\ninformation from multi-view images, exhibits the highest robustness, whereas\nBEVDet is fully compromised, with its precision reduced to zero.\n","authors":["Fu Wang","Yanghao Zhang","Xiangyu Yin","Guangliang Cheng","Zeyu Fu","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.13913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10929v6","updated":"2024-12-19T13:39:55Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v6.pdf","comment":"In process to IEEE Intelligent Vehicle Symposium 2025"},{"id":"http://arxiv.org/abs/2412.14846v1","updated":"2024-12-19T13:38:20Z","published":"2024-12-19T13:38:20Z","title":"Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy\n  with Pre-training, Data Augmentation and Dual Flow UNet","summary":"  Head and neck tumors and metastatic lymph nodes are crucial for treatment\nplanning and prognostic analysis. Accurate segmentation and quantitative\nanalysis of these structures require pixel-level annotation, making automated\nsegmentation techniques essential for the diagnosis and treatment of head and\nneck cancer. In this study, we investigated the effects of multiple strategies\non the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)\nimages. For the segmentation of pre-RT images, we utilized: 1) a fully\nsupervised learning approach, and 2) the same approach enhanced with\npre-trained weights and the MixUp data augmentation technique. For mid-RT\nimages, we introduced a novel computational-friendly network architecture that\nfeatures separate encoders for mid-RT images and registered pre-RT images with\ntheir labels. The mid-RT encoder branch integrates information from pre-RT\nimages and labels progressively during the forward propagation. We selected the\nhighest-performing model from each fold and used their predictions to create an\nensemble average for inference. In the final test, our models achieved a\nsegmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on\naggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at\nhttps://github.com/WltyBY/HNTS-MRG2024_train_code.\n","authors":["Litingyu Wang","Wenjun Liao","Shichuan Zhang","Guotai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14837v1","updated":"2024-12-19T13:27:58Z","published":"2024-12-19T13:27:58Z","title":"ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects","summary":"  3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models.\n","authors":["Qihang Cao","Huangxun Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14837v1.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2412.14835v1","updated":"2024-12-19T13:25:39Z","published":"2024-12-19T13:25:39Z","title":"Progressive Multimodal Reasoning via Active Retrieval","summary":"  Multi-step multimodal reasoning tasks pose significant challenges for\nmultimodal large language models (MLLMs), and finding effective ways to enhance\ntheir performance in such scenarios remains an unresolved issue. In this paper,\nwe propose AR-MCTS, a universal framework designed to progressively improve the\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\nTree Search (MCTS). Our approach begins with the development of a unified\nretrieval module that retrieves key supporting insights for solving complex\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\nautomated multimodal reasoning verification, we employ the MCTS algorithm\ncombined with an active retrieval mechanism, which enables the automatic\ngeneration of step-wise annotations. This strategy dynamically retrieves key\ninsights for each reasoning step, moving beyond traditional beam search\nsampling to improve the diversity and reliability of the reasoning space.\nAdditionally, we introduce a process reward model that aligns progressively to\nsupport the automatic verification of multimodal reasoning tasks. Experimental\nresults across three complex multimodal reasoning benchmarks confirm the\neffectiveness of the AR-MCTS framework in enhancing the performance of various\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\nsampling diversity and accuracy, yielding reliable multimodal reasoning.\n","authors":["Guanting Dong","Chenghao Zhang","Mengjie Deng","Yutao Zhu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.14835v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.14833v1","updated":"2024-12-19T13:21:04Z","published":"2024-12-19T13:21:04Z","title":"Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action\n  Recognition","summary":"  Skeleton-based action recognition using GCNs has achieved remarkable\nperformance, but recognizing ambiguous actions, such as \"waving\" and\n\"saluting\", remains a significant challenge. Existing methods typically rely on\na serial combination of GCNs and TCNs, where spatial and temporal features are\nextracted independently, leading to an unbalanced spatial-temporal information,\nwhich hinders accurate action recognition. Moreover, existing methods for\nambiguous actions often overemphasize local details, resulting in the loss of\ncrucial global context, which further complicates the task of differentiating\nambiguous actions. To address these challenges, we propose a lightweight\nplug-and-play module called Synchronized and Fine-grained Head (SF-Head),\ninserted between GCN and TCN layers. SF-Head first conducts Synchronized\nSpatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL),\nensuring a balanced interaction between the two types of features. It then\nperforms Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature\nConsistency Loss (F-CL), which aligns the aggregated feature with their\noriginal spatial-temporal feature. This aggregation step effectively combines\nboth global context and local details. Experimental results on NTU RGB+D 60,\nNTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in\ndistinguishing ambiguous actions. Our code will be made available at\nhttps://github.com/HaoHuang2003/SFHead.\n","authors":["Hao Huang","Yujie Lin","Siyu Chen","Haiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14833v1.pdf","comment":"20pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.14821v1","updated":"2024-12-19T13:12:15Z","published":"2024-12-19T13:12:15Z","title":"PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation","summary":"  Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}\n","authors":["Shoumeng Qiu","Xinrun Li","XiangYang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2412.14821v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14819v1","updated":"2024-12-19T13:10:38Z","published":"2024-12-19T13:10:38Z","title":"Multi-Level Embedding and Alignment Network with Consistency and\n  Invariance Learning for Cross-View Geo-Localization","summary":"  Cross-View Geo-Localization (CVGL) involves determining the localization of\ndrone images by retrieving the most similar GPS-tagged satellite images.\nHowever, the imaging gaps between platforms are often significant and the\nvariations in viewpoints are substantial, which limits the ability of existing\nmethods to effectively associate cross-view features and extract consistent and\ninvariant characteristics. Moreover, existing methods often overlook the\nproblem of increased computational and storage requirements when improving\nmodel performance. To handle these limitations, we propose a lightweight\nenhanced alignment network, called the Multi-Level Embedding and Alignment\nNetwork (MEAN). The MEAN network uses a progressive multi-level enhancement\nstrategy, global-to-local associations, and cross-domain alignment, enabling\nfeature communication across levels. This allows MEAN to effectively connect\nfeatures at different levels and learn robust cross-view consistent mappings\nand modality-invariant features. Moreover, MEAN adopts a shallow backbone\nnetwork combined with a lightweight branch design, effectively reducing\nparameter count and computational complexity. Experimental results on the\nUniversity-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter\ncount by 62.17% and computational complexity by 70.99% compared to\nstate-of-the-art models, while maintaining competitive or even superior\nperformance. The codes will be released soon.\n","authors":["Zhongwei Chen","Zhao-Xu Yang","Hai-Jun Rong"],"pdf_url":"https://arxiv.org/pdf/2412.14819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14816v1","updated":"2024-12-19T13:10:03Z","published":"2024-12-19T13:10:03Z","title":"Explainable Tampered Text Detection via Multimodal Large Models","summary":"  Recently, tampered text detection has attracted increasing attention due to\nits essential role in information security. Although existing methods can\ndetect the tampered text region, the interpretation of such detection remains\nunclear, making the prediction unreliable. To address this black-box problem,\nwe propose to explain the basis of tampered text detection with natural\nlanguage via large multimodal models. To fill the data gap for this task, we\npropose a large-scale, comprehensive dataset, ETTD, which contains both\npixel-level annotations indicating the tampered text region and natural\nlanguage annotations describing the anomaly of the tampered text. Multiple\nmethods are employed to improve the quality of the proposed data. For example,\na fused mask prompt is proposed to reduce confusion when querying GPT4o to\ngenerate anomaly descriptions. By weighting the input image with the mask\nannotation, the tampered region can be clearly indicated and the content in and\naround the tampered region can also be preserved. We also propose prompting\nGPT4o to recognize tampered texts and filtering out the responses with low OCR\naccuracy, which can effectively improve annotation quality in an automatic\nmanner. To further improve explainable tampered text detection, we propose a\nsimple yet effective model called TTD, which benefits from improved\nfine-grained perception by paying attention to the suspected region with\nauxiliary reference grounding query. Extensive experiments on both the ETTD\ndataset and the public dataset have verified the effectiveness of the proposed\nmethods. In-depth analysis is also provided to inspire further research. The\ndataset and code will be made publicly available.\n","authors":["Chenfan Qu","Jian Liu","Haoxing Chen","Baihan Yu","Jingjing Liu","Weiqiang Wang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.14816v1.pdf","comment":"The first work for explainable tampered text detection"},{"id":"http://arxiv.org/abs/2304.02488v4","updated":"2024-12-19T13:00:35Z","published":"2023-04-05T15:02:30Z","title":"SCB-dataset: A Dataset for Detecting Student Classroom Behavior","summary":"  The use of deep learning methods for automatic detection of students'\nclassroom behavior is a promising approach to analyze their class performance\nand enhance teaching effectiveness. However, the lack of publicly available\ndatasets on student behavior poses a challenge for researchers in this field.\nTo address this issue, we propose a Student Classroom Behavior dataset\n(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248\nlabels and 4,003 images, with a focus on hand-raising behavior. We evaluated\nthe dataset using the YOLOv7 algorithm, achieving a mean average precision\n(map) of up to 85.3%. We believe that our dataset can serve as a robust\nfoundation for future research in the field of student behavior detection and\npromote further advancements in this area.Our SCB-dataset can be downloaded\nfrom: https://github.com/Whiffe/SCB-dataset\n","authors":["Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2304.02488v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13735v2","updated":"2024-12-19T12:59:31Z","published":"2024-12-18T11:14:01Z","title":"3D Registration in 30 Years: A Survey","summary":"  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n","authors":["Jiaqi Yang","Chu'ai Zhang","Zhengbao Wang","Xinyue Cao","Xuan Ouyang","Xiyu Zhang","Zhenxuan Zeng","Zhao Zeng","Borui Lu","Zhiyi Xia","Qian Zhang","Yulan Guo","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14803v1","updated":"2024-12-19T12:48:40Z","published":"2024-12-19T12:48:40Z","title":"Video Prediction Policy: A Generalist Robot Policy with Predictive\n  Visual Representations","summary":"  Recent advancements in robotics have focused on developing generalist\npolicies capable of performing multiple tasks. Typically, these policies\nutilize pre-trained vision encoders to capture crucial information from current\nobservations. However, previous vision encoders, which trained on two-image\ncontrastive learning or single-image reconstruction, can not perfectly capture\nthe sequential information essential for embodied tasks. Recently, video\ndiffusion models (VDMs) have demonstrated the capability to accurately predict\nfuture image sequences, exhibiting a good understanding of physical dynamics.\nMotivated by the strong visual prediction capabilities of VDMs, we hypothesize\nthat they inherently possess visual representations that reflect the evolution\nof the physical world, which we term predictive visual representations.\nBuilding on this hypothesis, we propose the Video Prediction Policy (VPP), a\ngeneralist robotic policy conditioned on the predictive visual representations\nfrom VDMs. To further enhance these representations, we incorporate diverse\nhuman or robotic manipulation datasets, employing unified video-generation\ntraining objectives. VPP consistently outperforms existing methods across two\nsimulated and two real-world benchmarks. Notably, it achieves a 28.1\\% relative\nimprovement in the Calvin ABC-D benchmark compared to the previous\nstate-of-the-art and delivers a 28.8\\% increase in success rates for complex\nreal-world dexterous manipulation tasks.\n","authors":["Yucheng Hu","Yanjiang Guo","Pengchao Wang","Xiaoyu Chen","Yen-Jen Wang","Jianke Zhang","Koushil Sreenath","Chaochao Lu","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14803v1.pdf","comment":"The first two authors contribute equally. Project Page at\n  https://video-prediction-policy.github.io/"},{"id":"http://arxiv.org/abs/2410.05317v3","updated":"2024-12-19T12:38:23Z","published":"2024-10-05T03:47:06Z","title":"Accelerating Diffusion Transformers with Token-wise Feature Caching","summary":"  Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.\n","authors":["Chang Zou","Xuyang Liu","Ting Liu","Siteng Huang","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05317v3.pdf","comment":"In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix"},{"id":"http://arxiv.org/abs/2412.13803v2","updated":"2024-12-19T12:31:34Z","published":"2024-12-18T12:50:11Z","title":"M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation","summary":"  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M$^3$-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M$^3$-VOS, yielding several key insights. Notably, current\nappearancebased approaches show significant room for improvement when handling\nobjects with phase transitions. The inherent changes in disorder suggest that\nthe predictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-andplay model that improves its performance by\nreversal refinement. Our data and code will be publicly available at\nhttps://zixuan-chen.github.io/M-cubeVOS.github.io/.\n","authors":["Zixuan Chen","Jiaxin Li","Liming Tan","Yejie Guo","Junxuan Liang","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2412.13803v2.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.14790v1","updated":"2024-12-19T12:29:31Z","published":"2024-12-19T12:29:31Z","title":"YOLOv11 Optimization for Efficient Resource Utilization","summary":"  The objective of this research is to optimize the eleventh iteration of You\nOnly Look Once (YOLOv11) by developing size-specific modified versions of the\narchitecture. These modifications involve pruning unnecessary layers and\nreconfiguring the main architecture of YOLOv11. Each proposed version is\ntailored to detect objects of specific size ranges, from small to large. To\nensure proper model selection based on dataset characteristics, we introduced\nan object classifier program. This program identifies the most suitable\nmodified version for a given dataset. The proposed models were evaluated on\nvarious datasets and compared with the original YOLOv11 and YOLOv8 models. The\nexperimental results highlight significant improvements in computational\nresource efficiency, with the proposed models maintaining the accuracy of the\noriginal YOLOv11. In some cases, the modified versions outperformed the\noriginal model regarding detection performance. Furthermore, the proposed\nmodels demonstrated reduced model sizes and faster inference times. Models\nweights and the object size classifier can be found in this repository\n","authors":["Areeg Fagad Rasheed","M. Zarkoosh"],"pdf_url":"https://arxiv.org/pdf/2412.14790v1.pdf","comment":"12 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.09401v2","updated":"2024-12-19T12:23:39Z","published":"2024-12-12T16:08:03Z","title":"SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos","summary":"  In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM\nsystem for real-time and high-quality dense 3D reconstruction. SLAM3R provides\nan end-to-end solution by seamlessly integrating local 3D reconstruction and\nglobal coordinate registration through feed-forward neural networks. Given an\ninput video, the system first converts it into overlapping clips using a\nsliding window mechanism. Unlike traditional pose optimization-based methods,\nSLAM3R directly regresses 3D pointmaps from RGB images in each window and\nprogressively aligns and deforms these local pointmaps to create a globally\nconsistent scene reconstruction - all without explicitly solving any camera\nparameters. Experiments across datasets consistently show that SLAM3R achieves\nstate-of-the-art reconstruction accuracy and completeness while maintaining\nreal-time performance at 20+ FPS. Code and weights at:\nhttps://github.com/PKU-VCL-3DV/SLAM3R.\n","authors":["Yuzheng Liu","Siyan Dong","Shuzhe Wang","Yanchao Yang","Qingnan Fan","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16302v2","updated":"2024-12-19T12:14:03Z","published":"2024-07-23T08:57:11Z","title":"DeepClean: Integrated Distortion Identification and Algorithm Selection\n  for Rectifying Image Corruptions","summary":"  Distortion identification and rectification in images and videos is vital for\nachieving good performance in downstream vision applications. Instead of\nrelying on fixed trial-and-error based image processing pipelines, we propose a\ntwo-level sequential planning approach for automated image distortion\nclassification and rectification. At the higher level it detects the class of\ncorruptions present in the input image, if any. The lower level selects a\nspecific algorithm to be applied, from a set of externally provided candidate\nalgorithms. The entire two-level setup runs in the form of a single forward\npass during inference and it is to be queried iteratively until the retrieval\nof the original image. We demonstrate improvements compared to three baselines\non the object detection task on COCO image dataset with rich set of\ndistortions. The advantage of our approach is its dynamic reconfiguration,\nconditioned on the input image and generalisability to unseen candidate\nalgorithms at inference time, since it relies only on the comparison of their\noutput of the image embeddings.\n","authors":["Aditya Kapoor","Harshad Khadilkar","Jayvardhana Gubbi"],"pdf_url":"https://arxiv.org/pdf/2407.16302v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.14168v2","updated":"2024-12-19T11:59:46Z","published":"2024-12-18T18:59:50Z","title":"FashionComposer: Compositional Fashion Image Generation","summary":"  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n","authors":["Sihui Ji","Yiyang Wang","Xi Chen","Xiaogang Xu","Hao Luo","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14168v2.pdf","comment":"https://sihuiji.github.io/FashionComposer-Page"},{"id":"http://arxiv.org/abs/2412.14768v1","updated":"2024-12-19T11:51:45Z","published":"2024-12-19T11:51:45Z","title":"FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal\n  Keypoint Transformers for Pedestrian Fall Detection in Smart Cities","summary":"  In smart cities, detecting pedestrian falls is a major challenge to ensure\nthe safety and quality of life of citizens. In this study, we propose a novel\nfall detection system using FLAMe (Federated Learning with Attention\nMechanism), a federated learning (FL) based algorithm. FLAMe trains around\nimportant keypoint information and only transmits the trained important weights\nto the server, reducing communication costs and preserving data privacy.\nFurthermore, the lightweight keypoint transformer model is integrated into the\nFL framework to effectively learn spatio-temporal features. We validated the\nexperiment using 22,672 video samples from the \"Fall Accident Risk Behavior\nVideo-Sensor Pair data\" dataset from AI-Hub. As a result of the experiment, the\nFLAMe-based system achieved an accuracy of 94.02% with about 190,000\ntransmission parameters, maintaining performance similar to that of existing\ncentralized learning while maximizing efficiency by reducing communication\ncosts by about 40% compared to the existing FL algorithm, FedAvg. Therefore,\nthe FLAMe algorithm has demonstrated that it provides robust performance in the\ndistributed environment of smart cities and is a practical and effective\nsolution for public safety.\n","authors":["Byeonghun Kim","Byeongjoon Noh"],"pdf_url":"https://arxiv.org/pdf/2412.14768v1.pdf","comment":"8 pages, 7 figures, AAAI 2025 FLUID Workshop"},{"id":"http://arxiv.org/abs/2408.01812v3","updated":"2024-12-19T11:29:09Z","published":"2024-08-03T15:43:56Z","title":"SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and\n  BEV Paradigm","summary":"  Ground-to-aerial image synthesis focuses on generating realistic aerial\nimages from corresponding ground street view images while maintaining\nconsistent content layout, simulating a top-down view. The significant\nviewpoint difference leads to domain gaps between views, and dense urban scenes\nlimit the visible range of street views, making this cross-view generation task\nparticularly challenging. In this paper, we introduce SkyDiffusion, a novel\ncross-view generation method for synthesizing aerial images from street view\nimages, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The\nCurved-BEV method in SkyDiffusion converts street-view images into a BEV\nperspective, effectively bridging the domain gap, and employs a \"multi-to-one\"\nmapping strategy to address occlusion issues in dense urban scenes. Next,\nSkyDiffusion designed a BEV-guided diffusion model to generate\ncontent-consistent and realistic aerial images. Additionally, we introduce a\nnovel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image\nsynthesis applications, including disaster scene aerial synthesis, historical\nhigh-resolution satellite image synthesis, and low-altitude UAV image synthesis\ntasks. Experimental results demonstrate that SkyDiffusion outperforms\nstate-of-the-art methods on cross-view datasets across natural (CVUSA),\nsuburban (CVACT), urban (VIGOR-Chicago), and various application scenarios\n(G2A-3), achieving realistic and content-consistent aerial image generation.\nMore result and dataset information can be found at\nhttps://opendatalab.github.io/skydiffusion/ .\n","authors":["Junyan Ye","Jun He","Weijia Li","Zhutao Lv","Yi Lin","Jinhua Yu","Haote Yang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2408.01812v3.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.17981v3","updated":"2024-12-19T11:25:34Z","published":"2024-01-31T16:38:32Z","title":"From Training-Free to Adaptive: Empirical Insights into MLLMs'\n  Understanding of Detection Information","summary":"  Despite the impressive capabilities of Multimodal Large Language Models\n(MLLMs) in integrating text and image modalities, challenges remain in\naccurately interpreting detailed visual elements. Vision detection models excel\nat recognizing fine-grained image details, prompting researchers to use them to\nenhance MLLMs. One effective strategy is to infuse detection information in\ntext format, which has proven simple and effective. However, most studies\nutilize this method without training, leaving the potential of adaptive\ntraining largely unexplored. Adaptive training could significantly enhance\nMLLMs' comprehension of unique inputs while filtering out irrelevant\ninformation. This paper addresses the crucial question: How does training\nimpact MLLMs' understanding of infused textual detection information? We\nsystematically experiment with various representative models to evaluate the\neffects of training-free, retraining, and fine-tuning strategies. We also\nexamine the influence of training on MLLMs' original abilities and the\ninterchangeability of detection models. Our findings indicate that fine-tuning\na pre-trained MLLM to incorporate textual detection information delivers\nsuperior results compared to training-free and retraining methods, improving\nperformance by 6.71% across 10 widely recognized benchmarks. Furthermore,\nfine-tuning enables MLLMs to retain performance enhancements even when\ndetection models are swapped, indicating improved understanding of formatted\ntextual data. We release our codes to support further exploration of fusion\nstrategies for vision detection models and the enhancement of MLLMs'\nfine-grained multimodal capabilities.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2401.17981v3.pdf","comment":"32 pages, 22 tables, 7 figures"},{"id":"http://arxiv.org/abs/2410.23091v5","updated":"2024-12-19T11:18:58Z","published":"2024-10-30T15:06:44Z","title":"CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for\n  Adversarial Defense","summary":"  Despite ongoing efforts to defend neural classifiers from adversarial\nattacks, they remain vulnerable, especially to unseen attacks. In contrast,\nhumans are difficult to be cheated by subtle manipulations, since we make\njudgments only based on essential factors. Inspired by this observation, we\nattempt to model label generation with essential label-causative factors and\nincorporate label-non-causative factors to assist data generation. For an\nadversarial example, we aim to discriminate the perturbations as non-causative\nfactors and make predictions only based on the label-causative factors.\nConcretely, we propose a casual diffusion model (CausalDiff) that adapts\ndiffusion models for conditional data generation and disentangles the two types\nof casual factors by learning towards a novel casual information bottleneck\nobjective. Empirically, CausalDiff has significantly outperformed\nstate-of-the-art defense methods on various unseen attacks, achieving an\naverage robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on\nCIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition\nBenchmark). The code is available at\nhttps://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff\n","authors":["Mingkun Zhang","Keping Bi","Wei Chen","Quanrun Chen","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.23091v5.pdf","comment":"accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.11383v2","updated":"2024-12-19T11:12:30Z","published":"2024-09-17T17:34:24Z","title":"Training Datasets Generation for Machine Learning: Application to Vision\n  Based Navigation","summary":"  Vision Based Navigation consists in utilizing cameras as precision sensors\nfor GNC after extracting information from images. To enable the adoption of\nmachine learning for space applications, one of obstacles is the demonstration\nthat available training datasets are adequate to validate the algorithms. The\nobjective of the study is to generate datasets of images and metadata suitable\nfor training machine learning algorithms. Two use cases were selected and a\nrobust methodology was developed to validate the datasets including the ground\ntruth. The first use case is in-orbit rendezvous with a man-made object: a\nmockup of satellite ENVISAT. The second use case is a Lunar landing scenario.\nDatasets were produced from archival datasets (Chang'e 3), from the laboratory\nat DLR TRON facility and at Airbus Robotic laboratory, from SurRender software\nhigh fidelity image simulator using Model Capture and from Generative\nAdversarial Networks. The use case definition included the selection of\nalgorithms as benchmark: an AI-based pose estimation algorithm and a dense\noptical flow algorithm were selected. Eventually it is demonstrated that\ndatasets produced with SurRender and selected laboratory facilities are\nadequate to train machine learning algorithms.\n","authors":["Jérémy Lebreton","Ingo Ahrns","Roland Brochard","Christoph Haskamp","Hans Krüger","Matthieu Le Goff","Nicolas Menga","Nicolas Ollagnier","Ralf Regele","Francesco Capolupo","Massimo Casasco"],"pdf_url":"https://arxiv.org/pdf/2409.11383v2.pdf","comment":"6 pages, 4 figures, preprint of the proceedings of ESA SPAICE\n  conference 2024"},{"id":"http://arxiv.org/abs/2408.04594v3","updated":"2024-12-19T11:04:20Z","published":"2024-08-08T17:10:16Z","title":"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models","summary":"  High-performance Multimodal Large Language Models (MLLMs) are heavily\ndependent on data quality. To advance fine-grained image recognition within\nMLLMs, we introduce a novel data synthesis method inspired by contrastive\nlearning and image difference captioning. Our key idea involves challenging the\nmodel to discern both matching and distinct elements by scrutinizing object\ndifferences in detailed regions across similar images. We begin by generating\npairs of similar images that emphasize object variations. Following this, we\nemploy a Difference Area Generator to pinpoint object differences, and\nsubsequently, a Difference Captions Generator to articulate these differences.\nThis process results in a high-quality dataset of \"object replacement\" samples,\ntermed Img-Diff, which can be scaled as needed due to its automated nature. We\nleverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs,\nsuch as InternVL2, achieving substantial improvements across various image\ndifference and Visual Question Answering tasks. Notably, the trained models\nsignificantly outperform existing SOTA models like GPT-4V and Gemini on the\nMMVP benchmark. Additionally, we conduct comprehensive evaluations to validate\nthe dataset's diversity, quality, and robustness, offering several insights\ninto the synthesis of such contrastive datasets. We release our codes and\ndataset to encourage further research on multimodal data synthesis and MLLMs'\nfundamental capabilities for image understanding.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Bolin Ding","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04594v3.pdf","comment":"22 pages, 10 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.10681v2","updated":"2024-12-19T10:59:24Z","published":"2024-12-14T05:01:46Z","title":"One Pixel is All I Need","summary":"  Vision Transformers (ViTs) have achieved record-breaking performance in\nvarious visual tasks. However, concerns about their robustness against backdoor\nattacks have grown. Backdoor attacks involve associating a specific trigger\nwith a target label, causing the model to predict the attacker-specified label\nwhen the trigger is present, while correctly identifying clean images.We found\nthat ViTs exhibit higher attack success rates for quasi-triggers(patterns\ndifferent from but similar to the original training triggers)compared to CNNs.\nMoreover, some backdoor features in clean samples can suppress the original\ntrigger, making quasi-triggers more effective.To better understand and exploit\nthese vulnerabilities, we developed a tool called the Perturbation Sensitivity\nDistribution Map (PSDM). PSDM computes and sums gradients over many inputs to\nshow how sensitive the model is to small changes in the input. In ViTs, PSDM\nreveals a patch-like pattern where central pixels are more sensitive than\nedges. We use PSDM to guide the creation of quasi-triggers.Based on these\nfindings, we designed \"WorstVIT,\" a simple yet effective data poisoning\nbackdoor for ViT models. This attack requires an extremely low poisoning rate,\ntrains for just one epoch, and modifies a single pixel to successfully attack\nall validation images.\n","authors":["Deng Siqin","Zhou Xiaoyi"],"pdf_url":"https://arxiv.org/pdf/2412.10681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16729v3","updated":"2024-12-19T10:53:48Z","published":"2024-08-29T17:20:59Z","title":"Prediction-Feedback DETR for Temporal Action Detection","summary":"  Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.\n","authors":["Jihwan Kim","Miso Lee","Cheol-Ho Cho","Jihyun Lee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2408.16729v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2406.02507v3","updated":"2024-12-19T10:43:11Z","published":"2024-06-04T17:25:59Z","title":"Guiding a Diffusion Model with a Bad Version of Itself","summary":"  The primary axes of interest in image-generating diffusion models are image\nquality, the amount of variation in the results, and how well the results align\nwith a given condition, e.g., a class label or a text prompt. The popular\nclassifier-free guidance approach uses an unconditional model to guide a\nconditional model, leading to simultaneously better prompt alignment and\nhigher-quality images at the cost of reduced variation. These effects seem\ninherently entangled, and thus hard to control. We make the surprising\nobservation that it is possible to obtain disentangled control over image\nquality without compromising the amount of variation by guiding generation\nusing a smaller, less-trained version of the model itself rather than an\nunconditional model. This leads to significant improvements in ImageNet\ngeneration, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using\npublicly available networks. Furthermore, the method is also applicable to\nunconditional diffusion models, drastically improving their quality.\n","authors":["Tero Karras","Miika Aittala","Tuomas Kynkäänniemi","Jaakko Lehtinen","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2406.02507v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14719v1","updated":"2024-12-19T10:41:24Z","published":"2024-12-19T10:41:24Z","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","summary":"  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the\nambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to\nidentify the ambiguous sample, categorizing them into distinct sets of\nambiguous samples of false negatives and false positives, considering both\nbody- and action-level categories. \\textbf{Secondly}, we implement an ambiguous\ncontrastive refinement module to calibrate these ambiguous samples by\nregulating the distance between ambiguous samples and their corresponding\nprototypes. This calibration process aims to pull false negative\n($\\mathbb{FN}$) samples closer to their respective prototypes and push false\npositive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. \\textbf{Finally}, we propose a prototype-guided rectification to\nrectify prediction by incorporating the representability of prototypes.\nExtensive experiments conducted on the benchmark dataset demonstrate the\nsuperior performance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14719v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14706v1","updated":"2024-12-19T10:19:43Z","published":"2024-12-19T10:19:43Z","title":"EnergyMoGen: Compositional Human Motion Generation with Energy-Based\n  Diffusion Model in Latent Space","summary":"  Diffusion models, particularly latent diffusion models, have demonstrated\nremarkable success in text-driven human motion generation. However, it remains\nchallenging for latent diffusion models to effectively compose multiple\nsemantic concepts into a single, coherent motion sequence. To address this\nissue, we propose EnergyMoGen, which includes two spectrums of Energy-Based\nModels: (1) We interpret the diffusion model as a latent-aware energy-based\nmodel that generates motions by composing a set of diffusion models in latent\nspace; (2) We introduce a semantic-aware energy model based on cross-attention,\nwhich enables semantic composition and adaptive gradient descent for text\nembeddings. To overcome the challenges of semantic inconsistency and motion\ndistortion across these two spectrums, we introduce Synergistic Energy Fusion.\nThis design allows the motion latent diffusion model to synthesize\nhigh-quality, complex motions by combining multiple energy terms corresponding\nto textual descriptions. Experiments show that our approach outperforms\nexisting state-of-the-art models on various motion generation tasks, including\ntext-to-motion generation, compositional motion generation, and multi-concept\nmotion generation. Additionally, we demonstrate that our method can be used to\nextend motion datasets and improve the text-to-motion task.\n","authors":["Jianrong Zhang","Hehe Fan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14706v1.pdf","comment":"Project page: https://jiro-zhang.github.io/EnergyMoGen/"},{"id":"http://arxiv.org/abs/2412.14705v1","updated":"2024-12-19T10:17:50Z","published":"2024-12-19T10:17:50Z","title":"Event-assisted 12-stop HDR Imaging of Dynamic Scene","summary":"  High dynamic range (HDR) imaging is a crucial task in computational\nphotography, which captures details across diverse lighting conditions.\nTraditional HDR fusion methods face limitations in dynamic scenes with extreme\nexposure differences, as aligning low dynamic range (LDR) frames becomes\nchallenging due to motion and brightness variation. In this work, we propose a\nnovel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera\nsystem with an event camera and an RGB camera. The event camera provides\ntemporally dense, high dynamic range signals that improve alignment between LDR\nframes with large exposure differences, reducing ghosting artifacts caused by\nmotion. Also, a real-world finetuning strategy is proposed to increase the\ngeneralization of alignment module on real-world events. Additionally, we\nintroduce a diffusion-based fusion module that incorporates image priors from\npre-trained diffusion models to address artifacts in high-contrast regions and\nminimize errors from the alignment process. To support this work, we developed\nthe ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized\nevent signals, and validated our approach on both simulated and real-world\ndata. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, successfully extending HDR imaging to 12 stops in\ndynamic scenes.\n","authors":["Shi Guo","Zixuan Chen","Ziran Zhang","Yutian Chen","Gangwei Xu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2412.14705v1.pdf","comment":"Project page:\n  https://openimaginglab.github.io/Event-Assisted-12stops-HDR/"},{"id":"http://arxiv.org/abs/2410.17098v2","updated":"2024-12-19T10:03:18Z","published":"2024-10-22T15:22:53Z","title":"Activity Recognition on Avatar-Anonymized Datasets with Masked\n  Differential Privacy","summary":"  Privacy-preserving computer vision is an important emerging problem in\nmachine learning and artificial intelligence. Prevalent methods tackling this\nproblem use differential privacy (DP) or obfuscation techniques to protect the\nprivacy of individuals. In both cases, the utility of the trained model is\nsacrificed heavily in this process. In this work, we present an anonymization\npipeline that replaces sensitive human subjects in video datasets with\nsynthetic avatars within context, employing a combined rendering and stable\ndiffusion-based strategy. Additionally we propose masked differential privacy\n({MaskDP}) to protect non-anonymized but privacy sensitive background\ninformation. MaskDP allows for controlling sensitive regions where differential\nprivacy is applied, in contrast to applying DP on the entire input. This\ncombined methodology provides strong privacy protection while minimizing the\nusual performance penalty of privacy preserving methods. Experiments on\nmultiple challenging action recognition datasets demonstrate that our proposed\ntechniques result in better utility-privacy trade-offs compared to standard\ndifferentially private training in the especially demanding $\\epsilon<1$\nregime.\n","authors":["David Schneider","Sina Sajadmanesh","Vikash Sehwag","Saquib Sarfraz","Rainer Stiefelhagen","Lingjuan Lyu","Vivek Sharma"],"pdf_url":"https://arxiv.org/pdf/2410.17098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14692v1","updated":"2024-12-19T09:51:45Z","published":"2024-12-19T09:51:45Z","title":"Explicit Relational Reasoning Network for Scene Text Detection","summary":"  Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed.\n","authors":["Yuchen Su","Zhineng Chen","Yongkun Du","Zhilong Ji","Kai Hu","Jinfeng Bai","Xieping Gao"],"pdf_url":"https://arxiv.org/pdf/2412.14692v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14680v1","updated":"2024-12-19T09:32:53Z","published":"2024-12-19T09:32:53Z","title":"A Light-Weight Framework for Open-Set Object Detection with Decoupled\n  Feature Alignment in Joint Space","summary":"  Open-set object detection (OSOD) is highly desirable for robotic manipulation\nin unstructured environments. However, existing OSOD methods often fail to meet\nthe requirements of robotic applications due to their high computational burden\nand complex deployment. To address this issue, this paper proposes a\nlight-weight framework called Decoupled OSOD (DOSOD), which is a practical and\nhighly efficient solution to support real-time OSOD tasks in robotic systems.\nSpecifically, DOSOD builds upon the YOLO-World pipeline by integrating a\nvision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)\nadaptor is developed to transform text embeddings extracted by the VLM into a\njoint space, within which the detector learns the region representations of\nclass-agnostic proposals. Cross-modality features are directly aligned in the\njoint space, avoiding the complex feature interactions and thereby improving\ncomputational efficiency. DOSOD operates like a traditional closed-set detector\nduring the testing phase, effectively bridging the gap between closed-set and\nopen-set detection. Compared to the baseline YOLO-World, the proposed DOSOD\nsignificantly enhances real-time performance while maintaining comparable\naccuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\\%$, compared to\n$26.2\\%$ for YOLO-World-v1-S and $22.7\\%$ for YOLO-World-v2-S, using similar\nbackbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is\n$57.1\\%$ higher than YOLO-World-v1-S and $29.6\\%$ higher than YOLO-World-v2-S.\nMeanwhile, we demonstrate that the DOSOD model facilitates the deployment of\nedge devices. The codes and models are publicly available at\nhttps://github.com/D-Robotics-AI-Lab/DOSOD.\n","authors":["Yonghao He","Hu Su","Haiyong Yu","Cong Yang","Wei Sui","Cong Wang","Song Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14678v1","updated":"2024-12-19T09:31:53Z","published":"2024-12-19T09:31:53Z","title":"Efficient Few-Shot Neural Architecture Search by Counting the Number of\n  Nonlinear Functions","summary":"  Neural architecture search (NAS) enables finding the best-performing\narchitecture from a search space automatically. Most NAS methods exploit an\nover-parameterized network (i.e., a supernet) containing all possible\narchitectures (i.e., subnets) in the search space. However, the subnets that\nshare the same set of parameters are likely to have different characteristics,\ninterfering with each other during training. To address this, few-shot NAS\nmethods have been proposed that divide the space into a few subspaces and\nemploy a separate supernet for each subspace to limit the extent of weight\nsharing. They achieve state-of-the-art performance, but the computational cost\nincreases accordingly. We introduce in this paper a novel few-shot NAS method\nthat exploits the number of nonlinear functions to split the search space. To\nbe specific, our method divides the space such that each subspace consists of\nsubnets with the same number of nonlinear functions. Our splitting criterion is\nefficient, since it does not require comparing gradients of a supernet to split\nthe space. In addition, we have found that dividing the space allows us to\nreduce the channel dimensions required for each supernet, which enables\ntraining multiple supernets in an efficient manner. We also introduce a\nsupernet-balanced sampling (SBS) technique, sampling several subnets at each\ntraining step, to train different supernets evenly within a limited number of\ntraining steps. Extensive experiments on standard NAS benchmarks demonstrate\nthe effectiveness of our approach. Our code is available at\nhttps://cvlab.yonsei.ac.kr/projects/EFS-NAS.\n","authors":["Youngmin Oh","Hyunju Lee","Bumsub Ham"],"pdf_url":"https://arxiv.org/pdf/2412.14678v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14672v1","updated":"2024-12-19T09:24:10Z","published":"2024-12-19T09:24:10Z","title":"FiVL: A Framework for Improved Vision-Language Alignment","summary":"  Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. This issue extends to\nvision-language benchmarks, where it is difficult to make the image\nindispensable for accurate answer generation, particularly in vision\nquestion-answering tasks. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nto evaluate their effectiveness in achieving it. These datasets can be utilized\nfor both training and assessing an LVLM's ability to use image content as\nsubstantive evidence rather than relying solely on linguistic priors, providing\ninsights into the model's reliance on visual information. To demonstrate the\nutility of our dataset, we introduce an innovative training task that\noutperforms baselines alongside a validation method and application for\nexplainability. The code is available at https://github.com/IntelLabs/fivl.\n","authors":["Estelle Aflalo","Gabriela Ben Melech Stan","Tiep Le","Man Luo","Shachar Rosenman","Sayak Paul","Shao-Yen Tseng","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2412.14672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14671v1","updated":"2024-12-19T09:22:19Z","published":"2024-12-19T09:22:19Z","title":"MUSTER: Longitudinal Deformable Registration by Composition of\n  Consecutive Deformations","summary":"  Longitudinal imaging allows for the study of structural changes over time.\nOne approach to detecting such changes is by non-linear image registration.\nThis study introduces Multi-Session Temporal Registration (MUSTER), a novel\nmethod that facilitates longitudinal analysis of changes in extended series of\nmedical images. MUSTER improves upon conventional pairwise registration by\nincorporating more than two imaging sessions to recover longitudinal\ndeformations. Longitudinal analysis at a voxel-level is challenging due to\neffects of a changing image contrast as well as instrumental and environmental\nsources of bias between sessions. We show that local normalized\ncross-correlation as an image similarity metric leads to biased results and\npropose a robust alternative. We test the performance of MUSTER on a synthetic\nmulti-site, multi-session neuroimaging dataset and show that, in various\nscenarios, using MUSTER significantly enhances the estimated deformations\nrelative to pairwise registration. Additionally, we apply MUSTER on a sample of\nolder adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.\nThe results show that MUSTER can effectively identify patterns of\nneuro-degeneration from T1-weighted images and that these changes correlate\nwith changes in cognition, matching the performance of state of the art\nsegmentation methods. By leveraging GPU acceleration, MUSTER efficiently\nhandles large datasets, making it feasible also in situations with limited\ncomputational resources.\n","authors":["Edvard O. S. Grødem","Donatas Sederevičius","Esten H. Leonardsen","Bradley J. MacIntosh","Atle Bjørnerud","Till Schellhorn","Øystein Sørensen","Inge Amlien","Pablo F. Garrido","Anders M. Fjell"],"pdf_url":"https://arxiv.org/pdf/2412.14671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01220v3","updated":"2024-12-19T09:16:19Z","published":"2024-07-01T12:07:26Z","title":"Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation","summary":"  Understanding 3D scenes is a crucial challenge in computer vision research\nwith applications spanning multiple domains. Recent advancements in distilling\n2D vision-language foundation models into neural fields, like NeRF and 3DGS,\nenable open-vocabulary segmentation of 3D scenes from 2D multi-view images\nwithout the need for precise 3D annotations. However, while effective, these\nmethods typically rely on the per-pixel distillation of high-dimensional CLIP\nfeatures, introducing ambiguity and necessitating complex regularization\nstrategies, which adds inefficiency during training. This paper presents\nMaskField, which enables efficient 3D open-vocabulary segmentation with neural\nfields from a novel perspective. Unlike previous methods, MaskField decomposes\nthe distillation of mask and semantic features from foundation models by\nformulating a mask feature field and queries. MaskField overcomes ambiguous\nobject boundaries by naturally introducing SAM segmented object shapes without\nextra regularization during training. By circumventing the direct handling of\ndense high-dimensional CLIP features during training, MaskField is particularly\ncompatible with explicit scene representations like 3DGS. Our extensive\nexperiments show that MaskField not only surpasses prior state-of-the-art\nmethods but also achieves remarkably fast convergence. We hope that MaskField\nwill inspire further exploration into how neural fields can be trained to\ncomprehend 3D scenes from 2D models.\n","authors":["Zihan Gao","Lingling Li","Licheng Jiao","Fang Liu","Xu Liu","Wenping Ma","Yuwei Guo","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01220v3.pdf","comment":"15 pages, 9 figures, Code:https://github.com/keloee/MaskField"},{"id":"http://arxiv.org/abs/2412.14660v1","updated":"2024-12-19T09:10:07Z","published":"2024-12-19T09:10:07Z","title":"Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.\n","authors":["Zijun Chen","Wenbo Hu","Guande He","Zhijie Deng","Zheng Zhang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.14660v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2403.15031v2","updated":"2024-12-19T09:00:34Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.00143v2","updated":"2024-12-19T08:58:15Z","published":"2024-05-31T19:13:09Z","title":"Diversifying Query: Region-Guided Transformer for Temporal Sentence\n  Grounding","summary":"  Temporal sentence grounding is a challenging task that aims to localize the\nmoment spans relevant to a language description. Although recent DETR-based\nmodels have achieved notable progress by leveraging multiple learnable moment\nqueries, they suffer from overlapped and redundant proposals, leading to\ninaccurate predictions. We attribute this limitation to the lack of\ntask-related guidance for the learnable queries to serve a specific mode.\nFurthermore, the complex solution space generated by variable and\nopen-vocabulary language descriptions complicates optimization, making it\nharder for learnable queries to distinguish each other adaptively. To tackle\nthis limitation, we present a Region-Guided TRansformer (RGTR) for temporal\nsentence grounding, which diversifies moment queries to eliminate overlapped\nand redundant predictions. Instead of using learnable queries, RGTR adopts a\nset of anchor pairs as moment queries to introduce explicit regional guidance.\nEach anchor pair takes charge of moment prediction for a specific temporal\nregion, which reduces the optimization difficulty and ensures the diversity of\nthe final predictions. In addition, we design an IoU-aware scoring head to\nimprove proposal quality. Extensive experiments demonstrate the effectiveness\nof RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA\nand TACoS datasets. Codes are available at https://github.com/TensorsSun/RGTR\n","authors":["Xiaolong Sun","Liushuai Shi","Le Wang","Sanping Zhou","Kun Xia","Yabing Wang","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2406.00143v2.pdf","comment":"Accepted by AAAI-25. Code is available at\n  https://github.com/TensorsSun/RGTR"},{"id":"http://arxiv.org/abs/2412.11953v2","updated":"2024-12-19T08:52:56Z","published":"2024-12-16T16:37:03Z","title":"Reliable Breast Cancer Molecular Subtype Prediction based on\n  uncertainty-aware Bayesian Deep Learning by Mammography","summary":"  Breast cancer is a heterogeneous disease with different molecular subtypes,\nclinical behavior, treatment responses as well as survival outcomes. The\ndevelopment of a reliable, accurate, available and inexpensive method to\npredict the molecular subtypes using medical images plays an important role in\nthe diagnosis and prognosis of breast cancer. Recently, deep learning methods\nhave shown good performance in the breast cancer classification tasks using\nvarious medical images. Despite all that success, classical deep learning\ncannot deliver the predictive uncertainty. The uncertainty represents the\nvalidity of the predictions. Therefore, the high predicted uncertainty might\ncause a negative effect in the accurate diagnosis of breast cancer molecular\nsubtypes. To overcome this, uncertainty quantification methods are used to\ndetermine the predictive uncertainty. Accordingly, in this study, we proposed\nan uncertainty-aware Bayesian deep learning model using the full mammogram\nimages. In addition, to increase the performance of the multi-class molecular\nsubtype classification task, we proposed a novel hierarchical classification\nstrategy, named the two-stage classification strategy. The separate AUC of the\nproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,\nluminal and triple-negative classes, respectively. The proposed model not only\nhas a comparable performance to other studies in the field of breast cancer\nmolecular subtypes prediction, even using full mammography images, but it is\nalso more reliable, due to quantify the predictive uncertainty.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2412.11953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14643v1","updated":"2024-12-19T08:51:57Z","published":"2024-12-19T08:51:57Z","title":"RefHCM: A Unified Model for Referring Perceptions in Human-Centric\n  Scenarios","summary":"  Human-centric perceptions play a crucial role in real-world applications.\nWhile recent human-centric works have achieved impressive progress, these\nefforts are often constrained to the visual domain and lack interaction with\nhuman instructions, limiting their applicability in broader scenarios such as\nchatbots and sports analysis. This paper introduces Referring Human\nPerceptions, where a referring prompt specifies the person of interest in an\nimage. To tackle the new task, we propose RefHCM (Referring Human-Centric\nModel), a unified framework to integrate a wide range of human-centric\nreferring tasks. Specifically, RefHCM employs sequence mergers to convert raw\nmultimodal data -- including images, text, coordinates, and parsing maps --\ninto semantic tokens. This standardized representation enables RefHCM to\nreformulate diverse human-centric referring tasks into a sequence-to-sequence\nparadigm, solved using a plain encoder-decoder transformer architecture.\nBenefiting from a unified learning strategy, RefHCM effectively facilitates\nknowledge transfer across tasks and exhibits unforeseen capabilities in\nhandling complex reasoning. This work represents the first attempt to address\nreferring human perceptions with a general-purpose framework, while\nsimultaneously establishing a corresponding benchmark that sets new standards\nfor the field. Extensive experiments showcase RefHCM's competitive and even\nsuperior performance across multiple human-centric referring tasks. The code\nand data are publicly at https://github.com/JJJYmmm/RefHCM.\n","authors":["Jie Huang","Ruibing Hou","Jiahe Zhao","Hong Chang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2412.14643v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.14640v1","updated":"2024-12-19T08:51:01Z","published":"2024-12-19T08:51:01Z","title":"Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention\n  for Fine-Grained Few-Shot Learning","summary":"  Few-shot, fine-grained classification in computer vision poses significant\nchallenges due to the need to differentiate subtle class distinctions with\nlimited data. This paper presents a novel method that enhances the Contrastive\nLanguage-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided\nby real-time visual inputs. Unlike existing techniques such as Context\nOptimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by\nstatic prompts or visual token reliance, the proposed approach leverages a\ncross-attention mechanism to dynamically refine text prompts for the image at\nhand. This enables an image-specific alignment of textual features with image\npatches extracted from the Vision Transformer, making the model more effective\nfor datasets with high intra-class variance and low inter-class differences.\nThe method is evaluated on several datasets, including CUBirds, Oxford Flowers,\nand FGVC Aircraft, showing significant performance gains over static prompt\ntuning approaches. To ensure these performance gains translate into trustworthy\npredictions, we integrate Monte-Carlo Dropout in our approach to improve the\nreliability of the model predictions and uncertainty estimates. This\nintegration provides valuable insights into the model's predictive confidence,\nhelping to identify when predictions can be trusted and when additional\nverification is necessary. This dynamic approach offers a robust solution,\nadvancing the state-of-the-art for few-shot fine-grained classification.\n","authors":["Eric Brouwer","Jan Erik van Woerden","Gertjan Burghouts","Matias Valedenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2412.14640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18459v3","updated":"2024-12-19T08:47:07Z","published":"2024-04-29T06:35:34Z","title":"Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in\n  the Wild","summary":"  Large language models have evolved data-efficient generalists, benefiting\nfrom the universal language interface and large-scale pre-training. However,\nconstructing a data-efficient generalist for dense visual prediction presents a\ndistinct challenge due to the variation in label structures across different\ntasks. Consequently, generalization to unseen dense prediction tasks in the\nlow-data regime is not straightforward and has received less attention from\nprevious vision generalists. In this study, we explore a universal model that\ncan flexibly adapt to unseen dense label structures with a few examples,\nenabling it to serve as a data-efficient vision generalist in diverse\nreal-world scenarios. To this end, we base our method on a powerful\nmeta-learning framework and explore several axes to improve its performance and\nversatility for real-world problems, such as flexible adaptation mechanisms and\nscalability. We evaluate our model across a spectrum of unseen real-world\nscenarios where low-shot learning is desirable, including video, 3D, medical,\nbiological, and user-interactive tasks. Equipped with a generic architecture\nand an effective adaptation mechanism, our model flexibly adapts to all of\nthese tasks with at most 50 labeled images, showcasing a significant\nadvancement over existing data-efficient generalist approaches. Codes are\navailable at https://github.com/GitGyun/chameleon.\n","authors":["Donggyun Kim","Seongwoong Cho","Semin Kim","Chong Luo","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2404.18459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v3","updated":"2024-12-19T08:41:19Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Xue-Mei Dong","Jingqun Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12974v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14633v1","updated":"2024-12-19T08:38:59Z","published":"2024-12-19T08:38:59Z","title":"Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit\n  Post-Training Quantization in Vision Transformers","summary":"  Due to its efficiency, Post-Training Quantization (PTQ) has been widely\nadopted for compressing Vision Transformers (ViTs). However, when quantized\ninto low-bit representations, there is often a significant performance drop\ncompared to their full-precision counterparts. To address this issue,\nreconstruction methods have been incorporated into the PTQ framework to improve\nperformance in low-bit quantization settings. Nevertheless, existing related\nmethods predefine the reconstruction granularity and seldom explore the\nprogressive relationships between different reconstruction granularities, which\nleads to sub-optimal quantization results in ViTs. To this end, in this paper,\nwe propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for\naccurate PTQ, which significantly improves the performance of low-bit quantized\nvision transformers. Specifically, we define multi-head self-attention and\nmulti-layer perceptron modules along with their shortcuts as the finest\nreconstruction units. After reconstructing these two fine-grained units, we\ncombine them to form coarser blocks and reconstruct them at a coarser\ngranularity level. We iteratively perform this combination and reconstruction\nprocess, achieving progressive fine-to-coarse reconstruction. Additionally, we\nintroduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the\ndifficulty of training, thereby further enhancing model performance.\nExperimental results on the ImageNet dataset demonstrate that our proposed\nmethod achieves the best Top-1 accuracy among state-of-the-art methods,\nparticularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides,\nquantization results on the COCO dataset reveal the effectiveness and\ngeneralization of our proposed method on other computer vision tasks like\nobject detection and instance segmentation.\n","authors":["Rui Ding","Liang Yong","Sihuan Zhao","Jing Nie","Lihui Chen","Haijun Liu","Xichuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14631v1","updated":"2024-12-19T08:36:32Z","published":"2024-12-19T08:36:32Z","title":"Review of Fruit Tree Image Segmentation","summary":"  Fruit tree image segmentation is an essential problem in automating a variety\nof agricultural tasks such as phenotyping, harvesting, spraying, and pruning.\nMany research papers have proposed a diverse spectrum of solutions suitable to\nspecific tasks and environments. The review scope of this paper is confined to\nthe front views of fruit trees and based on 158 relevant papers collected using\na newly designed crawling review method. These papers are systematically\nreviewed based on a taxonomy that sequentially considers the method, image,\ntask, and fruit. This taxonomy will assist readers to intuitively grasp the big\npicture of these research activities. Our review reveals that the most\nnoticeable deficiency of the previous studies was the lack of a versatile\ndataset and segmentation model that could be applied to a variety of tasks and\nenvironments. Six important future research tasks are suggested, with the\nexpectation that these will pave the way to building a versatile tree\nsegmentation module.\n","authors":["Il-Seok Oh"],"pdf_url":"https://arxiv.org/pdf/2412.14631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14630v1","updated":"2024-12-19T08:33:33Z","published":"2024-12-19T08:33:33Z","title":"Unified Image Restoration and Enhancement: Degradation Calibrated Cycle\n  Reconstruction Diffusion Model","summary":"  Image restoration and enhancement are pivotal for numerous computer vision\napplications, yet unifying these tasks efficiently remains a significant\nchallenge. Inspired by the iterative refinement capabilities of diffusion\nmodels, we propose CycleRDM, a novel framework designed to unify restoration\nand enhancement tasks while achieving high-quality mapping. Specifically,\nCycleRDM first learns the mapping relationships among the degraded domain, the\nrough normal domain, and the normal domain through a two-stage diffusion\ninference process. Subsequently, we transfer the final calibration process to\nthe wavelet low-frequency domain using discrete wavelet transform, performing\nfine-grained calibration from a frequency domain perspective by leveraging\ntask-specific frequency spaces. To improve restoration quality, we design a\nfeature gain module for the decomposed wavelet high-frequency domain to\neliminate redundant features. Additionally, we employ multimodal textual\nprompts and Fourier transform to drive stable denoising and reduce randomness\nduring the inference process. After extensive validation, CycleRDM can be\neffectively generalized to a wide range of image restoration and enhancement\ntasks while requiring only a small number of training samples to be\nsignificantly superior on various benchmarks of reconstruction quality and\nperceptual quality. The source code will be available at\nhttps://github.com/hejh8/CycleRDM.\n","authors":["Minglong Xue","Jinhong He","Shivakumara Palaiahnakote","Mingliang Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11216v2","updated":"2024-12-19T08:32:20Z","published":"2024-12-15T15:13:14Z","title":"Distribution-Consistency-Guided Multi-modal Hashing","summary":"  Multi-modal hashing methods have gained popularity due to their fast speed\nand low storage requirements. Among them, the supervised methods demonstrate\nbetter performance by utilizing labels as supervisory signals compared with\nunsupervised methods. Currently, for almost all supervised multi-modal hashing\nmethods, there is a hidden assumption that training sets have no noisy labels.\nHowever, labels are often annotated incorrectly due to manual labeling in\nreal-world scenarios, which will greatly harm the retrieval performance. To\naddress this issue, we first discover a significant distribution consistency\npattern through experiments, i.e., the 1-0 distribution of the presence or\nabsence of each category in the label is consistent with the high-low\ndistribution of similarity scores of the hash codes relative to category\ncenters. Then, inspired by this pattern, we propose a novel\nDistribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to\nfilter and reconstruct noisy labels to enhance retrieval performance.\nSpecifically, the proposed method first randomly initializes several category\ncenters, which are used to compute the high-low distribution of similarity\nscores; Noisy and clean labels are then separately filtered out via the\ndiscovered distribution consistency pattern to mitigate the impact of noisy\nlabels; Subsequently, a correction strategy, which is indirectly designed via\nthe distribution consistency pattern, is applied to the filtered noisy labels,\ncorrecting high-confidence ones while treating low-confidence ones as unlabeled\nfor unsupervised learning, thereby further enhancing the model's performance.\nExtensive experiments on three widely used datasets demonstrate the superiority\nof the proposed method compared to state-of-the-art baselines in multi-modal\nretrieval tasks. The code is available at\nhttps://github.com/LiuJinyu1229/DCGMH.\n","authors":["Jin-Yu Liu","Xian-Ling Mao","Tian-Yi Che","Rong-Cheng Tu"],"pdf_url":"https://arxiv.org/pdf/2412.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14629v1","updated":"2024-12-19T08:31:42Z","published":"2024-12-19T08:31:42Z","title":"Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix\n  Factorization","summary":"  Robust Principal Component Analysis (RPCA) is a fundamental technique for\ndecomposing data into low-rank and sparse components, which plays a critical\nrole for applications such as image processing and anomaly detection.\nTraditional RPCA methods commonly use $\\ell_1$ norm regularization to enforce\nsparsity, but this approach can introduce bias and result in suboptimal\nestimates, particularly in the presence of significant noise or outliers.\nNon-convex regularization methods have been proposed to mitigate these\nchallenges, but they tend to be complex to optimize and sensitive to initial\nconditions, leading to potential instability in solutions. To overcome these\nchallenges, in this paper, we propose a novel RPCA model that integrates\nadaptive weighted least squares (AWLS) and low-rank matrix factorization\n(LRMF). The model employs a {self-attention-inspired} mechanism in its weight\nupdate process, allowing the weight matrix to dynamically adjust and emphasize\nsignificant components during each iteration. By employing a weighted F-norm\nfor the sparse component, our method effectively reduces bias while simplifying\nthe computational process compared to traditional $\\ell_1$-norm-based methods.\nWe use an alternating minimization algorithm, where each subproblem has an\nexplicit solution, thereby improving computational efficiency. Despite its\nsimplicity, numerical experiments demonstrate that our method outperforms\nexisting non-convex regularization approaches, offering superior performance\nand stability, as well as enhanced accuracy and robustness in practical\napplications.\n","authors":["Kexin Li","You-wei Wen","Xu Xiao","Mingchao Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14628v1","updated":"2024-12-19T08:30:54Z","published":"2024-12-19T08:30:54Z","title":"Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models","summary":"  Diffusion Models (DM) have democratized AI image generation through an\niterative denoising process. Quantization is a major technique to alleviate the\ninference cost and reduce the size of DM denoiser networks. However, as\ndenoisers evolve from variants of convolutional U-Nets toward newer Transformer\narchitectures, it is of growing importance to understand the quantization\nsensitivity of different weight layers, operations and architecture types to\nperformance. In this work, we address this challenge with Qua$^2$SeDiMo, a\nmixed-precision Post-Training Quantization framework that generates explainable\ninsights on the cost-effectiveness of various model weight quantization methods\nfor different denoiser operation types and block structures. We leverage these\ninsights to make high-quality mixed-precision quantization decisions for a\nmyriad of diffusion models ranging from foundational U-Nets to state-of-the-art\nTransformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,\n3.65-bit and 3.7-bit weight quantization on PixArt-${\\alpha}$,\nPixArt-${\\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our\nweight-quantization configurations with 6-bit activation quantization and\noutperform existing approaches in terms of quantitative metrics and generative\nimage quality.\n","authors":["Keith G. Mills","Mohammad Salameh","Ruichen Chen","Negar Hassanpour","Wei Lu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2412.14628v1.pdf","comment":"AAAI 2025; version includes supplementary material; 22 Pages, 18\n  Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2412.14623v1","updated":"2024-12-19T08:21:28Z","published":"2024-12-19T08:21:28Z","title":"FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors\n  Guided by Facial Recognizers","summary":"  Previous Deepfake detection methods perform well within their training\ndomains, but their effectiveness diminishes significantly with new synthesis\ntechniques. Recent studies have revealed that detection models often create\ndecision boundaries based on facial identity rather than synthetic artifacts,\nresulting in poor performance on cross-domain datasets. To address this\nlimitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a\nnovel training method that mitigates facial identity influence using a face\nrecognizer. Specifically, we first train a face recognizer using the same\nbackbone as the Deepfake detector. The recognizer is then frozen and employed\nduring the detector's training to reduce facial identity information. This is\nachieved by feeding input images into both the recognizer and the detector, and\nminimizing the similarity of their feature embeddings through our Facial\nIdentity Attenuating loss. This process encourages the detector to generate\nembeddings distinct from the recognizer, effectively reducing the impact of\nfacial identity. Extensive experiments demonstrate that our approach\nsignificantly enhances detection performance on both in-domain and cross-domain\ndatasets.\n","authors":["Younhun Kim","Myung-Joon Kwon","Wonjun Lee","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.14623v1.pdf","comment":"5 pages, 4 figures. In 2024 IEEE International Conference on Visual\n  Communications and Image Processing (VCIP) Oral"},{"id":"http://arxiv.org/abs/2409.17671v3","updated":"2024-12-19T08:19:41Z","published":"2024-09-26T09:30:37Z","title":"Leveraging Anthropometric Measurements to Improve Human Mesh Estimation\n  and Ensure Consistent Body Shapes","summary":"  The basic body shape (i.e., the body shape in T-pose) of a person does not\nchange within a single video. However, most SOTA human mesh estimation (HME)\nmodels output a slightly different, thus inconsistent basic body shape for each\nvideo frame. Furthermore, we find that SOTA 3D human pose estimation (HPE)\nmodels outperform HME models regarding the precision of the estimated 3D\nkeypoint positions. We solve the problem of inconsistent body shapes by\nleveraging anthropometric measurements like taken by tailors from humans. We\ncreate a model called A2B that converts given anthropometric measurements to\nbasic body shape parameters of human mesh models. We obtain superior and\nconsistent human meshes by combining the A2B model results with the keypoints\nof 3D HPE models using inverse kinematics. We evaluate our approach on\nchallenging datasets like ASPset or fit3D, where we can lower the MPJPE by over\n30 mm compared to SOTA HME models. Further, replacing estimates of the body\nshape parameters from existing HME models with A2B results not only increases\nthe performance of these HME models, but also guarantees consistent body\nshapes.\n","authors":["Katja Ludwig","Julian Lorenz","Daniel Kienzle","Tuan Bui","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2409.17671v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14619v1","updated":"2024-12-19T08:11:42Z","published":"2024-12-19T08:11:42Z","title":"Pitfalls of topology-aware image segmentation","summary":"  Topological correctness, i.e., the preservation of structural integrity and\nspecific characteristics of shape, is a fundamental requirement for medical\nimaging tasks, such as neuron or vessel segmentation. Despite the recent surge\nin topology-aware methods addressing this challenge, their real-world\napplicability is hindered by flawed benchmarking practices. In this paper, we\nidentify critical pitfalls in model evaluation that include inadequate\nconnectivity choices, overlooked topological artifacts in ground truth\nannotations, and inappropriate use of evaluation metrics. Through detailed\nempirical analysis, we uncover these issues' profound impact on the evaluation\nand ranking of segmentation methods. Drawing from our findings, we propose a\nset of actionable recommendations to establish fair and robust evaluation\nstandards for topology-aware medical image segmentation methods.\n","authors":["Alexander H. Berger","Laurin Lux","Alexander Weers","Martin Menten","Daniel Rueckert","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2412.14619v1.pdf","comment":"Code is available at\n  https://github.com/AlexanderHBerger/topo-pitfalls"},{"id":"http://arxiv.org/abs/2412.14613v1","updated":"2024-12-19T08:03:16Z","published":"2024-12-19T08:03:16Z","title":"HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic\n  Evaluation Using a Vision Language Model","summary":"  Vision-language models (VLMs) have shown impressive abilities in text and\nimage understanding. However, existing metrics for evaluating the text\ngenerated by VLMs focus exclusively on overall quality, leading to two\nlimitations: 1) it is challenging to identify which aspects of the text need\nimprovement from the overall score; 2) metrics may overlook specific evaluation\ncriteria when predicting an overall score. To address these limitations, we\npropose HarmonicEval, a reference-free evaluation metric that aggregates\ncriterion-wise scores to produce the overall score in a bottom-up manner.\nFurthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)\ndataset, which comprises 18,000 expert human judgments across four\nvision-language tasks. Our experiments demonstrate that HarmonicEval achieves\nhigher correlations with human judgments than conventional metrics while\nproviding numerical scores for each criterion.\n","authors":["Masanari Ohi","Masahiro Kaneko","Naoaki Okazaki","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2412.14613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19101v4","updated":"2024-12-19T08:00:44Z","published":"2024-06-27T11:28:36Z","title":"DocKylin: A Large Multimodal Model for Visual Document Understanding\n  with Efficient Visual Slimming","summary":"  Current multimodal large language models (MLLMs) face significant challenges\nin visual document understanding (VDU) tasks due to the high resolution, dense\ntext, and complex layouts typical of document images. These characteristics\ndemand a high level of detail perception ability from MLLMs. While increasing\ninput resolution improves detail perception capability, it also leads to longer\nsequences of visual tokens, increasing computational costs and straining the\nmodels' ability to handle long contexts. To address these challenges, we\nintroduce DocKylin, a document-centric MLLM that performs visual content\nslimming at both the pixel and token levels, thereby reducing token sequence\nlength in VDU scenarios. We introduce an Adaptive Pixel Slimming (APS)\npreprocessing module to perform pixel-level slimming, increasing the proportion\nof informative pixels. Moreover, we propose a novel Dynamic Token Slimming\n(DTS) module to conduct token-level slimming, filtering essential tokens and\nremoving others to adaptively create a more compact visual sequence.\nExperiments demonstrate DocKylin's promising performance across various VDU\nbenchmarks and the effectiveness of each component.\n","authors":["Jiaxin Zhang","Wentao Yang","Songxuan Lai","Zecheng Xie","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2406.19101v4.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14603v1","updated":"2024-12-19T07:49:40Z","published":"2024-12-19T07:49:40Z","title":"Successive optimization of optics and post-processing with\n  differentiable coherent PSF operator and field information","summary":"  Recently, the joint design of optical systems and downstream algorithms is\nshowing significant potential. However, existing rays-described methods are\nlimited to optimizing geometric degradation, making it difficult to fully\nrepresent the optical characteristics of complex, miniaturized lenses\nconstrained by wavefront aberration or diffraction effects. In this work, we\nintroduce a precise optical simulation model, and every operation in pipeline\nis differentiable. This model employs a novel initial value strategy to enhance\nthe reliability of intersection calculation on high aspherics. Moreover, it\nutilizes a differential operator to reduce memory consumption during coherent\npoint spread function calculations. To efficiently address various degradation,\nwe design a joint optimization procedure that leverages field information.\nGuided by a general restoration network, the proposed method not only enhances\nthe image quality, but also successively improves the optical performance\nacross multiple lenses that are already in professional level. This joint\noptimization pipeline offers innovative insights into the practical design of\nsophisticated optical systems and post-processing algorithms. The source code\nwill be made publicly available at\nhttps://github.com/Zrr-ZJU/Successive-optimization\n","authors":["Zheng Ren","Jingwen Zhou","Wenguan Zhang","Jiapu Yan","Bingkun Chen","Huajun Feng","Shiqi Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14598v1","updated":"2024-12-19T07:39:06Z","published":"2024-12-19T07:39:06Z","title":"Can We Get Rid of Handcrafted Feature Extractors? SparseViT:\n  Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization\n  Through Spare-Coding Transformer","summary":"  Non-semantic features or semantic-agnostic features, which are irrelevant to\nimage context but sensitive to image manipulations, are recognized as\nevidential to Image Manipulation Localization (IML). Since manual labels are\nimpossible, existing works rely on handcrafted methods to extract non-semantic\nfeatures. Handcrafted non-semantic features jeopardize IML model's\ngeneralization ability in unseen or complex scenarios. Therefore, for IML, the\nelephant in the room is: How to adaptively extract non-semantic features?\nNon-semantic features are context-irrelevant and manipulation-sensitive. That\nis, within an image, they are consistent across patches unless manipulation\noccurs. Then, spare and discrete interactions among image patches are\nsufficient for extracting non-semantic features. However, image semantics vary\ndrastically on different patches, requiring dense and continuous interactions\namong image patches for learning semantic representations. Hence, in this\npaper, we propose a Sparse Vision Transformer (SparseViT), which reformulates\nthe dense, global self-attention in ViT into a sparse, discrete manner. Such\nsparse self-attention breaks image semantics and forces SparseViT to adaptively\nextract non-semantic features for images. Besides, compared with existing IML\nmodels, the sparse self-attention mechanism largely reduced the model size (max\n80% in FLOPs), achieving stunning parameter efficiency and computation\nreduction. Extensive experiments demonstrate that, without any handcrafted\nfeature extractors, SparseViT is superior in both generalization and efficiency\nacross benchmark datasets.\n","authors":["Lei Su","Xiaochen Ma","Xuekang Zhu","Chaoqun Niu","Zeyu Lei","Ji-Zhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14598v1.pdf","comment":"12 page, 8 figures, published to AAAI"},{"id":"http://arxiv.org/abs/2412.14596v1","updated":"2024-12-19T07:31:40Z","published":"2024-12-19T07:31:40Z","title":"LDP: Generalizing to Multilingual Visual Information Extraction by\n  Language Decoupled Pretraining","summary":"  Visual Information Extraction (VIE) plays a crucial role in the comprehension\nof semi-structured documents, and several pre-trained models have been\ndeveloped to enhance performance. However, most of these works are monolingual\n(usually English). Due to the extremely unbalanced quantity and quality of\npre-training corpora between English and other languages, few works can extend\nto non-English scenarios. In this paper, we conduct systematic experiments to\nshow that vision and layout modality hold invariance among images with\ndifferent languages. If decoupling language bias from document images, a\nvision-layout-based model can achieve impressive cross-lingual generalization.\nAccordingly, we present a simple but effective multilingual training paradigm\nLDP (Language Decoupled Pre-training) for better utilization of monolingual\npre-training data. Our proposed model LDM (Language Decoupled Model) is first\npre-trained on the language-independent data, where the language knowledge is\ndecoupled by a diffusion model, and then the LDM is fine-tuned on the\ndownstream languages. Extensive experiments show that the LDM outperformed all\nSOTA multilingual pre-trained models, and also maintains competitiveness on\ndownstream monolingual/English benchmarks.\n","authors":["Huawen Shen","Gengluo Li","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14596v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14592v1","updated":"2024-12-19T07:23:17Z","published":"2024-12-19T07:23:17Z","title":"Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,\n  and Internal Properties","summary":"  Object anomaly detection is essential for industrial quality inspection, yet\ntraditional single-sensor methods face critical limitations. They fail to\ncapture the wide range of anomaly types, as single sensors are often\nconstrained to either external appearance, geometric structure, or internal\nproperties. To overcome these challenges, we introduce MulSen-AD, the first\nhigh-resolution, multi-sensor anomaly detection dataset tailored for industrial\napplications. MulSen-AD unifies data from RGB cameras, laser scanners, and\nlock-in infrared thermography, effectively capturing external appearance,\ngeometric deformations, and internal defects. The dataset spans 15 industrial\nproducts with diverse, real-world anomalies. We also present MulSen-AD Bench, a\nbenchmark designed to evaluate multi-sensor methods, and propose\nMulSen-TripleAD, a decision-level fusion algorithm that integrates these three\nmodalities for robust, unsupervised object anomaly detection. Our experiments\ndemonstrate that multi-sensor fusion substantially outperforms single-sensor\napproaches, achieving 96.1% AUROC in object-level detection accuracy. These\nresults highlight the importance of integrating multi-sensor data for\ncomprehensive industrial anomaly detection.\n","authors":["Wenqiao Li","Bozhong Zheng","Xiaohao Xu","Jinye Gan","Fading Lu","Xiang Li","Na Ni","Zheng Tian","Xiaonan Huang","Shenghua Gao","Yingna Wu"],"pdf_url":"https://arxiv.org/pdf/2412.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09583v4","updated":"2024-12-19T07:21:43Z","published":"2024-10-12T16:28:40Z","title":"POPoS: Improving Efficient and Robust Facial Landmark Detection with\n  Parallel Optimal Position Search","summary":"  Achieving a balance between accuracy and efficiency is a critical challenge\nin facial landmark detection (FLD). This paper introduces Parallel Optimal\nPosition Search (POPoS), a high-precision encoding-decoding framework designed\nto address the limitations of traditional FLD methods. POPoS employs three key\ncontributions: (1) Pseudo-range multilateration is utilized to correct heatmap\nerrors, improving landmark localization accuracy. By integrating multiple\nanchor points, it reduces the impact of individual heatmap inaccuracies,\nleading to robust overall positioning. (2) To enhance the pseudo-range accuracy\nof selected anchor points, a new loss function, named multilateration anchor\nloss, is proposed. This loss function enhances the accuracy of the distance\nmap, mitigates the risk of local optima, and ensures optimal solutions. (3) A\nsingle-step parallel computation algorithm is introduced, boosting\ncomputational efficiency and reducing processing time. Extensive evaluations\nacross five benchmark datasets demonstrate that POPoS consistently outperforms\nexisting methods, particularly excelling in low-resolution heatmaps scenarios\nwith minimal computational overhead. These advantages make POPoS a highly\nefficient and accurate tool for FLD, with broad applicability in real-world\nscenarios.\n","authors":["Chong-Yang Xiang","Jun-Yan He","Zhi-Qi Cheng","Xiao Wu","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2410.09583v4.pdf","comment":"Accepted to AAAI 2025, 9 pages, 6 figures. Code:\n  https://github.com/teslatasy/POPoS"},{"id":"http://arxiv.org/abs/2410.20815v2","updated":"2024-12-19T07:19:52Z","published":"2024-10-28T08:02:34Z","title":"Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian\n  Splatting","summary":"  Recently, Gaussian splatting has received more and more attention in the\nfield of static scene rendering. Due to the low computational overhead and\ninherent flexibility of explicit representations, plane-based explicit methods\nare popular ways to predict deformations for Gaussian-based dynamic scene\nrendering models. However, plane-based methods rely on the inappropriate\nlow-rank assumption and excessively decompose the space-time 4D encoding,\nresulting in overmuch feature overlap and unsatisfactory rendering quality. To\ntackle these problems, we propose Grid4D, a dynamic scene rendering model based\non Gaussian splatting and employing a novel explicit encoding method for the 4D\ninput through the hash encoding. Different from plane-based explicit\nrepresentations, we decompose the 4D encoding into one spatial and three\ntemporal 3D hash encodings without the low-rank assumption. Additionally, we\ndesign a novel attention module that generates the attention scores in a\ndirectional range to aggregate the spatial and temporal features. The\ndirectional attention enables Grid4D to more accurately fit the diverse\ndeformations across distinct scene components based on the spatial encoded\nfeatures. Moreover, to mitigate the inherent lack of smoothness in explicit\nrepresentation methods, we introduce a smooth regularization term that keeps\nour model from the chaos of deformation prediction. Our experiments demonstrate\nthat Grid4D significantly outperforms the state-of-the-art models in visual\nquality and rendering speed.\n","authors":["Jiawei Xu","Zexin Fan","Jian Yang","Jin Xie"],"pdf_url":"https://arxiv.org/pdf/2410.20815v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14587v1","updated":"2024-12-19T07:13:15Z","published":"2024-12-19T07:13:15Z","title":"Spike2Former: Efficient Spiking Transformer for High-performance Image\n  Segmentation","summary":"  Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly\nin image segmentation tasks. The reason is that directly converting neural\nnetworks with complex architectural designs for segmentation tasks into spiking\nversions leads to performance degradation and non-convergence. To address this\nchallenge, we first identify the modules in the architecture design that lead\nto the severe reduction in spike firing, make targeted improvements, and\npropose Spike2Former architecture. Second, we propose normalized integer\nspiking neurons to solve the training stability problem of SNNs with complex\narchitectures. We set a new state-of-the-art for SNNs in various semantic\nsegmentation datasets, with a significant improvement of +12.7% mIoU and 5.0\nefficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU\nand 6.6 efficiency on CityScapes.\n","authors":["Zhenxin Lei","Man Yao","Jiakui Hu","Xinhao Luo","Yanye Lu","Bo Xu","Guoqi Li"],"pdf_url":"https://arxiv.org/pdf/2412.14587v1.pdf","comment":"This work has been accepted on Association for the Advancement of\n  Artificial Intelligence 2025"},{"id":"http://arxiv.org/abs/2412.14585v1","updated":"2024-12-19T07:06:25Z","published":"2024-12-19T07:06:25Z","title":"HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video\n  Captioning","summary":"  With the growing demand for solutions to real-world video challenges,\ninterest in dense video captioning (DVC) has been on the rise. DVC involves the\nautomatic captioning and localization of untrimmed videos. Several studies\nhighlight the challenges of DVC and introduce improved methods utilizing prior\nknowledge, such as pre-training and external memory. In this research, we\npropose a model that leverages the prior knowledge of human-oriented\nhierarchical compact memory inspired by human memory hierarchy and cognition.\nTo mimic human-like memory recall, we construct a hierarchical memory and a\nhierarchical memory reading module. We build an efficient hierarchical compact\nmemory by employing clustering of memory events and summarization using large\nlanguage models. Comparative experiments demonstrate that this hierarchical\nmemory recall process improves the performance of DVC by achieving\nstate-of-the-art performance on YouCook2 and ViTT datasets.\n","authors":["Minkuk Kim","Hyeon Bae Kim","Jinyoung Moon","Jinwoo Choi","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2412.14585v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.14580v1","updated":"2024-12-19T07:00:03Z","published":"2024-12-19T07:00:03Z","title":"DiffSim: Taming Diffusion Models for Evaluating Visual Similarity","summary":"  Diffusion models have fundamentally transformed the field of generative\nmodels, making the assessment of similarity between customized model outputs\nand reference inputs critically important. However, traditional perceptual\nsimilarity metrics operate primarily at the pixel and patch levels, comparing\nlow-level colors and textures but failing to capture mid-level similarities and\ndifferences in image layout, object pose, and semantic content. Contrastive\nlearning-based CLIP and self-supervised learning-based DINO are often used to\nmeasure semantic similarity, but they highly compress image features,\ninadequately assessing appearance details. This paper is the first to discover\nthat pretrained diffusion models can be utilized for measuring visual\nsimilarity and introduces the DiffSim method, addressing the limitations of\ntraditional metrics in capturing perceptual consistency in custom generation\ntasks. By aligning features in the attention layers of the denoising U-Net,\nDiffSim evaluates both appearance and style similarity, showing superior\nalignment with human visual preferences. Additionally, we introduce the Sref\nand IP benchmarks to evaluate visual similarity at the level of style and\ninstance, respectively. Comprehensive evaluations across multiple benchmarks\ndemonstrate that DiffSim achieves state-of-the-art performance, providing a\nrobust tool for measuring visual coherence in generative models.\n","authors":["Yiren Song","Xiaokang Liu","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2412.14580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14579v1","updated":"2024-12-19T06:57:37Z","published":"2024-12-19T06:57:37Z","title":"GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D\n  Gaussian Splatting","summary":"  3D occupancy perception is gaining increasing attention due to its capability\nto offer detailed and precise environment representations. Previous\nweakly-supervised NeRF methods balance efficiency and accuracy, with mIoU\nvarying by 5-10 points due to sampling count along camera rays. Recently,\nreal-time Gaussian splatting has gained widespread popularity in 3D\nreconstruction, and the occupancy prediction task can also be viewed as a\nreconstruction task. Consequently, we propose GSRender, which naturally employs\n3D Gaussian Splatting for occupancy prediction, simplifying the sampling\nprocess. In addition, the limitations of 2D supervision result in duplicate\npredictions along the same camera ray. We implemented the Ray Compensation (RC)\nmodule, which mitigates this issue by compensating for features from adjacent\nframes. Finally, we redesigned the loss to eliminate the impact of dynamic\nobjects from adjacent frames. Extensive experiments demonstrate that our\napproach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while\nnarrowing the gap with 3D supervision methods. Our code will be released soon.\n","authors":["Qianpu Sun","Changyong Shu","Sifan Zhou","Zichen Yu","Yan Chen","Dawei Yang","Yuan Chun"],"pdf_url":"https://arxiv.org/pdf/2412.14579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14576v1","updated":"2024-12-19T06:52:12Z","published":"2024-12-19T06:52:12Z","title":"Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and\n  Progressive Correlation Network","summary":"  Alignment-free RGB-Thermal (RGB-T) salient object detection (SOD) aims to\nachieve robust performance in complex scenes by directly leveraging the\ncomplementary information from unaligned visible-thermal image pairs, without\nrequiring manual alignment. However, the labor-intensive process of collecting\nand annotating image pairs limits the scale of existing benchmarks, hindering\nthe advancement of alignment-free RGB-T SOD. In this paper, we construct a\nlarge-scale and high-diversity unaligned RGB-T SOD dataset named UVT20K,\ncomprising 20,000 image pairs, 407 scenes, and 1256 object categories. All\nsamples are collected from real-world scenarios with various challenges, such\nas low illumination, image clutter, complex salient objects, and so on. To\nsupport the exploration for further research, each sample in UVT20K is\nannotated with a comprehensive set of ground truths, including saliency masks,\nscribbles, boundaries, and challenge attributes. In addition, we propose a\nProgressive Correlation Network (PCNet), which models inter- and intra-modal\ncorrelations on the basis of explicit alignment to achieve accurate predictions\nin unaligned image pairs. Extensive experiments conducted on unaligned and\naligned datasets demonstrate the effectiveness of our method.Code and dataset\nare available at https://github.com/Angknpng/PCNet.\n","authors":["Kunpeng Wang","Keke Chen","Chenglong Li","Zhengzheng Tu","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2412.14576v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14571v1","updated":"2024-12-19T06:42:25Z","published":"2024-12-19T06:42:25Z","title":"SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar\n  Object Detection","summary":"  3D object detection is one of the fundamental perception tasks for autonomous\nvehicles. Fulfilling such a task with a 4D millimeter-wave radar is very\nattractive since the sensor is able to acquire 3D point clouds similar to Lidar\nwhile maintaining robust measurements under adverse weather. However, due to\nthe high sparsity and noise associated with the radar point clouds, the\nperformance of the existing methods is still much lower than expected. In this\npaper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation\n(SCKD) method for 4D radar-based 3D object detection. It characterizes the\ncapability of learning the feature from a Lidar-radar-fused teacher network\nwith semi-supervised distillation. We first propose an adaptive fusion module\nin the teacher network to boost its performance. Then, two feature distillation\nmodules are designed to facilitate the cross-modality knowledge transfer.\nFinally, a semi-supervised output distillation is proposed to increase the\neffectiveness and flexibility of the distillation framework. With the same\nnetwork structure, our radar-only student trained by SCKD boosts the mAP by\n10.38% over the baseline and outperforms the state-of-the-art works on the VoD\ndataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the\nmoderate difficulty level over the baseline when extra unlabeled data are\navailable. Code is available at https://github.com/Ruoyu-Xu/SCKD.\n","authors":["Ruoyu Xu","Zhiyu Xiang","Chenwei Zhang","Hanzhi Zhong","Xijun Zhao","Ruina Dang","Peng Xu","Tianyu Pu","Eryun Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14571v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.04749v2","updated":"2024-12-19T06:41:40Z","published":"2024-10-07T04:59:08Z","title":"LLaVA Needs More Knowledge: Retrieval Augmented Natural Language\n  Generation with Knowledge Graph for Explaining Thoracic Pathologies","summary":"  Generating Natural Language Explanations (NLEs) for model predictions on\nmedical images, particularly those depicting thoracic pathologies, remains a\ncritical and challenging task. Existing methodologies often struggle due to\ngeneral models' insufficient domain-specific medical knowledge and privacy\nconcerns associated with retrieval-based augmentation techniques. To address\nthese issues, we propose a novel Vision-Language framework augmented with a\nKnowledge Graph (KG)-based datastore, which enhances the model's understanding\nby incorporating additional domain-specific medical knowledge essential for\ngenerating accurate and informative NLEs. Our framework employs a KG-based\nretrieval mechanism that not only improves the precision of the generated\nexplanations but also preserves data privacy by avoiding direct data retrieval.\nThe KG datastore is designed as a plug-and-play module, allowing for seamless\nintegration with various model architectures. We introduce and evaluate three\ndistinct frameworks within this paradigm: KG-LLaVA, which integrates the\npre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining\nMedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts\nLLaVA by incorporating the Bio-ViT-L vision model. These frameworks are\nvalidated on the MIMIC-NLE dataset, where they achieve state-of-the-art\nresults, underscoring the effectiveness of KG augmentation in generating\nhigh-quality NLEs for thoracic pathologies.\n","authors":["Ameer Hamza"," Abdullah","Yong Hyun Ahn","Sungyoung Lee","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04749v2.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.14568v1","updated":"2024-12-19T06:39:28Z","published":"2024-12-19T06:39:28Z","title":"Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF\n  Separation","summary":"  Recent learning-based Multi-View Stereo models have demonstrated\nstate-of-the-art performance in sparse-view 3D reconstruction. However,\ndirectly applying 3D Gaussian Splatting (3DGS) as a refinement step following\nthese models presents challenges. We hypothesize that the excessive positional\ndegrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting\ncolor patterns at the cost of structural fidelity. To address this, we propose\nreprojection-based DoF separation, a method distinguishing positional DoFs in\nterms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To\nindependently manage each DoF, we introduce a reprojection process along with\ntailored constraints for each DoF. Through experiments across various datasets,\nwe confirm that separating the positional DoFs of Gaussians and applying\ntargeted constraints effectively suppresses geometric artifacts, producing\nreconstruction results that are both visually and geometrically plausible.\n","authors":["Yongsung Kim","Minjun Park","Jooyoung Choi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.14568v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.11530v2","updated":"2024-12-19T06:32:22Z","published":"2024-12-16T08:08:35Z","title":"RoMeO: Robust Metric Visual Odometry","summary":"  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a\nfundamental building block for many applications such as VR/AR and robotics.\nThis work focuses on monocular RGB VO where the input is a monocular RGB video\nwithout IMU or 3D sensors. Existing approaches lack robustness under this\nchallenging scenario and fail to generalize to unseen data (especially\noutdoors); they also cannot recover metric-scale poses. We propose Robust\nMetric Visual Odometry (RoMeO), a novel method that resolves these issues\nleveraging priors from pre-trained depth models. RoMeO incorporates both\nmonocular metric depth and multi-view stereo (MVS) models to recover\nmetric-scale, simplify correspondence search, provide better initialization and\nregularize optimization. Effective strategies are proposed to inject noise\nduring training and adaptively filter noisy depth priors, which ensure the\nrobustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the\nstate-of-the-art (SOTA) by a large margin across 6 diverse datasets covering\nboth indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO\nreduces the relative (align the trajectory scale with GT) and absolute\ntrajectory errors both by >50%. The performance gain also transfers to the full\nSLAM pipeline (with global BA & loop closure). Code will be released upon\nacceptance.\n","authors":["Junda Cheng","Zhipeng Cai","Zhaoxing Zhang","Wei Yin","Matthias Muller","Michael Paulitsch","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11973v6","updated":"2024-12-19T06:29:38Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) and Task-agnostic\nIncremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning\n(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is\ndesigned to prevent overfitting when the data samples are scarce. Furthermore,\nthe sparse reuse of WSN weights is considered for Video Incremental Learning\n(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It\nenables compact encoding of videos and identifies reusable subnetworks across\nvarying bandwidths. We have integrated FSO into different architectural\nframeworks for continual learning, including VIL, TIL, and FSCIL. Our\ncomprehensive experiments demonstrate FSO's effectiveness, significantly\nimproving task performance at various convolutional representational levels.\nSpecifically, FSO enhances higher-layer performance in TIL and FSCIL and\nlower-layer performance in VIL.\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v6.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (T-PAMI)"},{"id":"http://arxiv.org/abs/2412.14561v1","updated":"2024-12-19T06:26:16Z","published":"2024-12-19T06:26:16Z","title":"GBRIP: Granular Ball Representation for Imbalanced Partial Label\n  Learning","summary":"  Partial label learning (PLL) is a complicated weakly supervised\nmulti-classification task compounded by class imbalance. Currently, existing\nmethods only rely on inter-class pseudo-labeling from inter-class features,\noften overlooking the significant impact of the intra-class imbalanced features\ncombined with the inter-class. To address these limitations, we introduce\nGranular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for\nimbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and\nmulti-center loss to construct a granular ball-based nfeature space through\nunsupervised learning, effectively capturing the feature distribution within\neach class. GBRIP mitigates the impact of confusing features by systematically\nrefining label disambiguation and estimating imbalance distributions. The novel\nmulti-center loss function enhances learning by emphasizing the relationships\nbetween samples and their respective centers within the granular balls.\nExtensive experiments on standard benchmarks demonstrate that GBRIP outperforms\nexisting state-of-the-art methods, offering a robust solution to the challenges\nof imbalanced PLL.\n","authors":["Jintao Huang","Yiu-ming Cheung","Chi-man Vong","Wenbin Qian"],"pdf_url":"https://arxiv.org/pdf/2412.14561v1.pdf","comment":"AAAI25"},{"id":"http://arxiv.org/abs/2409.01179v3","updated":"2024-12-19T06:26:04Z","published":"2024-09-02T11:19:54Z","title":"Recoverable Compression: A Multimodal Vision Token Recovery Mechanism\n  Guided by Text Information","summary":"  With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance.\n","authors":["Yi Chen","Jian Xu","Xu-Yao Zhang","Wen-Zhuo Liu","Yang-Yang Liu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01179v3.pdf","comment":"AAAI2025 Accepted"},{"id":"http://arxiv.org/abs/2403.10650v3","updated":"2024-12-19T06:25:45Z","published":"2024-03-15T19:35:10Z","title":"PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time\n  Adaptation","summary":"  Real-world vision models in dynamic environments face rapid shifts in domain\ndistributions, leading to decreased recognition performance. Using unlabeled\ntest data, continuous test-time adaptation (CTTA) directly adjusts a\npre-trained source discriminative model to these changing domains. A highly\neffective CTTA method involves applying layer-wise adaptive learning rates for\nselectively adapting pre-trained layers. However, it suffers from the poor\nestimation of domain shift and the inaccuracies arising from the pseudo-labels.\nThis work aims to overcome these limitations by identifying layers for\nadaptation via quantifying model prediction uncertainty without relying on\npseudo-labels. We utilize the magnitude of gradients as a metric, calculated by\nbackpropagating the KL divergence between the softmax output and a uniform\ndistribution, to select layers for further adaptation. Subsequently, for the\nparameters exclusively belonging to these selected layers, with the remaining\nones frozen, we evaluate their sensitivity to approximate the domain shift and\nadjust their learning rates accordingly. We conduct extensive image\nclassification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,\ndemonstrating the superior efficacy of our method compared to prior approaches.\n","authors":["Sarthak Kumar Maharana","Baoming Zhang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10650v3.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14559v1","updated":"2024-12-19T06:22:19Z","published":"2024-12-19T06:22:19Z","title":"ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation\n  Model","summary":"  The scaling law has been validated in various domains, such as natural\nlanguage processing (NLP) and massive computer vision tasks; however, its\napplication to motion generation remains largely unexplored. In this paper, we\nintroduce a scalable motion generation framework that includes the motion\ntokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through\ncomprehensive experiments, we observe the scaling behavior of this system. For\nthe first time, we confirm the existence of scaling laws within the context of\nmotion generation. Specifically, our results demonstrate that the normalized\ntest loss of our prefix autoregressive models adheres to a logarithmic law in\nrelation to compute budgets. Furthermore, we also confirm the power law between\nNon-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect\nto compute budgets respectively. Leveraging the scaling law, we predict the\noptimal transformer size, vocabulary size, and data requirements for a compute\nbudget of $1e18$. The test loss of the system, when trained with the optimal\nmodel size, vocabulary size, and required data, aligns precisely with the\npredicted test loss, thereby validating the scaling law.\n","authors":["Shunlin Lu","Jingbo Wang","Zeyu Lu","Ling-Hao Chen","Wenxun Dai","Junting Dong","Zhiyang Dou","Bo Dai","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.14559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20633v3","updated":"2024-12-19T06:22:05Z","published":"2024-05-31T05:49:37Z","title":"Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust\n  Out-of-Distribution Human Action Detection","summary":"  Human action recognition is crucial in computer vision systems. However, in\nreal-world scenarios, human actions often fall outside the distribution of\ntraining data, requiring a model to both recognize in-distribution (ID) actions\nand reject out-of-distribution (OOD) ones. Despite its importance, there has\nbeen limited research on OOD detection in human actions. Existing works on OOD\ndetection mainly focus on image data with RGB structure, and many methods are\npost-hoc in nature. While these methods are convenient and computationally\nefficient, they often lack sufficient accuracy, fail to consider the exposure\nof OOD samples, and ignore the application in skeleton structure data. To\naddress these challenges, we propose a novel end-to-end skeleton-based model\ncalled Skeleton-OOD, which is committed to improving the effectiveness of OOD\ntasks while ensuring the accuracy of ID recognition. Through extensive\nexperiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400\ndatasets, Skeleton-OOD demonstrates the superior performance of our proposed\napproach compared to state-of-the-art methods. Our findings underscore the\neffectiveness of classic OOD detection techniques in the context of\nskeleton-based action recognition tasks, offering promising avenues for future\nresearch in this field. Code is available at\nhttps://github.com/YilliaJing/Skeleton-OOD.git.\n","authors":["Jing Xu","Anqi Zhu","Jingyu Lin","Qiuhong Ke","Cunjian Chen"],"pdf_url":"https://arxiv.org/pdf/2405.20633v3.pdf","comment":"Accepted by Neurocomputing"},{"id":"http://arxiv.org/abs/2412.14547v1","updated":"2024-12-19T05:55:18Z","published":"2024-12-19T05:55:18Z","title":"Bright-NeRF:Brightening Neural Radiance Field with Color Restoration\n  from Low-light Raw Images","summary":"  Neural Radiance Fields (NeRFs) have demonstrated prominent performance in\nnovel view synthesis. However, their input heavily relies on image acquisition\nunder normal light conditions, making it challenging to learn accurate scene\nrepresentation in low-light environments where images typically exhibit\nsignificant noise and severe color distortion. To address these challenges, we\npropose a novel approach, Bright-NeRF, which learns enhanced and high-quality\nradiance fields from multi-view low-light raw images in an unsupervised manner.\nOur method simultaneously achieves color restoration, denoising, and enhanced\nnovel view synthesis. Specifically, we leverage a physically-inspired model of\nthe sensor's response to illumination and introduce a chromatic adaptation loss\nto constrain the learning of response, enabling consistent color perception of\nobjects regardless of lighting conditions. We further utilize the raw data's\nproperties to expose the scene's intensity automatically. Additionally, we have\ncollected a multi-view low-light raw image dataset to advance research in this\nfield. Experimental results demonstrate that our proposed method significantly\noutperforms existing 2D and 3D approaches. Our code and dataset will be made\npublicly available.\n","authors":["Min Wang","Xin Huang","Guoqing Zhou","Qifeng Guo","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14547v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14546v1","updated":"2024-12-19T05:52:16Z","published":"2024-12-19T05:52:16Z","title":"{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation","summary":"  Small lesions play a critical role in early disease diagnosis and\nintervention of severe infections. Popular models often face challenges in\nsegmenting small lesions, as it occupies only a minor portion of an image,\nwhile down\\_sampling operations may inevitably lose focus on local features of\nsmall lesions. To tackle the challenges, we propose a {\\bf S}mall-{\\bf\nS}ize-{\\bf S}ensitive {\\bf Mamba} ({\\bf S$^3$-Mamba}), which promotes the\nsensitivity to small lesions across three dimensions: channel, spatial, and\ntraining strategy. Specifically, an Enhanced Visual State Space block is\ndesigned to focus on small lesions through multiple residual connections to\npreserve local features, and selectively amplify important details while\nsuppressing irrelevant ones through channel-wise attention. A Tensor-based\nCross-feature Multi-scale Attention is designed to integrate input image\nfeatures and intermediate-layer features with edge features and exploit the\nattentive support of features across multiple scales, thereby retaining spatial\ndetails of small lesions at various granularities. Finally, we introduce a\nnovel regularized curriculum learning to automatically assess lesion size and\nsample difficulty, and gradually focus from easy samples to hard ones like\nsmall lesions. Extensive experiments on three medical image segmentation\ndatasets show the superiority of our S$^3$-Mamba, especially in segmenting\nsmall lesions. Our code is available at\nhttps://github.com/ErinWang2023/S3-Mamba.\n","authors":["Gui Wang","Yuexiang Li","Wenting Chen","Meidan Ding","Wooi Ping Cheah","Rong Qu","Jianfeng Ren","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14546v1.pdf","comment":"Accept by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14545v1","updated":"2024-12-19T05:51:46Z","published":"2024-12-19T05:51:46Z","title":"Summary of Point Transformer with Federated Learning for Predicting\n  Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide\n  Images","summary":"  This study introduces a federated learning-based approach to predict HER2\nstatus from hematoxylin and eosin (HE)-stained whole slide images (WSIs),\nreducing costs and speeding up treatment decisions. To address label imbalance\nand feature representation challenges in multisite datasets, a point\ntransformer is proposed, incorporating dynamic label distribution, an auxiliary\nclassifier, and farthest cosine sampling. Extensive experiments demonstrate\nstate-of-the-art performance across four sites (2687 WSIs) and strong\ngeneralization to two unseen sites (229 WSIs).\n","authors":["Kamorudeen A. Amuda","Almustapha A. Wakili"],"pdf_url":"https://arxiv.org/pdf/2412.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16214v2","updated":"2024-12-19T05:50:06Z","published":"2024-07-23T06:42:55Z","title":"Diff-Shadow: Global-guided Diffusion Model for Shadow Removal","summary":"  We propose Diff-Shadow, a global-guided diffusion model for shadow removal.\nPrevious transformer-based approaches can utilize global information to relate\nshadow and non-shadow regions but are limited in their synthesis ability and\nrecover images with obvious boundaries. In contrast, diffusion-based methods\ncan generate better content but they are not exempt from issues related to\ninconsistent illumination. In this work, we combine the advantages of diffusion\nmodels and global guidance to achieve shadow-free restoration. Specifically, we\npropose a parallel UNets architecture: 1) the local branch performs the\npatch-based noise estimation in the diffusion process, and 2) the global branch\nrecovers the low-resolution shadow-free images. A Reweight Cross Attention\n(RCA) module is designed to integrate global contextual information of\nnon-shadow regions into the local branch. We further design a Global-guided\nSampling Strategy (GSS) that mitigates patch boundary issues and ensures\nconsistent illumination across shaded and unshaded regions in the recovered\nimage. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have\ndemonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art\nmethods, our method achieves a significant improvement in terms of PSNR,\nincreasing from 32.33dB to 33.69dB on the ISTD dataset.\n","authors":["Jinting Luo","Ru Li","Chengzhi Jiang","Xiaoming Zhang","Mingyan Han","Ting Jiang","Haoqiang Fan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.16214v2.pdf","comment":"Proceedings of the 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.08125v2","updated":"2024-12-19T05:46:29Z","published":"2024-12-11T06:21:33Z","title":"Progressive Multi-granular Alignments for Grounded Reasoning in Large\n  Vision-Language Models","summary":"  Existing Large Vision-Language Models (LVLMs) excel at matching concepts\nacross multi-modal inputs but struggle with compositional concepts and\nhigh-level relationships between entities. This paper introduces Progressive\nmulti-granular Vision-Language alignments (PromViL), a novel framework to\nenhance LVLMs' ability in performing grounded compositional visual reasoning\ntasks. Our approach constructs a hierarchical structure of multi-modal\nalignments, ranging from simple to complex concepts. By progressively aligning\ntextual descriptions with corresponding visual regions, our model learns to\nleverage contextual information from lower levels to inform higher-level\nreasoning. To facilitate this learning process, we introduce a data generation\nprocess that creates a novel dataset derived from Visual Genome, providing a\nwide range of nested compositional vision-language pairs. Experimental results\ndemonstrate that our PromViL framework significantly outperforms baselines on\nvarious visual grounding and compositional question answering tasks. The code\nis available at: https://github.com/lqh52/PromViL.\n","authors":["Quang-Hung Le","Long Hoang Dang","Ngan Le","Truyen Tran","Thao Minh Le"],"pdf_url":"https://arxiv.org/pdf/2412.08125v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.15093v1","updated":"2024-12-19T17:43:27Z","published":"2024-12-19T17:43:27Z","title":"Nano-ESG: Extracting Corporate Sustainability Information from News\n  Articles","summary":"  Determining the sustainability impact of companies is a highly complex\nsubject which has garnered more and more attention over the past few years.\nToday, investors largely rely on sustainability-ratings from established\nrating-providers in order to analyze how responsibly a company acts. However,\nthose ratings have recently been criticized for being hard to understand and\nnearly impossible to reproduce.\n  An independent way to find out about the sustainability practices of\ncompanies lies in the rich landscape of news article data. In this paper, we\nexplore a different approach to identify key opportunities and challenges of\ncompanies in the sustainability domain. We present a novel dataset of more than\n840,000 news articles which were gathered for major German companies between\nJanuary 2023 and September 2024. By applying a mixture of Natural Language\nProcessing techniques, we first identify relevant articles, before summarizing\nthem and extracting their sustainability-related sentiment and aspect using\nLarge Language Models (LLMs). Furthermore, we conduct an evaluation of the\nobtained data and determine that the LLM-produced answers are accurate. We\nrelease both datasets at https://github.com/Bailefan/Nano-ESG.\n","authors":["Fabian Billert","Stefan Conrad"],"pdf_url":"https://arxiv.org/pdf/2412.15093v1.pdf","comment":"To be published at ECIR 2025. Preprint"},{"id":"http://arxiv.org/abs/2301.03767v2","updated":"2024-12-19T16:45:52Z","published":"2023-01-10T03:10:32Z","title":"Metric Compatible Training for Online Backfilling in Large-Scale\n  Retrieval","summary":"  Backfilling is the process of re-extracting all gallery embeddings from\nupgraded models in image retrieval systems. It inevitably requires a\nprohibitively large amount of computational cost and even entails the downtime\nof the service. Although backward-compatible learning sidesteps this challenge\nby tackling query-side representations, this leads to suboptimal solutions in\nprinciple because gallery embeddings cannot benefit from model upgrades. We\naddress this dilemma by introducing an online backfilling algorithm, which\nenables us to achieve a progressive performance improvement during the\nbackfilling process while not sacrificing the final performance of new model\nafter the completion of backfilling. To this end, we first propose a simple\ndistance rank merge technique for online backfilling. Then, we incorporate a\nreverse transformation module for more effective and efficient merging, which\nis further enhanced by adopting a metric-compatible contrastive learning\napproach. These two components help to make the distances of old and new models\ncompatible, resulting in desirable merge results during backfilling with no\nextra computational overhead. Extensive experiments show the effectiveness of\nour framework on four standard benchmarks in various settings.\n","authors":["Seonguk Seo","Mustafa Gokhan Uzunbas","Bohyung Han","Sara Cao","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2301.03767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15005v1","updated":"2024-12-19T16:20:42Z","published":"2024-12-19T16:20:42Z","title":"DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start\n  Cross-Domain Recommendation","summary":"  Recommender systems are widely used in various real-world applications, but\nthey often encounter the persistent challenge of the user cold-start problem.\nCross-domain recommendation (CDR), which leverages user interactions from one\ndomain to improve prediction performance in another, has emerged as a promising\nsolution. However, users with similar preferences in the source domain may\nexhibit different interests in the target domain. Therefore, directly\ntransferring embeddings may introduce irrelevant source-domain collaborative\ninformation. In this paper, we propose a novel graph-based disentangled\ncontrastive learning framework to capture fine-grained user intent and filter\nout irrelevant collaborative information, thereby avoiding negative transfer.\nSpecifically, for each domain, we use a multi-channel graph encoder to capture\ndiverse user intents. We then construct the affinity graph in the embedding\nspace and perform multi-step random walks to capture high-order user similarity\nrelationships. Treating one domain as the target, we propose a disentangled\nintent-wise contrastive learning approach, guided by user similarity, to refine\nthe bridging of user intents across domains. Extensive experiments on four\nbenchmark CDR datasets demonstrate that DisCo consistently outperforms existing\nstate-of-the-art baselines, thereby validating the effectiveness of both DisCo\nand its components.\n","authors":["Hourun Li","Yifan Wang","Zhiping Xiao","Jia Yang","Changling Zhou","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2412.15005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14978v1","updated":"2024-12-19T15:53:21Z","published":"2024-12-19T15:53:21Z","title":"Spectrum-based Modality Representation Fusion Graph Convolutional\n  Network for Multimodal Recommendation","summary":"  Incorporating multi-modal features as side information has recently become a\ntrend in recommender systems. To elucidate user-item preferences, recent\nstudies focus on fusing modalities via concatenation, element-wise sum, or\nattention mechanisms. Despite having notable success, existing approaches do\nnot account for the modality-specific noise encapsulated within each modality.\nAs a result, direct fusion of modalities will lead to the amplification of\ncross-modality noise. Moreover, the variation of noise that is unique within\neach modality results in noise alleviation and fusion being more challenging.\nIn this work, we propose a new Spectrum-based Modality Representation (SMORE)\nfusion graph recommender that aims to capture both uni-modal and fusion\npreferences while simultaneously suppressing modality noise. Specifically,\nSMORE projects the multi-modal features into the frequency domain and leverages\nthe spectral space for fusion. To reduce dynamic contamination that is unique\nto each modality, we introduce a filter to attenuate and suppress the modality\nnoise adaptively while capturing the universal modality patterns effectively.\nFurthermore, we explore the item latent structures by designing a new\nmulti-modal graph learning module to capture associative semantic correlations\nand universal fusion patterns among similar items. Finally, we formulate a new\nmodality-aware preference module, which infuses behavioral features and\nbalances the uni- and multi-modal features for precise preference modeling.\nThis empowers SMORE with the ability to infer both user modality-specific and\nfusion preferences more accurately. Experiments on three real-world datasets\nshow the efficacy of our proposed model. The source code for this work has been\nmade publicly available at https://github.com/kennethorq/SMORE.\n","authors":["Rongqing Kenneth Ong","Andy W. H. Khong"],"pdf_url":"https://arxiv.org/pdf/2412.14978v1.pdf","comment":"Accepted to ACM Web Search and Data Mining (WSDM) 2025"},{"id":"http://arxiv.org/abs/2412.14967v1","updated":"2024-12-19T15:45:06Z","published":"2024-12-19T15:45:06Z","title":"ECLIPSE: Contrastive Dimension Importance Estimation with\n  Pseudo-Irrelevance Feedback for Dense Retrieval","summary":"  Recent advances in Information Retrieval have leveraged high-dimensional\nembedding spaces to improve the retrieval of relevant documents. Moreover, the\nManifold Clustering Hypothesis suggests that despite these high-dimensional\nrepresentations, documents relevant to a query reside on a lower-dimensional,\nquery-dependent manifold. While this hypothesis has inspired new retrieval\nmethods, existing approaches still face challenges in effectively separating\nnon-relevant information from relevant signals. We propose a novel methodology\nthat addresses these limitations by leveraging information from both relevant\nand non-relevant documents. Our method, ECLIPSE, computes a centroid based on\nirrelevant documents as a reference to estimate noisy dimensions present in\nrelevant ones, enhancing retrieval performance. Extensive experiments on three\nin-domain and one out-of-domain benchmarks demonstrate an average improvement\nof up to 19.50% (resp. 22.35%) in mAP(AP) and 11.42% (resp. 13.10%) in nDCG@10\nw.r.t. the DIME-based baseline (resp. the baseline using all dimensions). Our\nresults pave the way for more robust, pseudo-irrelevance-based retrieval\nsystems in future IR research.\n","authors":["Giulio D'Erasmo","Giovanni Trappolini","Nicola Tonellotto","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2412.14967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00326v5","updated":"2024-12-19T15:07:38Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v5.pdf","comment":"19 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.00390v2","updated":"2024-12-19T14:28:19Z","published":"2024-02-01T07:22:52Z","title":"DNS-Rec: Data-aware Neural Architecture Search for Recommender Systems","summary":"  In the era of data proliferation, efficiently sifting through vast\ninformation to extract meaningful insights has become increasingly crucial.\nThis paper addresses the computational overhead and resource inefficiency\nprevalent in existing Sequential Recommender Systems (SRSs). We introduce an\ninnovative approach combining pruning methods with advanced model designs.\nFurthermore, we delve into resource-constrained Neural Architecture Search\n(NAS), an emerging technique in recommender systems, to optimize models in\nterms of FLOPs, latency, and energy consumption while maintaining or enhancing\naccuracy. Our principal contribution is the development of a Data-aware Neural\nArchitecture Search for Recommender System (DNS-Rec). DNS-Rec is specifically\ndesigned to tailor compact network architectures for attention-based SRS\nmodels, thereby ensuring accuracy retention. It incorporates data-aware gates\nto enhance the performance of the recommendation network by learning\ninformation from historical user-item interactions. Moreover, DNS-Rec employs a\ndynamic resource constraint strategy, stabilizing the search process and\nyielding more suitable architectural solutions. We demonstrate the\neffectiveness of our approach through rigorous experiments conducted on three\nbenchmark datasets, which highlight the superiority of DNS-Rec in SRSs. Our\nfindings set a new standard for future research in efficient and accurate\nrecommendation systems, marking a significant step forward in this rapidly\nevolving field.\n","authors":["Sheng Zhang","Maolin Wang","Yao Zhao","Chenyi Zhuang","Jinjie Gu","Ruocheng Guo","Xiangyu Zhao","Zijian Zhang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2402.00390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14835v1","updated":"2024-12-19T13:25:39Z","published":"2024-12-19T13:25:39Z","title":"Progressive Multimodal Reasoning via Active Retrieval","summary":"  Multi-step multimodal reasoning tasks pose significant challenges for\nmultimodal large language models (MLLMs), and finding effective ways to enhance\ntheir performance in such scenarios remains an unresolved issue. In this paper,\nwe propose AR-MCTS, a universal framework designed to progressively improve the\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\nTree Search (MCTS). Our approach begins with the development of a unified\nretrieval module that retrieves key supporting insights for solving complex\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\nautomated multimodal reasoning verification, we employ the MCTS algorithm\ncombined with an active retrieval mechanism, which enables the automatic\ngeneration of step-wise annotations. This strategy dynamically retrieves key\ninsights for each reasoning step, moving beyond traditional beam search\nsampling to improve the diversity and reliability of the reasoning space.\nAdditionally, we introduce a process reward model that aligns progressively to\nsupport the automatic verification of multimodal reasoning tasks. Experimental\nresults across three complex multimodal reasoning benchmarks confirm the\neffectiveness of the AR-MCTS framework in enhancing the performance of various\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\nsampling diversity and accuracy, yielding reliable multimodal reasoning.\n","authors":["Guanting Dong","Chenghao Zhang","Mengjie Deng","Yutao Zhu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.14835v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2406.05666v9","updated":"2024-12-19T12:13:26Z","published":"2024-06-09T06:49:22Z","title":"Probability Distribution Learning and Its Application in Deep Learning","summary":"  This paper introduces a novel theoretical learning framework, termed\nprobability distribution learning (PD learning). Departing from the traditional\nstatistical learning framework, PD learning focuses on learning the underlying\nprobability distribution, which is modeled as a random variable within the\nprobability simplex. In this framework, the optimization objective is the\nlearning error, which quantifies the posterior expected discrepancy between the\nmodel's predicted distribution and the underlying true distribution, given\navailable sample data and prior knowledge. To optimize the learning error, this\npaper proposes the necessary conditions for loss functions, models, and\noptimization algorithms, ensuring that these conditions are met in real-world\nmachine learning scenarios. Based on these conditions, the non-convex\noptimization mechanism corresponding to model training can be theoretically\nresolved. Moreover, this paper provides model-dependent and model-independent\nbounds on learning error, offering new insights into the model's fitting and\ngeneralization capabilities. Furthermore, the paper applies the PD learning\nframework to elucidate the mechanisms by which various techniques, including\nrandom parameter initialization, over-parameterization, and dropout, influence\ndeep model training. Finally, the paper substantiates the key conclusions of\nthe proposed framework through experimental results.\n","authors":["Binchuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.05666v9.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2411.04677v3","updated":"2024-12-19T12:08:31Z","published":"2024-11-07T13:03:21Z","title":"Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval","summary":"  A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir.\n","authors":["Ferdinand Schlatt","Maik Fröbe","Matthias Hagen"],"pdf_url":"https://arxiv.org/pdf/2411.04677v3.pdf","comment":"Accepted as a demo at WSDM'25"},{"id":"http://arxiv.org/abs/2412.11216v2","updated":"2024-12-19T08:32:20Z","published":"2024-12-15T15:13:14Z","title":"Distribution-Consistency-Guided Multi-modal Hashing","summary":"  Multi-modal hashing methods have gained popularity due to their fast speed\nand low storage requirements. Among them, the supervised methods demonstrate\nbetter performance by utilizing labels as supervisory signals compared with\nunsupervised methods. Currently, for almost all supervised multi-modal hashing\nmethods, there is a hidden assumption that training sets have no noisy labels.\nHowever, labels are often annotated incorrectly due to manual labeling in\nreal-world scenarios, which will greatly harm the retrieval performance. To\naddress this issue, we first discover a significant distribution consistency\npattern through experiments, i.e., the 1-0 distribution of the presence or\nabsence of each category in the label is consistent with the high-low\ndistribution of similarity scores of the hash codes relative to category\ncenters. Then, inspired by this pattern, we propose a novel\nDistribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to\nfilter and reconstruct noisy labels to enhance retrieval performance.\nSpecifically, the proposed method first randomly initializes several category\ncenters, which are used to compute the high-low distribution of similarity\nscores; Noisy and clean labels are then separately filtered out via the\ndiscovered distribution consistency pattern to mitigate the impact of noisy\nlabels; Subsequently, a correction strategy, which is indirectly designed via\nthe distribution consistency pattern, is applied to the filtered noisy labels,\ncorrecting high-confidence ones while treating low-confidence ones as unlabeled\nfor unsupervised learning, thereby further enhancing the model's performance.\nExtensive experiments on three widely used datasets demonstrate the superiority\nof the proposed method compared to state-of-the-art baselines in multi-modal\nretrieval tasks. The code is available at\nhttps://github.com/LiuJinyu1229/DCGMH.\n","authors":["Jin-Yu Liu","Xian-Ling Mao","Tian-Yi Che","Rong-Cheng Tu"],"pdf_url":"https://arxiv.org/pdf/2412.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12470v2","updated":"2024-12-19T08:26:32Z","published":"2024-08-22T15:10:56Z","title":"DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems","summary":"  The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios.\n","authors":["Jiaju Chen","Chongming Gao","Shuai Yuan","Shuchang Liu","Qingpeng Cai","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.12470v2.pdf","comment":"Accepted by WSDM 2025"},{"id":"http://arxiv.org/abs/2412.14574v1","updated":"2024-12-19T06:44:59Z","published":"2024-12-19T06:44:59Z","title":"Sliding Windows Are Not the End: Exploring Full Ranking with\n  Long-Context Large Language Models","summary":"  Large Language Models (LLMs) have shown exciting performance in listwise\npassage ranking. Due to the limited input length, existing methods often adopt\nthe sliding window strategy. Such a strategy, though effective, is inefficient\nas it involves repetitive and serialized processing, which usually re-evaluates\nrelevant passages multiple times. As a result, it incurs redundant API costs,\nwhich are proportional to the number of inference tokens. The development of\nlong-context LLMs enables the full ranking of all passages within a single\ninference, avoiding redundant API costs. In this paper, we conduct a\ncomprehensive study of long-context LLMs for ranking tasks in terms of\nefficiency and effectiveness. Surprisingly, our experiments reveal that full\nranking with long-context LLMs can deliver superior performance in the\nsupervised fine-tuning setting with a huge efficiency improvement. Furthermore,\nwe identify two limitations of fine-tuning the full ranking model based on\nexisting methods: (1) sliding window strategy fails to produce a full ranking\nlist as a training label, and (2) the language modeling loss cannot emphasize\ntop-ranked passage IDs in the label. To alleviate these issues, we propose a\nnew complete listwise label construction approach and a novel importance-aware\nlearning objective for full ranking. Experiments show the superior performance\nof our method over baselines. Our codes are available at\n\\url{https://github.com/8421BCD/fullrank}.\n","authors":["Wenhan Liu","Xinyu Ma","Yutao Zhu","Ziliang Zhao","Shuaiqiang Wang","Dawei Yin","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2412.14574v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2405.00287v2","updated":"2024-12-19T05:48:08Z","published":"2024-05-01T02:27:59Z","title":"SCONE: A Novel Stochastic Sampling to Generate Contrastive Views and\n  Hard Negative Samples for Recommendation","summary":"  Graph-based collaborative filtering (CF) has emerged as a promising approach\nin recommender systems. Despite its achievements, graph-based CF models face\nchallenges due to data sparsity and negative sampling. In this paper, we\npropose a novel Stochastic sampling for i) COntrastive views and ii) hard\nNEgative samples (SCONE) to overcome these issues. SCONE generates dynamic\naugmented views and diverse hard negative samples via a unified stochastic\nsampling approach based on score-based generative models. Our extensive\nexperiments on 6 benchmark datasets show that SCONE consistently outperforms\nstate-of-the-art baselines. SCONE shows efficacy in addressing user sparsity\nand item popularity issues, while enhancing performance for both cold-start\nusers and long-tail items. Furthermore, our approach improves the diversity of\nthe recommendation and the uniformity of the representations. The code is\navailable at https://github.com/jeongwhanchoi/SCONE.\n","authors":["Chaejeong Lee","Jeongwhan Choi","Hyowon Wi","Sung-Bae Cho","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2405.00287v2.pdf","comment":"Accepted to WSDM 2025. Chaejeong Lee and Jeongwhan Choi are co-first\n  authors with equal contributions"},{"id":"http://arxiv.org/abs/2412.14518v1","updated":"2024-12-19T04:33:22Z","published":"2024-12-19T04:33:22Z","title":"Efficient Self-Supervised Video Hashing with Selective State Spaces","summary":"  Self-supervised video hashing (SSVH) is a practical task in video indexing\nand retrieval. Although Transformers are predominant in SSVH for their\nimpressive temporal modeling capabilities, they often suffer from computational\nand memory inefficiencies. Drawing inspiration from Mamba, an advanced\nstate-space model, we explore its potential in SSVH to achieve a better balance\nbetween efficacy and efficiency. We introduce S5VH, a Mamba-based video hashing\nmodel with an improved self-supervised learning paradigm. Specifically, we\ndesign bidirectional Mamba layers for both the encoder and decoder, which are\neffective and efficient in capturing temporal relationships thanks to the\ndata-dependent selective scanning mechanism with linear complexity. In our\nlearning strategy, we transform global semantics in the feature space into\nsemantically consistent and discriminative hash centers, followed by a center\nalignment loss as a global learning signal. Our self-local-global (SLG)\nparadigm significantly improves learning efficiency, leading to faster and\nbetter convergence. Extensive experiments demonstrate S5VH's improvements over\nstate-of-the-art methods, superior transferability, and scalable advantages in\ninference efficiency. Code is available at\nhttps://github.com/gimpong/AAAI25-S5VH.\n","authors":["Jinpeng Wang","Niu Lian","Jun Li","Yuting Wang","Yan Feng","Bin Chen","Yongbing Zhang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2412.14518v1.pdf","comment":"Accepted by AAAI'25. 9 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.14486v1","updated":"2024-12-19T03:19:18Z","published":"2024-12-19T03:19:18Z","title":"Moving Beyond LDA: A Comparison of Unsupervised Topic Modelling\n  Techniques for Qualitative Data Analysis of Online Communities","summary":"  Social media constitutes a rich and influential source of information for\nqualitative researchers. Although computational techniques like topic modelling\nassist with managing the volume and diversity of social media content,\nqualitative researcher's lack of programming expertise creates a significant\nbarrier to their adoption. In this paper we explore how BERTopic, an advanced\nLarge Language Model (LLM)-based topic modelling technique, can support\nqualitative data analysis of social media. We conducted interviews and hands-on\nevaluations in which qualitative researchers compared topics from three\nmodelling techniques: LDA, NMF, and BERTopic. BERTopic was favoured by 8 of 12\nparticipants for its ability to provide detailed, coherent clusters for deeper\nunderstanding and actionable insights. Participants also prioritised topic\nrelevance, logical organisation, and the capacity to reveal unexpected\nrelationships within the data. Our findings underscore the potential of\nLLM-based techniques for supporting qualitative analysis.\n","authors":["Amandeep Kaur","James R. Wallace"],"pdf_url":"https://arxiv.org/pdf/2412.14486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14476v1","updated":"2024-12-19T02:57:02Z","published":"2024-12-19T02:57:02Z","title":"HEC-GCN: Hypergraph Enhanced Cascading Graph Convolution Network for\n  Multi-Behavior Recommendation","summary":"  Multi-behavior recommendation (MBR) has garnered growing attention recently\ndue to its ability to mitigate the sparsity issue by inferring user preferences\nfrom various auxiliary behaviors to improve predictions for the target\nbehavior. Although existing research on MBR has yielded impressive results,\nthey still face two major limitations. First, previous methods mainly focus on\nmodeling fine-grained interaction information between users and items under\neach behavior, which may suffer from sparsity issue. Second, existing models\nusually concentrate on exploiting dependencies between two consecutive\nbehaviors, leaving intra- and inter-behavior consistency largely unexplored. To\nthe end, we propose a novel approach named Hypergraph Enhanced Cascading Graph\nConvolution Network for multi-behavior recommendation (HEC-GCN). To be\nspecific, we first explore both fine- and coarse-grained correlations among\nusers or items of each behavior by simultaneously modeling the\nbehavior-specific interaction graph and its corresponding hypergraph in a\ncascaded manner. Then, we propose a behavior consistency-guided alignment\nstrategy that ensures consistent representations between the interaction graph\nand its associated hypergraph for each behavior, while also maintaining\nrepresentation consistency across different behaviors. Extensive experiments\nand analyses on three public benchmark datasets demonstrate that our proposed\napproach is consistently superior to previous state-of-the-art methods due to\nits capability to effectively attenuate the sparsity issue as well as preserve\nboth intra- and inter-behavior consistencies. The code is available at\nhttps://github.com/marqu22/HEC-GCN.git.\n","authors":["Yabo Yin","Xiaofei Zhu","Wenshan Wang","Yihao Zhang","Pengfei Wang","Yixing Fan","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2412.14476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14768v3","updated":"2024-12-19T02:18:54Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14457v1","updated":"2024-12-19T02:17:35Z","published":"2024-12-19T02:17:35Z","title":"VISA: Retrieval Augmented Generation with Visual Source Attribution","summary":"  Generation with source attribution is important for enhancing the\nverifiability of retrieval-augmented generation (RAG) systems. However,\nexisting approaches in RAG primarily link generated content to document-level\nreferences, making it challenging for users to locate evidence among multiple\ncontent-rich retrieved documents. To address this challenge, we propose\nRetrieval-Augmented Generation with Visual Source Attribution (VISA), a novel\napproach that combines answer generation with visual source attribution.\nLeveraging large vision-language models (VLMs), VISA identifies the evidence\nand highlights the exact regions that support the generated answers with\nbounding boxes in the retrieved document screenshots. To evaluate its\neffectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia\nwebpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the\nmedical domain. Experimental results demonstrate the effectiveness of VISA for\nvisual source attribution on documents' original look, as well as highlighting\nthe challenges for improvement. Code, data, and model checkpoints will be\nreleased.\n","authors":["Xueguang Ma","Shengyao Zhuang","Bevan Koopman","Guido Zuccon","Wenhu Chen","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2412.14457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v3","updated":"2024-12-19T02:10:00Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v3.pdf","comment":"NeurIPS 2024, 26 pages"},{"id":"http://arxiv.org/abs/2412.14454v1","updated":"2024-12-19T02:09:59Z","published":"2024-12-19T02:09:59Z","title":"Are Longer Prompts Always Better? Prompt Selection in Large Language\n  Models for Recommendation Systems","summary":"  In large language models (LLM)-based recommendation systems (LLM-RSs),\naccurately predicting user preferences by leveraging the general knowledge of\nLLMs is possible without requiring extensive training data. By converting\nrecommendation tasks into natural language inputs called prompts, LLM-RSs can\nefficiently solve issues that have been difficult to address due to data\nscarcity but are crucial in applications such as cold-start and cross-domain\nproblems. However, when applying this in practice, selecting the prompt that\nmatches tasks and data is essential. Although numerous prompts have been\nproposed in LLM-RSs and representing the target user in prompts significantly\nimpacts recommendation accuracy, there are still no clear guidelines for\nselecting specific prompts.\n  In this paper, we categorize and analyze prompts from previous research to\nestablish practical prompt selection guidelines. Through 450 experiments with\n90 prompts and five real-world datasets, we examined the relationship between\nprompts and dataset characteristics in recommendation accuracy. We found that\nno single prompt consistently outperforms others; thus, selecting prompts on\nthe basis of dataset characteristics is crucial. Here, we propose a prompt\nselection method that achieves higher accuracy with minimal validation data.\nBecause increasing the number of prompts to explore raises costs, we also\nintroduce a cost-efficient strategy using high-performance and cost-efficient\nLLMs, significantly reducing exploration costs while maintaining high\nprediction accuracy. Our work offers valuable insights into the prompt\nselection, advancing accurate and efficient LLM-RSs.\n","authors":["Genki Kusano","Kosuke Akimoto","Kunihiro Takeoka"],"pdf_url":"https://arxiv.org/pdf/2412.14454v1.pdf","comment":"15 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2412.15212v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Scaling 4D Representations","summary":"  Scaling has not yet been convincingly demonstrated for pure self-supervised\nlearning from video. However, prior work has focused evaluations on\nsemantic-related tasks $\\unicode{x2013}$ action classification, ImageNet\nclassification, etc. In this paper we focus on evaluating self-supervised\nlearning on non-semantic vision tasks that are more spatial (3D) and temporal\n(+1D = 4D), such as camera pose estimation, point and object tracking, and\ndepth estimation. We show that by learning from very large video datasets,\nmasked auto-encoding (MAE) with transformer video models actually scales,\nconsistently improving performance on these 4D tasks, as model size increases\nfrom 20M all the way to the largest by far reported self-supervised video model\n$\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with\nmany recent image and video models demonstrates the benefits of scaling 4D\nrepresentations.\n","authors":["João Carreira","Dilara Gokay","Michael King","Chuhan Zhang","Ignacio Rocco","Aravindh Mahendran","Thomas Albert Keck","Joseph Heyward","Skanda Koppula","Etienne Pot","Goker Erdogan","Yana Hasson","Yi Yang","Klaus Greff","Guillaume Le Moing","Sjoerd van Steenkiste","Daniel Zoran","Drew A. Hudson","Pedro Vélez","Luisa Polanía","Luke Friedman","Chris Duvarney","Ross Goroshin","Kelsey Allen","Jacob Walker","Rishabh Kabra","Eric Aboussouan","Jennifer Sun","Thomas Kipf","Carl Doersch","Viorica Pătrăucean","Dima Damen","Pauline Luc","Mehdi S. M. Sajjadi","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2412.15212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15209v1","updated":"2024-12-19T18:59:44Z","published":"2024-12-19T18:59:44Z","title":"PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation","summary":"  Despite significant advancements in Large Vision-Language Models (LVLMs),\nexisting pixel-grounding models operate on single-image settings, limiting\ntheir ability to perform detailed, fine-grained comparisons across multiple\nimages. Conversely, current multi-image understanding models lack pixel-level\ngrounding. Our work addresses this gap by introducing the task of multi-image\npixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates\npixel-level grounding with robust multi-image reasoning capabilities to produce\ncontextually rich, pixel-grounded explanations. Central to PRIMA is an\nefficient vision module that queries fine-grained visual representations across\nmultiple images, reducing TFLOPs by $25.3\\%$. To support training and\nevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark\nconsisting of $\\sim$224K question-answer pairs that require fine-grained visual\nunderstanding across multiple images. Experimental results demonstrate PRIMA\noutperforms state-of-the-art baselines.\n","authors":["Muntasir Wahed","Kiet A. Nguyen","Adheesh Sunil Juvekar","Xinzhuo Li","Xiaona Zhou","Vedant Shah","Tianjiao Yu","Pinar Yanardag","Ismini Lourentzou"],"pdf_url":"https://arxiv.org/pdf/2412.15209v1.pdf","comment":"Project page: https://plan-lab.github.io/prima"},{"id":"http://arxiv.org/abs/2412.15208v1","updated":"2024-12-19T18:59:40Z","published":"2024-12-19T18:59:40Z","title":"OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving","summary":"  Since the advent of Multimodal Large Language Models (MLLMs), they have made\na significant impact across a wide range of real-world applications,\nparticularly in Autonomous Driving (AD). Their ability to process complex\nvisual data and reason about intricate driving scenarios has paved the way for\na new paradigm in end-to-end AD systems. However, the progress of developing\nend-to-end models for AD has been slow, as existing fine-tuning methods demand\nsubstantial resources, including extensive computational power, large-scale\ndatasets, and significant funding. Drawing inspiration from recent advancements\nin inference computing, we propose OpenEMMA, an open-source end-to-end\nframework based on MLLMs. By incorporating the Chain-of-Thought reasoning\nprocess, OpenEMMA achieves significant improvements compared to the baseline\nwhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates\neffectiveness, generalizability, and robustness across a variety of challenging\ndriving scenarios, offering a more efficient and effective approach to\nautonomous driving. We release all the codes in\nhttps://github.com/taco-group/OpenEMMA.\n","authors":["Shuo Xing","Chengyuan Qian","Yuping Wang","Hongyuan Hua","Kexin Tian","Yang Zhou","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15206v1","updated":"2024-12-19T18:59:33Z","published":"2024-12-19T18:59:33Z","title":"AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models\n  for Autonomous Driving","summary":"  Recent advancements in large vision language models (VLMs) tailored for\nautonomous driving (AD) have shown strong scene understanding and reasoning\ncapabilities, making them undeniable candidates for end-to-end driving systems.\nHowever, limited work exists on studying the trustworthiness of DriveVLMs -- a\ncritical factor that directly impacts public transportation safety. In this\npaper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for\nlarge vision-language models in autonomous driving (DriveVLMs), considering\ndiverse perspectives -- including trustfulness, safety, robustness, privacy,\nand fairness. We constructed the largest visual question-answering dataset for\ninvestigating trustworthiness issues in driving scenarios, comprising over 10k\nunique scenes and 18k queries. We evaluated six publicly available VLMs,\nspanning from generalist to specialist, from open-source to commercial models.\nOur exhaustive evaluations have unveiled previously undiscovered\nvulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found\nthat the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform\nspecialized models fine-tuned for driving in terms of overall trustworthiness.\nDriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing\nsensitive information. Additionally, both generalist and specialist VLMs remain\nsusceptible to adversarial attacks and struggle to ensure unbiased\ndecision-making across diverse environments and populations. Our findings call\nfor immediate and decisive action to address the trustworthiness of DriveVLMs\n-- an issue of critical importance to public safety and the welfare of all\ncitizens relying on autonomous transportation systems. Our benchmark is\npublicly available at \\url{https://github.com/taco-group/AutoTrust}, and the\nleaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.\n","authors":["Shuo Xing","Hongyuan Hua","Xiangbo Gao","Shenzhe Zhu","Renjie Li","Kexin Tian","Xiaopeng Li","Heng Huang","Tianbao Yang","Zhangyang Wang","Yang Zhou","Huaxiu Yao","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15206v1.pdf","comment":"55 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.15199v1","updated":"2024-12-19T18:58:36Z","published":"2024-12-19T18:58:36Z","title":"LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation","summary":"  This paper targets the challenge of real-time LiDAR re-simulation in dynamic\ndriving scenarios. Recent approaches utilize neural radiance fields combined\nwith the physical modeling of LiDAR sensors to achieve high-fidelity\nre-simulation results. Unfortunately, these methods face limitations due to\nhigh computational demands in large-scale scenes and cannot perform real-time\nLiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel\nframework that supports real-time, physically accurate LiDAR re-simulation for\ndriving scenes. Our primary contribution is the development of an efficient and\neffective rendering pipeline, which integrates Gaussian primitives and\nhardware-accelerated ray tracing technology. Specifically, we model the\nphysical properties of LiDAR sensors using Gaussian primitives with learnable\nparameters and incorporate scene graphs to handle scene dynamics. Building upon\nthis scene representation, our framework first constructs a bounding volume\nhierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views\nthrough a differentiable rendering algorithm. Importantly, our framework\nsupports realistic rendering with flexible scene editing operations and various\nsensor configurations. Extensive experiments across multiple public benchmarks\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nrendering quality and efficiency. Our project page is at\nhttps://zju3dv.github.io/lidar-rt.\n","authors":["Chenxu Zhou","Lvchang Fu","Sida Peng","Yunzhi Yan","Zhanhua Zhang","Yong Chen","Jiazhi Xia","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15199v1.pdf","comment":"Project page: https://zju3dv.github.io/lidar-rt"},{"id":"http://arxiv.org/abs/2412.15195v1","updated":"2024-12-19T18:58:14Z","published":"2024-12-19T18:58:14Z","title":"Preventing Local Pitfalls in Vector Quantization via Optimal Transport","summary":"  Vector-quantized networks (VQNs) have exhibited remarkable performance across\nvarious tasks, yet they are prone to training instability, which complicates\nthe training process due to the necessity for techniques such as subtle\ninitialization and model distillation. In this study, we identify the local\nminima issue as the primary cause of this instability. To address this, we\nintegrate an optimal transport method in place of the nearest neighbor search\nto achieve a more globally informed assignment. We introduce OptVQ, a novel\nvector quantization method that employs the Sinkhorn algorithm to optimize the\noptimal transport problem, thereby enhancing the stability and efficiency of\nthe training process. To mitigate the influence of diverse data distributions\non the Sinkhorn algorithm, we implement a straightforward yet effective\nnormalization strategy. Our comprehensive experiments on image reconstruction\ntasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses\ncurrent state-of-the-art VQNs in reconstruction quality.\n","authors":["Borui Zhang","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.15195v1.pdf","comment":"Code is available at https://github.com/zbr17/OptVQ"},{"id":"http://arxiv.org/abs/2412.15191v1","updated":"2024-12-19T18:57:21Z","published":"2024-12-19T18:57:21Z","title":"AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation","summary":"  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video\ngeneration that leverages the activations of frozen video and audio diffusion\nmodels for temporally-aligned cross-modal conditioning. The key to our\nframework is a Fusion Block that enables bidirectional information exchange\nbetween our backbone video and audio diffusion models through a\ntemporally-aligned self attention operation. Unlike prior work that uses\nfeature extractors pretrained for other tasks for the conditioning signal,\nAV-Link can directly leverage features obtained by the complementary modality\nin a single framework i.e. video features to generate audio, or audio features\nto generate video. We extensively evaluate our design choices and demonstrate\nthe ability of our method to achieve synchronized and high-quality audiovisual\ncontent, showcasing its potential for applications in immersive media\ngeneration. Project Page: snap-research.github.io/AVLink/\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Ivan Skorokhodov","Alper Canberk","Kwot Sin Lee","Vicente Ordonez","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2412.15191v1.pdf","comment":"Project Page: snap-research.github.io/AVLink/"},{"id":"http://arxiv.org/abs/2412.15188v1","updated":"2024-12-19T18:56:24Z","published":"2024-12-19T18:56:24Z","title":"LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation","summary":"  We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.\n","authors":["Weijia Shi","Xiaochuang Han","Chunting Zhou","Weixin Liang","Xi Victoria Lin","Luke Zettlemoyer","Lili Yu"],"pdf_url":"https://arxiv.org/pdf/2412.15188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15184v1","updated":"2024-12-19T18:55:17Z","published":"2024-12-19T18:55:17Z","title":"Data for Mathematical Copilots: Better Ways of Presenting Proofs for\n  Machine Learning","summary":"  The suite of datasets commonly used to train and evaluate the mathematical\ncapabilities of AI-based mathematical copilots (primarily large language\nmodels) exhibit several shortcomings. These limitations include a restricted\nscope of mathematical complexity, typically not exceeding lower\nundergraduate-level mathematics, binary rating protocols and other issues,\nwhich makes comprehensive proof-based evaluation suites difficult. We\nsystematically explore these limitations and contend that enhancing the\ncapabilities of large language models, or any forthcoming advancements in\nAI-based mathematical assistants (copilots or \"thought partners\"), necessitates\na paradigm shift in the design of mathematical datasets and the evaluation\ncriteria of mathematical ability: It is necessary to move away from\nresult-based datasets (theorem statement to theorem proof) and convert the rich\nfacets of mathematical research practice to data LLMs can train on. Examples of\nthese are mathematical workflows (sequences of atomic, potentially\nsubfield-dependent tasks that are often performed when creating new\nmathematics), which are an important part of the proof-discovery process.\nAdditionally, we advocate for mathematical dataset developers to consider the\nconcept of \"motivated proof\", introduced by G. P\\'olya in 1949, which can serve\nas a blueprint for datasets that offer a better proof learning signal,\nalleviating some of the mentioned limitations. Lastly, we introduce math\ndatasheets for datasets, extending the general, dataset-agnostic variants of\ndatasheets: We provide a questionnaire designed specifically for math datasets\nthat we urge dataset creators to include with their datasets. This will make\ncreators aware of potential limitations of their datasets while at the same\ntime making it easy for readers to assess it from the point of view of training\nand evaluating mathematical copilots.\n","authors":["Simon Frieder","Jonas Bayer","Katherine M. Collins","Julius Berner","Jacob Loader","András Juhász","Fabian Ruehle","Sean Welleck","Gabriel Poesia","Ryan-Rhys Griffiths","Adrian Weller","Anirudh Goyal","Thomas Lukasiewicz","Timothy Gowers"],"pdf_url":"https://arxiv.org/pdf/2412.15184v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2412.15182v1","updated":"2024-12-19T18:54:06Z","published":"2024-12-19T18:54:06Z","title":"STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning","summary":"  Robot learning is witnessing a significant increase in the size, diversity,\nand complexity of pre-collected datasets, mirroring trends in domains such as\nnatural language processing and computer vision. Many robot learning methods\ntreat such datasets as multi-task expert data and learn a multi-task,\ngeneralist policy by training broadly across them. Notably, while these\ngeneralist policies can improve the average performance across many tasks, the\nperformance of generalist policies on any one task is often suboptimal due to\nnegative transfer between partitions of the data, compared to task-specific\nspecialist policies. In this work, we argue for the paradigm of training\npolicies during deployment given the scenarios they encounter: rather than\ndeploying pre-trained policies to unseen problems in a zero-shot manner, we\nnon-parametrically retrieve and train models directly on relevant data at test\ntime. Furthermore, we show that many robotics tasks share considerable amounts\nof low-level behaviors and that retrieval at the \"sub\"-trajectory granularity\nenables significantly improved data utilization, generalization, and robustness\nin adapting policies to novel problems. In contrast, existing full-trajectory\nretrieval methods tend to underutilize the data and miss out on shared\ncross-task content. This work proposes STRAP, a technique for leveraging\npre-trained vision foundation models and dynamic time warping to retrieve\nsub-sequences of trajectories from large training corpora in a robust fashion.\nSTRAP outperforms both prior retrieval algorithms and multi-task learning\nmethods in simulated and real experiments, showing the ability to scale to much\nlarger offline datasets in the real world as well as the ability to learn\nrobust control policies with just a handful of real-world demonstrations.\n","authors":["Marius Memmel","Jacob Berg","Bingqing Chen","Abhishek Gupta","Jonathan Francis"],"pdf_url":"https://arxiv.org/pdf/2412.15182v1.pdf","comment":"Project website at https://weirdlabuw.github.io/strap/"},{"id":"http://arxiv.org/abs/2412.15178v1","updated":"2024-12-19T18:52:05Z","published":"2024-12-19T18:52:05Z","title":"HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages","summary":"  Large Language Model (LLM) based coding tools have been tremendously\nsuccessful as software development assistants, yet they are often designed for\ngeneral purpose programming tasks and perform poorly for more specialized\ndomains such as high performance computing. Creating specialized models and\ntools for these domains is crucial towards gaining the benefits of LLMs in\nareas such as HPC. While previous work has explored HPC-specific models, LLMs\nstill struggle to generate parallel code and it is not at all clear what\nhurdles are still holding back these LLMs and what must be done to overcome\nthem. In this work, we conduct an in-depth study along the many axes of\nfine-tuning a specialized HPC LLM in order to better understand the challenges.\nBased on our findings we fine-tune and evaluate a specialized HPC LLM that is\nshown to be the best performing open-source code LLM for parallel code\ngeneration to date.\n","authors":["Aman Chaturvedi","Daniel Nichols","Siddharth Singh","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2412.15178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15176v1","updated":"2024-12-19T18:51:06Z","published":"2024-12-19T18:51:06Z","title":"Rethinking Uncertainty Estimation in Natural Language Generation","summary":"  Large Language Models (LLMs) are increasingly employed in real-world\napplications, driving the need to evaluate the trustworthiness of their\ngenerated text. To this end, reliable uncertainty estimation is essential.\nSince current LLMs generate text autoregressively through a stochastic process,\nthe same prompt can lead to varying outputs. Consequently, leading uncertainty\nestimation methods generate and analyze multiple output sequences to determine\nthe LLM's uncertainty. However, generating output sequences is computationally\nexpensive, making these methods impractical at scale. In this work, we inspect\nthe theoretical foundations of the leading methods and explore new directions\nto enhance their computational efficiency. Building on the framework of proper\nscoring rules, we find that the negative log-likelihood of the most likely\noutput sequence constitutes a theoretically grounded uncertainty measure. To\napproximate this alternative measure, we propose G-NLL, which has the advantage\nof being obtained using only a single output sequence generated by greedy\ndecoding. This makes uncertainty estimation more efficient and straightforward,\nwhile preserving theoretical rigor. Empirical results demonstrate that G-NLL\nachieves state-of-the-art performance across various LLMs and tasks. Our work\nlays the foundation for efficient and reliable uncertainty estimation in\nnatural language generation, challenging the necessity of more computationally\ninvolved methods currently leading the field.\n","authors":["Lukas Aichberger","Kajetan Schweighofer","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2412.15176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18479v2","updated":"2024-12-19T18:49:00Z","published":"2024-11-27T16:22:33Z","title":"SoK: Watermarking for AI-Generated Content","summary":"  As the outputs of generative AI (GenAI) techniques improve in quality, it\nbecomes increasingly challenging to distinguish them from human-created\ncontent. Watermarking schemes are a promising approach to address the problem\nof distinguishing between AI and human-generated content. These schemes embed\nhidden signals within AI-generated content to enable reliable detection. While\nwatermarking is not a silver bullet for addressing all risks associated with\nGenAI, it can play a crucial role in enhancing AI safety and trustworthiness by\ncombating misinformation and deception. This paper presents a comprehensive\noverview of watermarking techniques for GenAI, beginning with the need for\nwatermarking from historical and regulatory perspectives. We formalize the\ndefinitions and desired properties of watermarking schemes and examine the key\nobjectives and threat models for existing approaches. Practical evaluation\nstrategies are also explored, providing insights into the development of robust\nwatermarking techniques capable of resisting various attacks. Additionally, we\nreview recent representative works, highlight open challenges, and discuss\npotential directions for this emerging field. By offering a thorough\nunderstanding of watermarking in GenAI, this work aims to guide researchers in\nadvancing watermarking methods and applications, and support policymakers in\naddressing the broader implications of GenAI.\n","authors":["Xuandong Zhao","Sam Gunn","Miranda Christ","Jaiden Fairoze","Andres Fabrega","Nicholas Carlini","Sanjam Garg","Sanghyun Hong","Milad Nasr","Florian Tramer","Somesh Jha","Lei Li","Yu-Xiang Wang","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2411.18479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06289v3","updated":"2024-12-19T18:47:54Z","published":"2024-12-09T08:24:11Z","title":"S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity","summary":"  Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents forgetting\nwhile simplifying optimization, delivers SOTA performance on both commonsense\nand arithmetic reasoning with 4.6% and 1.3% average improvements compared to\nLoRA, and surpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models.\n","authors":["Xinyu Yang","Jixuan Leng","Geyang Guo","Jiawei Zhao","Ryumei Nakada","Linjun Zhang","Huaxiu Yao","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2412.06289v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15163v1","updated":"2024-12-19T18:38:13Z","published":"2024-12-19T18:38:13Z","title":"Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents","summary":"  Social norms are standards of behaviour common in a society. However, when\nagents make decisions without considering how others are impacted, norms can\nemerge that lead to the subjugation of certain agents. We present RAWL-E, a\nmethod to create ethical norm-learning agents. RAWL-E agents operationalise\nmaximin, a fairness principle from Rawlsian ethics, in their decision-making\nprocesses to promote ethical norms by balancing societal well-being with\nindividual goals. We evaluate RAWL-E agents in simulated harvesting scenarios.\nWe find that norms emerging in RAWL-E agent societies enhance social welfare,\nfairness, and robustness, and yield higher minimum experience compared to those\nthat emerge in agent societies that do not implement Rawlsian ethics.\n","authors":["Jessica Woodgate","Paul Marshall","Nirav Ajmeri"],"pdf_url":"https://arxiv.org/pdf/2412.15163v1.pdf","comment":"14 pages, 7 figures, 8 tables (and supplementary material with\n  reproducibility and additional results), accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15150v1","updated":"2024-12-19T18:28:37Z","published":"2024-12-19T18:28:37Z","title":"Leveraging Color Channel Independence for Improved Unsupervised Object\n  Detection","summary":"  Object-centric architectures can learn to extract distinct object\nrepresentations from visual scenes, enabling downstream applications on the\nobject level. Similarly to autoencoder-based image models, object-centric\napproaches have been trained on the unsupervised reconstruction loss of images\nencoded by RGB color spaces. In our work, we challenge the common assumption\nthat RGB images are the optimal color space for unsupervised learning in\ncomputer vision. We discuss conceptually and empirically that other color\nspaces, such as HSV, bear essential characteristics for object-centric\nrepresentation learning, like robustness to lighting conditions. We further\nshow that models improve when requiring them to predict additional color\nchannels. Specifically, we propose to transform the predicted targets to the\nRGB-S space, which extends RGB with HSV's saturation component and leads to\nmarkedly better reconstruction and disentanglement for five common evaluation\ndatasets. The use of composite color spaces can be implemented with basically\nno computational overhead, is agnostic of the models' architecture, and is\nuniversally applicable across a wide range of visual computing tasks and\ntraining types. The findings of our approach encourage additional\ninvestigations in computer vision tasks beyond object-centric learning.\n","authors":["Bastian Jäckl","Yannick Metz","Udo Schlegel","Daniel A. Keim","Maximilian T. Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.15150v1.pdf","comment":"38 pages incl. references, 16 figures"},{"id":"http://arxiv.org/abs/2412.15129v1","updated":"2024-12-19T18:09:42Z","published":"2024-12-19T18:09:42Z","title":"Jet: A Modern Transformer-Based Normalizing Flow","summary":"  In the past, normalizing generative flows have emerged as a promising class\nof generative models for natural images. This type of model has many modeling\nadvantages: the ability to efficiently compute log-likelihood of the input\ndata, fast generation and simple overall structure. Normalizing flows remained\na topic of active research but later fell out of favor, as visual quality of\nthe samples was not competitive with other model classes, such as GANs,\nVQ-VAE-based approaches or diffusion models. In this paper we revisit the\ndesign of the coupling-based normalizing flow models by carefully ablating\nprior design choices and using computational blocks based on the Vision\nTransformer architecture, not convolutional neural networks. As a result, we\nachieve state-of-the-art quantitative and qualitative performance with a much\nsimpler architecture. While the overall visual quality is still behind the\ncurrent state-of-the-art models, we argue that strong normalizing flow models\ncan help advancing research frontier by serving as building components of more\npowerful generative models.\n","authors":["Alexander Kolesnikov","André Susano Pinto","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2412.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15127v1","updated":"2024-12-19T18:08:04Z","published":"2024-12-19T18:08:04Z","title":"Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness","summary":"  The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios.\n","authors":["Haotian Zheng","Jinke Ren","Yushan Sun","Ruichen Zhang","Wenbo Zhang","Zhen Li","Dusit Niyato","Shuguang Cui","Yatong Han"],"pdf_url":"https://arxiv.org/pdf/2412.15127v1.pdf","comment":"12 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2412.15118v1","updated":"2024-12-19T17:59:42Z","published":"2024-12-19T17:59:42Z","title":"Outcome-Refining Process Supervision for Code Generation","summary":"  Large Language Models have demonstrated remarkable capabilities in code\ngeneration, yet they often struggle with complex programming tasks that require\ndeep algorithmic reasoning. While process supervision through learned reward\nmodels shows promise in guiding reasoning steps, it requires expensive training\ndata and suffers from unreliable evaluation. We propose Outcome-Refining\nProcess Supervision, a novel paradigm that treats outcome refinement itself as\nthe process to be supervised. Our framework leverages concrete execution\nsignals to ground the supervision of reasoning steps, while using\ntree-structured exploration to maintain multiple solution trajectories\nsimultaneously. Experiments demonstrate that our approach enables even smaller\nmodels to achieve high success accuracy and performance metrics on competitive\nprogramming tasks, creates more reliable verification than traditional reward\nmodels without requiring training PRMs. Our approach achieves significant\nimprovements across 5 models and 3 datasets: an average of 26.9% increase in\ncorrectness and 42.2% in efficiency. The results suggest that providing\nstructured reasoning space with concrete verification signals is crucial for\nsolving complex programming tasks. We open-source all our code and data at:\nhttps://github.com/zhuohaoyu/ORPS\n","authors":["Zhuohao Yu","Weizheng Gu","Yidong Wang","Zhengran Zeng","Jindong Wang","Wei Ye","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15118v1.pdf","comment":"18 pages, 5 figures, Code: https://github.com/zhuohaoyu/ORPS"},{"id":"http://arxiv.org/abs/2409.18472v2","updated":"2024-12-19T17:57:43Z","published":"2024-09-27T06:18:55Z","title":"URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological\n  and Multilingual Knowledge Base","summary":"  URIEL is a knowledge base offering geographical, phylogenetic, and\ntypological vector representations for 7970 languages. It includes distance\nmeasures between these vectors for 4005 languages, which are accessible via the\nlang2vec tool. Despite being frequently cited, URIEL is limited in terms of\nlinguistic inclusion and overall usability. To tackle these challenges, we\nintroduce URIEL+, an enhanced version of URIEL and lang2vec that addresses\nthese limitations. In addition to expanding typological feature coverage for\n2898 languages, URIEL+ improves the user experience with robust, customizable\ndistance calculations to better suit the needs of users. These upgrades also\noffer competitive performance on downstream tasks and provide distances that\nbetter align with linguistic distance studies.\n","authors":["Aditya Khan","Mason Shipton","David Anugraha","Kaiyao Duan","Phuong H. Hoang","Eric Khiu","A. Seza Doğruöz","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2409.18472v2.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.04619v3","updated":"2024-12-19T17:51:34Z","published":"2024-12-05T21:12:37Z","title":"Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization","summary":"  Language models (LMs), like other neural networks, often favor shortcut\nheuristics based on surface-level patterns. Although LMs behave like n-gram\nmodels early in training, they must eventually learn hierarchical syntactic\nrepresentations to correctly apply grammatical rules out-of-distribution (OOD).\nIn this work, we use case studies of English grammar to explore how complex,\ndiverse training data drives models to generalize OOD. We construct a framework\nthat unifies our understanding of random variation with training dynamics, rule\nselection with memorization, and data diversity with complexity. We show that\nthese factors are nuanced, and that intermediate levels of diversity and\ncomplexity lead to inconsistent behavior across random seeds and to unstable\ntraining dynamics. Our findings emphasize the critical role of training data in\nshaping generalization patterns and illuminate how competing model strategies\nlead to inconsistent generalization outcomes across random seeds. Code is\navailable at https://github.com/sunnytqin/concept_comp.git.\n","authors":["Tian Qin","Naomi Saphra","David Alvarez-Melis"],"pdf_url":"https://arxiv.org/pdf/2412.04619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15100v1","updated":"2024-12-19T17:48:03Z","published":"2024-12-19T17:48:03Z","title":"Tests for model misspecification in simulation-based inference: from\n  local distortions to global model checks","summary":"  Model misspecification analysis strategies, such as anomaly detection, model\nvalidation, and model comparison are a key component of scientific model\ndevelopment. Over the last few years, there has been a rapid rise in the use of\nsimulation-based inference (SBI) techniques for Bayesian parameter estimation,\napplied to increasingly complex forward models. To move towards fully\nsimulation-based analysis pipelines, however, there is an urgent need for a\ncomprehensive simulation-based framework for model misspecification analysis.\nIn this work, we provide a solid and flexible foundation for a wide range of\nmodel discrepancy analysis tasks, using distortion-driven model\nmisspecification tests. From a theoretical perspective, we introduce the\nstatistical framework built around performing many hypothesis tests for\ndistortions of the simulation model. We also make explicit analytic connections\nto classical techniques: anomaly detection, model validation, and\ngoodness-of-fit residual analysis. Furthermore, we introduce an efficient\nself-calibrating training algorithm that is useful for practitioners. We\ndemonstrate the performance of the framework in multiple scenarios, making the\nconnection to classical results where they are valid. Finally, we show how to\nconduct such a distortion-driven model misspecification test for real\ngravitational wave data, specifically on the event GW150914.\n","authors":["Noemi Anau Montel","James Alvey","Christoph Weniger"],"pdf_url":"https://arxiv.org/pdf/2412.15100v1.pdf","comment":"11 pages, 5 figures. Code available on github (NoemiAM/mist) at\n  https://github.com/NoemiAM/mist"},{"id":"http://arxiv.org/abs/2412.15095v1","updated":"2024-12-19T17:45:08Z","published":"2024-12-19T17:45:08Z","title":"A Full Transformer-based Framework for Automatic Pain Estimation using\n  Videos","summary":"  The automatic estimation of pain is essential in designing an optimal pain\nmanagement system offering reliable assessment and reducing the suffering of\npatients. In this study, we present a novel full transformer-based framework\nconsisting of a Transformer in Transformer (TNT) model and a Transformer\nleveraging cross-attention and self-attention blocks. Elaborating on videos\nfrom the BioVid database, we demonstrate state-of-the-art performances, showing\nthe efficacy, efficiency, and generalization capability across all the primary\npain estimation tasks.\n","authors":["Stefanos Gkikas","Manolis Tsiknakis"],"pdf_url":"https://arxiv.org/pdf/2412.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15086v1","updated":"2024-12-19T17:33:56Z","published":"2024-12-19T17:33:56Z","title":"Learning Disentangled Equivariant Representation for Explicitly\n  Controllable 3D Molecule Generation","summary":"  We consider the conditional generation of 3D drug-like molecules with\n\\textit{explicit control} over molecular properties such as drug-like\nproperties (e.g., Quantitative Estimate of Druglikeness or Synthetic\nAccessibility score) and effectively binding to specific protein sites. To\ntackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and\nfactorize the latent space of our generative model into two disentangled\naspects: molecular properties and the remaining structural context of 3D\nmolecules. Our model ensures explicit control over these molecular attributes\nwhile maintaining equivariance of coordinate representation and invariance of\ndata likelihood. Furthermore, we introduce a novel alignment-based coordinate\nloss to adapt equivariant networks for auto-regressive de-novo 3D molecule\ngeneration from scratch. Extensive experiments validate our model's\neffectiveness on property-guided and context-guided molecule generation, both\nfor de-novo 3D molecule design and structure-based drug discovery against\nprotein targets.\n","authors":["Haoran Liu","Youzhi Luo","Tianxiao Li","James Caverlee","Martin Renqiang Min"],"pdf_url":"https://arxiv.org/pdf/2412.15086v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15084v1","updated":"2024-12-19T17:29:44Z","published":"2024-12-19T17:29:44Z","title":"AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward\n  Modeling","summary":"  In this paper, we introduce AceMath, a suite of frontier math models that\nexcel in solving complex math problems, along with highly effective reward\nmodels capable of evaluating generated solutions and reliably identifying the\ncorrect ones. To develop the instruction-tuned math models, we propose a\nsupervised fine-tuning (SFT) process that first achieves competitive\nperformance across general domains, followed by targeted fine-tuning for the\nmath domain using a carefully curated set of prompts and synthetically\ngenerated responses. The resulting model, AceMath-72B-Instruct greatly\noutperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop\nmath-specialized reward model, we first construct AceMath-RewardBench, a\ncomprehensive and robust benchmark for evaluating math reward models across\ndiverse problems and difficulty levels. After that, we present a systematic\napproach to build our math reward models. The resulting model, AceMath-72B-RM,\nconsistently outperforms state-of-the-art reward models. Furthermore, when\ncombining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest\naverage rm@8 score across the math reasoning benchmarks. We will release model\nweights, training data, and evaluation benchmarks at:\nhttps://research.nvidia.com/labs/adlr/acemath\n","authors":["Zihan Liu","Yang Chen","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2412.15084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15077v1","updated":"2024-12-19T17:26:07Z","published":"2024-12-19T17:26:07Z","title":"Till the Layers Collapse: Compressing a Deep Neural Network through the\n  Lenses of Batch Normalization Layers","summary":"  Today, deep neural networks are widely used since they can handle a variety\nof complex tasks. Their generality makes them very powerful tools in modern\ntechnology. However, deep neural networks are often overparameterized. The\nusage of these large models consumes a lot of computation resources. In this\npaper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers\n\\textbf{C}ollapse (TLC), which compresses deep neural networks through the\nlenses of batch normalization layers. By reducing the depth of these networks,\nour method decreases deep neural networks' computational requirements and\noverall latency. We validate our method on popular models such as Swin-T,\nMobileNet-V2, and RoBERTa, across both image classification and natural\nlanguage processing (NLP) tasks.\n","authors":["Zhu Liao","Nour Hezbri","Victor Quétu","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2412.15077v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15075v1","updated":"2024-12-19T17:24:15Z","published":"2024-12-19T17:24:15Z","title":"DroughtSet: Understanding Drought Through Spatial-Temporal Learning","summary":"  Drought is one of the most destructive and expensive natural disasters,\nseverely impacting natural resources and risks by depleting water resources and\ndiminishing agricultural yields. Under climate change, accurately predicting\ndrought is critical for mitigating drought-induced risks. However, the\nintricate interplay among the physical and biological drivers that regulate\ndroughts limits the predictability and understanding of drought, particularly\nat a subseasonal to seasonal (S2S) time scale. While deep learning has been\ndemonstrated with potential in addressing climate forecasting challenges, its\napplication to drought prediction has received relatively less attention. In\nthis work, we propose a new dataset, DroughtSet, which integrates relevant\npredictive features and three drought indices from multiple remote sensing and\nreanalysis datasets across the contiguous United States (CONUS). DroughtSet\nspecifically provides the machine learning community with a new real-world\ndataset to benchmark drought prediction models and more generally, time-series\nforecasting methods. Furthermore, we propose a spatial-temporal model SPDrought\nto predict and interpret S2S droughts. Our model learns from the spatial and\ntemporal information of physical and biological features to predict three types\nof droughts simultaneously. Multiple strategies are employed to quantify the\nimportance of physical and biological features for drought prediction. Our\nresults provide insights for researchers to better understand the\npredictability and sensitivity of drought to biological and physical\nconditions. We aim to contribute to the climate field by proposing a new tool\nto predict and understand the occurrence of droughts and provide the AI\ncommunity with a new benchmark to study deep learning applications in climate\nscience.\n","authors":["Xuwei Tan","Qian Zhao","Yanlan Liu","Xueru Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15075v1.pdf","comment":"Accepted by AAAI25"},{"id":"http://arxiv.org/abs/2408.15165v2","updated":"2024-12-19T17:11:11Z","published":"2024-08-27T16:03:18Z","title":"Latent Ewald summation for machine learning of long-range interactions","summary":"  Machine learning interatomic potentials (MLIPs) often neglect long-range\ninteractions, such as electrostatic and dispersion forces. In this work, we\nintroduce a straightforward and efficient method to account for long-range\ninteractions by learning a latent variable from local atomic descriptors and\napplying an Ewald summation to this variable. We demonstrate that in systems\nincluding charged and polar molecular dimers, bulk water, and water-vapor\ninterface, standard short-ranged MLIPs can lead to unphysical predictions even\nwhen employing message passing. The long-range models effectively eliminate\nthese artifacts, with only about twice the computational cost of short-range\nMLIPs.\n","authors":["Bingqing Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.15165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15058v1","updated":"2024-12-19T17:06:53Z","published":"2024-12-19T17:06:53Z","title":"MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging\n  Datasets with In-Context Guidance","summary":"  Medical researchers and clinicians often need to perform novel segmentation\ntasks on a set of related images. Existing methods for segmenting a new dataset\nare either interactive, requiring substantial human effort for each image, or\nrequire an existing set of manually labeled images. We introduce a system,\nMultiverSeg, that enables practitioners to rapidly segment an entire new\ndataset without requiring access to any existing labeled data from that task or\ndomain. Along with the image to segment, the model takes user interactions such\nas clicks, bounding boxes or scribbles as input, and predicts a segmentation.\nAs the user segments more images, those images and segmentations become\nadditional inputs to the model, providing context. As the context set of\nlabeled images grows, the number of interactions required to segment each new\nimage decreases. We demonstrate that MultiverSeg enables users to interactively\nsegment new datasets efficiently, by amortizing the number of interactions per\nimage to achieve an accurate segmentation. Compared to using a state-of-the-art\ninteractive segmentation method, using MultiverSeg reduced the total number of\nscribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images\nfrom unseen tasks. We release code and model weights at\nhttps://multiverseg.csail.mit.edu\n","authors":["Hallee E. Wong","Jose Javier Gonzalez Ortiz","John Guttag","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2412.15058v1.pdf","comment":"Project Website: https://multiverseg.csail.mit.edu Keywords:\n  interactive segmentation, in-context learning, medical image analysis,\n  biomedical imaging, image annotation, visual prompting"},{"id":"http://arxiv.org/abs/2407.17710v2","updated":"2024-12-19T16:48:59Z","published":"2024-07-25T02:05:15Z","title":"Revisiting Machine Unlearning with Dimensional Alignment","summary":"  Machine unlearning, an emerging research topic focusing on compliance with\ndata privacy regulations, enables trained models to remove the information\nlearned from specific data. While many existing methods indirectly address this\nissue by intentionally injecting incorrect supervisions, they can drastically\nand unpredictably alter the decision boundaries and feature spaces, leading to\ntraining instability and undesired side effects. To fundamentally approach this\ntask, we first analyze the changes in latent feature spaces between original\nand retrained models, and observe that the feature representations of samples\nnot involved in training are closely aligned with the feature manifolds of\npreviously seen samples in training. Based on these findings, we introduce a\nnovel evaluation metric for machine unlearning, coined dimensional alignment,\nwhich measures the alignment between the eigenspaces of the forget and retain\nset samples. We employ this metric as a regularizer loss to build a robust and\nstable unlearning framework, which is further enhanced by integrating a\nself-distillation loss and an alternating training scheme. Our framework\neffectively eliminates information from the forget set and preserves knowledge\nfrom the retain set. Lastly, we identify critical flaws in established\nevaluation metrics for machine unlearning, and introduce new evaluation tools\nthat more accurately reflect the fundamental goals of machine unlearning.\n","authors":["Seonguk Seo","Dongwan Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2407.17710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03767v2","updated":"2024-12-19T16:45:52Z","published":"2023-01-10T03:10:32Z","title":"Metric Compatible Training for Online Backfilling in Large-Scale\n  Retrieval","summary":"  Backfilling is the process of re-extracting all gallery embeddings from\nupgraded models in image retrieval systems. It inevitably requires a\nprohibitively large amount of computational cost and even entails the downtime\nof the service. Although backward-compatible learning sidesteps this challenge\nby tackling query-side representations, this leads to suboptimal solutions in\nprinciple because gallery embeddings cannot benefit from model upgrades. We\naddress this dilemma by introducing an online backfilling algorithm, which\nenables us to achieve a progressive performance improvement during the\nbackfilling process while not sacrificing the final performance of new model\nafter the completion of backfilling. To this end, we first propose a simple\ndistance rank merge technique for online backfilling. Then, we incorporate a\nreverse transformation module for more effective and efficient merging, which\nis further enhanced by adopting a metric-compatible contrastive learning\napproach. These two components help to make the distances of old and new models\ncompatible, resulting in desirable merge results during backfilling with no\nextra computational overhead. Extensive experiments show the effectiveness of\nour framework on four standard benchmarks in various settings.\n","authors":["Seonguk Seo","Mustafa Gokhan Uzunbas","Bohyung Han","Sara Cao","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2301.03767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15032v1","updated":"2024-12-19T16:44:01Z","published":"2024-12-19T16:44:01Z","title":"DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT\n  Space","summary":"  This paper explores image modeling from the frequency space and introduces\nDCTdiff, an end-to-end diffusion generative paradigm that efficiently models\nimages in the discrete cosine transform (DCT) space. We investigate the design\nspace of DCTdiff and reveal the key design factors. Experiments on different\nframeworks (UViT, DiT), generation tasks, and various diffusion samplers\ndemonstrate that DCTdiff outperforms pixel-based diffusion models regarding\ngenerative quality and training efficiency. Remarkably, DCTdiff can seamlessly\nscale up to high-resolution generation without using the latent diffusion\nparadigm. Finally, we illustrate several intriguing properties of DCT image\nmodeling. For example, we provide a theoretical proof of why `image diffusion\ncan be seen as spectral autoregression', bridging the gap between diffusion and\nautoregressive models. The effectiveness of DCTdiff and the introduced\nproperties suggest a promising direction for image modeling in the frequency\nspace. The code is at \\url{https://github.com/forever208/DCTdiff}.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Haozhe Jia","Lanmiao Liu","Martin Beneš","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2412.15032v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2406.14742v2","updated":"2024-12-19T16:37:38Z","published":"2024-06-20T21:13:39Z","title":"Latent Variable Sequence Identification for Cognitive Models with Neural\n  Network Estimators","summary":"  Extracting time-varying latent variables from computational cognitive models\nis a key step in model-based neural analysis, which aims to understand the\nneural correlates of cognitive processes. However, existing methods only allow\nresearchers to infer latent variables that explain subjects' behavior in a\nrelatively small class of cognitive models. For example, a broad class of\nrelevant cognitive models with analytically intractable likelihood is currently\nout of reach from standard techniques, based on Maximum a Posteriori parameter\nestimation. Here, we present an approach that extends neural Bayes estimation\nto learn a direct mapping between experimental data and the targeted latent\nvariable space using recurrent neural networks and simulated datasets. We show\nthat our approach achieves competitive performance in inferring latent variable\nsequences in both tractable and intractable models. Furthermore, the approach\nis generalizable across different computational models and is adaptable for\nboth continuous and discrete latent spaces. We then demonstrate its\napplicability in real world datasets. Our work underscores that combining\nrecurrent neural networks and simulation-based inference to identify latent\nvariable sequences can enable researchers to access a wider class of cognitive\nmodels for model-based neural analyses, and thus test a broader set of\ntheories.\n","authors":["Ti-Fen Pan","Jing-Jing Li","Bill Thompson","Anne Collins"],"pdf_url":"https://arxiv.org/pdf/2406.14742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15023v1","updated":"2024-12-19T16:37:19Z","published":"2024-12-19T16:37:19Z","title":"Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and\n  Semantic Controls","summary":"  Sound designers and Foley artists usually sonorize a scene, such as from a\nmovie or video game, by manually annotating and sonorizing each action of\ninterest in the video. In our case, the intent is to leave full creative\ncontrol to sound designers with a tool that allows them to bypass the more\nrepetitive parts of their work, thus being able to focus on the creative\naspects of sound production. We achieve this presenting Stable-V2A, a two-stage\nmodel consisting of: an RMS-Mapper that estimates an envelope representative of\nthe audio characteristics associated with the input video; and Stable-Foley, a\ndiffusion model based on Stable Audio Open that generates audio semantically\nand temporally aligned with the target video. Temporal alignment is guaranteed\nby the use of the envelope as a ControlNet input, while semantic alignment is\nachieved through the use of sound representations chosen by the designer as\ncross-attention conditioning of the diffusion process. We train and test our\nmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. In\naddition, to test our model on a case study of interest, we introduce Walking\nThe Maps, a dataset of videos extracted from video games depicting animated\ncharacters walking in different locations. Samples and code available on our\ndemo page at https://ispamm.github.io/Stable-V2A.\n","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunità","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.15023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14512v3","updated":"2024-12-19T16:37:00Z","published":"2024-08-25T04:32:45Z","title":"LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings","summary":"  Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.\n","authors":["Duo Wang","Yuan Zuo","Fengzhi Li","Junjie Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15557v2","updated":"2024-12-19T16:32:43Z","published":"2024-05-24T13:44:30Z","title":"Learning from Linear Algebra: A Graph Neural Network Approach to\n  Preconditioner Design for Conjugate Gradient Solvers","summary":"  Large linear systems are ubiquitous in modern computational science and\nengineering. The main recipe for solving them is the use of Krylov subspace\niterative methods with well-designed preconditioners. Deep learning models can\nbe used as nonlinear preconditioners during the iteration of linear solvers\nsuch as the conjugate gradient (CG) method. Neural network models require an\nenormous number of parameters to approximate well in this setup. Another\napproach is to take advantage of small graph neural networks (GNNs) to\nconstruct preconditioners with predefined sparsity patterns. Recently, GNNs\nhave been shown to be a promising tool for designing preconditioners to reduce\nthe overall computational cost of iterative methods by constructing them more\nefficiently than with classical linear algebra techniques. However,\npreconditioners designed with these approaches cannot outperform those designed\nwith classical methods in terms of the number of iterations in CG. In our work,\nwe recall well-established preconditioners from linear algebra and use them as\na starting point for training the GNN to obtain preconditioners that reduce the\ncondition number of the system more significantly. Numerical experiments show\nthat our approach outperforms both classical and neural network-based methods\nfor an important class of parametric partial differential equations. We also\nprovide a heuristic justification for the loss function used and show that\npreconditioners obtained by learning with this loss function reduce the\ncondition number in a more desirable way for CG.\n","authors":["Vladislav Trifonov","Alexander Rudikov","Oleg Iliev","Yuri M. Laevsky","Ivan Oseledets","Ekaterina Muravleva"],"pdf_url":"https://arxiv.org/pdf/2405.15557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15010v1","updated":"2024-12-19T16:22:37Z","published":"2024-12-19T16:22:37Z","title":"Robust Federated Learning in the Face of Covariate Shift: A Magnitude\n  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation","summary":"  The development of highly sophisticated neural networks has allowed for fast\nprogress in every field of computer vision, however, applications where\nannotated data is prohibited due to privacy or security concerns remain\nchallenging. Federated Learning (FL) offers a promising framework for\nindividuals aiming to collaboratively develop a shared model while preserving\ndata privacy. Nevertheless, our findings reveal that variations in data\ndistribution among clients can profoundly affect FL methodologies, primarily\ndue to instabilities in the aggregation process. We also propose a novel FL\nframework to mitigate the adverse effects of covariate shifts among federated\nclients by combining individual parameter pruning and regularization techniques\nto improve the robustness of individual clients' models to aggregate. Each\nclient's model is optimized through magnitude-based pruning and the addition of\ndropout and noise injection layers to build more resilient decision pathways in\nthe networks and improve the robustness of the model's parameter aggregation\nstep. The proposed framework is capable of extracting robust representations\neven in the presence of very large covariate shifts among client data\ndistributions and in the federation of a small number of clients. Empirical\nfindings substantiate the effectiveness of our proposed methodology across\ncommon benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.\nFurthermore, we introduce the CelebA-Gender dataset, specifically designed to\nevaluate performance on a more realistic domain. The proposed method is capable\nof extracting robust representations even in the presence of both high and low\ncovariate shifts among client data distributions.\n","authors":["Ozgu Goksu","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2412.15010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15005v1","updated":"2024-12-19T16:20:42Z","published":"2024-12-19T16:20:42Z","title":"DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start\n  Cross-Domain Recommendation","summary":"  Recommender systems are widely used in various real-world applications, but\nthey often encounter the persistent challenge of the user cold-start problem.\nCross-domain recommendation (CDR), which leverages user interactions from one\ndomain to improve prediction performance in another, has emerged as a promising\nsolution. However, users with similar preferences in the source domain may\nexhibit different interests in the target domain. Therefore, directly\ntransferring embeddings may introduce irrelevant source-domain collaborative\ninformation. In this paper, we propose a novel graph-based disentangled\ncontrastive learning framework to capture fine-grained user intent and filter\nout irrelevant collaborative information, thereby avoiding negative transfer.\nSpecifically, for each domain, we use a multi-channel graph encoder to capture\ndiverse user intents. We then construct the affinity graph in the embedding\nspace and perform multi-step random walks to capture high-order user similarity\nrelationships. Treating one domain as the target, we propose a disentangled\nintent-wise contrastive learning approach, guided by user similarity, to refine\nthe bridging of user intents across domains. Extensive experiments on four\nbenchmark CDR datasets demonstrate that DisCo consistently outperforms existing\nstate-of-the-art baselines, thereby validating the effectiveness of both DisCo\nand its components.\n","authors":["Hourun Li","Yifan Wang","Zhiping Xiao","Jia Yang","Changling Zhou","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2412.15005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12012v5","updated":"2024-12-19T16:15:27Z","published":"2024-01-22T14:59:11Z","title":"TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for\n  Lazy Clients","summary":"  Federated learning is a distributed collaborative machine learning paradigm\nthat has gained strong momentum in recent years. In federated learning, a\ncentral server periodically coordinates models with clients and aggregates the\nmodels trained locally by clients without necessitating access to local data.\nDespite its potential, the implementation of federated learning continues to\nencounter several challenges, predominantly the slow convergence that is\nlargely due to data heterogeneity. The slow convergence becomes particularly\nproblematic in cross-device federated learning scenarios where clients may be\nstrongly limited by computing power and storage space, and hence counteracting\nmethods that induce additional computation or memory cost on the client side\nsuch as auxiliary objective terms and larger training iterations can be\nimpractical. In this paper, we propose a novel federated aggregation strategy,\nTurboSVM-FL, that poses no additional computation burden on the client side and\ncan significantly accelerate convergence for federated classification task,\nespecially when clients are \"lazy\" and train their models solely for few epochs\nfor next global aggregation. TurboSVM-FL extensively utilizes support vector\nmachine to conduct selective aggregation and max-margin spread-out\nregularization on class embeddings. We evaluate TurboSVM-FL on multiple\ndatasets including FEMNIST, CelebA, and Shakespeare using user-independent\nvalidation with non-iid data distribution. Our results show that TurboSVM-FL\ncan significantly outperform existing popular algorithms on convergence rate\nand reduce communication rounds while delivering better test metrics including\naccuracy, F1 score, and MCC.\n","authors":["Mengdi Wang","Anna Bodonhelyi","Efe Bozkir","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2401.12012v5.pdf","comment":"Proceedings of the AAAI Conference on Artificial Intelligence 2024\n  (AAAI'24)"},{"id":"http://arxiv.org/abs/2405.08044v2","updated":"2024-12-19T16:08:31Z","published":"2024-05-13T13:55:34Z","title":"Mitigating federated learning contribution allocation instability\n  through randomized aggregation","summary":"  Federated learning (FL) is a collaborative and privacy-preserving Machine\nLearning paradigm, allowing the development of robust models without the need\nto centralise sensitive data. A critical challenge in FL lies in fairly and\naccurately allocating contributions from diverse participants. Inaccurate\nallocation can undermine trust, lead to unfair compensation, and thus\nparticipants may lack the incentive to join or actively contribute to the\nfederation.\n  Various remuneration strategies have been proposed to date, including\nauction-based approaches and Shapley-value based methods, the latter offering a\nmeans to quantify the contribution of each participant. However, little to no\nwork has studied the stability of these contribution evaluation methods.\n  In this paper, we focus on calculating contributions using gradient-based\nmodel reconstruction techniques with Shapley values. We first show that\nbaseline Shapley values do not accurately reflect clients' contributions,\nleading to unstable reward allocations amongst participants in a cross-silo\nfederation. We then introduce \\textsc{FedRandom}, a new method that mitigates\nthese shortcomings with additional data samplings, and show its efficacy at\nincreasing the stability of contribution evaluation in federated learning.\n","authors":["Arno Geimer","Beltran Fiz","Radu State"],"pdf_url":"https://arxiv.org/pdf/2405.08044v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08606v2","updated":"2024-12-19T16:05:31Z","published":"2024-02-13T17:12:01Z","title":"Arbitrary Polynomial Separations in Trainable Quantum Machine Learning","summary":"  Recent theoretical results in quantum machine learning have demonstrated a\ngeneral trade-off between the expressive power of quantum neural networks\n(QNNs) and their trainability; as a corollary of these results, practical\nexponential separations in expressive power over classical machine learning\nmodels are believed to be infeasible as such QNNs take a time to train that is\nexponential in the model size. We here circumvent these negative results by\nconstructing a hierarchy of efficiently trainable QNNs that exhibit\nunconditionally provable, polynomial memory separations of arbitrary constant\ndegree over classical neural networks -- including state-of-the-art models,\nsuch as Transformers -- in performing a classical sequence modeling task. This\nconstruction is also computationally efficient, as each unit cell of the\nintroduced class of QNNs only has constant gate complexity. We show that\ncontextuality -- informally, a quantitative notion of semantic ambiguity -- is\nthe source of the expressivity separation, suggesting that other learning tasks\nwith this property may be a natural setting for the use of quantum learning\nalgorithms.\n","authors":["Eric R. Anschuetz","Xun Gao"],"pdf_url":"https://arxiv.org/pdf/2402.08606v2.pdf","comment":"24 pages, 3 figures, strengthened and simplified results and\n  presentation"},{"id":"http://arxiv.org/abs/2412.14988v1","updated":"2024-12-19T16:00:10Z","published":"2024-12-19T16:00:10Z","title":"Stitch Contrast and Segment_Learning a Human Action Segmentation Model\n  Using Trimmed Skeleton Videos","summary":"  Existing skeleton-based human action classification models rely on\nwell-trimmed action-specific skeleton videos for both training and testing,\nprecluding their scalability to real-world applications where untrimmed videos\nexhibiting concatenated actions are predominant. To overcome this limitation,\nrecently introduced skeleton action segmentation models involve un-trimmed\nskeleton videos into end-to-end training. The model is optimized to provide\nframe-wise predictions for any length of testing videos, simultaneously\nrealizing action localization and classification. Yet, achieving such an\nimprovement im-poses frame-wise annotated skeleton videos, which remains\ntime-consuming in practice. This paper features a novel framework for\nskeleton-based action segmentation trained on short trimmed skeleton videos,\nbut that can run on longer un-trimmed videos. The approach is implemented in\nthree steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral\nskeleton stitching scheme that treats trimmed skeleton videos as elementary\nhuman motions that compose a semantic space and can be sampled to generate\nmulti-action stitched se-quences. Contrast learns contrastive representations\nfrom stitched sequences with a novel discrimination pretext task that enables a\nskeleton encoder to learn meaningful action-temporal contexts to improve action\nsegmentation. Finally, Segment relates the proposed method to action\nsegmentation by learning a segmentation layer while handling particular da-ta\navailability. Experiments involve a trimmed source dataset and an untrimmed\ntarget dataset in an adaptation formulation for real-world skeleton-based human\naction segmentation to evaluate the effectiveness of the proposed method.\n","authors":["Haitao Tian","Pierre Payeur"],"pdf_url":"https://arxiv.org/pdf/2412.14988v1.pdf","comment":"Accepted as AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08941v3","updated":"2024-12-19T15:59:19Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2404.15379v3","updated":"2024-12-19T15:54:17Z","published":"2024-04-23T07:16:13Z","title":"Clustering of timed sequences -- Application to the analysis of care\n  pathways","summary":"  Improving the future of healthcare starts by better understanding the current\nactual practices in hospital settings. This motivates the objective of\ndiscovering typical care pathways from patient data. Revealing typical care\npathways can be achieved through clustering. The difficulty in clustering care\npathways, represented by sequences of timestamped events, lies in defining a\nsemantically appropriate metric and clustering algorithms. In this article, we\nadapt two methods developed for time series to the clustering of timed\nsequences: the drop-DTW metric and the DBA approach for the construction of\naveraged time sequences. These methods are then applied in clustering\nalgorithms to propose original and sound clustering algorithms for timed\nsequences. This approach is experimented with and evaluated on synthetic and\nreal-world data.\n","authors":["Thomas Guyet","Pierre Pinson","Enoal Gesny"],"pdf_url":"https://arxiv.org/pdf/2404.15379v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01420v2","updated":"2024-12-19T15:51:33Z","published":"2024-12-02T12:00:27Z","title":"Task Adaptation of Reinforcement Learning-based NAS Agents through\n  Transfer Learning","summary":"  Recently, a novel paradigm has been proposed for reinforcement learning-based\nNAS agents, that revolves around the incremental improvement of a given\narchitecture. We assess the abilities of such reinforcement learning agents to\ntransfer between different tasks. We perform our evaluation using the\nTrans-NASBench-101 benchmark, and consider the efficacy of the transferred\nagents, as well as how quickly they can be trained. We find that pretraining an\nagent on one task benefits the performance of the agent in another task in all\nbut 1 task when considering final performance. We also show that the training\nprocedure for an agent can be shortened significantly by pretraining it on\nanother task. Our results indicate that these effects occur regardless of the\nsource or target task, although they are more pronounced for some tasks than\nfor others. Our results show that transfer learning can be an effective tool in\nmitigating the computational cost of the initial training procedure for\nreinforcement learning-based NAS agents.\n","authors":["Amber Cassimon","Siegfried Mercelis","Kevin Mets"],"pdf_url":"https://arxiv.org/pdf/2412.01420v2.pdf","comment":"15 Pages, 13 Figures, Corrected data in Figure 5"},{"id":"http://arxiv.org/abs/2412.14964v1","updated":"2024-12-19T15:44:01Z","published":"2024-12-19T15:44:01Z","title":"Knowledge Injection via Prompt Distillation","summary":"  In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights.\n","authors":["Kalle Kujanpää","Harri Valpola","Alexander Ilin"],"pdf_url":"https://arxiv.org/pdf/2412.14964v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.14963v1","updated":"2024-12-19T15:43:05Z","published":"2024-12-19T15:43:05Z","title":"IDOL: Instant Photorealistic 3D Human Creation from a Single Image","summary":"  Creating a high-fidelity, animatable 3D full-body avatar from a single image\nis a challenging task due to the diverse appearance and poses of humans and the\nlimited availability of high-quality training data. To achieve fast and\nhigh-quality human reconstruction, this work rethinks the task from the\nperspectives of dataset, model, and representation. First, we introduce a\nlarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K\ndiverse, photorealistic sets of human images. Each set contains 24-view frames\nin specific human poses, generated using a pose-controllable\nimage-to-multi-view model. Next, leveraging the diversity in views, poses, and\nappearances within HuGe100K, we develop a scalable feed-forward transformer\nmodel to predict a 3D human Gaussian representation in a uniform space from a\ngiven human image. This model is trained to disentangle human pose, body shape,\nclothing geometry, and texture. The estimated Gaussians can be animated without\npost-processing. We conduct comprehensive experiments to validate the\neffectiveness of the proposed dataset and method. Our model demonstrates the\nability to efficiently reconstruct photorealistic humans at 1K resolution from\na single input image using a single GPU instantly. Additionally, it seamlessly\nsupports various applications, as well as shape and texture editing tasks.\n","authors":["Yiyu Zhuang","Jiaxi Lv","Hao Wen","Qing Shuai","Ailing Zeng","Hao Zhu","Shifeng Chen","Yujiu Yang","Xun Cao","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14963v1.pdf","comment":"21 pages, 15 figures, includes main content, supplementary materials,\n  and references"},{"id":"http://arxiv.org/abs/2412.03795v2","updated":"2024-12-19T15:43:00Z","published":"2024-12-05T01:25:34Z","title":"Samudra: An AI Global Ocean Emulator for Climate","summary":"  AI emulators for forecasting have emerged as powerful tools that can\noutperform conventional numerical predictions. The next frontier is to build\nemulators for long climate simulations with skill across a range of\nspatiotemporal scales, a particularly important goal for the ocean. Our work\nbuilds a skillful global emulator of the ocean component of a state-of-the-art\nclimate model. We emulate key ocean variables, sea surface height, horizontal\nvelocities, temperature, and salinity, across their full depth. We use a\nmodified ConvNeXt UNet architecture trained on multidepth levels of ocean data.\nWe show that the ocean emulator - Samudra - which exhibits no drift relative to\nthe truth, can reproduce the depth structure of ocean variables and their\ninterannual variability. Samudra is stable for centuries and 150 times faster\nthan the original ocean model. Samudra struggles to capture the correct\nmagnitude of the forcing trends and simultaneously remains stable, requiring\nfurther work.\n","authors":["Surya Dheeshjith","Adam Subel","Alistair Adcroft","Julius Busecke","Carlos Fernandez-Granda","Shubham Gupta","Laure Zanna"],"pdf_url":"https://arxiv.org/pdf/2412.03795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00641v2","updated":"2024-12-19T15:42:00Z","published":"2024-08-01T15:30:43Z","title":"Enhancing Ethereum Fraud Detection via Generative and Contrastive\n  Self-supervision","summary":"  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be available in\nhttps://github.com/GISec-Team/Meta-IFD.\n","authors":["Chenxiang Jin","Jiajun Zhou","Chenxuan Xie","Shanqing Yu","Qi Xuan","Xiaoniu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00641v2.pdf","comment":"Accepted by IEEE Transactions on Information Forensics & Security"},{"id":"http://arxiv.org/abs/2412.14954v1","updated":"2024-12-19T15:36:30Z","published":"2024-12-19T15:36:30Z","title":"Corn Ear Detection and Orientation Estimation Using Deep Learning","summary":"  Monitoring growth behavior of maize plants such as the development of ears\ncan give key insights into the plant's health and development. Traditionally,\nthe measurement of the angle of ears is performed manually, which can be\ntime-consuming and prone to human error. To address these challenges, this\npaper presents a computer vision-based system for detecting and tracking ears\nof corn in an image sequence. The proposed system could accurately detect,\ntrack, and predict the ear's orientation, which can be useful in monitoring\ntheir growth behavior. This can significantly save time compared to manual\nmeasurement and enables additional areas of ear orientation research and\npotential increase in efficiencies for maize production. Using an object\ndetector with keypoint detection, the algorithm proposed could detect 90\npercent of all ears. The cardinal estimation had a mean absolute error (MAE) of\n18 degrees, compared to a mean 15 degree difference between two people\nmeasuring by hand. These results demonstrate the feasibility of using computer\nvision techniques for monitoring maize growth and can lead to further research\nin this area.\n","authors":["Nathan Sprague","John Evans","Michael Mardikes"],"pdf_url":"https://arxiv.org/pdf/2412.14954v1.pdf","comment":"22 pages;15 figures"},{"id":"http://arxiv.org/abs/2411.10958v2","updated":"2024-12-19T15:26:20Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrixes $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$\nto enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS)\nof SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on\nRTX4090, respectively. Comprehensive experiments confirm that our approach\nincurs negligible end-to-end metrics loss across diverse models, including\nthose for large language processing, image generation, and video generation.\nThe codes are available at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10839v2","updated":"2024-12-19T15:25:41Z","published":"2024-08-20T13:34:17Z","title":"Benchmarking Large Language Models for Math Reasoning Tasks","summary":"  The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.\n","authors":["Kathrin Seßler","Yao Rong","Emek Gözlüklü","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2408.10839v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2302.11947v2","updated":"2024-12-19T15:13:46Z","published":"2023-02-23T11:44:43Z","title":"Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight\n  Convolutional Neural Networks","summary":"  The health and safety hazards posed by worn crane lifting ropes mandate\nperiodic inspection for damage. This task is time-consuming, prone to human\nerror, halts operation, and may result in the premature disposal of ropes.\nTherefore, we propose using efficient deep learning and computer vision methods\nto automate the process of detecting damaged ropes. Specifically, we present a\nvision-based system for detecting damage in synthetic fiber rope images using\nlightweight convolutional neural networks. We develop a camera-based apparatus\nto photograph the lifting rope's surface, while in operation, and capture the\nprogressive wear-and-tear as well as the more significant degradation in the\nrope's health state. Experts from Konecranes annotate the collected images in\naccordance with the rope's condition; normal or damaged. Then, we pre-process\nthe images, systematically design a deep learning model, evaluate its detection\nand prediction performance, analyze its computational complexity, and compare\nit with various other models. Experimental results show the proposed model\noutperforms other similar techniques with 96.5% accuracy, 94.8% precision,\n98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the\nmodel's real-time operation, low memory footprint, robustness to various\nenvironmental and operational conditions, and adequacy for deployment in\nindustrial applications such as lifting, mooring, towing, climbing, and\nsailing.\n","authors":["Tuomas Jalonen","Mohammad Al-Sa'd","Roope Mellanen","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2302.11947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09423v3","updated":"2024-12-19T15:10:18Z","published":"2023-07-18T16:43:03Z","title":"Scaling Laws for Imitation Learning in Single-Agent Games","summary":"  Imitation Learning (IL) is one of the most widely used methods in machine\nlearning. Yet, many works find it is often unable to fully recover the\nunderlying expert behavior, even in constrained environments like single-agent\ngames. However, none of these works deeply investigate the role of scaling up\nthe model and data size. Inspired by recent work in Natural Language Processing\n(NLP) where \"scaling up\" has resulted in increasingly more capable LLMs, we\ninvestigate whether carefully scaling up model and data size can bring similar\nimprovements in the imitation learning setting for single-agent games. We first\ndemonstrate our findings on a variety of Atari games, and thereafter focus on\nthe extremely challenging game of NetHack. In all games, we find that IL loss\nand mean return scale smoothly with the compute budget (FLOPs) and are strongly\ncorrelated, resulting in power laws for training compute-optimal IL agents.\nFinally, we forecast and train several NetHack agents with IL and find they\noutperform prior state-of-the-art by 1.5x in all settings. Our work both\ndemonstrates the scaling behavior of imitation learning in a variety of\nsingle-agent games, as well as the viability of scaling up current approaches\nfor increasingly capable agents in NetHack, a game that remains elusively hard\nfor current AI systems.\n","authors":["Jens Tuyls","Dhruv Madeka","Kari Torkkola","Dean Foster","Karthik Narasimhan","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2307.09423v3.pdf","comment":"Accepted at TMLR 2024"},{"id":"http://arxiv.org/abs/2412.14916v1","updated":"2024-12-19T14:50:10Z","published":"2024-12-19T14:50:10Z","title":"From Point to probabilistic gradient boosting for claim frequency and\n  severity prediction","summary":"  Gradient boosting for decision tree algorithms are increasingly used in\nactuarial applications as they show superior predictive performance over\ntraditional generalized linear models. Many improvements and sophistications to\nthe first gradient boosting machine algorithm exist. We present in a unified\nnotation, and contrast, all the existing point and probabilistic gradient\nboosting for decision tree algorithms: GBM, XGBoost, DART, LightGBM, CatBoost,\nEGBM, PGBM, XGBoostLSS, cyclic GBM, and NGBoost. In this comprehensive\nnumerical study, we compare their performance on five publicly available\ndatasets for claim frequency and severity, of various size and comprising\ndifferent number of (high cardinality) categorical variables. We explain how\nvarying exposure-to-risk can be handled with boosting in frequency models. We\ncompare the algorithms on the basis of computational efficiency, predictive\nperformance, and model adequacy. LightGBM and XGBoostLSS win in terms of\ncomputational efficiency. The fully interpretable EGBM achieves competitive\npredictive performance compared to the black box algorithms considered. We find\nthat there is no trade-off between model adequacy and predictive accuracy: both\nare achievable simultaneously.\n","authors":["Dominik Chevalier","Marie-Pier Côté"],"pdf_url":"https://arxiv.org/pdf/2412.14916v1.pdf","comment":"26 pages, 4 figures, 26 tables, 7 algorithms"},{"id":"http://arxiv.org/abs/2311.18512v2","updated":"2024-12-19T14:46:05Z","published":"2023-11-30T12:40:23Z","title":"Union-over-Intersections: Object Detection beyond Winner-Takes-All","summary":"  This paper revisits the problem of predicting box locations in object\ndetection architectures. Typically, each box proposal or box query aims to\ndirectly maximize the intersection-over-union score with the ground truth,\nfollowed by a winner-takes-all non-maximum suppression where only the highest\nscoring box in each region is retained. We observe that both steps are\nsub-optimal: the first involves regressing proposals to the entire ground\ntruth, which is a difficult task even with large receptive fields, and the\nsecond neglects valuable information from boxes other than the top candidate.\nInstead of regressing proposals to the whole ground truth, we propose a simpler\napproach: regress only to the area of intersection between the proposal and the\nground truth. This avoids the need for proposals to extrapolate beyond their\nvisual scope, improving localization accuracy. Rather than adopting a\nwinner-takes-all strategy, we take the union over the regressed intersections\nof all boxes in a region to generate the final box outputs. Our plug-and-play\nmethod integrates seamlessly into proposal-based, grid-based, and query-based\ndetection architectures with minimal modifications, consistently improving\nobject localization and instance segmentation. We demonstrate its broad\napplicability and versatility across various detection and segmentation tasks.\n","authors":["Aritra Bhowmik","Pascal Mettes","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2311.18512v2.pdf","comment":"17 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2412.14075v2","updated":"2024-12-19T14:41:03Z","published":"2024-12-18T17:19:55Z","title":"Online MDP with Transition Prototypes: A Robust Adaptive Approach","summary":"  In this work, we consider an online robust Markov Decision Process (MDP)\nwhere we have the information of finitely many prototypes of the underlying\ntransition kernel. We consider an adaptively updated ambiguity set of the\nprototypes and propose an algorithm that efficiently identifies the true\nunderlying transition kernel while guaranteeing the performance of the\ncorresponding robust policy. To be more specific, we provide a sublinear regret\nof the subsequent optimal robust policy. We also provide an early stopping\nmechanism and a worst-case performance bound of the value function. In\nnumerical experiments, we demonstrate that our method outperforms existing\napproaches, particularly in the early stage with limited data. This work\ncontributes to robust MDPs by considering possible prior information about the\nunderlying transition probability and online learning, offering both\ntheoretical insights and practical algorithms for improved decision-making\nunder uncertainty.\n","authors":["Shuo Sun","Meng Qi","Zuo-Jun Max Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11544v4","updated":"2024-12-19T14:33:00Z","published":"2024-06-17T13:42:28Z","title":"Do Parameters Reveal More than Loss for Membership Inference?","summary":"  Membership inference attacks are used as a key tool for disclosure auditing.\nThey aim to infer whether an individual record was used to train a model. While\nsuch evaluations are useful to demonstrate risk, they are computationally\nexpensive and often make strong assumptions about potential adversaries' access\nto models and training environments, and thus do not provide tight bounds on\nleakage from potential attacks. We show how prior claims around black-box\naccess being sufficient for optimal membership inference do not hold for\nstochastic gradient descent, and that optimal membership inference indeed\nrequires white-box access. Our theoretical results lead to a new white-box\ninference attack, IHA (Inverse Hessian Attack), that explicitly uses model\nparameters by taking advantage of computing inverse-Hessian vector products.\nOur results show that both auditors and adversaries may be able to benefit from\naccess to model parameters, and we advocate for further research into white-box\nmethods for membership inference.\n","authors":["Anshuman Suri","Xiao Zhang","David Evans"],"pdf_url":"https://arxiv.org/pdf/2406.11544v4.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2412.14897v1","updated":"2024-12-19T14:28:00Z","published":"2024-12-19T14:28:00Z","title":"Diffusion priors for Bayesian 3D reconstruction from incomplete\n  measurements","summary":"  Many inverse problems are ill-posed and need to be complemented by prior\ninformation that restricts the class of admissible models. Bayesian approaches\nencode this information as prior distributions that impose generic properties\non the model such as sparsity, non-negativity or smoothness. However, in case\nof complex structured models such as images, graphs or three-dimensional (3D)\nobjects,generic prior distributions tend to favor models that differ largely\nfrom those observed in the real world. Here we explore the use of diffusion\nmodels as priors that are combined with experimental data within a Bayesian\nframework. We use 3D point clouds to represent 3D objects such as household\nitems or biomolecular complexes formed from proteins and nucleic acids. We\ntrain diffusion models that generate coarse-grained 3D structures at a medium\nresolution and integrate these with incomplete and noisy experimental data. To\ndemonstrate the power of our approach, we focus on the reconstruction of\nbiomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which\nis an important inverse problem in structural biology. We find that posterior\nsampling with diffusion model priors allows for 3D reconstruction from very\nsparse, low-resolution and partial observations.\n","authors":["Julian L. Möbius","Michael Habeck"],"pdf_url":"https://arxiv.org/pdf/2412.14897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01519v3","updated":"2024-12-19T14:26:22Z","published":"2024-09-03T01:26:21Z","title":"Hybridization of Persistent Homology with Neural Networks for\n  Time-Series Prediction: A Case Study in Wave Height","summary":"  Time-series prediction is an active area of research across various fields,\noften challenged by the fluctuating influence of short-term and long-term\nfactors. In this study, we introduce a feature engineering method that enhances\nthe predictive performance of neural network models. Specifically, we leverage\ncomputational topology techniques to derive valuable topological features from\ninput data, boosting the predictive accuracy of our models. Our focus is on\npredicting wave heights, utilizing models based on topological features within\nfeedforward neural networks (FNNs), recurrent neural networks (RNNs), long\nshort-term memory networks (LSTM), and RNNs with gated recurrent units (GRU).\nFor time-ahead predictions, the enhancements in $R^2$ score were significant\nfor FNNs, RNNs, LSTM, and GRU models. Additionally, these models also showed\nsignificant reductions in maximum errors and mean squared errors.\n","authors":["Zixin Lin","Nur Fariha Syaqina Zulkepli","Mohd Shareduwan Mohd Kasihmuddin","R. U. Gobithaasan"],"pdf_url":"https://arxiv.org/pdf/2409.01519v3.pdf","comment":"the paper contain errors"},{"id":"http://arxiv.org/abs/2405.14573v4","updated":"2024-12-19T14:19:02Z","published":"2024-05-23T13:48:54Z","title":"AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents","summary":"  Autonomous agents that execute human tasks by controlling computers can\nenhance human productivity and application accessibility. However, progress in\nthis field will be driven by realistic and reproducible benchmarks. We present\nAndroidWorld, a fully functional Android environment that provides reward\nsignals for 116 programmatic tasks across 20 real-world Android apps. Unlike\nexisting interactive environments, which provide a static test set,\nAndroidWorld dynamically constructs tasks that are parameterized and expressed\nin natural language in unlimited ways, thus enabling testing on a much larger\nand more realistic suite of tasks. To ensure reproducibility, each task\nincludes dedicated initialization, success-checking, and tear-down logic, which\nmodifies and inspects the device's system state. We experiment with baseline\nagents to test AndroidWorld and provide initial results on the benchmark. Our\nbest agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for\nfuture work. Furthermore, we adapt a popular desktop web agent to work on\nAndroid, which we find to be less effective on mobile, suggesting future\nresearch is needed to achieve universal, cross-platform agents. Finally, we\nalso conduct a robustness analysis, showing that task variations can\nsignificantly affect agent performance, demonstrating that without such\ntesting, agent performance metrics may not fully reflect practical challenges.\nAndroidWorld and the experiments in this paper are available at\ngithub.com/google-research/android_world.\n","authors":["Christopher Rawles","Sarah Clinckemaillie","Yifan Chang","Jonathan Waltz","Gabrielle Lau","Marybeth Fair","Alice Li","William Bishop","Wei Li","Folawiyo Campbell-Ajala","Daniel Toyama","Robert Berry","Divya Tyamagundlu","Timothy Lillicrap","Oriana Riva"],"pdf_url":"https://arxiv.org/pdf/2405.14573v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16684v2","updated":"2024-12-19T14:18:15Z","published":"2024-09-25T07:20:59Z","title":"Erase then Rectify: A Training-Free Parameter Editing Approach for\n  Cost-Effective Graph Unlearning","summary":"  Graph unlearning, which aims to eliminate the influence of specific nodes,\nedges, or attributes from a trained Graph Neural Network (GNN), is essential in\napplications where privacy, bias, or data obsolescence is a concern. However,\nexisting graph unlearning techniques often necessitate additional training on\nthe remaining data, leading to significant computational costs, particularly\nwith large-scale graphs. To address these challenges, we propose a two-stage\ntraining-free approach, Erase then Rectify (ETR), designed for efficient and\nscalable graph unlearning while preserving the model utility. Specifically, we\nfirst build a theoretical foundation showing that masking parameters critical\nfor unlearned samples enables effective unlearning. Building on this insight,\nthe Erase stage strategically edits model parameters to eliminate the impact of\nunlearned samples and their propagated influence on intercorrelated nodes. To\nfurther ensure the GNN's utility, the Rectify stage devises a gradient\napproximation method to estimate the model's gradient on the remaining dataset,\nwhich is then used to enhance model performance. Overall, ETR achieves graph\nunlearning without additional training or full training data access,\nsignificantly reducing computational overhead and preserving data privacy.\nExtensive experiments on seven public datasets demonstrate the consistent\nsuperiority of ETR in model utility, unlearning efficiency, and unlearning\neffectiveness, establishing it as a promising solution for real-world graph\nunlearning challenges.\n","authors":["Zhe-Rui Yang","Jindong Han","Chang-Dong Wang","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16684v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14869v1","updated":"2024-12-19T14:06:44Z","published":"2024-12-19T14:06:44Z","title":"AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional\n  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature\n  Screening","summary":"  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood\nwithin the skull, which occurs due to the rupture of blood vessels in or around\nthe brain. If this condition is not diagnosed in a timely manner and\nappropriately treated, it can lead to serious complications such as decreased\nconsciousness, permanent neurological disabilities, or even death.The primary\naim of this study is to detect the occurrence or non-occurrence of ICH,\nfollowed by determining the type of subdural hemorrhage (SDH). These tasks are\nframed as two separate binary classification problems. By adding two layers to\nthe co-scale convolutional attention (CCA) classifier architecture, we\nintroduce a novel approach for ICH detection. In the first layer, after\nextracting features from different slices of computed tomography (CT) scan\nimages, we combine these features and select the 50 components that capture the\nhighest variance in the data, considering them as informative features. We then\nassess the discriminative power of these features using the bootstrap forest\nalgorithm, discarding those that lack sufficient discriminative ability between\ndifferent classes. This algorithm explicitly determines the contribution of\neach feature to the final prediction, assisting us in developing an explainable\nAI model. The features feed into a boosting neural network as a latent feature\nspace. In the second layer, we introduce a novel uncertainty-based fuzzy\nintegral operator to fuse information from different CT scan slices. This\noperator, by accounting for the dependencies between consecutive slices,\nsignificantly improves detection accuracy.\n","authors":["Mehdi Hosseini Chagahi","Md. Jalil Piran","Niloufar Delfan","Behzad Moshiri","Jaber Hatam Parikhan"],"pdf_url":"https://arxiv.org/pdf/2412.14869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14865v1","updated":"2024-12-19T14:00:03Z","published":"2024-12-19T14:00:03Z","title":"Hierarchical Subspaces of Policies for Continual Offline Reinforcement\n  Learning","summary":"  In dynamic domains such as autonomous robotics and video game simulations,\nagents must continuously adapt to new tasks while retaining previously acquired\nskills. This ongoing process, known as Continual Reinforcement Learning,\npresents significant challenges, including the risk of forgetting past\nknowledge and the need for scalable solutions as the number of tasks increases.\nTo address these issues, we introduce HIerarchical LOW-rank Subspaces of\nPolicies (HILOW), a novel framework designed for continual learning in offline\nnavigation settings. HILOW leverages hierarchical policy subspaces to enable\nflexible and efficient adaptation to new tasks while preserving existing\nknowledge. We demonstrate, through a careful experimental study, the\neffectiveness of our method in both classical MuJoCo maze environments and\ncomplex video game-like simulations, showcasing competitive performance and\nsatisfying adaptability according to classical continual learning metrics, in\nparticular regarding memory usage. Our work provides a promising framework for\nreal-world applications where continuous learning from pre-collected data is\nessential.\n","authors":["Anthony Kobanda","Rémy Portelas","Odalric-Ambrym Maillard","Ludovic Denoyer"],"pdf_url":"https://arxiv.org/pdf/2412.14865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14854v1","updated":"2024-12-19T13:48:49Z","published":"2024-12-19T13:48:49Z","title":"Surrogate-assisted multi-objective design of complex multibody systems","summary":"  The optimization of large-scale multibody systems is a numerically\nchallenging task, in particular when considering multiple conflicting criteria\nat the same time. In this situation, we need to approximate the Pareto set of\noptimal compromises, which is significantly more expensive than finding a\nsingle optimum in single-objective optimization. To prevent large costs, the\nusage of surrogate models, constructed from a small but informative number of\nexpensive model evaluations, is a very popular and widely studied approach. The\ncentral challenge then is to ensure a high quality (that is, near-optimality)\nof the solutions that were obtained using the surrogate model, which can be\nhard to guarantee with a single pre-computed surrogate. We present a\nback-and-forth approach between surrogate modeling and multi-objective\noptimization to improve the quality of the obtained solutions. Using the\nexample of an expensive-to-evaluate multibody system, we compare different\nstrategies regarding multi-objective optimization, sampling and also surrogate\nmodeling, to identify the most promising approach in terms of computational\nefficiency and solution quality.\n","authors":["Augustina C. Amakor","Manuel B. Berkemeier","Meike Wohlleben","Walter Sextro","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2412.14854v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.01566"},{"id":"http://arxiv.org/abs/2410.10929v6","updated":"2024-12-19T13:39:55Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v6.pdf","comment":"In process to IEEE Intelligent Vehicle Symposium 2025"},{"id":"http://arxiv.org/abs/2408.11778v2","updated":"2024-12-19T13:34:56Z","published":"2024-08-21T17:08:05Z","title":"Sum of Squares Circuits","summary":"  Designing expressive generative models that support exact and efficient\ninference is a core question in probabilistic ML. Probabilistic circuits (PCs)\noffer a framework where this tractability-vs-expressiveness trade-off can be\nanalyzed theoretically. Recently, squared PCs encoding subtractive mixtures via\nnegative parameters have emerged as tractable models that can be exponentially\nmore expressive than monotonic PCs, i.e., PCs with positive parameters only. In\nthis paper, we provide a more precise theoretical characterization of the\nexpressiveness relationships among these models. First, we prove that squared\nPCs can be less expressive than monotonic ones. Second, we formalize a novel\nclass of PCs -- sum of squares PCs -- that can be exponentially more expressive\nthan both squared and monotonic PCs. Around sum of squares PCs, we build an\nexpressiveness hierarchy that allows us to precisely unify and separate\ndifferent tractable model classes such as Born Machines and PSD models, and\nother recently introduced tractable probabilistic models by using complex\nparameters. Finally, we empirically show the effectiveness of sum of squares\ncircuits in performing distribution estimation.\n","authors":["Lorenzo Loconte","Stefan Mengel","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2408.11778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09116v3","updated":"2024-12-19T13:27:49Z","published":"2024-12-12T09:51:18Z","title":"How to Re-enable PDE Loss for Physical Systems Modeling Under Partial\n  Observation","summary":"  In science and engineering, machine learning techniques are increasingly\nsuccessful in physical systems modeling (predicting future states of physical\nsystems). Effectively integrating PDE loss as a constraint of system transition\ncan improve the model's prediction by overcoming generalization issues due to\ndata scarcity, especially when data acquisition is costly. However, in many\nreal-world scenarios, due to sensor limitations, the data we can obtain is\noften only partial observation, making the calculation of PDE loss seem to be\ninfeasible, as the PDE loss heavily relies on high-resolution states. We\ncarefully study this problem and propose a novel framework named Re-enable PDE\nLoss under Partial Observation (RPLPO). The key idea is that although enabling\nPDE loss to constrain system transition solely is infeasible, we can re-enable\nPDE loss by reconstructing the learnable high-resolution state and constraining\nsystem transition simultaneously. Specifically, RPLPO combines an encoding\nmodule for reconstructing learnable high-resolution states with a transition\nmodule for predicting future states. The two modules are jointly trained by\ndata and PDE loss. We conduct experiments in various physical systems to\ndemonstrate that RPLPO has significant improvement in generalization, even when\nobservation is sparse, irregular, noisy, and PDE is inaccurate.\n","authors":["Haodong Feng","Yue Wang","Dixia Fan"],"pdf_url":"https://arxiv.org/pdf/2412.09116v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2305.09565v2","updated":"2024-12-19T13:27:40Z","published":"2023-05-16T16:02:18Z","title":"Toward Falsifying Causal Graphs Using a Permutation-Based Test","summary":"  Understanding causal relationships among the variables of a system is\nparamount to explain and control its behavior. For many real-world systems,\nhowever, the true causal graph is not readily available and one must resort to\npredictions made by algorithms or domain experts. Therefore, metrics that\nquantitatively assess the goodness of a causal graph provide helpful checks\nbefore using it in downstream tasks. Existing metrics provide an\n$\\textit{absolute}$ number of inconsistencies between the graph and the\nobserved data, and without a baseline, practitioners are left to answer the\nhard question of how many such inconsistencies are acceptable or expected.\nHere, we propose a novel consistency metric by constructing a baseline through\nnode permutations. By comparing the number of inconsistencies with those on the\nbaseline, we derive an interpretable metric that captures whether the graph is\nsignificantly better than random. Evaluating on both simulated and real data\nsets from various domains, including biology and cloud monitoring, we\ndemonstrate that the true graph is not falsified by our metric, whereas the\nwrong graphs given by a hypothetical user are likely to be falsified.\n","authors":["Elias Eulig","Atalanti A. Mastakouri","Patrick Blöbaum","Michaela Hardt","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2305.09565v2.pdf","comment":"Camera-ready version for AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14834v1","updated":"2024-12-19T13:24:01Z","published":"2024-12-19T13:24:01Z","title":"Entropy Regularized Task Representation Learning for Offline\n  Meta-Reinforcement Learning","summary":"  Offline meta-reinforcement learning aims to equip agents with the ability to\nrapidly adapt to new tasks by training on data from a set of different tasks.\nContext-based approaches utilize a history of state-action-reward transitions\n-- referred to as the context -- to infer representations of the current task,\nand then condition the agent, i.e., the policy and value function, on the task\nrepresentations. Intuitively, the better the task representations capture the\nunderlying tasks, the better the agent can generalize to new tasks.\nUnfortunately, context-based approaches suffer from distribution mismatch, as\nthe context in the offline data does not match the context at test time,\nlimiting their ability to generalize to the test tasks. This leads to the task\nrepresentations overfitting to the offline training data. Intuitively, the task\nrepresentations should be independent of the behavior policy used to collect\nthe offline data. To address this issue, we approximately minimize the mutual\ninformation between the distribution over the task representations and behavior\npolicy by maximizing the entropy of behavior policy conditioned on the task\nrepresentations. We validate our approach in MuJoCo environments, showing that\ncompared to baselines, our task representations more faithfully represent the\nunderlying tasks, leading to outperforming prior methods in both\nin-distribution and out-of-distribution tasks.\n","authors":["Mohammadreza nakhaei","Aidan Scannell","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2412.14834v1.pdf","comment":"7 Pages, Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2406.02765v5","updated":"2024-12-19T13:16:21Z","published":"2024-06-04T20:33:29Z","title":"Discovering Continuous-Time Memory-Based Symbolic Policies using Genetic\n  Programming","summary":"  Artificial intelligence techniques are increasingly being applied to solve\ncontrol problems, but often rely on black-box methods without transparent\noutput generation. To improve the interpretability and transparency in control\nsystems, models can be defined as white-box symbolic policies described by\nmathematical expressions. For better performance in partially observable and\nvolatile environments, the symbolic policies are extended with memory\nrepresented by continuous-time latent variables, governed by differential\nequations. Genetic programming is used for optimisation, resulting in\ninterpretable policies consisting of symbolic expressions. Our results show\nthat symbolic policies with memory compare with black-box policies on a variety\nof control tasks. Furthermore, the benefit of the memory in symbolic policies\nis demonstrated on experiments where memory-less policies fall short. Overall,\nwe present a method for evolving high-performing symbolic policies that offer\ninterpretability and transparency, which lacks in black-box models.\n","authors":["Sigur de Vries","Sander Keemink","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2406.02765v5.pdf","comment":"21 pages including references and appendix, 5 figures, 1 algorithm, 5\n  tables"},{"id":"http://arxiv.org/abs/2412.14814v1","updated":"2024-12-19T13:09:06Z","published":"2024-12-19T13:09:06Z","title":"Answer Set Networks: Casting Answer Set Programming into Deep Learning","summary":"  Although Answer Set Programming (ASP) allows constraining neural-symbolic\n(NeSy) systems, its employment is hindered by the prohibitive costs of\ncomputing stable models and the CPU-bound nature of state-of-the-art solvers.\nTo this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on\nGraph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep\nProbabilistic Logic Programming (DPPL). Specifically, we show how to translate\nASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded\nproblem by leveraging GPU's batching and parallelization capabilities. Our\nexperimental evaluations demonstrate that ASNs outperform state-of-the-art\nCPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following\ntwo contributions based on the strengths of ASNs. Namely, we are the first to\nshow the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs\nto guide the training with logic. Further, we show the \"constitutional\nnavigation\" of drones, i.e., encoding public aviation laws in an ASN for\nrouting Unmanned Aerial Vehicles in uncertain environments.\n","authors":["Arseny Skryagin","Daniel Ochs","Phillip Deibert","Simon Kohaut","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2412.14814v1.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.10341v2","updated":"2024-12-19T13:03:10Z","published":"2024-12-13T18:38:47Z","title":"Shape error prediction in 5-axis machining using graph neural networks","summary":"  This paper presents an innovative method for predicting shape errors in\n5-axis machining using graph neural networks. The graph structure is defined\nwith nodes representing workpiece surface points and edges denoting the\nneighboring relationships. The dataset encompasses data from a material removal\nsimulation, process data, and post-machining quality information. Experimental\nresults show that the presented approach can generalize the shape error\nprediction for the investigated workpiece geometry. Moreover, by modelling\nspatial and temporal connections within the workpiece, the approach handles a\nlow number of labels compared to non-graphical methods such as Support Vector\nMachines.\n","authors":["Julia Huuk","Abheek Dhingra","Eirini Ntoutsi","Berend Denkena"],"pdf_url":"https://arxiv.org/pdf/2412.10341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14810v1","updated":"2024-12-19T13:00:03Z","published":"2024-12-19T13:00:03Z","title":"MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data","summary":"  In healthcare, the integration of multimodal data is pivotal for developing\ncomprehensive diagnostic and predictive models. However, managing missing data\nremains a significant challenge in real-world applications. We introduce MARIA\n(Multimodal Attention Resilient to Incomplete datA), a novel transformer-based\ndeep learning model designed to address these challenges through an\nintermediate fusion strategy. Unlike conventional approaches that depend on\nimputation, MARIA utilizes a masked self-attention mechanism, which processes\nonly the available data without generating synthetic values. This approach\nenables it to effectively handle incomplete datasets, enhancing robustness and\nminimizing biases introduced by imputation methods. We evaluated MARIA against\n10 state-of-the-art machine learning and deep learning models across 8\ndiagnostic and prognostic tasks. The results demonstrate that MARIA outperforms\nexisting methods in terms of performance and resilience to varying levels of\ndata incompleteness, underscoring its potential for critical healthcare\napplications.\n","authors":["Camillo Maria Caruso","Paolo Soda","Valerio Guarrasi"],"pdf_url":"https://arxiv.org/pdf/2412.14810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14802v1","updated":"2024-12-19T12:48:17Z","published":"2024-12-19T12:48:17Z","title":"Stack Trace Deduplication: Faster, More Accurately, and in More\n  Realistic Scenarios","summary":"  In large-scale software systems, there are often no fully-fledged bug reports\nwith human-written descriptions when an error occurs. In this case, developers\nrely on stack traces, i.e., series of function calls that led to the error.\nSince there can be tens and hundreds of thousands of them describing the same\nissue from different users, automatic deduplication into categories is\nnecessary to allow for processing. Recent works have proposed powerful deep\nlearning-based approaches for this, but they are evaluated and compared in\nisolation from real-life workflows, and it is not clear whether they will\nactually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel\nmodel, an industry-based dataset, and a multi-faceted evaluation. Our model\nconsists of two parts - (1) an embedding model with byte-pair encoding and\napproximate nearest neighbor search to quickly find the most relevant stack\ntraces to the incoming one, and (2) a reranker that re-ranks the most fitting\nstack traces, taking into account the repeated frames between them. To\ncomplement the existing datasets collected from open-source projects, we share\nwith the community SlowOps - a dataset of stack traces from IntelliJ-based\nproducts developed by JetBrains, which has an order of magnitude more stack\ntraces per category. Finally, we carry out an evaluation that strives to be\nrealistic: measuring not only the accuracy of categorization, but also the\noperation time and the ability to create new categories. The evaluation shows\nthat our model strikes a good balance - it outperforms other models on both\nopen-source datasets and SlowOps, while also being faster on time than most. We\nrelease all of our code and data, and hope that our work can pave the way to\nfurther practice-oriented research in the area.\n","authors":["Egor Shibaev","Denis Sushentsev","Yaroslav Golubev","Aleksandr Khvorov"],"pdf_url":"https://arxiv.org/pdf/2412.14802v1.pdf","comment":"Published at SANER'25. 11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.14801v1","updated":"2024-12-19T12:47:21Z","published":"2024-12-19T12:47:21Z","title":"Extending TWIG: Zero-Shot Predictive Hyperparameter Selection for KGEs\n  based on Graph Structure","summary":"  Knowledge Graphs (KGs) have seen increasing use across various domains --\nfrom biomedicine and linguistics to general knowledge modelling. In order to\nfacilitate the analysis of knowledge graphs, Knowledge Graph Embeddings (KGEs)\nhave been developed to automatically analyse KGs and predict new facts based on\nthe information in a KG, a task called \"link prediction\". Many existing studies\nhave documented that the structure of a KG, KGE model components, and KGE\nhyperparameters can significantly change how well KGEs perform and what\nrelationships they are able to learn. Recently, the Topologically-Weighted\nIntelligence Generation (TWIG) model has been proposed as a solution to\nmodelling how each of these elements relate. In this work, we extend the\nprevious research on TWIG and evaluate its ability to simulate the output of\nthe KGE model ComplEx in the cross-KG setting. Our results are twofold. First,\nTWIG is able to summarise KGE performance on a wide range of hyperparameter\nsettings and KGs being learned, suggesting that it represents a general\nknowledge of how to predict KGE performance from KG structure. Second, we show\nthat TWIG can successfully predict hyperparameter performance on unseen KGs in\nthe zero-shot setting. This second observation leads us to propose that, with\nadditional research, optimal hyperparameter selection for KGE models could be\ndetermined in a pre-hoc manner using TWIG-like methods, rather than by using a\nfull hyperparameter search.\n","authors":["Jeffrey Sardina","John D. Kelleher","Declan O'Sullivan"],"pdf_url":"https://arxiv.org/pdf/2412.14801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11448v3","updated":"2024-12-19T12:46:27Z","published":"2024-12-16T05:02:50Z","title":"TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated\n  Learning","summary":"  Due to the sensitivity of data, Federated Learning (FL) is employed to enable\ndistributed machine learning while safeguarding data privacy and accommodating\nthe requirements of various devices. However, in the context of\nsemi-decentralized FL, clients' communication and training states are dynamic.\nThis variability arises from local training fluctuations, heterogeneous data\ndistributions, and intermittent client participation. Most existing studies\nprimarily focus on stable client states, neglecting the dynamic challenges\ninherent in real-world scenarios. To tackle this issue, we propose a\nTRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client\nstates and contributions, enhancing model training efficiency through selective\nclient participation. We focus on a semi-decentralized FL framework where edge\nservers and clients train a shared global model using unreliable intra-cluster\nmodel aggregation and inter-cluster model consensus. First, we propose an\nadaptive hidden semi-Markov model to estimate clients' communication states and\ncontributions. Next, we address a client-server association optimization\nproblem to minimize global training loss. Using convergence analysis, we\npropose a greedy client scheduling algorithm. Finally, our experiments\nconducted on real-world datasets demonstrate that TRAIL outperforms\nstate-of-the-art baselines, achieving an improvement of 8.7% in test accuracy\nand a reduction of 15.3% in training loss.\n","authors":["Gangqiang Hu","Jianfeng Lu","Jianmin Han","Shuqin Cao","Jing Liu","Hao Fu"],"pdf_url":"https://arxiv.org/pdf/2412.11448v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05317v3","updated":"2024-12-19T12:38:23Z","published":"2024-10-05T03:47:06Z","title":"Accelerating Diffusion Transformers with Token-wise Feature Caching","summary":"  Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.\n","authors":["Chang Zou","Xuyang Liu","Ting Liu","Siteng Huang","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05317v3.pdf","comment":"In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix"},{"id":"http://arxiv.org/abs/2406.05666v9","updated":"2024-12-19T12:13:26Z","published":"2024-06-09T06:49:22Z","title":"Probability Distribution Learning and Its Application in Deep Learning","summary":"  This paper introduces a novel theoretical learning framework, termed\nprobability distribution learning (PD learning). Departing from the traditional\nstatistical learning framework, PD learning focuses on learning the underlying\nprobability distribution, which is modeled as a random variable within the\nprobability simplex. In this framework, the optimization objective is the\nlearning error, which quantifies the posterior expected discrepancy between the\nmodel's predicted distribution and the underlying true distribution, given\navailable sample data and prior knowledge. To optimize the learning error, this\npaper proposes the necessary conditions for loss functions, models, and\noptimization algorithms, ensuring that these conditions are met in real-world\nmachine learning scenarios. Based on these conditions, the non-convex\noptimization mechanism corresponding to model training can be theoretically\nresolved. Moreover, this paper provides model-dependent and model-independent\nbounds on learning error, offering new insights into the model's fitting and\ngeneralization capabilities. Furthermore, the paper applies the PD learning\nframework to elucidate the mechanisms by which various techniques, including\nrandom parameter initialization, over-parameterization, and dropout, influence\ndeep model training. Finally, the paper substantiates the key conclusions of\nthe proposed framework through experimental results.\n","authors":["Binchuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.05666v9.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2412.09265v4","updated":"2024-12-19T12:11:13Z","published":"2024-12-12T13:22:02Z","title":"Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation","summary":"  Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.\n","authors":["Bofang Jia","Pengxiang Ding","Can Cui","Mingyang Sun","Pengfang Qian","Siteng Huang","Zhaoxin Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09265v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09826v4","updated":"2024-12-19T12:07:06Z","published":"2023-02-20T08:19:19Z","title":"On the Expressivity of Persistent Homology in Graph Learning","summary":"  Persistent homology, a technique from computational topology, has recently\nshown strong empirical performance in the context of graph classification.\nBeing able to capture long range graph properties via higher-order topological\nfeatures, such as cycles of arbitrary length, in combination with multi-scale\ntopological descriptors, has improved predictive performance for data sets with\nprominent topological structures, such as molecules. At the same time, the\ntheoretical properties of persistent homology have not been formally assessed\nin this context. This paper intends to bridge the gap between computational\ntopology and graph machine learning by providing a brief introduction to\npersistent homology in the context of graphs, as well as a theoretical\ndiscussion and empirical analysis of its expressivity for graph learning tasks.\n","authors":["Rubén Ballester","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2302.09826v4.pdf","comment":"Accepted at the 3rd Learning on Graphs Conference (LoG) 2024"},{"id":"http://arxiv.org/abs/2412.14779v1","updated":"2024-12-19T12:05:13Z","published":"2024-12-19T12:05:13Z","title":"Agent-Temporal Credit Assignment for Optimal Policy Preservation in\n  Sparse Multi-Agent Reinforcement Learning","summary":"  In multi-agent environments, agents often struggle to learn optimal policies\ndue to sparse or delayed global rewards, particularly in long-horizon tasks\nwhere it is challenging to evaluate actions at intermediate time steps. We\nintroduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach\ndesigned to address the agent-temporal credit assignment problem by\nredistributing sparse rewards both temporally and across agents. TAR$^2$\ndecomposes sparse global rewards into time-step-specific rewards and calculates\nagent-specific contributions to these rewards. We theoretically prove that\nTAR$^2$ is equivalent to potential-based reward shaping, ensuring that the\noptimal policy remains unchanged. Empirical results demonstrate that TAR$^2$\nstabilizes and accelerates the learning process. Additionally, we show that\nwhen TAR$^2$ is integrated with single-agent reinforcement learning algorithms,\nit performs as well as or better than traditional multi-agent reinforcement\nlearning methods.\n","authors":["Aditya Kapoor","Sushant Swamy","Kale-ab Tessera","Mayank Baranwal","Mingfei Sun","Harshad Khadilkar","Stefano V. Albrecht"],"pdf_url":"https://arxiv.org/pdf/2412.14779v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.05428v2","updated":"2024-12-19T11:57:19Z","published":"2024-08-10T04:21:04Z","title":"Generalized Encouragement-Based Instrumental Variables for\n  Counterfactual Regression","summary":"  In causal inference, encouragement designs (EDs) are widely used to analyze\ncausal effects, when randomized controlled trials (RCTs) are impractical or\ncompliance to treatment cannot be perfectly enforced. Unlike RCTs, which\ndirectly allocate treatments, EDs randomly assign encouragement policies that\npositively motivate individuals to engage in a specific treatment. These random\nencouragements act as instrumental variables (IVs), facilitating the\nidentification of causal effects through leveraging exogenous perturbations in\ndiscrete treatment scenarios. However, real-world applications of encouragement\ndesigns often face challenges such as incomplete randomization, limited\nexperimental data, and significantly fewer encouragements compared to\ntreatments, hindering precise causal effect estimation. To address this, this\npaper introduces novel theories and algorithms for identifying the Conditional\nAverage Treatment Effect (CATE) using variations in encouragement. Further, by\nleveraging both observational and encouragement data, we propose a generalized\nIV estimator, named Encouragement-based Counterfactual Regression (EnCounteR),\nto effectively estimate the causal effects. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the superiority of EnCounteR over\nexisting methods.\n","authors":["Anpeng Wu","Kun Kuang","Ruoxuan Xiong","Xiangwei Chen","Zexu Sun","Fei Wu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14771v1","updated":"2024-12-19T11:55:51Z","published":"2024-12-19T11:55:51Z","title":"ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine","summary":"  Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments.\n","authors":["Rabee Qasem","Mohannad Hendi","Banan Tantour"],"pdf_url":"https://arxiv.org/pdf/2412.14771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11654v2","updated":"2024-12-19T11:47:34Z","published":"2024-12-16T10:56:58Z","title":"Smoothness Really Matters: A Simple Yet Effective Approach for\n  Unsupervised Graph Domain Adaptation","summary":"  Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution\nshifts between domains by transferring knowledge from labeled source graphs to\ngiven unlabeled target graphs. Existing UGDA methods primarily focus on\naligning features in the latent space learned by graph neural networks (GNNs)\nacross domains, often overlooking structural shifts, resulting in limited\neffectiveness when addressing structurally complex transfer scenarios. Given\nthe sensitivity of GNNs to local structural features, even slight discrepancies\nbetween source and target graphs could lead to significant shifts in node\nembeddings, thereby reducing the effectiveness of knowledge transfer. To\naddress this issue, we introduce a novel approach for UGDA called Target-Domain\nStructural Smoothing (TDSS). TDSS is a simple and effective method designed to\nperform structural smoothing directly on the target graph, thereby mitigating\nstructural distribution shifts and ensuring the consistency of node\nrepresentations. Specifically, by integrating smoothing techniques with\nneighborhood sampling, TDSS maintains the structural coherence of the target\ngraph while mitigating the risk of over-smoothing. Our theoretical analysis\nshows that TDSS effectively reduces target risk by improving model smoothness.\nEmpirical results on three real-world datasets demonstrate that TDSS\noutperforms recent state-of-the-art baselines, achieving significant\nimprovements across six transfer scenarios. The code is available in\nhttps://github.com/cwei01/TDSS.\n","authors":["Wei Chen","Guo Ye","Yakun Wang","Zhao Zhang","Libang Zhang","Daxin Wang","Zhiqiang Zhang","Fuzhen Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.11654v2.pdf","comment":"11 pages, Accpected by AAAI2025"},{"id":"http://arxiv.org/abs/2311.07326v2","updated":"2024-12-19T11:41:28Z","published":"2023-11-13T13:27:59Z","title":"MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and\n  Activation Functions","summary":"  Mathematical formulas serve as the means of communication between humans and\nnature, encapsulating the operational laws governing natural phenomena. The\nconcise formulation of these laws is a crucial objective in scientific research\nand an important challenge for artificial intelligence (AI). While traditional\nartificial neural networks (MLP) excel at data fitting, they often yield\nuninterpretable black box results that hinder our understanding of the\nrelationship between variables x and predicted values y. Moreover, the fixed\nnetwork architecture in MLP often gives rise to redundancy in both network\nstructure and parameters. To address these issues, we propose MetaSymNet, a\nnovel neural network that dynamically adjusts its structure in real-time,\nallowing for both expansion and contraction. This adaptive network employs the\nPANGU meta function as its activation function, which is a unique type capable\nof evolving into various basic functions during training to compose\nmathematical formulas tailored to specific needs. We then evolve the neural\nnetwork into a concise, interpretable mathematical expression. To evaluate\nMetaSymNet's performance, we compare it with four state-of-the-art symbolic\nregression algorithms across more than 10 public datasets comprising 222\nformulas. Our experimental results demonstrate that our algorithm outperforms\nothers consistently regardless of noise presence or absence. Furthermore, we\nassess MetaSymNet against MLP and SVM regarding their fitting ability and\nextrapolation capability, these are two essential aspects of machine learning\nalgorithms. The findings reveal that our algorithm excels in both areas.\nFinally, we compared MetaSymNet with MLP using iterative pruning in network\nstructure complexity. The results show that MetaSymNet's network structure\ncomplexity is obviously less than MLP under the same goodness of fit.\n","authors":["Yanjie Li","Weijun Li","Lina Yu","Min Wu","Jinyi Liu","Wenqiang Li","Meilan Hao","Shu Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2311.07326v2.pdf","comment":"This work has been accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14753v1","updated":"2024-12-19T11:34:22Z","published":"2024-12-19T11:34:22Z","title":"Opportunities and limitations of explaining quantum machine learning","summary":"  A common trait of many machine learning models is that it is often difficult\nto understand and explain what caused the model to produce the given output.\nWhile the explainability of neural networks has been an active field of\nresearch in the last years, comparably little is known for quantum machine\nlearning models. Despite a few recent works analyzing some specific aspects of\nexplainability, as of now there is no clear big picture perspective as to what\ncan be expected from quantum learning models in terms of explainability. In\nthis work, we address this issue by identifying promising research avenues in\nthis direction and lining out the expected future results. We additionally\npropose two explanation methods designed specifically for quantum machine\nlearning models, as first of their kind to the best of our knowledge. Next to\nour pre-view of the field, we compare both existing and novel methods to\nexplain the predictions of quantum learning models. By studying explainability\nin quantum machine learning, we can contribute to the sustainable development\nof the field, preventing trust issues in the future.\n","authors":["Elies Gil-Fuster","Jonas R. Naujoks","Grégoire Montavon","Thomas Wiegand","Wojciech Samek","Jens Eisert"],"pdf_url":"https://arxiv.org/pdf/2412.14753v1.pdf","comment":"16+16 pages, 3+4 figures"},{"id":"http://arxiv.org/abs/2412.14750v1","updated":"2024-12-19T11:29:57Z","published":"2024-12-19T11:29:57Z","title":"Deep Learning Based Recalibration of SDSS and DESI BAO Alleviates Hubble\n  and Clustering Tensions","summary":"  Conventional calibration of Baryon Acoustic Oscillations (BAO) data relies on\nestimation of the sound horizon at drag epoch $r_d$ from early universe\nobservations by assuming a cosmological model. We present a recalibration of\ntwo independent BAO datasets, SDSS and DESI, by employing deep learning\ntechniques for model-independent estimation of $r_d$, and explore the impacts\non $\\Lambda$CDM cosmological parameters. Significant reductions in both Hubble\n($H_0$) and clustering ($S_8$) tensions are observed for both the recalibrated\ndatasets. Moderate shifts in some other parameters hint towards further\nexploration of such data-driven approaches.\n","authors":["Rahul Shah","Purba Mukherjee","Soumadeep Saha","Utpal Garain","Supratik Pal"],"pdf_url":"https://arxiv.org/pdf/2412.14750v1.pdf","comment":"5 pages, 2 figures, 2 tables. Comments are welcome"},{"id":"http://arxiv.org/abs/2412.14744v1","updated":"2024-12-19T11:22:52Z","published":"2024-12-19T11:22:52Z","title":"A parametric algorithm is optimal for non-parametric regression of\n  smooth functions","summary":"  We address the regression problem for a general function $f:[-1,1]^d\\to\n\\mathbb R$ when the learner selects the training points $\\{x_i\\}_{i=1}^n$ to\nachieve a uniform error bound across the entire domain. In this setting, known\nhistorically as nonparametric regression, we aim to establish a sample\ncomplexity bound that depends solely on the function's degree of smoothness.\nAssuming periodicity at the domain boundaries, we introduce PADUA, an algorithm\nthat, with high probability, provides performance guarantees optimal up to\nconstant or logarithmic factors across all problem parameters. Notably, PADUA\nis the first parametric algorithm with optimal sample complexity for this\nsetting. Due to this feature, we prove that, differently from the\nnon-parametric state of the art, PADUA enjoys optimal space complexity in the\nprediction phase. To validate these results, we perform numerical experiments\nover functions coming from real audio data, where PADUA shows comparable\nperformance to state-of-the-art methods, while requiring only a fraction of the\ncomputational time.\n","authors":["Davide Maran","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2412.14744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14741v1","updated":"2024-12-19T11:17:31Z","published":"2024-12-19T11:17:31Z","title":"Active Inference and Human--Computer Interaction","summary":"  Active Inference is a closed-loop computational theoretical basis for\nunderstanding behaviour, based on agents with internal probabilistic generative\nmodels that encode their beliefs about how hidden states in their environment\ncause their sensations. We review Active Inference and how it could be applied\nto model the human-computer interaction loop. Active Inference provides a\ncoherent framework for managing generative models of humans, their\nenvironments, sensors and interface components. It informs off-line design and\nsupports real-time, online adaptation. It provides model-based explanations for\nbehaviours observed in HCI, and new tools to measure important concepts such as\nagency and engagement. We discuss how Active Inference offers a new basis for a\ntheory of interaction in HCI, tools for design of modern, complex sensor-based\nsystems, and integration of artificial intelligence technologies, enabling it\nto cope with diversity in human users and contexts. We discuss the practical\nchallenges in implementing such Active Inference-based systems.\n","authors":["Roderick Murray-Smith","John H. Williamson","Sebastian Stein"],"pdf_url":"https://arxiv.org/pdf/2412.14741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14739v1","updated":"2024-12-19T11:15:02Z","published":"2024-12-19T11:15:02Z","title":"On the Use of Deep Learning Models for Semantic Clone Detection","summary":"  Detecting and tracking code clones can ease various software development and\nmaintenance tasks when changes in a code fragment should be propagated over all\nits copies. Several deep learning-based clone detection models have appeared in\nthe literature for detecting syntactic and semantic clones, widely evaluated\nwith the BigCloneBench dataset. However, class imbalance and the small number\nof semantic clones make BigCloneBench less ideal for interpreting model\nperformance. Researchers also use other datasets such as GoogleCodeJam,\nOJClone, and SemanticCloneBench to understand model generalizability. To\novercome the limitations of existing datasets, the GPT-assisted semantic and\ncross-language clone dataset GPTCloneBench has been released. However, how\nthese models compare across datasets remains unclear. In this paper, we propose\na multi-step evaluation approach for five state-of-the-art clone detection\nmodels leveraging existing benchmark datasets, including GPTCloneBench, and\nusing mutation operators to study model ability. Specifically, we examine three\nhighly-performing single-language models (ASTNN, GMN, CodeBERT) on\nBigCloneBench, SemanticCloneBench, and GPTCloneBench, testing their robustness\nwith mutation operations. Additionally, we compare them against cross-language\nmodels (C4, CLCDSA) known for detecting semantic clones. While single-language\nmodels show high F1 scores for BigCloneBench, their performance on\nSemanticCloneBench varies (up to 20%). Interestingly, the cross-language model\n(C4) shows superior performance (around 7%) on SemanticCloneBench over other\nmodels and performs similarly on BigCloneBench and GPTCloneBench. On\nmutation-based datasets, C4 has more robust performance (less than 1%\ndifference) compared to single-language models, which show high variability.\n","authors":["Subroto Nag Pinku","Debajyoti Mondal","Chanchal K. Roy"],"pdf_url":"https://arxiv.org/pdf/2412.14739v1.pdf","comment":"Accepted at the 40th IEEE International Conference on Software\n  Maintenance and Evolution (ICSME 2024)"},{"id":"http://arxiv.org/abs/2409.11383v2","updated":"2024-12-19T11:12:30Z","published":"2024-09-17T17:34:24Z","title":"Training Datasets Generation for Machine Learning: Application to Vision\n  Based Navigation","summary":"  Vision Based Navigation consists in utilizing cameras as precision sensors\nfor GNC after extracting information from images. To enable the adoption of\nmachine learning for space applications, one of obstacles is the demonstration\nthat available training datasets are adequate to validate the algorithms. The\nobjective of the study is to generate datasets of images and metadata suitable\nfor training machine learning algorithms. Two use cases were selected and a\nrobust methodology was developed to validate the datasets including the ground\ntruth. The first use case is in-orbit rendezvous with a man-made object: a\nmockup of satellite ENVISAT. The second use case is a Lunar landing scenario.\nDatasets were produced from archival datasets (Chang'e 3), from the laboratory\nat DLR TRON facility and at Airbus Robotic laboratory, from SurRender software\nhigh fidelity image simulator using Model Capture and from Generative\nAdversarial Networks. The use case definition included the selection of\nalgorithms as benchmark: an AI-based pose estimation algorithm and a dense\noptical flow algorithm were selected. Eventually it is demonstrated that\ndatasets produced with SurRender and selected laboratory facilities are\nadequate to train machine learning algorithms.\n","authors":["Jérémy Lebreton","Ingo Ahrns","Roland Brochard","Christoph Haskamp","Hans Krüger","Matthieu Le Goff","Nicolas Menga","Nicolas Ollagnier","Ralf Regele","Francesco Capolupo","Massimo Casasco"],"pdf_url":"https://arxiv.org/pdf/2409.11383v2.pdf","comment":"6 pages, 4 figures, preprint of the proceedings of ESA SPAICE\n  conference 2024"},{"id":"http://arxiv.org/abs/2412.14738v1","updated":"2024-12-19T11:10:48Z","published":"2024-12-19T11:10:48Z","title":"Boosting GNN Performance via Training Sample Selection Based on\n  Adversarial Robustness Evaluation","summary":"  Graph Neural Networks (GNNs) have established themselves as one of the most\npowerful neural network architectures, excelling in leveraging graph topology\nand node features for various tasks. However, GNNs are inherently vulnerable to\nnoise in their inputs. Such noise can significantly degrade their performance.\nTo address this challenge, we propose a novel approach that employs adversarial\nrobustness evaluation techniques to identify nodes in the graph that are most\nsusceptible to noise. By selecting and constructing a training set composed of\nthese particularly noise-prone nodes, we then use them to train a Graph\nConvolutional Network (GCN). Our experimental results demonstrate that this\nstrategy leads to substantial improvements in the GCN's performance.\n","authors":["Yongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10099v2","updated":"2024-12-19T11:06:39Z","published":"2024-04-15T19:15:32Z","title":"Feature selection in linear SVMs via a hard cardinality constraint: a\n  scalable SDP decomposition approach","summary":"  In this paper, we study the embedded feature selection problem in linear\nSupport Vector Machines (SVMs), in which a cardinality constraint is employed,\nleading to an interpretable classification model. The problem is NP-hard due to\nthe presence of the cardinality constraint, even though the original linear SVM\namounts to a problem solvable in polynomial time. To handle the hard problem,\nwe first introduce two mixed-integer formulations for which novel semidefinite\nrelaxations are proposed. Exploiting the sparsity pattern of the relaxations,\nwe decompose the problems and obtain equivalent relaxations in a much smaller\ncone, making the conic approaches scalable. To make the best usage of the\ndecomposed relaxations, we propose heuristics using the information of its\noptimal solution. Moreover, an exact procedure is proposed by solving a\nsequence of mixed-integer decomposed semidefinite optimization problems.\nNumerical results on classical benchmarking datasets are reported, showing the\nefficiency and effectiveness of our approach.\n","authors":["Immanuel Bomze","Federico D'Onofrio","Laura Palagi","Bo Peng"],"pdf_url":"https://arxiv.org/pdf/2404.10099v2.pdf","comment":"Submitted to European Journal of Operational Research. arXiv admin\n  note: text overlap with arXiv:1808.02435 by other authors"},{"id":"http://arxiv.org/abs/2309.11036v2","updated":"2024-12-19T10:58:39Z","published":"2023-09-20T03:31:11Z","title":"Scalable Acceleration for Classification-Based Derivative-Free\n  Optimization","summary":"  Derivative-free optimization algorithms play an important role in scientific\nand engineering design optimization problems, especially when derivative\ninformation is not accessible. In this paper, we study the framework of\nsequential classification-based derivative-free optimization algorithms. By\nintroducing learning theoretic concept hypothesis-target shattering rate, we\nrevisit the computational complexity upper bound of SRACOS (Hu, Qian, and Yu\n2017). Inspired by the revisited upper bound, we propose an algorithm named\nRACE-CARS, which adds a random region-shrinking step compared with SRACOS. We\nfurther establish theorems showing the acceleration by region shrinking.\nExperiments on the synthetic functions as well as black-box tuning for\nlanguage-model-as-a-service demonstrate empirically the efficiency of\nRACE-CARS. An ablation experiment on the introduced hyperparameters is also\nconducted, revealing the mechanism of RACE-CARS and putting forward an\nempirical hyper-parameter tuning guidance.\n","authors":["Tianyi Han","Jingya Li","Zhipeng Guo","Yuan Jin"],"pdf_url":"https://arxiv.org/pdf/2309.11036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14730v1","updated":"2024-12-19T10:56:18Z","published":"2024-12-19T10:56:18Z","title":"Generative AI for Banks: Benchmarks and Algorithms for Synthetic\n  Financial Transaction Data","summary":"  The banking sector faces challenges in using deep learning due to data\nsensitivity and regulatory constraints, but generative AI may offer a solution.\nThus, this study identifies effective algorithms for generating synthetic\nfinancial transaction data and evaluates five leading models - Conditional\nTabular Generative Adversarial Networks (CTGAN), DoppelGANger (DGAN),\nWasserstein GAN, Financial Diffusion (FinDiff), and Tabular Variational\nAutoEncoders (TVAE) - across five criteria: fidelity, synthesis quality,\nefficiency, privacy, and graph structure. While none of the algorithms is able\nto replicate the real data's graph structure, each excels in specific areas:\nDGAN is ideal for privacy-sensitive tasks, FinDiff and TVAE excel in data\nreplication and augmentation, and CTGAN achieves a balance across all five\ncriteria, making it suitable for general applications with moderate privacy\nconcerns. As a result, our findings offer valuable insights for choosing the\nmost suitable algorithm.\n","authors":["Fabian Sven Karst","Sook-Yee Chong","Abigail A. Antenor","Enyu Lin","Mahei Manhai Li","Jan Marco Leimeister"],"pdf_url":"https://arxiv.org/pdf/2412.14730v1.pdf","comment":"Presented at the 34th Workshop on Information Technologies and\n  Systems (WITS 2024)"},{"id":"http://arxiv.org/abs/2412.14724v1","updated":"2024-12-19T10:47:31Z","published":"2024-12-19T10:47:31Z","title":"FROC: Building Fair ROC from a Trained Classifier","summary":"  This paper considers the problem of fair probabilistic binary classification\nwith binary protected groups. The classifier assigns scores, and a practitioner\npredicts labels using a certain cut-off threshold based on the desired\ntrade-off between false positives vs. false negatives. It derives these\nthresholds from the ROC of the classifier. The resultant classifier may be\nunfair to one of the two protected groups in the dataset. It is desirable that\nno matter what threshold the practitioner uses, the classifier should be fair\nto both the protected groups; that is, the $\\mathcal{L}_p$ norm between FPRs\nand TPRs of both the protected groups should be at most $\\varepsilon$. We call\nsuch fairness on ROCs of both the protected attributes\n$\\varepsilon_p$-Equalized ROC. Given a classifier not satisfying\n$\\varepsilon_1$-Equalized ROC, we aim to design a post-processing method to\ntransform the given (potentially unfair) classifier's output (score) to a\nsuitable randomized yet fair classifier. That is, the resultant classifier must\nsatisfy $\\varepsilon_1$-Equalized ROC. First, we introduce a threshold query\nmodel on the ROC curves for each protected group. The resulting classifier is\nbound to face a reduction in AUC. With the proposed query model, we provide a\nrigorous theoretical analysis of the minimal AUC loss to achieve\n$\\varepsilon_1$-Equalized ROC. To achieve this, we design a linear time\nalgorithm, namely \\texttt{FROC}, to transform a given classifier's output to a\nprobabilistic classifier that satisfies $\\varepsilon_1$-Equalized ROC. We prove\nthat under certain theoretical conditions, \\texttt{FROC}\\ achieves the\ntheoretical optimal guarantees. We also study the performance of our\n\\texttt{FROC}\\ on multiple real-world datasets with many trained classifiers.\n","authors":["Avyukta Manjunatha Vummintala","Shantanu Das","Sujit Gujar"],"pdf_url":"https://arxiv.org/pdf/2412.14724v1.pdf","comment":"51 pages, The 39th Annual AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2406.02507v3","updated":"2024-12-19T10:43:11Z","published":"2024-06-04T17:25:59Z","title":"Guiding a Diffusion Model with a Bad Version of Itself","summary":"  The primary axes of interest in image-generating diffusion models are image\nquality, the amount of variation in the results, and how well the results align\nwith a given condition, e.g., a class label or a text prompt. The popular\nclassifier-free guidance approach uses an unconditional model to guide a\nconditional model, leading to simultaneously better prompt alignment and\nhigher-quality images at the cost of reduced variation. These effects seem\ninherently entangled, and thus hard to control. We make the surprising\nobservation that it is possible to obtain disentangled control over image\nquality without compromising the amount of variation by guiding generation\nusing a smaller, less-trained version of the model itself rather than an\nunconditional model. This leads to significant improvements in ImageNet\ngeneration, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using\npublicly available networks. Furthermore, the method is also applicable to\nunconditional diffusion models, drastically improving their quality.\n","authors":["Tero Karras","Miika Aittala","Tuomas Kynkäänniemi","Jaakko Lehtinen","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2406.02507v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14719v1","updated":"2024-12-19T10:41:24Z","published":"2024-12-19T10:41:24Z","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","summary":"  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the\nambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to\nidentify the ambiguous sample, categorizing them into distinct sets of\nambiguous samples of false negatives and false positives, considering both\nbody- and action-level categories. \\textbf{Secondly}, we implement an ambiguous\ncontrastive refinement module to calibrate these ambiguous samples by\nregulating the distance between ambiguous samples and their corresponding\nprototypes. This calibration process aims to pull false negative\n($\\mathbb{FN}$) samples closer to their respective prototypes and push false\npositive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. \\textbf{Finally}, we propose a prototype-guided rectification to\nrectify prediction by incorporating the representability of prototypes.\nExtensive experiments conducted on the benchmark dataset demonstrate the\nsuperior performance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14719v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14718v1","updated":"2024-12-19T10:33:19Z","published":"2024-12-19T10:33:19Z","title":"A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical\n  Forecasting Reconciliation and Adjustment","summary":"  Ads demand forecasting for Walmart's ad products plays a critical role in\nenabling effective resource planning, allocation, and management of ads\nperformance. In this paper, we introduce a comprehensive demand forecasting\nsystem that tackles hierarchical time series forecasting in business settings.\nThough traditional hierarchical reconciliation methods ensure forecasting\ncoherence, they often trade off accuracy for coherence especially at lower\nlevels and fail to capture the seasonality unique to each time-series in the\nhierarchy. Thus, we propose a novel framework \"Multi-Stage Hierarchical\nForecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)\" to address\nthe challenges of preserving seasonality, ensuring coherence, and improving\naccuracy. Our system first utilizes diverse models, ensembled through Bayesian\nOptimization (BO), achieving base forecasts. The generated base forecasts are\nthen passed into the Multi-Stage HiFoReAd framework. The initial stage refines\nthe hierarchy using Top-Down forecasts and \"harmonic alignment.\" The second\nstage aligns the higher levels' forecasts using MinTrace algorithm, following\nwhich the last two levels undergo \"harmonic alignment\" and \"stratified\nscaling\", to eventually achieve accurate and coherent forecasts across the\nwhole hierarchy. Our experiments on Walmart's internal Ads-demand dataset and 3\nother public datasets, each with 4 hierarchical levels, demonstrate that the\naverage Absolute Percentage Error from the cross-validation sets improve from\n3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet)\nas well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the\nforecasts at all hierarchical levels are proved to be coherent. The proposed\nframework has been deployed and leveraged by Walmart's ads, sales and\noperations teams to track future demands, make informed decisions and plan\nresources.\n","authors":["Zhengchao Yang","Mithun Ghosh","Anish Saha","Dong Xu","Konstantin Shmakov","Kuang-chih Lee"],"pdf_url":"https://arxiv.org/pdf/2412.14718v1.pdf","comment":"Published in 2024 IEEE International Conference on Big Data (BigData)"},{"id":"http://arxiv.org/abs/2412.11242v2","updated":"2024-12-19T10:33:13Z","published":"2024-12-15T16:47:16Z","title":"TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs","summary":"  Specializing large language models (LLMs) for local deployment in\ndomain-specific use cases is necessary for strong performance while meeting\nlatency and privacy constraints. However, conventional task-specific adaptation\napproaches do not show simultaneous memory saving and inference speedup at\ndeployment time. Practical compression techniques like quantization and pruning\nrequire dedicated hardware or kernel support to achieve measured inference\nspeedup. We develop TrimLLM based on the layer-wise specialization phenomenon\nwe empirically observed and verified on contemporary LLMs. TrimLLM reduces the\ndepth of LLMs via progressive layer dropping. We show it retains LLMs' capacity\nin specific domains and achieves inference speedup irrespective of hardware and\ndeep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for\ninference; models adapted on medical, legal, and financial datasets all\ndemonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to\n$3.1\\times$ speedup on A100 when compared to state-of-the-art model compression\nalgorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio.\n","authors":["Lanxiang Hu","Tajana Rosing","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14717v1","updated":"2024-12-19T10:31:25Z","published":"2024-12-19T10:31:25Z","title":"Computing Gram Matrix for SMILES Strings using RDKFingerprint and\n  Sinkhorn-Knopp Algorithm","summary":"  In molecular structure data, SMILES (Simplified Molecular Input Line Entry\nSystem) strings are used to analyze molecular structure design. Numerical\nfeature representation of SMILES strings is a challenging task. This work\nproposes a kernel-based approach for encoding and analyzing molecular\nstructures from SMILES strings. The proposed approach involves computing a\nkernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal\ncomponent analysis (PCA) for dimensionality reduction. The resulting\nlow-dimensional embeddings are then used for classification and regression\nanalysis. The kernel matrix is computed by converting the SMILES strings into\nmolecular structures using the Morgan Fingerprint, which computes a fingerprint\nfor each molecule. The distance matrix is computed using the pairwise kernels\nfunction. The Sinkhorn-Knopp algorithm is used to compute the final kernel\nmatrix that satisfies the constraints of a probability distribution. This is\nachieved by iteratively adjusting the kernel matrix until the marginal\ndistributions of the rows and columns match the desired marginal distributions.\nWe provided a comprehensive empirical analysis of the proposed kernel method to\nevaluate its goodness with greater depth. The suggested method is assessed for\ndrug subcategory prediction (classification task) and solubility AlogPS\n``Aqueous solubility and Octanol/Water partition coefficient\" (regression task)\nusing the benchmark SMILES string dataset. The outcomes show the proposed\nmethod outperforms several baseline methods in terms of supervised analysis and\nhas potential uses in molecular design and drug discovery. Overall, the\nsuggested method is a promising avenue for kernel methods-based molecular\nstructure analysis and design.\n","authors":["Sarwan Ali","Haris Mansoor","Prakash Chourasia","Imdad Ullah Khan","Murray Patterson"],"pdf_url":"https://arxiv.org/pdf/2412.14717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14714v1","updated":"2024-12-19T10:25:21Z","published":"2024-12-19T10:25:21Z","title":"Holistic Adversarially Robust Pruning","summary":"  Neural networks can be drastically shrunk in size by removing redundant\nparameters. While crucial for the deployment on resource-constraint hardware,\noftentimes, compression comes with a severe drop in accuracy and lack of\nadversarial robustness. Despite recent advances, counteracting both aspects has\nonly succeeded for moderate compression rates so far. We propose a novel\nmethod, HARP, that copes with aggressive pruning significantly better than\nprior work. For this, we consider the network holistically. We learn a global\ncompression strategy that optimizes how many parameters (compression rate) and\nwhich parameters (scoring connections) to prune specific to each layer\nindividually. Our method fine-tunes an existing model with dynamic\nregularization, that follows a step-wise incremental function balancing the\ndifferent objectives. It starts by favoring robustness before shifting focus on\nreaching the target compression rate and only then handles the objectives\nequally. The learned compression strategies allow us to maintain the\npre-trained model natural accuracy and its adversarial robustness for a\nreduction by 99% of the network original size. Moreover, we observe a crucial\ninfluence of non-uniform compression across layers.\n","authors":["Qi Zhao","Christian Wressnegger"],"pdf_url":"https://arxiv.org/pdf/2412.14714v1.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2412.14711v1","updated":"2024-12-19T10:21:20Z","published":"2024-12-19T10:21:20Z","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing","summary":"  Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to\nscale up model capacity without increasing the computation budget. However,\nvanilla TopK routers are trained in a discontinuous, non-differentiable way,\nlimiting their performance and scalability. To address this issue, we propose\nReMoE, a fully differentiable MoE architecture that offers a simple yet\neffective drop-in replacement for the conventional TopK+Softmax routing,\nutilizing ReLU as the router instead. We further propose methods to regulate\nthe router's sparsity while balancing the load among experts. ReMoE's\ncontinuous nature enables efficient dynamic allocation of computation across\ntokens and layers, while also exhibiting domain specialization. Our experiments\ndemonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across\nvarious model sizes, expert counts, and levels of granularity. Furthermore,\nReMoE exhibits superior scalability with respect to the number of experts,\nsurpassing traditional MoE architectures. The implementation based on\nMegatron-LM is available at https://github.com/thu-ml/ReMoE.\n","authors":["Ziteng Wang","Jianfei Chen","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.14711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11479v2","updated":"2024-12-19T10:21:11Z","published":"2024-08-21T09:44:43Z","title":"Learning Deep Dissipative Dynamics","summary":"  This study challenges strictly guaranteeing ``dissipativity'' of a dynamical\nsystem represented by neural networks learned from given time-series data.\nDissipativity is a crucial indicator for dynamical systems that generalizes\nstability and input-output stability, known to be valid across various systems\nincluding robotics, biological systems, and molecular dynamics. By analytically\nproving the general solution to the nonlinear Kalman-Yakubovich-Popov (KYP)\nlemma, which is the necessary and sufficient condition for dissipativity, we\npropose a differentiable projection that transforms any dynamics represented by\nneural networks into dissipative ones and a learning method for the transformed\ndynamics. Utilizing the generality of dissipativity, our method strictly\nguarantee stability, input-output stability, and energy conservation of trained\ndynamical systems. Finally, we demonstrate the robustness of our method against\nout-of-domain input through applications to robotic arms and fluid dynamics.\nCode is https://github.com/kojima-r/DeepDissipativeModel\n","authors":["Yuji Okamoto","Ryosuke Kojima"],"pdf_url":"https://arxiv.org/pdf/2408.11479v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08555v2","updated":"2024-12-19T10:12:38Z","published":"2024-12-11T17:17:02Z","title":"Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks\n  Defending against Poisoning Attacks","summary":"  Recent studies have revealed the vulnerability of graph neural networks\n(GNNs) to adversarial poisoning attacks on node classification tasks. Current\ndefensive methods require substituting the original GNNs with defense models,\nregardless of the original's type. This approach, while targeting adversarial\nrobustness, compromises the enhancements developed in prior research to boost\nGNNs' practical performance. Here we introduce Grimm, the first plug-and-play\ndefense model. With just a minimal interface requirement for extracting\nfeatures from any layer of the protected GNNs, Grimm is thus enabled to\nseamlessly rectify perturbations. Specifically, we utilize the feature\ntrajectories (FTs) generated by GNNs, as they evolve through epochs, to reflect\nthe training status of the networks. We then theoretically prove that the FTs\nof victim nodes will inevitably exhibit discriminable anomalies. Consequently,\ninspired by the natural parallelism between the biological nervous and immune\nsystems, we construct Grimm, a comprehensive artificial immune system for GNNs.\nGrimm not only detects abnormal FTs and rectifies adversarial edges during\ntraining but also operates efficiently in parallel, thereby mirroring the\nconcurrent functionalities of its biological counterparts. We experimentally\nconfirm that Grimm offers four empirically validated advantages: 1)\nHarmlessness, as it does not actively interfere with GNN training; 2)\nParallelism, ensuring monitoring, detection, and rectification functions\noperate independently of the GNN training process; 3) Generalizability,\ndemonstrating compatibility with mainstream GNNs such as GCN, GAT, and\nGraphSAGE; and 4) Transferability, as the detectors for abnormal FTs can be\nefficiently transferred across different systems for one-step rectification.\n","authors":["Ao Liu","Wenshan Li","Beibei Li","Wengang Ma","Tao Li","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.08555v2.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2202.06374v5","updated":"2024-12-19T10:12:00Z","published":"2022-02-13T18:04:00Z","title":"Holdouts set for safe predictive model updating","summary":"  Predictive risk scores for adverse outcomes are increasingly crucial in\nguiding health interventions. Such scores may need to be periodically updated\ndue to change in the distributions they model. However, directly updating risk\nscores used to guide intervention can lead to biased risk estimates. To address\nthis, we propose updating using a `holdout set' - a subset of the population\nthat does not receive interventions guided by the risk score. Balancing the\nholdout set size is essential to ensure good performance of the updated risk\nscore whilst minimising the number of held out samples. We prove that this\napproach reduces adverse outcome frequency to an asymptotically optimal level\nand argue that often there is no competitive alternative. We describe\nconditions under which an optimal holdout size (OHS) can be readily identified,\nand introduce parametric and semi-parametric algorithms for OHS estimation. We\napply our methods to the ASPRE risk score for pre-eclampsia to recommend a plan\nfor updating it in the presence of change in the underlying data distribution.\nWe show that, in order to minimise the number of pre-eclampsia cases over time,\nthis is best achieved using a holdout set of around 10,000 individuals.\n","authors":["Sami Haidar-Wehbe","Samuel R Emerson","Louis J M Aslett","James Liley"],"pdf_url":"https://arxiv.org/pdf/2202.06374v5.pdf","comment":"Manuscript includes supplementary materials and figures"},{"id":"http://arxiv.org/abs/2412.07675v3","updated":"2024-12-19T10:11:42Z","published":"2024-12-10T17:02:58Z","title":"RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting","summary":"  Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.\n","authors":["Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2412.07675v3.pdf","comment":"Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117"},{"id":"http://arxiv.org/abs/2412.14701v1","updated":"2024-12-19T10:10:57Z","published":"2024-12-19T10:10:57Z","title":"Taming the Memory Beast: Strategies for Reliable ML Training on\n  Kubernetes","summary":"  Kubernetes offers a powerful orchestration platform for machine learning\ntraining, but memory management can be challenging due to specialized needs and\nresource constraints. This paper outlines how Kubernetes handles memory\nrequests, limits, Quality of Service classes, and eviction policies for ML\nworkloads, with special focus on GPU memory and ephemeral storage. Common\npitfalls such as overcommitment, memory leaks, and ephemeral volume exhaustion\nare examined. We then provide best practices for stable, scalable memory\nutilization to help ML practitioners prevent out-of-memory events and ensure\nhigh-performance ML training pipelines.\n","authors":["Jaideep Ray"],"pdf_url":"https://arxiv.org/pdf/2412.14701v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2412.08160v4","updated":"2024-12-19T10:01:27Z","published":"2024-12-11T07:32:38Z","title":"DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with\n  Selective State Space Models","summary":"  Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns,\nwidely existing in the real world. Nevertheless, the structure incompleteness,\nnoise, and redundancy result in poor robustness for Dynamic Graph Neural\nNetworks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising\nway to optimize graph structures. However, aside from encountering unacceptable\nquadratic complexity, it overly relies on heuristic priors, making it hard to\ndiscover underlying predictive patterns. How to efficiently refine the dynamic\nstructures, capture intrinsic dependencies, and learn robust representations,\nremains under-explored. In this work, we propose the novel DG-Mamba, a robust\nand efficient Dynamic Graph structure learning framework with the Selective\nState Space Models (Mamba). To accelerate the spatio-temporal structure\nlearning, we propose a kernelized dynamic message-passing operator that reduces\nthe quadratic time complexity to linear. To capture global intrinsic dynamics,\nwe establish the dynamic graph as a self-contained system with State Space\nModel. By discretizing the system states with the cross-snapshot graph\nadjacency, we enable the long-distance dependencies capturing with the\nselective snapshot scan. To endow learned dynamic structures more expressive\nwith informativeness, we propose the self-supervised Principle of Relevant\nInformation for DGSL to regularize the most relevant yet least redundant\ninformation, enhancing global robustness. Extensive experiments demonstrate the\nsuperiority of the robustness and efficiency of our DG-Mamba compared with the\nstate-of-the-art baselines against adversarial attacks.\n","authors":["Haonan Yuan","Qingyun Sun","Zhaonan Wang","Xingcheng Fu","Cheng Ji","Yongjian Wang","Bo Jin","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2412.08160v4.pdf","comment":"Accepted by the Main Technical Track of the 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2412.14695v1","updated":"2024-12-19T09:56:01Z","published":"2024-12-19T09:56:01Z","title":"Lorentzian Residual Neural Networks","summary":"  Hyperbolic neural networks have emerged as a powerful tool for modeling\nhierarchical data structures prevalent in real-world datasets. Notably,\nresidual connections, which facilitate the direct flow of information across\nlayers, have been instrumental in the success of deep neural networks. However,\ncurrent methods for constructing hyperbolic residual networks suffer from\nlimitations such as increased model complexity, numerical instability, and\nerrors due to multiple mappings to and from the tangent space. To address these\nlimitations, we introduce LResNet, a novel Lorentzian residual neural network\nbased on the weighted Lorentzian centroid in the Lorentz model of hyperbolic\ngeometry. Our method enables the efficient integration of residual connections\nin Lorentz hyperbolic neural networks while preserving their hierarchical\nrepresentation capabilities. We demonstrate that our method can theoretically\nderive previous methods while offering improved stability, efficiency, and\neffectiveness. Extensive experiments on both graph and vision tasks showcase\nthe superior performance and robustness of our method compared to\nstate-of-the-art Euclidean and hyperbolic alternatives. Our findings highlight\nthe potential of \\method for building more expressive neural networks in\nhyperbolic embedding space as a generally applicable method to multiple\narchitectures, including CNNs, GNNs, and graph Transformers.\n","authors":["Neil He","Menglin Yang","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2412.14695v1.pdf","comment":"12 pages, 3 figures, KDD 2025"},{"id":"http://arxiv.org/abs/2410.05016v2","updated":"2024-12-19T09:49:25Z","published":"2024-10-07T13:15:07Z","title":"T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data","summary":"  Self-supervision is often used for pre-training to foster performance on a\ndownstream task by constructing meaningful representations of samples.\nSelf-supervised learning (SSL) generally involves generating different views of\nthe same sample and thus requires data augmentations that are challenging to\nconstruct for tabular data. This constitutes one of the main challenges of\nself-supervision for structured data. In the present work, we propose a novel\naugmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on\na Joint Embedding Predictive Architecture (JEPA) and is akin to mask\nreconstruction in the latent space. It involves predicting the latent\nrepresentation of one subset of features from the latent representation of a\ndifferent subset within the same sample, thereby learning rich representations\nwithout augmentations. We use our method as a pre-training technique and train\nseveral deep classifiers on the obtained representation. Our experimental\nresults demonstrate a substantial improvement in both classification and\nregression tasks, outperforming models trained directly on samples in their\noriginal data space. Moreover, T-JEPA enables some methods to consistently\noutperform or match the performance of traditional methods likes Gradient\nBoosted Decision Trees. To understand why, we extensively characterize the\nobtained representations and show that T-JEPA effectively identifies relevant\nfeatures for downstream tasks without access to the labels. Additionally, we\nintroduce regularization tokens, a novel regularization method critical for\ntraining of JEPA-based models on structured data.\n","authors":["Hugo Thimonier","José Lucas De Melo Costa","Fabrice Popineau","Arpad Rimmel","Bich-Liên Doan"],"pdf_url":"https://arxiv.org/pdf/2410.05016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14689v1","updated":"2024-12-19T09:43:39Z","published":"2024-12-19T09:43:39Z","title":"How to Synthesize Text Data without Model Collapse?","summary":"  Model collapse in synthetic data indicates that iterative training on\nself-generated data leads to a gradual decline in performance. With the\nproliferation of AI models, synthetic data will fundamentally reshape the web\ndata ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend\nof synthetic and human-produced data. In this paper, we focus on two questions:\nwhat is the impact of synthetic data on language model training, and how to\nsynthesize data without model collapse? We first pre-train language models\nacross different proportions of synthetic data, revealing a negative\ncorrelation between the proportion of synthetic data and model performance. We\nfurther conduct statistical analysis on synthetic data to uncover\ndistributional shift phenomenon and over-concentration of n-gram features.\nInspired by the above findings, we propose token editing on human-produced data\nto obtain semi-synthetic data. As a proof of concept, we theoretically\ndemonstrate that token-level editing can prevent model collapse, as the test\nerror is constrained by a finite upper bound. We conduct extensive experiments\non pre-training from scratch, continual pre-training, and supervised\nfine-tuning. The results validate our theoretical proof that token-level\nediting improves data quality and enhances model performance.\n","authors":["Xuekai Zhu","Daixuan Cheng","Hengli Li","Kaiyan Zhang","Ermo Hua","Xingtai Lv","Ning Ding","Zhouhan Lin","Zilong Zheng","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12327v2","updated":"2024-12-19T09:34:30Z","published":"2024-12-16T19:54:57Z","title":"Leveraging Group Classification with Descending Soft Labeling for Deep\n  Imbalanced Regression","summary":"  Deep imbalanced regression (DIR), where the target values have a highly\nskewed distribution and are also continuous, is an intriguing yet\nunder-explored problem in machine learning.\n  While recent works have already shown that incorporating various\nclassification-based regularizers can produce enhanced outcomes, the role of\nclassification remains elusive in DIR.\n  Moreover, such regularizers (e.g., contrastive penalties) merely focus on\nlearning discriminative features of data, which inevitably results in ignorance\nof either continuity or similarity across the data.\n  To address these issues, we first bridge the connection between the\nobjectives of DIR and classification from a Bayesian perspective.\n  Consequently, this motivates us to decompose the objective of DIR into a\ncombination of classification and regression tasks, which naturally guides us\ntoward a divide-and-conquer manner to solve the DIR problem.\n  Specifically, by aggregating the data at nearby labels into the same groups,\nwe introduce an ordinal group-aware contrastive learning loss along with a\nmulti-experts regressor to tackle the different groups of data thereby\nmaintaining the data continuity.\n  Meanwhile, considering the similarity between the groups, we also propose a\nsymmetric descending soft labeling strategy to exploit the intrinsic similarity\nacross the data, which allows classification to facilitate regression more\neffectively.\n  Extensive experiments on real-world datasets also validate the effectiveness\nof our method.\n","authors":["Ruizhi Pu","Gezheng Xu","Ruiyi Fang","Binkun Bao","Charles X. Ling","Boyu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06401v2","updated":"2024-12-19T09:30:05Z","published":"2024-08-12T12:09:25Z","title":"Langevin dynamics for high-dimensional optimization: the case of\n  multi-spiked tensor PCA","summary":"  We study nonconvex optimization in high dimensions through Langevin dynamics,\nfocusing on the multi-spiked tensor PCA problem. This tensor estimation problem\ninvolves recovering $r$ hidden signal vectors (spikes) from noisy Gaussian\ntensor observations using maximum likelihood estimation. We study the number of\nsamples required for Langevin dynamics to efficiently recover the spikes and\ndetermine the necessary separation condition on the signal-to-noise ratios\n(SNRs) for exact recovery, distinguishing the cases $p \\ge 3$ and $p=2$, where\n$p$ denotes the order of the tensor. In particular, we show that the sample\ncomplexity required for recovering the spike associated with the largest SNR\nmatches the well-known algorithmic threshold for the single-spike case, while\nthis threshold degrades when recovering all $r$ spikes. As a key step, we\nprovide a detailed characterization of the trajectory and interactions of\nlow-dimensional projections that capture the high-dimensional dynamics.\n","authors":["Gérard Ben Arous","Cédric Gerbelot","Vanessa Piccolo"],"pdf_url":"https://arxiv.org/pdf/2408.06401v2.pdf","comment":"65 pages"},{"id":"http://arxiv.org/abs/2412.06926v3","updated":"2024-12-19T09:24:39Z","published":"2024-12-09T19:11:54Z","title":"When Every Token Counts: Optimal Segmentation for Low-Resource Language\n  Models","summary":"  Traditional greedy tokenization methods have been a critical step in Natural\nLanguage Processing (NLP), influencing how text is converted into tokens and\ndirectly impacting model performance. While subword tokenizers like Byte-Pair\nEncoding (BPE) are widely used, questions remain about their optimality across\nmodel scales and languages. In this work, we demonstrate through extensive\nexperiments that an optimal BPE configuration significantly reduces token count\ncompared to greedy segmentation, yielding improvements in token-saving\npercentages and performance benefits, particularly for smaller models. We\nevaluate tokenization performance across various intrinsic and extrinsic tasks,\nincluding generation and classification. Our findings suggest that\ncompression-optimized tokenization strategies could provide substantial\nadvantages for multilingual and low-resource language applications,\nhighlighting a promising direction for further research and inclusive NLP.\n","authors":["Bharath Raj S","Garvit Suri","Vikrant Dewangan","Raghav Sonavane"],"pdf_url":"https://arxiv.org/pdf/2412.06926v3.pdf","comment":"LoResLM @ COLING 2025"},{"id":"http://arxiv.org/abs/2412.14668v1","updated":"2024-12-19T09:20:27Z","published":"2024-12-19T09:20:27Z","title":"LoLaFL: Low-Latency Federated Learning via Forward-only Propagation","summary":"  Federated learning (FL) has emerged as a widely adopted paradigm for enabling\nedge learning with distributed data while ensuring data privacy. However, the\ntraditional FL with deep neural networks trained via backpropagation can hardly\nmeet the low-latency learning requirements in the sixth generation (6G) mobile\nnetworks. This challenge mainly arises from the high-dimensional model\nparameters to be transmitted and the numerous rounds of communication required\nfor convergence due to the inherent randomness of the training process. To\naddress this issue, we adopt the state-of-the-art principle of maximal coding\nrate reduction to learn linear discriminative features and extend the resultant\nwhite-box neural network into FL, yielding the novel framework of Low-Latency\nFederated Learning (LoLaFL) via forward-only propagation. LoLaFL enables\nlayer-wise transmissions and aggregation with significantly fewer communication\nrounds, thereby considerably reducing latency. Additionally, we propose two\n\\emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on\nthe proof that the optimal NN parameter aggregation in LoLaFL should be\nharmonic-mean-like. The second scheme further exploits the low-rank structures\nof the features and transmits the low-rank-approximated covariance matrices of\nfeatures to achieve additional latency reduction. Theoretic analysis and\nexperiments are conducted to evaluate the performance of LoLaFL. In comparison\nwith traditional FL, the two nonlinear aggregation schemes for LoLaFL can\nachieve reductions in latency of over 91\\% and 98\\%, respectively, while\nmaintaining comparable accuracies.\n","authors":["Jierui Zhang","Jianhao Huang","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2412.14668v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.14660v1","updated":"2024-12-19T09:10:07Z","published":"2024-12-19T09:10:07Z","title":"Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.\n","authors":["Zijun Chen","Wenbo Hu","Guande He","Zhijie Deng","Zheng Zhang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.14660v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.14655v1","updated":"2024-12-19T09:06:39Z","published":"2024-12-19T09:06:39Z","title":"Trainable Adaptive Activation Function Structure (TAAFS) Enhances Neural\n  Network Force Field Performance with Only Dozens of Additional Parameters","summary":"  At the heart of neural network force fields (NNFFs) is the architecture of\nneural networks, where the capacity to model complex interactions is typically\nenhanced through widening or deepening multilayer perceptrons (MLPs) or by\nincreasing layers of graph neural networks (GNNs). These enhancements, while\nimproving the model's performance, often come at the cost of a substantial\nincrease in the number of parameters. By applying the Trainable Adaptive\nActivation Function Structure (TAAFS), we introduce a method that selects\ndistinct mathematical formulations for non-linear activations, thereby\nincreasing the precision of NNFFs with an insignificant addition to the\nparameter count. In this study, we integrate TAAFS into a variety of neural\nnetwork models, resulting in observed accuracy improvements, and further\nvalidate these enhancements through molecular dynamics (MD) simulations using\nDeepMD.\n","authors":["Enji Li"],"pdf_url":"https://arxiv.org/pdf/2412.14655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15031v2","updated":"2024-12-19T09:00:34Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.14650v1","updated":"2024-12-19T08:59:49Z","published":"2024-12-19T08:59:49Z","title":"Permutation recovery of spikes in noisy high-dimensional tensor\n  estimation","summary":"  We study the dynamics of gradient flow in high dimensions for the\nmulti-spiked tensor problem, where the goal is to estimate $r$ unknown signal\nvectors (spikes) from noisy Gaussian tensor observations. Specifically, we\nanalyze the maximum likelihood estimation procedure, which involves optimizing\na highly nonconvex random function. We determine the sample complexity required\nfor gradient flow to efficiently recover all spikes, without imposing any\nassumptions on the separation of the signal-to-noise ratios (SNRs). More\nprecisely, our results provide the sample complexity required to guarantee\nrecovery of the spikes up to a permutation. Our work builds on our companion\npaper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and\ndetermines the sample complexity and separation conditions for the SNRs\nnecessary for ensuring exact recovery of the spikes (where the recovered\npermutation matches the identity). During the recovery process, the\ncorrelations between the estimators and the hidden vectors increase in a\nsequential manner. The order in which these correlations become significant\ndepends on their initial values and the corresponding SNRs, which ultimately\ndetermines the permutation of the recovered spikes.\n","authors":["Gérard Ben Arous","CĆedric Gerbelot","Vanessa Piccolo"],"pdf_url":"https://arxiv.org/pdf/2412.14650v1.pdf","comment":"29 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2408.06401"},{"id":"http://arxiv.org/abs/2406.16606v2","updated":"2024-12-19T08:53:52Z","published":"2024-06-24T12:46:16Z","title":"Cherry on the Cake: Fairness is NOT an Optimization Problem","summary":"  In Fair AI literature, the practice of maliciously creating unfair models\nthat nevertheless satisfy fairness constraints is known as \"cherry-picking\". A\ncherry-picking model is a model that makes mistakes on purpose, selecting bad\nindividuals from a minority class instead of better candidates from the same\nminority. The model literally cherry-picks whom to select to superficially meet\nthe fairness constraints while making minimal changes to the unfair model. This\npractice has been described as \"blatantly unfair\" and has a negative impact on\nalready marginalized communities, undermining the intended purpose of fairness\nmeasures specifically designed to protect these communities. A common\nassumption is that cherry-picking arises solely from malicious intent and that\nmodels designed only to optimize fairness metrics would avoid this behavior. We\nshow that this is not the case: models optimized to minimize fairness metrics\nwhile maximizing performance are often forced to cherry-pick to some degree. In\nother words, cherry-picking might be an inevitable outcome of the optimization\nprocess itself. To demonstrate this, we use tools from fair cake-cutting, a\nmathematical subfield that studies the problem of fairly dividing a resource,\nreferred to as the \"cake,\" among a number of participants. This concept is\nconnected to supervised multi-label classification: any dataset can be thought\nof as a cake that needs to be distributed among different labels, and the model\nis the function that divides the cake. We adapt these classical results for\nmachine learning and demonstrate how this connection can be prolifically used\nfor fairness and classification in general.\n","authors":["Marco Favier","Toon Calders"],"pdf_url":"https://arxiv.org/pdf/2406.16606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14640v1","updated":"2024-12-19T08:51:01Z","published":"2024-12-19T08:51:01Z","title":"Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention\n  for Fine-Grained Few-Shot Learning","summary":"  Few-shot, fine-grained classification in computer vision poses significant\nchallenges due to the need to differentiate subtle class distinctions with\nlimited data. This paper presents a novel method that enhances the Contrastive\nLanguage-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided\nby real-time visual inputs. Unlike existing techniques such as Context\nOptimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by\nstatic prompts or visual token reliance, the proposed approach leverages a\ncross-attention mechanism to dynamically refine text prompts for the image at\nhand. This enables an image-specific alignment of textual features with image\npatches extracted from the Vision Transformer, making the model more effective\nfor datasets with high intra-class variance and low inter-class differences.\nThe method is evaluated on several datasets, including CUBirds, Oxford Flowers,\nand FGVC Aircraft, showing significant performance gains over static prompt\ntuning approaches. To ensure these performance gains translate into trustworthy\npredictions, we integrate Monte-Carlo Dropout in our approach to improve the\nreliability of the model predictions and uncertainty estimates. This\nintegration provides valuable insights into the model's predictive confidence,\nhelping to identify when predictions can be trusted and when additional\nverification is necessary. This dynamic approach offers a robust solution,\nadvancing the state-of-the-art for few-shot fine-grained classification.\n","authors":["Eric Brouwer","Jan Erik van Woerden","Gertjan Burghouts","Matias Valedenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2412.14640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14492v2","updated":"2024-12-19T08:46:02Z","published":"2024-05-23T12:25:22Z","title":"Iterative Methods for Full-Scale Gaussian Process Approximations for\n  Large Spatial Data","summary":"  Gaussian processes are flexible probabilistic regression models which are\nwidely used in statistics and machine learning. However, a drawback is their\nlimited scalability to large data sets. To alleviate this, we consider\nfull-scale approximations (FSAs) that combine predictive process methods and\ncovariance tapering, thus approximating both global and local structures. We\nshow how iterative methods can be used to reduce the computational costs for\ncalculating likelihoods, gradients, and predictive distributions with FSAs. We\nintroduce a novel preconditioner and show that it accelerates the conjugate\ngradient method's convergence speed and mitigates its sensitivity with respect\nto the FSA parameters and the eigenvalue structure of the original covariance\nmatrix, and we demonstrate empirically that it outperforms a state-of-the-art\npivoted Cholesky preconditioner. Further, we present a novel, accurate, and\nfast way to calculate predictive variances relying on stochastic estimations\nand iterative methods. In both simulated and real-world data experiments, we\nfind that our proposed methodology achieves the same accuracy as Cholesky-based\ncomputations with a substantial reduction in computational time. Finally, we\nalso compare different approaches for determining inducing points in predictive\nprocess and FSA models. All methods are implemented in a free C++ software\nlibrary with high-level Python and R packages.\n","authors":["Tim Gyger","Reinhard Furrer","Fabio Sigrist"],"pdf_url":"https://arxiv.org/pdf/2405.14492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16149v3","updated":"2024-12-19T08:34:15Z","published":"2024-03-24T13:43:43Z","title":"Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey","summary":"  The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to network traffic analysis in other fields such as mobile\napps and websites, CIoT presents unique characteristics, introducing new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for studying CIoT security and privacy risks, this\nsurvey reviews 303 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs.\n","authors":["Yan Jia","Yuxin Song","Zihou Liu","Qingyin Tan","Yang Song","Yu Zhang","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16149v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14629v1","updated":"2024-12-19T08:31:42Z","published":"2024-12-19T08:31:42Z","title":"Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix\n  Factorization","summary":"  Robust Principal Component Analysis (RPCA) is a fundamental technique for\ndecomposing data into low-rank and sparse components, which plays a critical\nrole for applications such as image processing and anomaly detection.\nTraditional RPCA methods commonly use $\\ell_1$ norm regularization to enforce\nsparsity, but this approach can introduce bias and result in suboptimal\nestimates, particularly in the presence of significant noise or outliers.\nNon-convex regularization methods have been proposed to mitigate these\nchallenges, but they tend to be complex to optimize and sensitive to initial\nconditions, leading to potential instability in solutions. To overcome these\nchallenges, in this paper, we propose a novel RPCA model that integrates\nadaptive weighted least squares (AWLS) and low-rank matrix factorization\n(LRMF). The model employs a {self-attention-inspired} mechanism in its weight\nupdate process, allowing the weight matrix to dynamically adjust and emphasize\nsignificant components during each iteration. By employing a weighted F-norm\nfor the sparse component, our method effectively reduces bias while simplifying\nthe computational process compared to traditional $\\ell_1$-norm-based methods.\nWe use an alternating minimization algorithm, where each subproblem has an\nexplicit solution, thereby improving computational efficiency. Despite its\nsimplicity, numerical experiments demonstrate that our method outperforms\nexisting non-convex regularization approaches, offering superior performance\nand stability, as well as enhanced accuracy and robustness in practical\napplications.\n","authors":["Kexin Li","You-wei Wen","Xu Xiao","Mingchao Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14628v1","updated":"2024-12-19T08:30:54Z","published":"2024-12-19T08:30:54Z","title":"Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models","summary":"  Diffusion Models (DM) have democratized AI image generation through an\niterative denoising process. Quantization is a major technique to alleviate the\ninference cost and reduce the size of DM denoiser networks. However, as\ndenoisers evolve from variants of convolutional U-Nets toward newer Transformer\narchitectures, it is of growing importance to understand the quantization\nsensitivity of different weight layers, operations and architecture types to\nperformance. In this work, we address this challenge with Qua$^2$SeDiMo, a\nmixed-precision Post-Training Quantization framework that generates explainable\ninsights on the cost-effectiveness of various model weight quantization methods\nfor different denoiser operation types and block structures. We leverage these\ninsights to make high-quality mixed-precision quantization decisions for a\nmyriad of diffusion models ranging from foundational U-Nets to state-of-the-art\nTransformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,\n3.65-bit and 3.7-bit weight quantization on PixArt-${\\alpha}$,\nPixArt-${\\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our\nweight-quantization configurations with 6-bit activation quantization and\noutperform existing approaches in terms of quantitative metrics and generative\nimage quality.\n","authors":["Keith G. Mills","Mohammad Salameh","Ruichen Chen","Negar Hassanpour","Wei Lu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2412.14628v1.pdf","comment":"AAAI 2025; version includes supplementary material; 22 Pages, 18\n  Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2302.09526v4","updated":"2024-12-19T08:22:30Z","published":"2023-02-19T09:55:18Z","title":"Mixed Semi-Supervised Generalized-Linear-Regression with Applications to\n  Deep-Learning and Interpolators","summary":"  We present a methodology for using unlabeled data to design semi supervised\nlearning (SSL) methods that improve the prediction performance of supervised\nlearning for regression tasks. The main idea is to design different mechanisms\nfor integrating the unlabeled data, and include in each of them a mixing\nparameter $\\alpha$, controlling the weight given to the unlabeled data.\nFocusing on Generalized Linear Models (GLM) and linear interpolators classes of\nmodels, we analyze the characteristics of different mixing mechanisms, and\nprove that in all cases, it is invariably beneficial to integrate the unlabeled\ndata with some nonzero mixing ratio $\\alpha>0$, in terms of predictive\nperformance. Moreover, we provide a rigorous framework to estimate the best\nmixing ratio $\\alpha^*$ where mixed SSL delivers the best predictive\nperformance, while using the labeled and unlabeled data on hand.\n  The effectiveness of our methodology in delivering substantial improvement\ncompared to the standard supervised models, in a variety of settings, is\ndemonstrated empirically through extensive simulation, in a manner that\nsupports the theoretical analysis. We also demonstrate the applicability of our\nmethodology (with some intuitive modifications) to improve more complex models,\nsuch as deep neural networks, in real-world regression tasks.\n","authors":["Oren Yuval","Saharon Rosset"],"pdf_url":"https://arxiv.org/pdf/2302.09526v4.pdf","comment":"58 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.14031v2","updated":"2024-12-19T08:21:15Z","published":"2024-12-18T16:51:47Z","title":"Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization\n  Perspective","summary":"  We analyze the convergence of Gauss-Newton dynamics for training neural\nnetworks with smooth activation functions. In the underparameterized regime,\nthe Gauss-Newton gradient flow induces a Riemannian gradient flow on a\nlow-dimensional, smooth, embedded submanifold of the Euclidean output space.\nUsing tools from Riemannian optimization, we prove \\emph{last-iterate}\nconvergence of the Riemannian gradient flow to the optimal in-class predictor\nat an \\emph{exponential rate} that is independent of the conditioning of the\nGram matrix, \\emph{without} requiring explicit regularization. We further\ncharacterize the critical impacts of the neural network scaling factor and the\ninitialization on the convergence behavior. In the overparameterized regime, we\nshow that the Levenberg-Marquardt dynamics with an appropriately chosen damping\nfactor yields robustness to ill-conditioned kernels, analogous to the\nunderparameterized regime. These findings demonstrate the potential of\nGauss-Newton methods for efficiently optimizing neural networks, particularly\nin ill-conditioned problems where kernel and Gram matrices have small singular\nvalues.\n","authors":["Semih Cayci"],"pdf_url":"https://arxiv.org/pdf/2412.14031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14620v1","updated":"2024-12-19T08:13:20Z","published":"2024-12-19T08:13:20Z","title":"Continuous latent representations for modeling precipitation with deep\n  learning","summary":"  The sparse and spatio-temporally discontinuous nature of precipitation data\npresents significant challenges for simulation and statistical processing for\nbias correction and downscaling. These include incorrect representation of\nintermittency and extreme values (critical for hydrology applications), Gibbs\nphenomenon upon regridding, and lack of fine scales details. To address these\nchallenges, a common approach is to transform the precipitation variable\nnonlinearly into one that is more malleable. In this work, we explore how deep\nlearning can be used to generate a smooth, spatio-temporally continuous\nvariable as a proxy for simulation of precipitation data. We develop a normally\ndistributed field called pseudo-precipitation (PP) as an alternative for\nsimulating precipitation. The practical applicability of this variable is\ninvestigated by applying it for downscaling precipitation from \\(1\\degree\\)\n(\\(\\sim\\) 100 km) to \\(0.25\\degree\\) (\\(\\sim\\) 25 km).\n","authors":["Gokul Radhakrishnan","Rahul Sundar","Nishant Parashar","Antoine Blanchard","Daiwei Wang","Boyko Dodov"],"pdf_url":"https://arxiv.org/pdf/2412.14620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14619v1","updated":"2024-12-19T08:11:42Z","published":"2024-12-19T08:11:42Z","title":"Pitfalls of topology-aware image segmentation","summary":"  Topological correctness, i.e., the preservation of structural integrity and\nspecific characteristics of shape, is a fundamental requirement for medical\nimaging tasks, such as neuron or vessel segmentation. Despite the recent surge\nin topology-aware methods addressing this challenge, their real-world\napplicability is hindered by flawed benchmarking practices. In this paper, we\nidentify critical pitfalls in model evaluation that include inadequate\nconnectivity choices, overlooked topological artifacts in ground truth\nannotations, and inappropriate use of evaluation metrics. Through detailed\nempirical analysis, we uncover these issues' profound impact on the evaluation\nand ranking of segmentation methods. Drawing from our findings, we propose a\nset of actionable recommendations to establish fair and robust evaluation\nstandards for topology-aware medical image segmentation methods.\n","authors":["Alexander H. Berger","Laurin Lux","Alexander Weers","Martin Menten","Daniel Rueckert","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2412.14619v1.pdf","comment":"Code is available at\n  https://github.com/AlexanderHBerger/topo-pitfalls"},{"id":"http://arxiv.org/abs/2412.14602v1","updated":"2024-12-19T07:48:14Z","published":"2024-12-19T07:48:14Z","title":"Towards Scalable and Deep Graph Neural Networks via Noise Masking","summary":"  In recent years, Graph Neural Networks (GNNs) have achieved remarkable\nsuccess in many graph mining tasks. However, scaling them to large graphs is\nchallenging due to the high computational and storage costs of repeated feature\npropagation and non-linear transformation during training. One commonly\nemployed approach to address this challenge is model-simplification, which only\nexecutes the Propagation (P) once in the pre-processing, and Combine (C) these\nreceptive fields in different ways and then feed them into a simple model for\nbetter performance. Despite their high predictive performance and scalability,\nthese methods still face two limitations. First, existing approaches mainly\nfocus on exploring different C methods from the model perspective, neglecting\nthe crucial problem of performance degradation with increasing P depth from the\ndata-centric perspective, known as the over-smoothing problem. Second,\npre-processing overhead takes up most of the end-to-end processing time,\nespecially for large-scale graphs. To address these limitations, we present\nrandom walk with noise masking (RMask), a plug-and-play module compatible with\nthe existing model-simplification works. This module enables the exploration of\ndeeper GNNs while preserving their scalability. Unlike the previous\nmodel-simplification works, we focus on continuous P and found that the noise\nexisting inside each P is the cause of the over-smoothing issue, and use the\nefficient masking mechanism to eliminate them. Experimental results on six\nreal-world datasets demonstrate that model-simplification works equipped with\nRMask yield superior performance compared to their original version and can\nmake a good trade-off between accuracy and efficiency.\n","authors":["Yuxuan Liang","Wentao Zhang","Zeang Sheng","Ling Yang","Quanqing Xu","Jiawei Jiang","Yunhai Tong","Bin Cu"],"pdf_url":"https://arxiv.org/pdf/2412.14602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14599v1","updated":"2024-12-19T07:42:07Z","published":"2024-12-19T07:42:07Z","title":"Fast inverse lithography based on a model-driven block stacking\n  convolutional neural network","summary":"  In the realm of lithography, Optical Proximity Correction (OPC) is a crucial\nresolution enhancement technique that optimizes the transmission function of\nphotomasks on a pixel-based to effectively counter Optical Proximity Effects\n(OPE). However, conventional pixel-based OPC methods often generate patterns\nthat pose manufacturing challenges, thereby leading to the increased cost in\npractical scenarios. This paper presents a novel inverse lithographic approach\nto OPC, employing a model-driven, block stacking deep learning framework that\nexpedites the generation of masks conducive to manufacturing. This method is\nfounded on vector lithography modelling and streamlines the training process by\neliminating the requirement for extensive labeled datasets. Furthermore,\ndiversity of mask patterns is enhanced by employing a wave function collapse\nalgorithm, which facilitates the random generation of a multitude of target\npatterns, therefore significantly expanding the range of mask paradigm.\nNumerical experiments have substantiated the efficacy of the proposed\nend-to-end approach, highlighting its superior capability to manage mask\ncomplexity within the context of advanced OPC lithography. This advancement is\nanticipated to enhance the feasibility and economic viability of OPC technology\nwithin actual manufacturing environments.\n","authors":["Ruixiang Chen","Yang Zhao","Haoqin Li","Rui Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14599v1.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.04979v3","updated":"2024-12-19T07:33:48Z","published":"2024-01-10T07:51:02Z","title":"DualDynamics: Synergizing Implicit and Explicit Methods for Robust\n  Irregular Time Series Analysis","summary":"  Real-world time series analysis faces significant challenges when dealing\nwith irregular and incomplete data. While Neural Differential Equation (NDE)\nbased methods have shown promise, they struggle with limited expressiveness,\nscalability issues, and stability concerns. Conversely, Neural Flows offer\nstability but falter with irregular data. We introduce 'DualDynamics', a novel\nframework that synergistically combines NDE-based method and Neural Flow-based\nmethod. This approach enhances expressive power while balancing computational\ndemands, addressing critical limitations of existing techniques. We demonstrate\nDualDynamics' effectiveness across diverse tasks: classification of robustness\nto dataset shift, irregularly-sampled series analysis, interpolation of missing\ndata, and forecasting with partial observations. Our results show consistent\noutperformance over state-of-the-art methods, indicating DualDynamics'\npotential to advance irregular time series analysis significantly.\n","authors":["YongKyung Oh","Dongyoung Lim","Sungil Kim"],"pdf_url":"https://arxiv.org/pdf/2401.04979v3.pdf","comment":"Published at the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.14596v1","updated":"2024-12-19T07:31:40Z","published":"2024-12-19T07:31:40Z","title":"LDP: Generalizing to Multilingual Visual Information Extraction by\n  Language Decoupled Pretraining","summary":"  Visual Information Extraction (VIE) plays a crucial role in the comprehension\nof semi-structured documents, and several pre-trained models have been\ndeveloped to enhance performance. However, most of these works are monolingual\n(usually English). Due to the extremely unbalanced quantity and quality of\npre-training corpora between English and other languages, few works can extend\nto non-English scenarios. In this paper, we conduct systematic experiments to\nshow that vision and layout modality hold invariance among images with\ndifferent languages. If decoupling language bias from document images, a\nvision-layout-based model can achieve impressive cross-lingual generalization.\nAccordingly, we present a simple but effective multilingual training paradigm\nLDP (Language Decoupled Pre-training) for better utilization of monolingual\npre-training data. Our proposed model LDM (Language Decoupled Model) is first\npre-trained on the language-independent data, where the language knowledge is\ndecoupled by a diffusion model, and then the LDM is fine-tuned on the\ndownstream languages. Extensive experiments show that the LDM outperformed all\nSOTA multilingual pre-trained models, and also maintains competitiveness on\ndownstream monolingual/English benchmarks.\n","authors":["Huawen Shen","Gengluo Li","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14596v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2409.05929v2","updated":"2024-12-19T07:31:18Z","published":"2024-09-09T10:40:50Z","title":"Alt-MoE: Multimodal Alignment via Alternating Optimization of\n  Multi-directional MoE with Unimodal Models","summary":"  Recent Large Multi-Modal Models (LMMs) have made significant advancements in\nmulti-modal alignment by employing lightweight connection modules to facilitate\nthe representation and fusion of knowledge from existing pre-trained uni-modal\nmodels. However, these methods still rely on modality-specific and\ndirection-specific connectors, leading to compartmentalized knowledge\nrepresentations and reduced computational efficiency, which limits the model's\nability to form unified multi-modal representations. To address these issues,\nwe introduce a novel training framework, Alt-MoE, which employs the Mixture of\nExperts (MoE) as a unified multi-directional connector across modalities, and\nemploys a multi-step sequential alternating unidirectional alignment strategy,\nwhich converges to bidirectional alignment over iterations. The extensive\nempirical studies revealed the following key points: 1) Alt-MoE achieves\ncompetitive results by integrating diverse knowledge representations from\nuni-modal models. This approach seamlessly fuses the specialized expertise of\nexisting high-performance uni-modal models, effectively synthesizing their\ndomain-specific knowledge into a cohesive multi-modal representation. 2)\nAlt-MoE efficiently scales to new tasks and modalities without altering its\nmodel architecture or training strategy. Furthermore, Alt-MoE operates in\nlatent space, supporting vector pre-storage and real-time retrieval via\nlightweight multi-directional MoE, thereby facilitating massive data\nprocessing. Our methodology has been validated on several well-performing\nuni-modal models (LLAMA3, Qwen2, and DINOv2), achieving competitive results on\na wide range of downstream tasks and datasets.\n","authors":["Hongyang Lei","Xiaolong Cheng","Dan Wang","Kun Fan","Qi Qin","Huazhen Huang","Yetao Wu","Qingqing Gu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.05929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14592v1","updated":"2024-12-19T07:23:17Z","published":"2024-12-19T07:23:17Z","title":"Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,\n  and Internal Properties","summary":"  Object anomaly detection is essential for industrial quality inspection, yet\ntraditional single-sensor methods face critical limitations. They fail to\ncapture the wide range of anomaly types, as single sensors are often\nconstrained to either external appearance, geometric structure, or internal\nproperties. To overcome these challenges, we introduce MulSen-AD, the first\nhigh-resolution, multi-sensor anomaly detection dataset tailored for industrial\napplications. MulSen-AD unifies data from RGB cameras, laser scanners, and\nlock-in infrared thermography, effectively capturing external appearance,\ngeometric deformations, and internal defects. The dataset spans 15 industrial\nproducts with diverse, real-world anomalies. We also present MulSen-AD Bench, a\nbenchmark designed to evaluate multi-sensor methods, and propose\nMulSen-TripleAD, a decision-level fusion algorithm that integrates these three\nmodalities for robust, unsupervised object anomaly detection. Our experiments\ndemonstrate that multi-sensor fusion substantially outperforms single-sensor\napproaches, achieving 96.1% AUROC in object-level detection accuracy. These\nresults highlight the importance of integrating multi-sensor data for\ncomprehensive industrial anomaly detection.\n","authors":["Wenqiao Li","Bozhong Zheng","Xiaohao Xu","Jinye Gan","Fading Lu","Xiang Li","Na Ni","Zheng Tian","Xiaonan Huang","Shenghua Gao","Yingna Wu"],"pdf_url":"https://arxiv.org/pdf/2412.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14590v1","updated":"2024-12-19T07:15:15Z","published":"2024-12-19T07:15:15Z","title":"MixLLM: LLM Quantization with Global Mixed-precision between\n  Output-features and Highly-efficient System Design","summary":"  Quantization has become one of the most effective methodologies to compress\nLLMs into smaller size. However, the existing quantization solutions still show\nlimitations of either non-negligible accuracy drop or system inefficiency. In\nthis paper, we make a comprehensive analysis of the general quantization\nprinciples on their effect to the triangle of accuracy, memory consumption and\nsystem efficiency. We propose MixLLM that explores the new optimization space\nof mixed-precision quantization between output features based on the insight\nthat different output features matter differently in the model. MixLLM\nidentifies the output features with high salience in the global view rather\nthan within each single layer, effectively assigning the larger bit-width to\noutput features that need it most to achieve good accuracy with low memory\nconsumption. We present the sweet spot of quantization configuration of\nalgorithm-system co-design that leads to high accuracy and system efficiency.\nTo address the system challenge, we design the two-step dequantization to make\nuse of the int8 Tensor Core easily and fast data type conversion to reduce\ndequantization overhead significantly, and present the software pipeline to\noverlap the memory access, dequantization and the MatMul to the best. Extensive\nexperiments show that with only 10% more bits, the PPL increasement can be\nreduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on\naverage MMLU-Pro improves by 0.93 over the SOTA of three popular models. In\naddition to its superior accuracy, MixLLM also achieves state-of-the-art system\nefficiency.\n","authors":["Zhen Zheng","Xiaonan Song","Chuanjie Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14590v1.pdf","comment":"The code will be released in the future"},{"id":"http://arxiv.org/abs/2407.02419v3","updated":"2024-12-19T07:07:51Z","published":"2024-07-02T16:44:14Z","title":"Quantum Curriculum Learning","summary":"  Quantum machine learning (QML) requires significant quantum resources to\naddress practical real-world problems. When the underlying quantum information\nexhibits hierarchical structures in the data, limitations persist in training\ncomplexity and generalization. Research should prioritize both the efficient\ndesign of quantum architectures and the development of learning strategies to\noptimize resource usage. We propose a framework called quantum curriculum\nlearning (Q-CurL) for quantum data, where the curriculum introduces simpler\ntasks or data to the learning model before progressing to more challenging\nones. Q-CurL exhibits robustness to noise and data limitations, which is\nparticularly relevant for current and near-term noisy intermediate-scale\nquantum devices. We achieve this through a curriculum design based on quantum\ndata density ratios and a dynamic learning schedule that prioritizes the most\ninformative quantum data. Empirical evidence shows that Q-CurL significantly\nenhances training convergence and generalization for unitary learning and\nimproves the robustness of quantum phase recognition tasks. Q-CurL is effective\nwith broad physical learning applications in condensed matter physics and\nquantum chemistry.\n","authors":["Quoc Hoan Tran","Yasuhiro Endo","Hirotaka Oshima"],"pdf_url":"https://arxiv.org/pdf/2407.02419v3.pdf","comment":"main 6 pages, supplementary materials 11 pages (update the\n  supplementary materials with more explanation on data-based Q-CurL)"},{"id":"http://arxiv.org/abs/2302.03390v5","updated":"2024-12-19T07:05:03Z","published":"2023-02-07T10:51:53Z","title":"Learning Discretized Neural Networks under Ricci Flow","summary":"  In this paper, we study Discretized Neural Networks (DNNs) composed of\nlow-precision weights and activations, which suffer from either infinite or\nzero gradients due to the non-differentiable discrete function during training.\nMost training-based DNNs in such scenarios employ the standard Straight-Through\nEstimator (STE) to approximate the gradient w.r.t. discrete values. However,\nthe use of STE introduces the problem of gradient mismatch, arising from\nperturbations in the approximated gradient. To address this problem, this paper\nreveals that this mismatch can be interpreted as a metric perturbation in a\nRiemannian manifold, viewed through the lens of duality theory. Building on\ninformation geometry, we construct the Linearly Nearly Euclidean (LNE) manifold\nfor DNNs, providing a background for addressing perturbations. By introducing a\npartial differential equation on metrics, i.e., the Ricci flow, we establish\nthe dynamical stability and convergence of the LNE metric with the $L^2$-norm\nperturbation. In contrast to previous perturbation theories with convergence\nrates in fractional powers, the metric perturbation under the Ricci flow\nexhibits exponential decay in the LNE manifold. Experimental results across\nvarious datasets demonstrate that our method achieves superior and more stable\nperformance for DNNs compared to other representative training-based methods.\n","authors":["Jun Chen","Hanwen Chen","Mengmeng Wang","Guang Dai","Ivor W. Tsang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2302.03390v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00284v3","updated":"2024-12-19T06:52:07Z","published":"2023-02-01T07:31:25Z","title":"Selective Uncertainty Propagation in Offline RL","summary":"  We consider the finite-horizon offline reinforcement learning (RL) setting,\nand are motivated by the challenge of learning the policy at any step h in\ndynamic programming (DP) algorithms. To learn this, it is sufficient to\nevaluate the treatment effect of deviating from the behavioral policy at step h\nafter having optimized the policy for all future steps. Since the policy at any\nstep can affect next-state distributions, the related distributional shift\nchallenges can make this problem far more statistically hard than estimating\nsuch treatment effects in the stochastic contextual bandit setting. However,\nthe hardness of many real-world RL instances lies between the two regimes. We\ndevelop a flexible and general method called selective uncertainty propagation\nfor confidence interval construction that adapts to the hardness of the\nassociated distribution shift challenges. We show benefits of our approach on\ntoy environments and demonstrate the benefits of these techniques for offline\npolicy learning.\n","authors":["Sanath Kumar Krishnamurthy","Tanmay Gangwani","Sumeet Katariya","Branislav Kveton","Shrey Modi","Anshuka Rangi"],"pdf_url":"https://arxiv.org/pdf/2302.00284v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07728v3","updated":"2024-12-19T06:51:17Z","published":"2024-03-12T15:07:20Z","title":"CAP: A General Algorithm for Online Selective Conformal Prediction with\n  FCR Control","summary":"  We study the problem of post-selection predictive inference in an online\nfashion. To avoid devoting resources to unimportant units, a preliminary\nselection of the current individual before reporting its prediction interval is\ncommon and meaningful in online predictive tasks. Since the online selection\ncauses a temporal multiplicity in the selected prediction intervals, it is\nimportant to control the real-time false coverage-statement rate (FCR) which\nmeasures the overall miscoverage level. We develop a general framework named\nCAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on\nhistorical data to construct a calibration set if the current individual is\nselected and then outputs a conformal prediction interval for the unobserved\nlabel. We provide tractable procedures for constructing the calibration set for\npopular online selection rules. We proved that CAP can achieve an exact\nselection-conditional coverage guarantee in the finite-sample and\ndistribution-free regimes. To account for the distribution shift in online\ndata, we also embed CAP into some recent dynamic conformal prediction\nalgorithms and show that the proposed method can deliver long-run FCR control.\nNumerical results on both synthetic and real data corroborate that CAP can\neffectively control FCR around the target level and yield more narrowed\nprediction intervals over existing baselines across various settings.\n","authors":["Yajie Bao","Yuyang Huo","Haojie Ren","Changliang Zou"],"pdf_url":"https://arxiv.org/pdf/2403.07728v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14572v1","updated":"2024-12-19T06:42:57Z","published":"2024-12-19T06:42:57Z","title":"Accelerated Patient-Specific Calibration via Differentiable Hemodynamics\n  Simulations","summary":"  One of the goals of personalized medicine is to tailor diagnostics to\nindividual patients. Diagnostics are performed in practice by measuring\nquantities, called biomarkers, that indicate the existence and progress of a\ndisease. In common cardiovascular diseases, such as hypertension, biomarkers\nthat are closely related to the clinical representation of a patient can be\npredicted using computational models. Personalizing computational models\ntranslates to considering patient-specific flow conditions, for example, the\ncompliance of blood vessels that cannot be a priori known and quantities such\nas the patient geometry that can be measured using imaging. Therefore, a\npatient is identified by a set of measurable and nonmeasurable parameters\nneeded to well-define a computational model; else, the computational model is\nnot personalized, meaning it is prone to large prediction errors. Therefore, to\npersonalize a computational model, sufficient information needs to be extracted\nfrom the data. The current methods by which this is done are either\ninefficient, due to relying on slow-converging optimization methods, or hard to\ninterpret, due to using `black box` deep-learning algorithms. We propose a\npersonalized diagnostic procedure based on a differentiable 0D-1D Navier-Stokes\nreduced order model solver and fast parameter inference methods that take\nadvantage of gradients through the solver. By providing a faster method for\nperforming parameter inference and sensitivity analysis through\ndifferentiability while maintaining the interpretability of well-understood\nmathematical models and numerical methods, the best of both worlds is combined.\nThe performance of the proposed solver is validated against a well-established\nprocess on different geometries, and different parameter inference processes\nare successfully performed.\n","authors":["Diego Renner","Georgios Kissas"],"pdf_url":"https://arxiv.org/pdf/2412.14572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14569v1","updated":"2024-12-19T06:40:21Z","published":"2024-12-19T06:40:21Z","title":"Global Spatio-Temporal Fusion-based Traffic Prediction Algorithm with\n  Anomaly Aware","summary":"  Traffic prediction is an indispensable component of urban planning and\ntraffic management. Achieving accurate traffic prediction hinges on the ability\nto capture the potential spatio-temporal relationships among road sensors.\nHowever, the majority of existing works focus on local short-term\nspatio-temporal correlations, failing to fully consider the interactions of\ndifferent sensors in the long-term state. In addition, these works do not\nanalyze the influences of anomalous factors, or have insufficient ability to\nextract personalized features of anomalous factors, which make them\nineffectively capture their spatio-temporal influences on traffic prediction.\nTo address the aforementioned issues, We propose a global spatio-temporal\nfusion-based traffic prediction algorithm that incorporates anomaly awareness.\nInitially, based on the designed anomaly detection network, we construct an\nefficient anomalous factors impacting module (AFIM), to evaluate the\nspatio-temporal impact of unexpected external events on traffic prediction.\nFurthermore, we propose a multi-scale spatio-temporal feature fusion module\n(MTSFFL) based on the transformer architecture, to obtain all possible both\nlong and short term correlations among different sensors in a wide-area traffic\nenvironment for accurate prediction of traffic flow. Finally, experiments are\nimplemented based on real-scenario public transportation datasets (PEMS04 and\nPEMS08) to demonstrate that our approach can achieve state-of-the-art\nperformance.\n","authors":["Chaoqun Liu","Xuanpeng Li","Chen Gong","Guangyu Li"],"pdf_url":"https://arxiv.org/pdf/2412.14569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21853v2","updated":"2024-12-19T06:39:24Z","published":"2024-10-29T08:28:23Z","title":"Learning Infinitesimal Generators of Continuous Symmetries from Data","summary":"  Exploiting symmetry inherent in data can significantly improve the sample\nefficiency of a learning procedure and the generalization of learned models.\nWhen data clearly reveals underlying symmetry, leveraging this symmetry can\nnaturally inform the design of model architectures or learning strategies. Yet,\nin numerous real-world scenarios, identifying the specific symmetry within a\ngiven data distribution often proves ambiguous. To tackle this, some existing\nworks learn symmetry in a data-driven manner, parameterizing and learning\nexpected symmetry through data. However, these methods often rely on explicit\nknowledge, such as pre-defined Lie groups, which are typically restricted to\nlinear or affine transformations. In this paper, we propose a novel symmetry\nlearning algorithm based on transformations defined with one-parameter groups,\ncontinuously parameterized transformations flowing along the directions of\nvector fields called infinitesimal generators. Our method is built upon minimal\ninductive biases, encompassing not only commonly utilized symmetries rooted in\nLie groups but also extending to symmetries derived from nonlinear generators.\nTo learn these symmetries, we introduce a notion of a validity score that\nexamine whether the transformed data is still valid for the given task. The\nvalidity score is designed to be fully differentiable and easily computable,\nenabling effective searches for transformations that achieve symmetries innate\nto the data. We apply our method mainly in two domains: image data and partial\ndifferential equations, and demonstrate its advantages. Our codes are available\nat \\url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.\n","authors":["Gyeonghoon Ko","Hyunsu Kim","Juho Lee"],"pdf_url":"https://arxiv.org/pdf/2410.21853v2.pdf","comment":"Neurips 2024"},{"id":"http://arxiv.org/abs/2412.14566v1","updated":"2024-12-19T06:35:54Z","published":"2024-12-19T06:35:54Z","title":"AIArena: A Blockchain-Based Decentralized AI Training Platform","summary":"  The rapid advancement of AI has underscored critical challenges in its\ndevelopment and implementation, largely due to centralized control by a few\nmajor corporations. This concentration of power intensifies biases within AI\nmodels, resulting from inadequate governance and oversight mechanisms.\nAdditionally, it limits public involvement and heightens concerns about the\nintegrity of model generation. Such monopolistic control over data and AI\noutputs threatens both innovation and fair data usage, as users inadvertently\ncontribute data that primarily benefits these corporations. In this work, we\npropose AIArena, a blockchain-based decentralized AI training platform designed\nto democratize AI development and alignment through on-chain incentive\nmechanisms. AIArena fosters an open and collaborative environment where\nparticipants can contribute models and computing resources. Its on-chain\nconsensus mechanism ensures fair rewards for participants based on their\ncontributions. We instantiate and implement AIArena on the public Base\nblockchain Sepolia testnet, and the evaluation results demonstrate the\nfeasibility of AIArena in real-world applications.\n","authors":["Zhipeng Wang","Rui Sun","Elizabeth Lui","Tuo Zhou","Yizhe Wen","Jiahao Sun"],"pdf_url":"https://arxiv.org/pdf/2412.14566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08282v2","updated":"2024-12-19T06:35:21Z","published":"2024-12-11T10:57:16Z","title":"How Does the Smoothness Approximation Method Facilitate Generalization\n  for Federated Adversarial Learning?","summary":"  Federated Adversarial Learning (FAL) is a robust framework for resisting\nadversarial attacks on federated learning. Although some FAL studies have\ndeveloped efficient algorithms, they primarily focus on convergence performance\nand overlook generalization. Generalization is crucial for evaluating algorithm\nperformance on unseen data. However, generalization analysis is more\nchallenging due to non-smooth adversarial loss functions. A common approach to\naddressing this issue is to leverage smoothness approximation. In this paper,\nwe develop algorithm stability measures to evaluate the generalization\nperformance of two popular FAL algorithms: \\textit{Vanilla FAL (VFAL)} and {\\it\nSlack FAL (SFAL)}, using three different smooth approximation methods: 1)\n\\textit{Surrogate Smoothness Approximation (SSA)}, (2) \\textit{Randomized\nSmoothness Approximation (RSA)}, and (3) \\textit{Over-Parameterized Smoothness\nApproximation (OPSA)}. Based on our in-depth analysis, we answer the question\nof how to properly set the smoothness approximation method to mitigate\ngeneralization error in FAL. Moreover, we identify RSA as the most effective\nmethod for reducing generalization error. In highly data-heterogeneous\nscenarios, we also recommend employing SFAL to mitigate the deterioration of\ngeneralization performance caused by heterogeneity. Based on our theoretical\nresults, we provide insights to help develop more efficient FAL algorithms,\nsuch as designing new metrics and dynamic aggregation rules to mitigate\nheterogeneity.\n","authors":["Wenjun Ding","Ying An","Lixing Chen","Shichao Kan","Fan Wu","Zhe Qu"],"pdf_url":"https://arxiv.org/pdf/2412.08282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11973v6","updated":"2024-12-19T06:29:38Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) and Task-agnostic\nIncremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning\n(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is\ndesigned to prevent overfitting when the data samples are scarce. Furthermore,\nthe sparse reuse of WSN weights is considered for Video Incremental Learning\n(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It\nenables compact encoding of videos and identifies reusable subnetworks across\nvarying bandwidths. We have integrated FSO into different architectural\nframeworks for continual learning, including VIL, TIL, and FSCIL. Our\ncomprehensive experiments demonstrate FSO's effectiveness, significantly\nimproving task performance at various convolutional representational levels.\nSpecifically, FSO enhances higher-layer performance in TIL and FSCIL and\nlower-layer performance in VIL.\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v6.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (T-PAMI)"},{"id":"http://arxiv.org/abs/2412.14561v1","updated":"2024-12-19T06:26:16Z","published":"2024-12-19T06:26:16Z","title":"GBRIP: Granular Ball Representation for Imbalanced Partial Label\n  Learning","summary":"  Partial label learning (PLL) is a complicated weakly supervised\nmulti-classification task compounded by class imbalance. Currently, existing\nmethods only rely on inter-class pseudo-labeling from inter-class features,\noften overlooking the significant impact of the intra-class imbalanced features\ncombined with the inter-class. To address these limitations, we introduce\nGranular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for\nimbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and\nmulti-center loss to construct a granular ball-based nfeature space through\nunsupervised learning, effectively capturing the feature distribution within\neach class. GBRIP mitigates the impact of confusing features by systematically\nrefining label disambiguation and estimating imbalance distributions. The novel\nmulti-center loss function enhances learning by emphasizing the relationships\nbetween samples and their respective centers within the granular balls.\nExtensive experiments on standard benchmarks demonstrate that GBRIP outperforms\nexisting state-of-the-art methods, offering a robust solution to the challenges\nof imbalanced PLL.\n","authors":["Jintao Huang","Yiu-ming Cheung","Chi-man Vong","Wenbin Qian"],"pdf_url":"https://arxiv.org/pdf/2412.14561v1.pdf","comment":"AAAI25"},{"id":"http://arxiv.org/abs/2403.10650v3","updated":"2024-12-19T06:25:45Z","published":"2024-03-15T19:35:10Z","title":"PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time\n  Adaptation","summary":"  Real-world vision models in dynamic environments face rapid shifts in domain\ndistributions, leading to decreased recognition performance. Using unlabeled\ntest data, continuous test-time adaptation (CTTA) directly adjusts a\npre-trained source discriminative model to these changing domains. A highly\neffective CTTA method involves applying layer-wise adaptive learning rates for\nselectively adapting pre-trained layers. However, it suffers from the poor\nestimation of domain shift and the inaccuracies arising from the pseudo-labels.\nThis work aims to overcome these limitations by identifying layers for\nadaptation via quantifying model prediction uncertainty without relying on\npseudo-labels. We utilize the magnitude of gradients as a metric, calculated by\nbackpropagating the KL divergence between the softmax output and a uniform\ndistribution, to select layers for further adaptation. Subsequently, for the\nparameters exclusively belonging to these selected layers, with the remaining\nones frozen, we evaluate their sensitivity to approximate the domain shift and\nadjust their learning rates accordingly. We conduct extensive image\nclassification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,\ndemonstrating the superior efficacy of our method compared to prior approaches.\n","authors":["Sarthak Kumar Maharana","Baoming Zhang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10650v3.pdf","comment":"AAAI 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2412.15156v1","updated":"2024-12-19T18:32:21Z","published":"2024-12-19T18:32:21Z","title":"Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM","summary":"  Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.\n","authors":["Yatai Ji","Jiacheng Zhang","Jie Wu","Shilong Zhang","Shoufa Chen","Chongjian GE","Peize Sun","Weifeng Chen","Wenqi Shao","Xuefeng Xiao","Weilin Huang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2412.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15023v1","updated":"2024-12-19T16:37:19Z","published":"2024-12-19T16:37:19Z","title":"Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and\n  Semantic Controls","summary":"  Sound designers and Foley artists usually sonorize a scene, such as from a\nmovie or video game, by manually annotating and sonorizing each action of\ninterest in the video. In our case, the intent is to leave full creative\ncontrol to sound designers with a tool that allows them to bypass the more\nrepetitive parts of their work, thus being able to focus on the creative\naspects of sound production. We achieve this presenting Stable-V2A, a two-stage\nmodel consisting of: an RMS-Mapper that estimates an envelope representative of\nthe audio characteristics associated with the input video; and Stable-Foley, a\ndiffusion model based on Stable Audio Open that generates audio semantically\nand temporally aligned with the target video. Temporal alignment is guaranteed\nby the use of the envelope as a ControlNet input, while semantic alignment is\nachieved through the use of sound representations chosen by the designer as\ncross-attention conditioning of the diffusion process. We train and test our\nmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. In\naddition, to test our model on a case study of interest, we introduce Walking\nThe Maps, a dataset of videos extracted from video games depicting animated\ncharacters walking in different locations. Samples and code available on our\ndemo page at https://ispamm.github.io/Stable-V2A.\n","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunità","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.15023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14978v1","updated":"2024-12-19T15:53:21Z","published":"2024-12-19T15:53:21Z","title":"Spectrum-based Modality Representation Fusion Graph Convolutional\n  Network for Multimodal Recommendation","summary":"  Incorporating multi-modal features as side information has recently become a\ntrend in recommender systems. To elucidate user-item preferences, recent\nstudies focus on fusing modalities via concatenation, element-wise sum, or\nattention mechanisms. Despite having notable success, existing approaches do\nnot account for the modality-specific noise encapsulated within each modality.\nAs a result, direct fusion of modalities will lead to the amplification of\ncross-modality noise. Moreover, the variation of noise that is unique within\neach modality results in noise alleviation and fusion being more challenging.\nIn this work, we propose a new Spectrum-based Modality Representation (SMORE)\nfusion graph recommender that aims to capture both uni-modal and fusion\npreferences while simultaneously suppressing modality noise. Specifically,\nSMORE projects the multi-modal features into the frequency domain and leverages\nthe spectral space for fusion. To reduce dynamic contamination that is unique\nto each modality, we introduce a filter to attenuate and suppress the modality\nnoise adaptively while capturing the universal modality patterns effectively.\nFurthermore, we explore the item latent structures by designing a new\nmulti-modal graph learning module to capture associative semantic correlations\nand universal fusion patterns among similar items. Finally, we formulate a new\nmodality-aware preference module, which infuses behavioral features and\nbalances the uni- and multi-modal features for precise preference modeling.\nThis empowers SMORE with the ability to infer both user modality-specific and\nfusion preferences more accurately. Experiments on three real-world datasets\nshow the efficacy of our proposed model. The source code for this work has been\nmade publicly available at https://github.com/kennethorq/SMORE.\n","authors":["Rongqing Kenneth Ong","Andy W. H. Khong"],"pdf_url":"https://arxiv.org/pdf/2412.14978v1.pdf","comment":"Accepted to ACM Web Search and Data Mining (WSDM) 2025"},{"id":"http://arxiv.org/abs/2310.14778v3","updated":"2024-12-19T11:49:06Z","published":"2023-10-23T10:29:33Z","title":"Audio-Visual Speaker Tracking: Progress, Challenges, and Future\n  Directions","summary":"  Audio-visual speaker tracking has drawn increasing attention over the past\nfew years due to its academic values and wide application. Audio and visual\nmodalities can provide complementary information for localization and tracking.\nWith audio and visual information, the Bayesian-based filter can solve the\nproblem of data association, audio-visual fusion and track management. In this\npaper, we conduct a comprehensive overview of audio-visual speaker tracking. To\nour knowledge, this is the first extensive survey over the past five years. We\nintroduce the family of Bayesian filters and summarize the methods for\nobtaining audio-visual measurements. In addition, the existing trackers and\ntheir performance on AV16.3 dataset are summarized. In the past few years, deep\nlearning techniques have thrived, which also boosts the development of audio\nvisual speaker tracking. The influence of deep learning techniques in terms of\nmeasurement extraction and state estimation is also discussed. At last, we\ndiscuss the connections between audio-visual speaker tracking and other areas\nsuch as speech separation and distributed speaker tracking.\n","authors":["Jinzheng Zhao","Yong Xu","Xinyuan Qian","Davide Berghi","Peipei Wu","Meng Cui","Jianyuan Sun","Philip J. B. Jackson","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14778v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14518v1","updated":"2024-12-19T04:33:22Z","published":"2024-12-19T04:33:22Z","title":"Efficient Self-Supervised Video Hashing with Selective State Spaces","summary":"  Self-supervised video hashing (SSVH) is a practical task in video indexing\nand retrieval. Although Transformers are predominant in SSVH for their\nimpressive temporal modeling capabilities, they often suffer from computational\nand memory inefficiencies. Drawing inspiration from Mamba, an advanced\nstate-space model, we explore its potential in SSVH to achieve a better balance\nbetween efficacy and efficiency. We introduce S5VH, a Mamba-based video hashing\nmodel with an improved self-supervised learning paradigm. Specifically, we\ndesign bidirectional Mamba layers for both the encoder and decoder, which are\neffective and efficient in capturing temporal relationships thanks to the\ndata-dependent selective scanning mechanism with linear complexity. In our\nlearning strategy, we transform global semantics in the feature space into\nsemantically consistent and discriminative hash centers, followed by a center\nalignment loss as a global learning signal. Our self-local-global (SLG)\nparadigm significantly improves learning efficiency, leading to faster and\nbetter convergence. Extensive experiments demonstrate S5VH's improvements over\nstate-of-the-art methods, superior transferability, and scalable advantages in\ninference efficiency. Code is available at\nhttps://github.com/gimpong/AAAI25-S5VH.\n","authors":["Jinpeng Wang","Niu Lian","Jun Li","Yuting Wang","Yan Feng","Bin Chen","Yongbing Zhang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2412.14518v1.pdf","comment":"Accepted by AAAI'25. 9 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.13609v2","updated":"2024-12-19T03:12:19Z","published":"2024-12-18T08:36:35Z","title":"Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production","summary":"  Sign Language Production (SLP) aims to generate semantically consistent sign\nvideos from textual statements, where the conversion from textual glosses to\nsign poses (G2P) is a crucial step. Existing G2P methods typically treat sign\nposes as discrete three-dimensional coordinates and directly fit them, which\noverlooks the relative positional relationships among joints. To this end, we\nprovide a new perspective, constraining joint associations and gesture details\nby modeling the limb bones to improve the accuracy and naturalness of the\ngenerated poses. In this work, we propose a pioneering iconicity disentangled\ndiffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD\nincorporates a novel Iconicity Disentanglement (ID) module to bridge the gap\nbetween relative positions among joints. The ID module disentangles the\nconventional 3D joint representation into a 4D bone representation, comprising\nthe 3D spatial direction vector and 1D spatial distance vector between adjacent\njoints. Additionally, an Attribute Controllable Diffusion (ACD) module is\nintroduced to further constrain joint associations, in which the attribute\nseparation layer aims to separate the bone direction and length attributes, and\nthe attribute control layer is designed to guide the pose generation by\nleveraging the above attributes. The ACD module utilizes the gloss embeddings\nas semantic conditions and finally generates sign poses from noise embeddings.\nExtensive experiments on PHOENIX14T and USTC-CSL datasets validate the\neffectiveness of our method. The code is available at:\nhttps://github.com/NaVi-start/Sign-IDD.\n","authors":["Shengeng Tang","Jiayi He","Dan Guo","Yanyan Wei","Feng Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.13609v2.pdf","comment":"Accepted by AAAI 2025"}],"Performance":[{"id":"http://arxiv.org/abs/2411.10958v2","updated":"2024-12-19T15:26:20Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrixes $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$\nto enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS)\nof SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on\nRTX4090, respectively. Comprehensive experiments confirm that our approach\nincurs negligible end-to-end metrics loss across diverse models, including\nthose for large language processing, image generation, and video generation.\nThe codes are available at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19460v2","updated":"2024-12-19T04:49:52Z","published":"2024-10-25T10:45:17Z","title":"Accelerating AI Performance using Anderson Extrapolation on GPUs","summary":"  We present a novel approach for accelerating AI performance by leveraging\nAnderson extrapolation, a vector-to-vector mapping technique based on a window\nof historical iterations. By identifying the crossover point (Fig. 1) where a\nmixing penalty is incurred, the method focuses on reducing iterations to\nconvergence, with fewer more compute-intensive but generally cacheable\niterations, balancing speed and memory usage with accuracy and algorithmic\nstability, respectively. We demonstrate significant improvements, in both\ntraining and inference, motivated by scalability and efficiency extensions to\nthe realm of high-performance computing (HPC).\n","authors":["Saleem Abdul Fattah Ahmed Al Dajani","David E. Keyes"],"pdf_url":"https://arxiv.org/pdf/2410.19460v2.pdf","comment":"6 pages, 6 figures, 1 table, Accepted by NeurIPS 2024 Workshop MLNCP\n  https://openreview.net/forum?id=wkP2ZFRn9e"}],"Database":[{"id":"http://arxiv.org/abs/2412.14832v1","updated":"2024-12-19T13:20:32Z","published":"2024-12-19T13:20:32Z","title":"Federated Heavy Hitter Analytics with Local Differential Privacy","summary":"  Federated heavy hitter analytics enables service providers to better\nunderstand the preferences of cross-party users by analyzing the most frequent\nitems. As with federated learning, it faces challenges of privacy concerns,\nstatistical heterogeneity, and expensive communication. Local differential\nprivacy (LDP), as the \\textit{de facto} standard for privacy-preserving data\ncollection, solves the privacy challenge by letting each user perturb her data\nlocally and report the sanitized version. However, in federated settings,\napplying LDP complicates the other two challenges, due to the deteriorated\nutility by the injected LDP noise or increasing communication/computation costs\nby perturbation mechanism. To tackle these problems, we propose a novel\ntarget-aligning prefix tree mechanism satisfying $\\epsilon$-LDP, for federated\nheavy hitter analytics. In particular, we propose an adaptive extension\nstrategy to address the inconsistencies between covering necessary prefixes and\nestimating heavy hitters within a party to enhance the utility. We also present\na consensus-based pruning strategy that utilizes noisy prior knowledge from\nother parties to further align the inconsistency between finding heavy hitters\nin each party and providing reasonable frequency information to identify the\nglobal ones. To the best of our knowledge, our study is the first solution to\nthe federated heavy hitter analytics in a cross-party setting while satisfying\nthe stringent $\\epsilon$-LDP. Comprehensive experiments on both real-world and\nsynthetic datasets confirm the effectiveness of our proposed mechanism.\n","authors":["Yuemin Zhang","Qingqing Ye","Haibo Hu"],"pdf_url":"https://arxiv.org/pdf/2412.14832v1.pdf","comment":"Accepted by SIGMOD 2025"},{"id":"http://arxiv.org/abs/2412.14679v1","updated":"2024-12-19T09:32:00Z","published":"2024-12-19T09:32:00Z","title":"On Enforcing Satisfiable, Coherent, and Minimal Sets of Self-Map\n  Constraints in MatBase","summary":"  This paper rigorously and concisely defines, in the context of our\n(Elementary) Mathematical Data Model ((E)MDM), the mathematical concepts of\nself-map, compound mapping, totality, one-to-oneness, non-primeness, ontoness,\nbijectivity, default value, (null-)reflexivity, irreflexivity, (null-)symmetry,\nasymmetry, (null-)idempotency, anti-idempotency, (null-)equivalence,\nacyclicity, (null-)representative system mapping, the properties that relate\nthem, and the corresponding corollaries on the coherence and minimality of sets\nmade of such mapping properties viewed as database constraints. Its main\ncontribution is the pseudocode algorithm used by MatBase, our intelligent\ndatabase management system prototype based on both (E)MDM, the relational, and\nthe entity-relationship data models, for enforcing self-map, atomic, and\ncompound mapping constraint sets. We prove that this algorithm guarantees the\nsatisfiability, coherence, and minimality of such sets, while being very fast,\nsolid, complete, and minimal. In the sequel, we also presented the relevant\nMatBase user interface as well as the tables of its metacatalog used by this\nalgorithm.\n","authors":["Christian Mancas"],"pdf_url":"https://arxiv.org/pdf/2412.14679v1.pdf","comment":"Submitted to the PriMera Scientific Engineering Journal on 18 Dec.\n  2024. arXiv admin note: substantial text overlap with arXiv:2410.23485"},{"id":"http://arxiv.org/abs/2402.02070v3","updated":"2024-12-19T06:33:08Z","published":"2024-02-03T07:30:37Z","title":"HotRAP: Hot Record Retention and Promotion for LSM-trees with Tiered\n  Storage","summary":"  The multi-level design of Log-Structured Merge-trees (LSM-trees) naturally\nfits the tiered storage architecture: the upper levels (recently\ninserted/updated records) are kept in fast storage to guarantee performance\nwhile the lower levels (the majority of records) are placed in slower but\ncheaper storage to reduce cost. However, frequently accessed records may have\nbeen compacted and reside in slow storage. Existing algorithms are inefficient\nin promoting these ``hot'' records to fast storage, leading to compromised read\nperformance. We present HotRAP, a key-value store based on RocksDB that can\ntimely promote hot records individually from slow to fast storage and keep them\nin fast storage while they are hot. HotRAP uses an on-disk data structure (a\nspecially-made LSM-tree) to track the hotness of keys and includes three\npathways to ensure that hot records reach fast storage with short delays. Our\nexperiments show that HotRAP outperforms state-of-the-art LSM-trees on tiered\nstorage by up to 5.4$\\times$ compared to the second best under read-only and\nread-write-balanced YCSB workloads with common access skew patterns, and by up\nto 1.9$\\times$ compared to the second best under Twitter production workloads.\n","authors":["Jiansheng Qiu","Fangzhou Yuan","Mingyu Gao","Huanchen Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02070v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14519v1","updated":"2024-12-19T04:34:09Z","published":"2024-12-19T04:34:09Z","title":"Optimizing Big Active Data Management Systems","summary":"  Within the dynamic world of Big Data, traditional systems typically operate\nin a passive mode, processing and responding to user queries by returning the\nrequested data. However, this methodology falls short of meeting the evolving\ndemands of users who not only wish to analyze data but also to receive\nproactive updates on topics of interest. To bridge this gap, Big Active Data\n(BAD) frameworks have been proposed to support extensive data subscriptions and\nanalytics for millions of subscribers. As data volumes and the number of\ninterested users continue to increase, the imperative to optimize BAD systems\nfor enhanced scalability, performance, and efficiency becomes paramount. To\nthis end, this paper introduces three main optimizations, namely: strategic\naggregation, intelligent modifications to the query plan, and early result\nfiltering, all aimed at reinforcing a BAD platform's capability to actively\nmanage and efficiently process soaring rates of incoming data and distribute\nnotifications to larger numbers of subscribers.\n","authors":["Shahrzad Haji Amin Shirazi","Xikui Wang","Michael J. Carey","Vassilis J. Tsotras"],"pdf_url":"https://arxiv.org/pdf/2412.14519v1.pdf","comment":null}]}}